<Paper ID=ument1> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Results on the SHARC test set, averaged over
		3 independent runs for GPT2 and BISON, reporting mi-
		cro accuracy and macro accuracy in terms of the clas-
		siﬁcation task and BLEU-1 and BLEU-4 on instances
		for which a clariﬁcation question was generated
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		We report the results of our approach, the various
		baselines, as well as the previous state-of-the-art
		(SOTA) scores where applicable in Table 1 and 2
		for SHARC and in Table 3 for DAILY DIALOG
	</Extractive Summary>
	<Extractive Summary> =
		We submitted the best BISON model out of the
		random three of Table 1 to be evaluated on the hid-
		den test set and report results in comparison to the
		best model on the leaderboard,3 E3 (Zhong and
		Zettlemoyer, 2019) in Table 2
	</Extractive Summary>
</Paper ID=ument1>


<Paper ID=ument1> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Results on the ofﬁcial hidden SHARC test
		set of our model compared to the best model on the
		leaderboard, E3 (Zhong and Zettlemoyer, 2019)
	</Abstractive Summary>
</Paper ID=ument1>


<Paper ID=ument1> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: BLEU n-gram scores for n = 1, 2, 3, 4 on the
		DailyDialog test set, averaged over 3 independent runs
		for GPT2 and BISON
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		We report the results of our approach, the various
		baselines, as well as the previous state-of-the-art
		(SOTA) scores where applicable in Table 1 and 2
		for SHARC and in Table 3 for DAILY DIALOG
	</Extractive Summary>
	<Extractive Summary> =
		On the DAILY DIALOG dataset the information
		retrieval-based method (IR in Table 3) introduced
		by Li et al
	</Extractive Summary>
</Paper ID=ument1>


<Paper ID=ument1> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Comparison of BISON to a setup where BI-
		SON isn’t allowed to attend to future tokens, i
	</Abstractive Summary>
	<Extractive Summary> =
		to future tokens during prediction (see Table 4)
	</Extractive Summary>
</Paper ID=ument1>


<Paper ID=ument1> <Table ID =5>
	<Abstractive Summary> =
		4
		Table 5: Best end-to-end models that do not use a
		pre-trained language model in comparison with BISON
		that uses randomly initialized weights for SHARC and
		DAILY DIALOG (DD), averaged over 3 runs
	</Abstractive Summary>
	<Extractive Summary> =
		Re-
		sults are presented in Table 5 for SHARC and
		DAILY DIALOG
	</Extractive Summary>
</Paper ID=ument1>


<Paper ID=ument1> <Table ID =6>
	<Abstractive Summary> =
		8
		Table 6: BLEU-4 using various sequence generation
		strategies for BISON on SHARC and DAILY DIALOG
	</Abstractive Summary>
</Paper ID=ument1>


<Paper ID=ument1> <Table ID =7>
	<Abstractive Summary> =
		5
		Table 7: Average attention weights and standard de-
		viation when predicting from left-to-right on both
		SHARC and DAILY DIALOG (DD) for different parts
		of the sequence, where α1 is for the input sequence
		x, α2/¯α2 is for the already produced sequence y and
		α3/¯α3 is for the sequence of remaining placeholder to-
		kens p
	</Abstractive Summary>
	<Extractive Summary> =
		Averaging over all H
		attention heads, αk =
		1
		H
		�H
		h=1 αk
		h, leads to the
		results reported in Table 7 for both datasets
	</Extractive Summary>
	<Extractive Summary> =
		the conditioning input
		x (see α1 in Table 7)
	</Extractive Summary>
	<Extractive Summary> =
		To directly compare the relationship within the
		sequence generation, we re-normalize over α2 and
		α3, leading to new values ¯α2 and ¯α3 (see Table 7)
	</Extractive Summary>
</Paper ID=ument1>


<Paper ID=ument10> <Table ID =1>
	<Abstractive Summary> =
		What kind of
		food would you like?
		Table 1: An example of the multi-domain task-oriented
		dialog between the user (U) and the system (S)
	</Abstractive Summary>
</Paper ID=ument10>


<Paper ID=ument10> <Table ID =2>
	<Abstractive Summary> =
		95
		Table 2: Hyper-parameter settings
	</Abstractive Summary>
</Paper ID=ument10>


<Paper ID=ument10> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Performance of different dialog agents on
		the multi-domain dialog corpus by interacting with the
		agenda-based user simulator
	</Abstractive Summary>
	<Extractive Summary> =
		The agreement on the superiority
		of GDPL between objective rating in Table 3 and
		human preference here also indicates that the au-
		tomatic metrics used in our experiments is reliable
		to reﬂect user satisfaction to some extent
	</Extractive Summary>
</Paper ID=ument10>


<Paper ID=ument10> <Table ID =4>
	<Abstractive Summary> =
		238
		Table 4: KL-divergence between different dialog pol-
		icy and the human dialog KL(πturns||pturns), where
		πturns denotes the discrete distribution over the num-
		ber of dialog turns of simulated sessions between the
		policy π and the agenda-based user simulator, and
		pturns for the real human-human dialog
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows that GDPL has the
		smallest KL-divergence to the human on the num-
		ber of dialog turns over the baselines, which im-
		plies that GDPL behaves more like the human
	</Extractive Summary>
</Paper ID=ument10>


<Paper ID=ument10> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: Performance of different agents on the neural
		user simulator
	</Abstractive Summary>
</Paper ID=ument10>


<Paper ID=ument10> <Table ID =6>
	<Abstractive Summary> =
		Efﬁciency
		Quality
		Success
		W
		D
		L
		W
		D
		L
		W
		D
		L
		ACER
		55
		25
		20
		44
		32
		24
		52
		30
		18
		PPO
		74
		13
		13
		56
		26
		18
		59
		31
		10
		ALDM
		69
		19
		12
		49
		25
		26
		61
		24
		15
		Table 6: The count of human preference on dialog ses-
		sion pairs that GDPL wins (W), draws with (D) or loses
		to (L) other methods based on different criteria
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 presents the results of human evalua-
		tion
	</Extractive Summary>
</Paper ID=ument10>


<Paper ID=ument10> <Table ID =7>
	<Abstractive Summary> =
		62
		135
		Table 7: Return distribution of GDPL on each metric
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Reward Evaluation
		To provide an insight into the learned reward func-
		tion itself, Table 7 provides a quantitative evalua-
		tion on the learned rewards by showing the dis-
		tribution of the return R = �
		t γtrt according to
		each metric
	</Extractive Summary>
</Paper ID=ument10>


<Paper ID=ument100> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Test F1 scores for our method (TMP), 4 cross-lingual baselines and a model trained on monolingual data
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		Our technique outperforms previous state-of-the-
		art cross-lingual methods on Spanish, German, Chi-
		nese, Hindi and Tamil and performs competitively
		on Dutch (Table 1)
	</Extractive Summary>
</Paper ID=ument100>


<Paper ID=ument100> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Performance of MP and fast-align on Forward,
		Reverse and Parallel settings in terms of F1
	</Abstractive Summary>
	<Extractive Summary> =
		Results in Table 2 show
		that MP once again outperforms fast-align
	</Extractive Summary>
</Paper ID=ument100>


<Paper ID=ument100> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Alignment performance on 100 sentences
	</Abstractive Summary>
	<Extractive Summary> =
		MP outperforms
		fast-align for both the languages (Table 3)
	</Extractive Summary>
</Paper ID=ument100>


<Paper ID=ument100> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4: Ablation study for Spanish and Hindi
		Sources of errors in TMP
		We also analyze mis-
		takes made by TMP in aligning entities
	</Abstractive Summary>
	<Extractive Summary> =
		Ablation of features for alignment
		We also
		conduct an ablation study (Table 4) to understand
		the sources of our gains beyond a base model that
		uses translations only from Google Translate and
		orthographic afﬁx matching
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4, we also see that the number of entities
		tagged (as a fraction of total entities) increase with
		the introduction of almost every feature (however,
		all matches might not be correct)
	</Extractive Summary>
</Paper ID=ument100>


<Paper ID=ument1000> <Table ID =1>
	<Abstractive Summary> =
		The American president Donald Trump has ad-
		dressed the General Assembly of the United Na-
		tions for the ﬁrst time
		Table 1: Examples of matching overlapping mentions
	</Abstractive Summary>
	<Extractive Summary> =
		For instance, in Table 1, examples 1
		and 2 show cases where annotators disagreed on
		including descriptive clauses in the event men-
		tion
	</Extractive Summary>
</Paper ID=ument1000>


<Paper ID=ument1000> <Table ID =2>
	<Abstractive Summary> =
		19
		Annotator A
		Annotator B
		De Amerikaanse president Donald Trump heeft
		voor het eerst de Algemene Vergadering van de
		Verenigde Naties toegesproken
		de Algemene Vergadering van de Verenigde
		Naties
		The American president Donald Trump ad-
		dressed the General Assembly of the United Na-
		tions for the ﬁrst time
		the General Assembly of the United Nations
		[president, heeft, vergadering, van, verenigde
		naties, toegesproken]
		[vergadering, van, verenigde naties]
		Table 2: Overlapping but non-matching mentions and the sets of their syntactic heads
	</Abstractive Summary>
	<Extractive Summary> =
		Such a matching function must mimic hu-
		man judgment in ﬁnding that the span pairs in Ta-
		ble 1 match, but the pair in Table 2 does not
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the syntactic heads extracted in this
		way from two non-matching annotations
	</Extractive Summary>
	<Extractive Summary> =
		Table 2)
	</Extractive Summary>
</Paper ID=ument1000>


<Paper ID=ument1000> <Table ID =3>
	<Abstractive Summary> =
		The negative skew
		indicates that most overlapping annotations are not
		Set of pairs
		Count
		Total
		182
		OT
		44
		OP
		61
		OF
		77
		HT
		17
		HF
		44
		Table 3: Annotation pair statistics over the four evalu-
		ation documents
	</Abstractive Summary>
</Paper ID=ument1000>


<Paper ID=ument1000> <Table ID =4>
	<Abstractive Summary> =
		72
		Table 4: Results of the different matching functions
		the same events annotated differently, but simply
		mentions of different events that happen to overlap
		(e
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 reports on the different matching func-
		tions (Section 4
	</Extractive Summary>
</Paper ID=ument1000>


<Paper ID=ument1000> <Table ID =5>
	<Abstractive Summary> =
		28
		Table 5: Scores of all Dice functions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 gives the scores for Dice functions on
		each threshold
	</Extractive Summary>
</Paper ID=ument1000>


<Paper ID=ument1001> <Table ID =1>
	<Abstractive Summary> =
		5791
		Table 1: Number of questions, workers and answers
	</Abstractive Summary>
	<Extractive Summary> =
		First, Table 1 lists the statistics of the three collec-
		tions
	</Extractive Summary>
</Paper ID=ument1001>


<Paper ID=ument1001> <Table ID =2>
	<Abstractive Summary> =
		3637
		(b) GLEU
		Table 2: Results of extractive answer aggregation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists the results
	</Extractive Summary>
</Paper ID=ument1001>


<Paper ID=ument1002> <Table ID =1>
	<Abstractive Summary> =
		45
		High-agreement pairs
		80%
		67%
		68%
		Low-agreement pairs
		20%
		15%
		15%
		Positive pairs
		20%
		17%
		25%
		Table 1: Speech-level annotation statistics (top) and re-
		sults (bottom), comparing the use of 3 different groups
		of annotators
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		The Experts column of Table 1 sum-
		marizes the annotation statistics and results
	</Extractive Summary>
	<Extractive Summary> =
		Results
		Column Crowd in Table 1 shows the
		agreement and quality measurements of this ex-
		periment
	</Extractive Summary>
	<Extractive Summary> =
		Results
		After several iterations, we assembled
		a group of 28 annotators which achieved simi-
		lar agreement to that of the expert annotators (see
		column Channel in Table 1), working at a much
		higher pace
	</Extractive Summary>
</Paper ID=ument1002>


<Paper ID=ument1002> <Table ID =2>
	<Abstractive Summary> =
		Speech
		Explicit
		Implicit
		No mention
		Positive
		150
		137
		102
		Negative
		301
		436
		1,889
		Table 2: A comparison of speech-level labels (Explicit,
		Implicit, No mention) to sentence-level based labels: a
		Positive claim is one which is positive for least one of
		the labeled sentences; a Negative claim is one which
		is negative for all labeled sentences
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 compares labels from both setups
	</Extractive Summary>
</Paper ID=ument1002>


<Paper ID=ument1003> <Table ID =1>
	<Abstractive Summary> =
		42
		type
		#videos
		#video clips
		in-house
		10 (group A)
		1,736
		crowdsourced
		10 (group B)
		1,861
		all
		20
		3,597
		Table 1: Statistics of the data
		label
		(score)
		guidelines
		up
		(+1)
		Watch the video clip and select up if your
		feeling matches one of the pictures below
	</Abstractive Summary>
	<Extractive Summary> =
		First, as shown in Table 1,
		10 TED Talks videos were divided into group A
		and group B
	</Extractive Summary>
</Paper ID=ument1003>


<Paper ID=ument1003> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Annotation guidelines for the change in ten-
		sion
		5
		Annotation
		5
	</Abstractive Summary>
</Paper ID=ument1003>


<Paper ID=ument1003> <Table ID =3>
	<Abstractive Summary> =
		type
		down
		similar
		up
		sum
		train
		153
		819
		243
		1,215
		test
		69
		342
		110
		521
		all
		222
		1,161
		353
		1,736
		Table 3: Statistics of the data for training the model
		5
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Crowdsourcing Annotation
		Of the data collected via in-house annotations to
		the Group A videos, 70% were used as the training
		set and 30% were used as the test set to train and
		evaluate the model (Table 3)
	</Extractive Summary>
</Paper ID=ument1003>


<Paper ID=ument1003> <Table ID =4>
	<Abstractive Summary> =
		63
		Table 4: Comparison of the performance on the test set
		according to the features used
		5
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 compares the perfor-
		mance according to the features used
	</Extractive Summary>
</Paper ID=ument1003>


<Paper ID=ument1003> <Table ID =5>
	<Abstractive Summary> =
		69
		Table 5: Statistics for agreement, time of annotation results
		As the result of the annotation, 11,166 anno-
		tation values were obtained for 10 videos with
		1,861 video clips (group B)
	</Abstractive Summary>
</Paper ID=ument1003>


<Paper ID=ument1004> <Table ID =1>
	<Abstractive Summary> =
		6%
		Table 1: WER and relaxed WER for measuring Quality
		of Transcriptions
		5
	</Abstractive Summary>
</Paper ID=ument1004>


<Paper ID=ument1005> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Comparison of median logistic classiﬁer ﬁt scores (out of 100 percent ﬁt) across categories deﬁned in
		CSLB The suitcase
		Table 1: Examples from Winograd Schema Challenge
		(WSC)
		Table 1: An example from the DREAM dataset; the as-
		terisk (*) denotes the correct answer9
		Table 1: Reported results on COPA33
		Table 1: Crowd-sourced labeling of phrases generated
		by the pattern approach of section 2
		Table 1: Example classes of errors when automatically generating inference pattern graphs4
		Table 1: Statistics of ReCoRD
		amples, most of which require commonsense
		reasoning Each passage is concatenated with
		Edge Relation
		Event Phrase
		RelatedTo
		A is related to B
		FormOf
		A is a form of B
		PartOf
		A is a part of B
		UsedFor
		A is used for B
		AtLocation
		A is at B
		Causes
		A causes B
		Synonym
		A is synonym of B
		Antonym
		A is antonym of B
		DerivedFrom
		A is derived from B
		Table 1: Event Phrases
		edge relation from ConceptNet0
		Table 1: Accuracy results for various models
		When did they wait for their train?
		a) before buying the ticket
		b) after buying a ticket
		Table 1: Example of a prompt from the shared task
		dataset, an everyday commonsense reasoning dataset The intuition behind
		why this multi-stage ﬁne-tuning strategy works is
		94
		Dataset
		Options
		Sentence A
		Sentence B
		RACE
		4
		passage
		query+option
		SWAG
		4
		query
		option
		Task 1
		2
		passage
		query+option
		Table 1: Structure of inputs for the two supplementary
		tasks and the target task dataset
		Dataset
		Train
		Dev
		RACE
		87866
		4887
		SWAG
		73546
		20006
		Task 1
		14191
		2020
		Table 2: Basic statistics for the three datasets involved
		in solving task 1
		that (a) to let the pre-trained LMs to adopt to the
		similar contextual environment, (b) and make the
		model more suitable for this speciﬁc task forma-
		tion5K
		Table 1: Datasets used in this paper for ﬁne-tuning
		c) meaningless causal relations: break the ice by
		seeing it; killing each other by slashing the rate;
		109
		Figure 4: Flowchart of causal relations extraction from text
		Table 1: Most frequent 3-grams for extracted result verbs
		Table 2: Samples of extracted causal relations
		7
		Conclusion and Further Work
		Commonsense inferences allow us to equip and
		empower cognitive robots with an ability to under-
		stand high-level natural language commands (or
		instructions)2%
		Table 1: The result of crowdsourcing,
		2016), paragraph reconstruction (Li and Juraf-
		sky, 2017), and hard coreference resolution (Peng
		Event
		Plausible?
		bird-construct-nest
		�
		bottle-contain-elephant
		�
		gorilla-ride-camel
		�
		lake-fuse-tie
		�
		Table 1:
		Example events from Wang et al75
		Table 1: Accuracy of the probing model compared with the baselines including previous approaches on the at-
		tributes in the Verb Physics dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows a comparison of ﬁt scores across
		different types of attribute categories
	</Extractive Summary>
	<Extractive Summary> =
		As shown in the
		examples in Table 1, in order to infer what the pro-
		noun “they” refers to in the ﬁrst two statements,
		one has to leverage the commonsense knowledge
		that “demonstrators can cause violence and city
		councilmen usually fear violence
	</Extractive Summary>
	<Extractive Summary> =
		An example from DREAM that
		requires commonsense is shown in Table 1, and
		an example from CommonsenseQA is shown in
		Table 2
	</Extractive Summary>
	<Extractive Summary> =
		Recent studies show
		that BERT and RoBERTa achieve considerable
		improvements on COPA (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 is a sum-
		mary of the annotation results
	</Extractive Summary>
	<Extractive Summary> =
		Instead, we discovered
		that both the clustering and connection processes
		are susceptible to a number of common opportu-
		nities for error (described in Table 1) that limit
		this process in practice
	</Extractive Summary>
	<Extractive Summary> =
		Here the
		user can also remove bad edges (two table rows
		that were automatically connected, but whose con-
		nection isn’t meaningful), or remove subsets of the
		column links on edges that are partially correct
		(see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 presents the structure of the inputs for
		the three datasets, where k is the number of op-
		tions for each query
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 lists datasets selected in this pa-
		per
	</Extractive Summary>
	<Extractive Summary> =
		Totally 60 3-grams were ex-
		tracted (see Table 1 for details)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the ratio of event pairs with each
		label
	</Extractive Summary>
	<Extractive Summary> =
		2
		The Quality of Original Datasets
		As shown in Table 1, only 84
	</Extractive Summary>
	<Extractive Summary> =
		This is the problem of determining if a
		given event, represented as an s-v-o triple, is phys-
		ically plausible (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results and Discussion
		The probing model (Figure 1) with pre-trained
		representations has better accuracy than previous
		approaches which use extra information in addi-
		tion to the words being compared (Table 1)
	</Extractive Summary>
</Paper ID=ument1005>


<Paper ID=ument1005> <Table ID =2>
	<Abstractive Summary> =
		0)
		Visual
		is triangular, is long and thin, is upright,
		has two feet, does swing, is rigid
		does come in pairs, has a back, has a bar-
		rel, has a bushy tail, has a clasp
		Encyclopedic
		is hardy, has types, is found in bible, is
		American, does play, is necessary essen-
		tial
		does grow on plants, does grow on trees,
		does live in rivers, does live in trees, does
		photosynthesize, has a crew
		Functional
		does work, does spin, does support, does
		drink, does breathe, does hang
		does DIY, does carry transport goods,
		does chop, does drive
		Perceptual
		is chewy, does rattle, is wet, does squeak,
		is rough, has a strong smell
		does bend, has a sting, has pollen, has
		soft ﬂesh, is citrus, is fermented
		Taxonomic
		is a home, is a dried fruit, is a garden tool,
		is a vessel, is a toy, is an ingredient
		is a bird of prey, is a boat, is a body part,
		is a cat, is a citrus fruit, is a crustacean
		Table 2: Fine-grained comparison across categories between attributes that lack some level of ﬁt (left) and perfectly
		ﬁt attributes (right) with classiﬁcation using BERT representations1
		Datasets
		Corpus
		#Train
		#Dev
		#Test
		WNLI
		-
		634 + 71
		146
		PDP60
		-
		-
		60
		WSC
		-
		-
		285
		WSCR
		1322
		564
		-
		Table 2: Summary of the three benchmark datasets:
		WSC, PDP60 and WNLI, and the additional dataset
		WSCR New York
		Table 2:
		An example from the CommonsenseQA
		dataset; the asterisk (*) denotes the correct answer2
		Table 2: Applicability (Appcom/p/word2vec/
		Sound
		Smell
		honking cars
		burning rubber
		snoring
		chlorine
		gunshots
		citrus blossoms
		live music
		fresh paint
		Table 2: Examples of sound and smell concepts recog-
		nized by our methodcom
		57
		Measure
		Count
		Graph Nodes:
		Nodes before merging
		700
		Nodes after merging
		540 (77%)
		Graph Edges:
		Edges before curation
		637
		Edges after curation
		771 (21%)
		Grid Row-to-Row Connections:
		Row-to-row connections before curation
		1384
		Row-to-row connections modiﬁed
		631 (46%)
		Row-to-row connections removed
		224 (16%)
		Grid Edge Constraints:
		Edge constraints before curation
		2101
		Edge constraints removed
		133 (6%)
		Edge constraints marked optional
		27 (1%)
		Table 2: Manual edits done to the automatically gen-
		erated graph and grid during the merging and curation
		steps
		68
		Rank
		Team Name
		Architecture
		Commonsense
		Other Resources
		Tasks
		1
		PSH–SJTU
		Transformer (XLNet)
		-
		RACE, SWAG
		1, 2
		2
		IIT-KGP
		Transformer
		(BERT
		+ XLNet)
		-
		RACE
		1
		3
		BLCU-NLP
		Transformer (BERT)
		-
		ReCoRD, RACE
		1
		4
		JDA
		Transformer (BERT)
		ConceptNet,
		Atomic, Webchild
		Wikipedia
		1
		5
		KARNA
		Transformer (BERT)
		ConceptNet
		-
		1
		Table 2: Overview of participating systems
		DocQA
		(Clark and Gardner, 2018) is a strong
		baseline model for extractive QA The model is trained on
		76
		Figure 1: Model Overview
		Paramter
		Value
		learningrate
		2e-5
		maxseqlength
		210
		batchsize
		4
		epochs
		3
		Optimizer
		ADAM
		Table 2: Hyperparameters
		Google Colab GPU for 2 epochs3
		Yes
		(Jain and Singh, 2019)
		Table 2: Performance comparison among participants
		of the COIN Shared Task 1, depicting use of common-
		sense knowledge bases7
		Table 2: Results with B The intuition behind
		why this multi-stage ﬁne-tuning strategy works is
		94
		Dataset
		Options
		Sentence A
		Sentence B
		RACE
		4
		passage
		query+option
		SWAG
		4
		query
		option
		Task 1
		2
		passage
		query+option
		Table 1: Structure of inputs for the two supplementary
		tasks and the target task dataset
		Dataset
		Train
		Dev
		RACE
		87866
		4887
		SWAG
		73546
		20006
		Task 1
		14191
		2020
		Table 2: Basic statistics for the three datasets involved
		in solving task 1
		that (a) to let the pre-trained LMs to adopt to the
		similar contextual environment, (b) and make the
		model more suitable for this speciﬁc task forma-
		tion7
		-
		Table 2: Main results
		c) meaningless causal relations: break the ice by
		seeing it; killing each other by slashing the rate;
		109
		Figure 4: Flowchart of causal relations extraction from text
		Table 1: Most frequent 3-grams for extracted result verbs
		Table 2: Samples of extracted causal relations
		7
		Conclusion and Further Work
		Commonsense inferences allow us to equip and
		empower cognitive robots with an ability to under-
		stand high-level natural language commands (or
		instructions)
		Train
		Dev
		Test
		New Test
		Wikihow
		1,287,360
		26,820
		26,820
		858 (174)
		Descript
		23,320
		2,915
		2,915
		2,203 (199)
		Table 2: Statistics of the datasets We follow
		the same evaluation procedure as previous work
		124
		Wikipedia
		male-have-income
		village-have-population
		event-take-place
		NELL
		login-post-comment
		use-constitute-acceptance
		modules-have-options
		Table 2: Most frequent s-v-o triples for each corpus893
		Table 2: The probing model trained on the Verb Physics
		size dataset and evaluated on (Bagherinezhad et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows examples of both
		levels of ﬁt
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the datasets which are used
		in our experiments
	</Extractive Summary>
	<Extractive Summary> =
		The applicability αk of
		a token k counts how often this token occurs in an
		alternative with one label, but not the other:
		αk =
		n
		�
		i=1
		1
		�
		∃j, k ∈ T(i)
		j
		∧ k /∈ T(i)
		¬j
		�
		The productivity πk of a token is the proportion of
		applicable instances for which it predicts the cor-
		rect answer:
		πk =
		�n
		i=1 1
		�
		∃j, k ∈ T(i)
		j
		∧ k /∈ T(i)
		¬j ∧ yi = j
		�
		αk
		Finally, the coverage ξk of a token is the propor-
		tion of applicable instances among all instances:
		ξk = αk
		n
		Table 2 shows the ﬁve tokens with highest cov-
		erage
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, we obtain high accuracy
		across all models
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 gives a summary of the par-
		ticipating systems
	</Extractive Summary>
	<Extractive Summary> =
		This is also evident from Table 2 as the
		top 3 systems (unofﬁcial leaderboard)3 do not use
		any kind of commonsense knowledge bases
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results and Analysis
		Table 2 demonstrates results of various trained
		models, consisting of three groups
	</Extractive Summary>
	<Extractive Summary> =
		Yet the probing model achieves at
		least 4% higher accuracy (Table 2)
	</Extractive Summary>
</Paper ID=ument1005>


<Paper ID=ument1005> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Results for attribute classiﬁcation with Con-
		ceptNet as a knowledge graph source
		For the WNLI instances, we convert them to
		the format of WSC as illustrated in Table 3: we
		ﬁrst detect pronouns in the premise using spaCy6;
		then given the detected pronoun, we search its left
		of the premise in hypothesis to ﬁnd the longest
		common substring (LCS) ignoring character case the
		lamplight
		Table 3: Examples of transforming WNLI to WSC for-
		mat
		25
		Options
		Extracted ConceptNet triples
		Bank
		(revolving door AtLocation bank) (bank RelatedTo security)
		Library
		(revolving door AtLocation library)
		Department Store
		(revolving door AtLocation store) (security IsA department)
		Mall
		(revolving door AtLocation mall)
		New York
		(revolving door AtLocation New York)
		Table 3: Extracted ConceptNet relations for sample shown in Table 2798
		Table 3: Results of human performance evaluation of
		the original COPA and Balanced COPA56
		Table 3: Performance of the various models on the task
		of sense recognition for products
		5
		5
		Measurements
		22
		34
		Navigation lost at sea
		6
		7
		Physical Changes
		13
		14
		Seeing
		19
		29
		Soil erosion*
		6
		6
		Solutions - Dissolving substances*
		4
		5
		Sources of Heat*
		3
		2
		Sunlight as a source of energy*
		14
		30
		Sunlight location and shadow size*
		7
		7
		Taste*
		9
		11
		Taxonomic Inheritance
		2
		1
		Texture*
		4
		3
		Thermal Conductivity
		27
		34
		Touch-Hardness*
		4
		3
		Table 3: A list of high-level inference patterns discov-
		ered in the corpus of explanations for Matter science
		exam questions using this tool97
		Table 3:
		Performance of participating systems and
		baselines for task 1, in total (acc), on commonsense-
		based questions (acccs), and on out-of-domain ques-
		tions that belong to the ﬁve held-out scenarios
		(accOOD)1%
		-
		Table 3: Results
		• w/o Qand PA Rel :
		Model with context
		containing passage and relation between
		question and answer3
		Table 3: Question type comparison between different
		models on the shared task: previous state-of-the-art
		TriAN (Wang, 2018), BERTLARGE, and B76%
		-
		Table 3: Main results on the task 1
		Description
		ﬁrst
		stage
		second
		stage
		t: tokens max length
		350
		300
		e: ﬁne-tune epoch
		{3,4}
		{3,4}
		α: learning rate
		3e-5
		1e-5
		b: batch size
		32
		64
		g:gradient accumulation step
		4
		8
		Table 3: Hyper-parameters settings used during train-
		ing
		We present a method for acquir-
		Table 3: Examples of causal relations for the 3-gram
		“open the window”
		ing the knowledge needed to transform high-level
		result-verb commands into action-verb commands
		for further implementation into primitive actions
		Table 3: Event prediction performance evaluated by automatic evaluation metrics89
		Table 3: Mean accuracy of classifying plausible events
		for models trained in a supervised setting52
		Table 3: Accuracy of probing models (averaged across
		the ﬁve attributes) on the Verb Physics dev sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows our results
	</Extractive Summary>
	<Extractive Summary> =
		, the cookstove, the
		kitchen and the lamplight as shown in Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		The human evaluation
		shows that our mirrored instances are compara-
		ble in difﬁculty to the original ones (see Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		Manual inspection of the curated graph using the
		Graph View revealed 29 high-level inference pat-
		terns shown in Table 3, each containing between
		3 and 66 nodes, and up to 107 edges
	</Extractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 3 shows the performance of the participating
		systems and the baselines on the task 1 data
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results
		The main experimental accuracy results for task
		1 are shown in Table 3, in which human perfor-
		mance is provided by task organizers
	</Extractive Summary>
	<Extractive Summary> =
		As a result, Table 3 illustrates that pre-training on
		RACE plays a signiﬁcant role in the system
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 lists the evaluation results
	</Extractive Summary>
	<Extractive Summary> =
		(2018): we
		perform 10-fold cross validation on the dataset of
		3,062 s-v-o triples, and report the mean accuracy
		of running this procedure 20 times all with the
		same model initialization (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		jority class baseline for this dataset), but the drop
		in accuracy is higher than 10% for GloVe and
		ELMo (Table 3): Our model is not simply rely-
		ing on lexical memorization
	</Extractive Summary>
</Paper ID=ument1005>


<Paper ID=ument1005> <Table ID =4>
	<Abstractive Summary> =
		I brought my
		husband his breakfast with the orange juice,
		and he said that the juice was his favorite
		part!
		How were the oranges sliced?
		a) in half
		b) in eighths
		When did they plug the juicer in?
		a) after squeezing oranges
		b) after removing it from the box
		Table 4: Example of a prompt from MCScript 20
		-
		-
		Table 4: Test results
		Figure 2: Comparison with SSM and MLM on WNLI examples
		Input sentence
		Generated ATOMIC relations
		Utterance 1
		(xAttr dedicated) (xWant to get to work)
		Utterance 2
		(xAttr far) (xReact happy) (xWant to get to their destination)
		Option A
		(xAttr calm) (xWant to avoid the road)
		Option B
		(xAttr careless) (xReact annoyed) (xEffect get tired)
		Option C
		(xAttr frustrated) (xEffect get tired) (xWant to get out of car)
		Table 4: Sample generated ATOMIC relations for sample shown in Table 10∗
		Table 4: Model performance on the COPA test set (Overall), on Easy instances with superﬁcial cues, and on Hard
		instances without superﬁcial cues43
		Table 4: Fleiss κ
		Change of State
		Freezing means changing from a liquid to a solid by
		reducing heat energy
		A liquid is a kind of state of matter
		Water is in the liquid state, called liquid water, for
		temperatures between 0 C and 100 C
		A solid is a kind of state of matter
		Water is in the solid state, called liquid water, for
		temperatures between -273 C and 0 C
		Cooling means reducing heat energy
		Freezing is when liquids are cooled below freezing point
		Phase Changes
		Boiling means changing from a liquid to a gas by
		adding heat energy
		Boiling is a kind of phase change
		A phase change is when a substance changes from
		one state to another state
		Temperature changes can cause phase changes
		Alloys
		Alloys are made of two or more metals
		Bronze is a kind of alloy
		Bronze is made of copper and tin
		Tin is a kind of metal
		Copper is a kind of metal
		Containers contain objects
		A container is a kind of object
		If a container contains something, then that container
		touches that something
		A bowl is a kind of container
		A container contains objects
		A rock is a kind of object
		Table 4: A small subset of example combinations of
		knowledge base facts that satisfy the constraints of in-
		ference patterns extracted from the explanation corpus12
		Table 4: Performance (EM and F1) of human, partici-
		pating systems and baselines for task 24
		Table 4:
		Hyperparameters used throughout experi-
		ments74 %
		Table 4: The main results on task 2
		Table 4: Next events generated by the deterministic and probabilistic models
		Event
		Plausible?
		BERT
		GT
		dentist-capsize-canoe
		�
		�
		stove-heat-air
		�
		�
		sun-cool-water
		�
		�
		chair-crush-water
		�
		�
		Table 4: Interpreting log-likelihood as conﬁdence, ex-
		ample events for which BERT was highly conﬁdent and
		either correct or incorrect with respect to the ground
		truth (GT) label68
		Table 4: Evaluation on Bagherinezhad et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		5
		Figure 3: Linear regression ﬁt of accuracy on MCScript
		2
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the sample ATOMIC relations, gen-
		erated using the DREAM example in Table 1
	</Extractive Summary>
	<Extractive Summary> =
		7
		As Table 4 shows, previous models perform simi-
		larly on both subsets, with the exception of Sasaki
		et al
	</Extractive Summary>
	<Extractive Summary> =
		Argument reasoning comprehension is a
		high level natural language understanding task re-
		quiring world knowledge and complex reasoning
		skills, while COPA can be largely solved with
		associative reasoning, as the performance of the
		PMI-based baselines shows (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows EM (%) and F1 (%) of hu-
		man performance, the PSH-SJTU system as well
		as baselines on the development and test sets of
		task 2
	</Extractive Summary>
	<Extractive Summary> =
		1
		Hyperparameters
		Seen in Table 4 is a list of hyperparameters for our
		experiments
	</Extractive Summary>
	<Extractive Summary> =
		For task 2, the results are presented on Table 4,
		where the bolded models are our submissions on
		the test leaderboard
	</Extractive Summary>
	<Extractive Summary> =
		4
		Qualitative Analysis
		Table 4 shows next events generated by the deter-
		ministic and probabilistic models, with Table 4a
		being an example from Wikihow
	</Extractive Summary>
	<Extractive Summary> =
		Table 4b shows an example from Descript
	</Extractive Summary>
	<Extractive Summary> =
		For the reference
		next events of “board bus” (Table 4b), “pay for the
		bus” and its variants dominate, but we are unsure
		if they are truly more typical than “place your lug-
		gage overhead or beneath seat”
	</Extractive Summary>
	<Extractive Summary> =
		, 2016): Using just one word when training
		and evaluating sees a drop of about 12 to 15% in
		accuracy (Table 4)
	</Extractive Summary>
</Paper ID=ument1005>


<Paper ID=ument1005> <Table ID =5>
	<Abstractive Summary> =
		5
		44 K
		Table 5:
		Test set results from the implicit method
		on MCScript 23
		Table 5: Ablation study of the two component models
		in HNN8
		Table 5: Results on DREAM; the asterisk (*) denotes
		results taken from leaderboard0)
		Table 5: Results of ﬁne-tuned models on Balanced COPA85
		Table 5: Accuracy of the linear models (LM) and mem-
		ory networks models (MM) on the sound-source rela-
		tion, acids)
		4
		3
		3
		Containers contain things
		-
		-
		-
		Containers (Abstracted)
		5
		5
		1000
		Containers (Application)
		6
		6
		15
		Cooking Food
		-
		-
		-
		Cooking (Core)
		6
		6
		26
		Cooking a particular food
		7
		7
		338
		Cooking (Containers for cooking)
		6
		6
		1
		Electrical Conductivity
		-
		-
		-
		Dangers of Electric Shock
		4
		3
		414
		Electrical Insulation
		15
		23
		46
		Electrical Circuits in Devices
		7
		11
		18
		Friction
		-
		-
		-
		Friction (core)
		16
		31
		3
		General Motion *
		3
		3
		6
		Ice Wedging *
		4
		4
		2
		Magnetism
		-
		-
		-
		Magnetic Objects
		5
		4
		10
		Manufacturers use material for products
		-
		-
		-
		Manufacturers use materials for products (core)
		4
		3
		19
		Measurements
		-
		-
		-
		Measurement Tools
		4
		4
		130
		Observations (Celestial Bodies)
		5
		6
		6
		Observations (Distant Objects)
		5
		6
		208
		Observations (Microscopic Things)
		6
		6
		4
		Observations (Small Things)
		6
		6
		94
		Navigation-Direction-Being lost at sea
		-
		-
		-
		Navigation (core)
		3
		2
		1
		Navigation (being lost/boat)
		6
		7
		2
		Physical Changes
		-
		-
		-
		Physical Changes (Changing Shape)
		9
		10
		832
		Seeing
		-
		-
		-
		Things that can see and what they can see
		6
		6
		1000
		Soil erosion *
		6
		6
		28
		Solutions - Dissolving substances *
		4
		5
		1
		Sources of Heat *
		3
		2
		6
		Sunlight as a source of energy *
		14
		30
		80
		Sunlight location and shadow size *
		7
		7
		312
		Taste *
		9
		11
		26
		Taxonomic Inheritance
		2
		1
		1000
		Texture *
		4
		3
		2
		Thermal Conductivity
		-
		-
		-
		Thermal Conductivity (Core)
		21
		26
		1000
		Thermal Conductors 0
		5
		4
		9
		Thermal Conductors 1
		5
		4
		8
		Thermal Insulators
		5
		5
		5
		Touch-Hardness *
		4
		3
		8
		Table 5: An extended list of inference patterns discovered in the corpus of explanations for Matter science exam questions using
		this tool685
		Table 5: Performance of participating systems and baselines for task 1 on the 5 most common question types56
		Table 5: Accuracy of classifying plausible events for
		models trained on a corpus in a self-supervised manner
		Examples of Orders Formed Around a Word
		head < knee < meal < chair < back < place <
		street < world < gate < air < ﬂoor < room
		eye < chair < child < king < daughter < wife <
		boy < messenger < father < coach < horse < door <
		house < gate < train < room < sun
		Table 5: Two examples for orderings formed around
		the words chair and gate for the size attribute using
		GloVe
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results and Analysis
		Table 5 shows the results from the implicit
		method
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 summarizes the experimen-
		tal results
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results
		DREAM results are shown in Table 5,
		and
		CommonsenseQA results are shown in Table
		6
	</Extractive Summary>
	<Extractive Summary> =
		A full list of patterns is
		provided in Table 5 (see Appendix)
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the systems’ performance on sin-
		gle question types for task 1
	</Extractive Summary>
	<Extractive Summary> =
		2), we report
		both the validation and test accuracies of classify-
		ing physically plausible events (Table 5)
	</Extractive Summary>
</Paper ID=ument1005>


<Paper ID=ument1005> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6: Test set results for knowledge base embed-
		dings on MCScript 27
		Table 6: Ablation study of the ranking loss0
		Table 6: Results on CommonsenseQA; the asterisk (*)
		denotes results taken from leaderboard1
		Table 6: Results of non-ﬁne-tuned models on Balanced COPA80
		Table 6: Accuracy on the sound-scene relation75
		Table 6: The best accuracies obtained by a Linear
		Model compared with the best accuracies obtained by a
		shallow Neural Network
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows our results with explicit knowl-
		edge embeddings
	</Extractive Summary>
	<Extractive Summary> =
		We can also combine the explicit knowledge
		base embeddings and the implicit RACE ﬁne-
		tuning, yielding the highest accuracy (with all KB
		+ RACE (subset) in Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 6, the lin-
		ear model with the shortest path achieves the best
		accuracy of 81%
	</Extractive Summary>
	<Extractive Summary> =
		Per Table 6, linear modela are
		almost at par (accuracy within 1%) with shal-
		low fully connected neural networks on the Verb
		Physics dev set
	</Extractive Summary>
</Paper ID=ument1005>


<Paper ID=ument1005> <Table ID =7>
	<Abstractive Summary> =
		0
		Bidirectional
		Yes
		Table 7:
		Hyperparameters used throughout experi-
		ments0)
		Table 7: Accuracies for each DREAM question type: M means Matching, S means Summary, L means Logic
		inference, C means Commonsense inference, and A means Arithmatic inference40
		Table 7: Sensitivity of BERT-large to superﬁcial cues
		identiﬁed in §2 (unit: 10−2)82
		Table 7: Accuracy on the sound-sentiment relation12
		Table 7: Label-Wise Accuracy: The GloVe, ELMo, and
		BERT representations (fed to a linear model) struggle
		to capture the relationship word1 ≈ word2 (label 2)
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Hyperparameters
		Seen in Table 7 is a list of hyperparameters for our
		experiments
	</Extractive Summary>
	<Extractive Summary> =
		As can be seen in Table 7, the linear model
		with LSTM encoded phrases achieved the highest
		accuracy of 84%
	</Extractive Summary>
	<Extractive Summary> =
		This might explain the relative
		struggles GloVe, ELMo, and BERT face classify-
		ing comparisons labeled 2 (Table 7)
	</Extractive Summary>
</Paper ID=ument1005>


<Paper ID=ument1005> <Table ID =8>
	<Abstractive Summary> =
		7)
		Table 8: Accuracies for each CommonsenseQA question type: AtLoc
	</Abstractive Summary>
</Paper ID=ument1005>


<Paper ID=ument1007> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Comparison of median logistic classiﬁer ﬁt scores (out of 100 percent ﬁt) across categories deﬁned in
		CSLB
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows a comparison of ﬁt scores across
		different types of attribute categories
	</Extractive Summary>
</Paper ID=ument1007>


<Paper ID=ument1007> <Table ID =2>
	<Abstractive Summary> =
		0)
		Visual
		is triangular, is long and thin, is upright,
		has two feet, does swing, is rigid
		does come in pairs, has a back, has a bar-
		rel, has a bushy tail, has a clasp
		Encyclopedic
		is hardy, has types, is found in bible, is
		American, does play, is necessary essen-
		tial
		does grow on plants, does grow on trees,
		does live in rivers, does live in trees, does
		photosynthesize, has a crew
		Functional
		does work, does spin, does support, does
		drink, does breathe, does hang
		does DIY, does carry transport goods,
		does chop, does drive
		Perceptual
		is chewy, does rattle, is wet, does squeak,
		is rough, has a strong smell
		does bend, has a sting, has pollen, has
		soft ﬂesh, is citrus, is fermented
		Taxonomic
		is a home, is a dried fruit, is a garden tool,
		is a vessel, is a toy, is an ingredient
		is a bird of prey, is a boat, is a body part,
		is a cat, is a citrus fruit, is a crustacean
		Table 2: Fine-grained comparison across categories between attributes that lack some level of ﬁt (left) and perfectly
		ﬁt attributes (right) with classiﬁcation using BERT representations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows examples of both
		levels of ﬁt
	</Extractive Summary>
</Paper ID=ument1007>


<Paper ID=ument1007> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Results for attribute classiﬁcation with Con-
		ceptNet as a knowledge graph source
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows our results
	</Extractive Summary>
</Paper ID=ument1007>


<Paper ID=ument1007> <Table ID =4>
	<Abstractive Summary> =
		I brought my
		husband his breakfast with the orange juice,
		and he said that the juice was his favorite
		part!
		How were the oranges sliced?
		a) in half
		b) in eighths
		When did they plug the juicer in?
		a) after squeezing oranges
		b) after removing it from the box
		Table 4: Example of a prompt from MCScript 2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		6
		Figure 3: Linear regression ﬁt of accuracy on MCScript
		2
	</Extractive Summary>
</Paper ID=ument1007>


<Paper ID=ument1007> <Table ID =5>
	<Abstractive Summary> =
		5
		44 K
		Table 5:
		Test set results from the implicit method
		on MCScript 2
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results and Analysis
		Table 5 shows the results from the implicit
		method
	</Extractive Summary>
</Paper ID=ument1007>


<Paper ID=ument1007> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6: Test set results for knowledge base embed-
		dings on MCScript 2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows our results with explicit knowl-
		edge embeddings
	</Extractive Summary>
	<Extractive Summary> =
		We can also combine the explicit knowledge
		base embeddings and the implicit RACE ﬁne-
		tuning, yielding the highest accuracy (with all KB
		+ RACE (subset) in Table 6)
	</Extractive Summary>
</Paper ID=ument1007>


<Paper ID=ument1007> <Table ID =7>
	<Abstractive Summary> =
		0
		Bidirectional
		Yes
		Table 7:
		Hyperparameters used throughout experi-
		ments
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Hyperparameters
		Seen in Table 7 is a list of hyperparameters for our
		experiments
	</Extractive Summary>
</Paper ID=ument1007>


<Paper ID=ument1008> <Table ID =1>
	<Abstractive Summary> =
		The suitcase
		Table 1: Examples from Winograd Schema Challenge
		(WSC)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in the
		examples in Table 1, in order to infer what the pro-
		noun “they” refers to in the ﬁrst two statements,
		one has to leverage the commonsense knowledge
		that “demonstrators can cause violence and city
		councilmen usually fear violence
	</Extractive Summary>
</Paper ID=ument1008>


<Paper ID=ument1008> <Table ID =2>
	<Abstractive Summary> =
		1
		Datasets
		Corpus
		#Train
		#Dev
		#Test
		WNLI
		-
		634 + 71
		146
		PDP60
		-
		-
		60
		WSC
		-
		-
		285
		WSCR
		1322
		564
		-
		Table 2: Summary of the three benchmark datasets:
		WSC, PDP60 and WNLI, and the additional dataset
		WSCR
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the datasets which are used
		in our experiments
	</Extractive Summary>
</Paper ID=ument1008>


<Paper ID=ument1008> <Table ID =3>
	<Abstractive Summary> =
		For the WNLI instances, we convert them to
		the format of WSC as illustrated in Table 3: we
		ﬁrst detect pronouns in the premise using spaCy6;
		then given the detected pronoun, we search its left
		of the premise in hypothesis to ﬁnd the longest
		common substring (LCS) ignoring character case the
		lamplight
		Table 3: Examples of transforming WNLI to WSC for-
		mat
	</Abstractive Summary>
	<Extractive Summary> =
		, the cookstove, the
		kitchen and the lamplight as shown in Table 3)
	</Extractive Summary>
</Paper ID=ument1008>


<Paper ID=ument1008> <Table ID =4>
	<Abstractive Summary> =
		0
		-
		-
		Table 4: Test results
		Figure 2: Comparison with SSM and MLM on WNLI examples
	</Abstractive Summary>
</Paper ID=ument1008>


<Paper ID=ument1008> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: Ablation study of the two component models
		in HNN
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 summarizes the experimen-
		tal results
	</Extractive Summary>
</Paper ID=ument1008>


<Paper ID=ument1008> <Table ID =6>
	<Abstractive Summary> =
		7
		Table 6: Ablation study of the ranking loss
	</Abstractive Summary>
</Paper ID=ument1008>


<Paper ID=ument1009> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example from the DREAM dataset; the as-
		terisk (*) denotes the correct answer
	</Abstractive Summary>
	<Extractive Summary> =
		An example from DREAM that
		requires commonsense is shown in Table 1, and
		an example from CommonsenseQA is shown in
		Table 2
	</Extractive Summary>
</Paper ID=ument1009>


<Paper ID=ument1009> <Table ID =2>
	<Abstractive Summary> =
		New York
		Table 2:
		An example from the CommonsenseQA
		dataset; the asterisk (*) denotes the correct answer
	</Abstractive Summary>
</Paper ID=ument1009>


<Paper ID=ument1009> <Table ID =3>
	<Abstractive Summary> =
		26
		Options
		Extracted ConceptNet triples
		Bank
		(revolving door AtLocation bank) (bank RelatedTo security)
		Library
		(revolving door AtLocation library)
		Department Store
		(revolving door AtLocation store) (security IsA department)
		Mall
		(revolving door AtLocation mall)
		New York
		(revolving door AtLocation New York)
		Table 3: Extracted ConceptNet relations for sample shown in Table 2
	</Abstractive Summary>
</Paper ID=ument1009>


<Paper ID=ument1009> <Table ID =4>
	<Abstractive Summary> =
		Input sentence
		Generated ATOMIC relations
		Utterance 1
		(xAttr dedicated) (xWant to get to work)
		Utterance 2
		(xAttr far) (xReact happy) (xWant to get to their destination)
		Option A
		(xAttr calm) (xWant to avoid the road)
		Option B
		(xAttr careless) (xReact annoyed) (xEffect get tired)
		Option C
		(xAttr frustrated) (xEffect get tired) (xWant to get out of car)
		Table 4: Sample generated ATOMIC relations for sample shown in Table 1
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the sample ATOMIC relations, gen-
		erated using the DREAM example in Table 1
	</Extractive Summary>
</Paper ID=ument1009>


<Paper ID=ument1009> <Table ID =5>
	<Abstractive Summary> =
		8
		Table 5: Results on DREAM; the asterisk (*) denotes
		results taken from leaderboard
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		DREAM results are shown in Table 5,
		and
		CommonsenseQA results are shown in Table
		6
	</Extractive Summary>
</Paper ID=ument1009>


<Paper ID=ument1009> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: Results on CommonsenseQA; the asterisk (*)
		denotes results taken from leaderboard
	</Abstractive Summary>
</Paper ID=ument1009>


<Paper ID=ument1009> <Table ID =7>
	<Abstractive Summary> =
		0)
		Table 7: Accuracies for each DREAM question type: M means Matching, S means Summary, L means Logic
		inference, C means Commonsense inference, and A means Arithmatic inference
	</Abstractive Summary>
</Paper ID=ument1009>


<Paper ID=ument1009> <Table ID =8>
	<Abstractive Summary> =
		7)
		Table 8: Accuracies for each CommonsenseQA question type: AtLoc
	</Abstractive Summary>
</Paper ID=ument1009>


<Paper ID=ument101> <Table ID =1>
	<Abstractive Summary> =
		52
		Table 1: Dataset statistics
	</Abstractive Summary>
</Paper ID=ument101>


<Paper ID=ument101> <Table ID =2>
	<Abstractive Summary> =
		38
		Table 2: Aggregating crowdsourced labels: estimating true labels for documents labelled by the crowd
	</Abstractive Summary>
</Paper ID=ument101>


<Paper ID=ument101> <Table ID =3>
	<Abstractive Summary> =
		46
		Table 3: Counts of different types of span errors
	</Abstractive Summary>
</Paper ID=ument101>


<Paper ID=ument1010> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1: Reported results on COPA
	</Abstractive Summary>
	<Extractive Summary> =
		Recent studies show
		that BERT and RoBERTa achieve considerable
		improvements on COPA (see Table 1)
	</Extractive Summary>
</Paper ID=ument1010>


<Paper ID=ument1010> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Applicability (App
	</Abstractive Summary>
	<Extractive Summary> =
		The applicability αk of
		a token k counts how often this token occurs in an
		alternative with one label, but not the other:
		αk =
		n
		�
		i=1
		1
		�
		∃j, k ∈ T(i)
		j
		∧ k /∈ T(i)
		¬j
		�
		The productivity πk of a token is the proportion of
		applicable instances for which it predicts the cor-
		rect answer:
		πk =
		�n
		i=1 1
		�
		∃j, k ∈ T(i)
		j
		∧ k /∈ T(i)
		¬j ∧ yi = j
		�
		αk
		Finally, the coverage ξk of a token is the propor-
		tion of applicable instances among all instances:
		ξk = αk
		n
		Table 2 shows the ﬁve tokens with highest cov-
		erage
	</Extractive Summary>
</Paper ID=ument1010>


<Paper ID=ument1010> <Table ID =3>
	<Abstractive Summary> =
		798
		Table 3: Results of human performance evaluation of
		the original COPA and Balanced COPA
	</Abstractive Summary>
	<Extractive Summary> =
		The human evaluation
		shows that our mirrored instances are compara-
		ble in difﬁculty to the original ones (see Table 3)
	</Extractive Summary>
</Paper ID=ument1010>


<Paper ID=ument1010> <Table ID =4>
	<Abstractive Summary> =
		0∗
		Table 4: Model performance on the COPA test set (Overall), on Easy instances with superﬁcial cues, and on Hard
		instances without superﬁcial cues
	</Abstractive Summary>
	<Extractive Summary> =
		7
		As Table 4 shows, previous models perform simi-
		larly on both subsets, with the exception of Sasaki
		et al
	</Extractive Summary>
	<Extractive Summary> =
		Argument reasoning comprehension is a
		high level natural language understanding task re-
		quiring world knowledge and complex reasoning
		skills, while COPA can be largely solved with
		associative reasoning, as the performance of the
		PMI-based baselines shows (Table 4)
	</Extractive Summary>
</Paper ID=ument1010>


<Paper ID=ument1010> <Table ID =5>
	<Abstractive Summary> =
		0)
		Table 5: Results of ﬁne-tuned models on Balanced COPA
	</Abstractive Summary>
</Paper ID=ument1010>


<Paper ID=ument1010> <Table ID =6>
	<Abstractive Summary> =
		1
		Table 6: Results of non-ﬁne-tuned models on Balanced COPA
	</Abstractive Summary>
</Paper ID=ument1010>


<Paper ID=ument1010> <Table ID =7>
	<Abstractive Summary> =
		40
		Table 7: Sensitivity of BERT-large to superﬁcial cues
		identiﬁed in §2 (unit: 10−2)
	</Abstractive Summary>
</Paper ID=ument1010>


<Paper ID=ument1011> <Table ID =1>
	<Abstractive Summary> =
		33
		Table 1: Crowd-sourced labeling of phrases generated
		by the pattern approach of section 2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 is a sum-
		mary of the annotation results
	</Extractive Summary>
</Paper ID=ument1011>


<Paper ID=ument1011> <Table ID =2>
	<Abstractive Summary> =
		com/p/word2vec/
		Sound
		Smell
		honking cars
		burning rubber
		snoring
		chlorine
		gunshots
		citrus blossoms
		live music
		fresh paint
		Table 2: Examples of sound and smell concepts recog-
		nized by our method
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, we obtain high accuracy
		across all models
	</Extractive Summary>
</Paper ID=ument1011>


<Paper ID=ument1011> <Table ID =3>
	<Abstractive Summary> =
		56
		Table 3: Performance of the various models on the task
		of sense recognition
	</Abstractive Summary>
</Paper ID=ument1011>


<Paper ID=ument1011> <Table ID =4>
	<Abstractive Summary> =
		43
		Table 4: Fleiss κ
	</Abstractive Summary>
</Paper ID=ument1011>


<Paper ID=ument1011> <Table ID =5>
	<Abstractive Summary> =
		85
		Table 5: Accuracy of the linear models (LM) and mem-
		ory networks models (MM) on the sound-source rela-
		tion
	</Abstractive Summary>
</Paper ID=ument1011>


<Paper ID=ument1011> <Table ID =6>
	<Abstractive Summary> =
		80
		Table 6: Accuracy on the sound-scene relation
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6, the lin-
		ear model with the shortest path achieves the best
		accuracy of 81%
	</Extractive Summary>
</Paper ID=ument1011>


<Paper ID=ument1011> <Table ID =7>
	<Abstractive Summary> =
		82
		Table 7: Accuracy on the sound-sentiment relation
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 7, the linear model
		with LSTM encoded phrases achieved the highest
		accuracy of 84%
	</Extractive Summary>
</Paper ID=ument1011>


<Paper ID=ument1012> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example classes of errors when automatically generating inference pattern graphs
	</Abstractive Summary>
	<Extractive Summary> =
		Instead, we discovered
		that both the clustering and connection processes
		are susceptible to a number of common opportu-
		nities for error (described in Table 1) that limit
		this process in practice
	</Extractive Summary>
	<Extractive Summary> =
		Here the
		user can also remove bad edges (two table rows
		that were automatically connected, but whose con-
		nection isn’t meaningful), or remove subsets of the
		column links on edges that are partially correct
		(see Table 1)
	</Extractive Summary>
</Paper ID=ument1012>


<Paper ID=ument1012> <Table ID =2>
	<Abstractive Summary> =
		com
		58
		Measure
		Count
		Graph Nodes:
		Nodes before merging
		700
		Nodes after merging
		540 (77%)
		Graph Edges:
		Edges before curation
		637
		Edges after curation
		771 (21%)
		Grid Row-to-Row Connections:
		Row-to-row connections before curation
		1384
		Row-to-row connections modiﬁed
		631 (46%)
		Row-to-row connections removed
		224 (16%)
		Grid Edge Constraints:
		Edge constraints before curation
		2101
		Edge constraints removed
		133 (6%)
		Edge constraints marked optional
		27 (1%)
		Table 2: Manual edits done to the automatically gen-
		erated graph and grid during the merging and curation
		steps
	</Abstractive Summary>
</Paper ID=ument1012>


<Paper ID=ument1012> <Table ID =3>
	<Abstractive Summary> =
		for products
		5
		5
		Measurements
		22
		34
		Navigation lost at sea
		6
		7
		Physical Changes
		13
		14
		Seeing
		19
		29
		Soil erosion*
		6
		6
		Solutions - Dissolving substances*
		4
		5
		Sources of Heat*
		3
		2
		Sunlight as a source of energy*
		14
		30
		Sunlight location and shadow size*
		7
		7
		Taste*
		9
		11
		Taxonomic Inheritance
		2
		1
		Texture*
		4
		3
		Thermal Conductivity
		27
		34
		Touch-Hardness*
		4
		3
		Table 3: A list of high-level inference patterns discov-
		ered in the corpus of explanations for Matter science
		exam questions using this tool
	</Abstractive Summary>
	<Extractive Summary> =
		Manual inspection of the curated graph using the
		Graph View revealed 29 high-level inference pat-
		terns shown in Table 3, each containing between
		3 and 66 nodes, and up to 107 edges
	</Extractive Summary>
</Paper ID=ument1012>


<Paper ID=ument1012> <Table ID =4>
	<Abstractive Summary> =
		Change of State
		Freezing means changing from a liquid to a solid by
		reducing heat energy
		A liquid is a kind of state of matter
		Water is in the liquid state, called liquid water, for
		temperatures between 0 C and 100 C
		A solid is a kind of state of matter
		Water is in the solid state, called liquid water, for
		temperatures between -273 C and 0 C
		Cooling means reducing heat energy
		Freezing is when liquids are cooled below freezing point
		Phase Changes
		Boiling means changing from a liquid to a gas by
		adding heat energy
		Boiling is a kind of phase change
		A phase change is when a substance changes from
		one state to another state
		Temperature changes can cause phase changes
		Alloys
		Alloys are made of two or more metals
		Bronze is a kind of alloy
		Bronze is made of copper and tin
		Tin is a kind of metal
		Copper is a kind of metal
		Containers contain objects
		A container is a kind of object
		If a container contains something, then that container
		touches that something
		A bowl is a kind of container
		A container contains objects
		A rock is a kind of object
		Table 4: A small subset of example combinations of
		knowledge base facts that satisfy the constraints of in-
		ference patterns extracted from the explanation corpus
	</Abstractive Summary>
</Paper ID=ument1012>


<Paper ID=ument1012> <Table ID =5>
	<Abstractive Summary> =
		, acids)
		4
		3
		3
		Containers contain things
		-
		-
		-
		Containers (Abstracted)
		5
		5
		1000
		Containers (Application)
		6
		6
		15
		Cooking Food
		-
		-
		-
		Cooking (Core)
		6
		6
		26
		Cooking a particular food
		7
		7
		338
		Cooking (Containers for cooking)
		6
		6
		1
		Electrical Conductivity
		-
		-
		-
		Dangers of Electric Shock
		4
		3
		414
		Electrical Insulation
		15
		23
		46
		Electrical Circuits in Devices
		7
		11
		18
		Friction
		-
		-
		-
		Friction (core)
		16
		31
		3
		General Motion *
		3
		3
		6
		Ice Wedging *
		4
		4
		2
		Magnetism
		-
		-
		-
		Magnetic Objects
		5
		4
		10
		Manufacturers use material for products
		-
		-
		-
		Manufacturers use materials for products (core)
		4
		3
		19
		Measurements
		-
		-
		-
		Measurement Tools
		4
		4
		130
		Observations (Celestial Bodies)
		5
		6
		6
		Observations (Distant Objects)
		5
		6
		208
		Observations (Microscopic Things)
		6
		6
		4
		Observations (Small Things)
		6
		6
		94
		Navigation-Direction-Being lost at sea
		-
		-
		-
		Navigation (core)
		3
		2
		1
		Navigation (being lost/boat)
		6
		7
		2
		Physical Changes
		-
		-
		-
		Physical Changes (Changing Shape)
		9
		10
		832
		Seeing
		-
		-
		-
		Things that can see and what they can see
		6
		6
		1000
		Soil erosion *
		6
		6
		28
		Solutions - Dissolving substances *
		4
		5
		1
		Sources of Heat *
		3
		2
		6
		Sunlight as a source of energy *
		14
		30
		80
		Sunlight location and shadow size *
		7
		7
		312
		Taste *
		9
		11
		26
		Taxonomic Inheritance
		2
		1
		1000
		Texture *
		4
		3
		2
		Thermal Conductivity
		-
		-
		-
		Thermal Conductivity (Core)
		21
		26
		1000
		Thermal Conductors 0
		5
		4
		9
		Thermal Conductors 1
		5
		4
		8
		Thermal Insulators
		5
		5
		5
		Touch-Hardness *
		4
		3
		8
		Table 5: An extended list of inference patterns discovered in the corpus of explanations for Matter science exam questions using
		this tool
	</Abstractive Summary>
	<Extractive Summary> =
		A full list of patterns is
		provided in Table 5 (see Appendix)
	</Extractive Summary>
</Paper ID=ument1012>


<Paper ID=ument1013> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: Statistics of ReCoRD
		amples, most of which require commonsense
		reasoning
	</Abstractive Summary>
</Paper ID=ument1013>


<Paper ID=ument1013> <Table ID =2>
	<Abstractive Summary> =
		69
		Rank
		Team Name
		Architecture
		Commonsense
		Other Resources
		Tasks
		1
		PSH–SJTU
		Transformer (XLNet)
		-
		RACE, SWAG
		1, 2
		2
		IIT-KGP
		Transformer
		(BERT
		+ XLNet)
		-
		RACE
		1
		3
		BLCU-NLP
		Transformer (BERT)
		-
		ReCoRD, RACE
		1
		4
		JDA
		Transformer (BERT)
		ConceptNet,
		Atomic, Webchild
		Wikipedia
		1
		5
		KARNA
		Transformer (BERT)
		ConceptNet
		-
		1
		Table 2: Overview of participating systems
		DocQA
		(Clark and Gardner, 2018) is a strong
		baseline model for extractive QA
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 gives a summary of the par-
		ticipating systems
	</Extractive Summary>
</Paper ID=ument1013>


<Paper ID=ument1013> <Table ID =3>
	<Abstractive Summary> =
		97
		Table 3:
		Performance of participating systems and
		baselines for task 1, in total (acc), on commonsense-
		based questions (acccs), and on out-of-domain ques-
		tions that belong to the ﬁve held-out scenarios
		(accOOD)
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 3 shows the performance of the participating
		systems and the baselines on the task 1 data
	</Extractive Summary>
</Paper ID=ument1013>


<Paper ID=ument1013> <Table ID =4>
	<Abstractive Summary> =
		12
		Table 4: Performance (EM and F1) of human, partici-
		pating systems and baselines for task 2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows EM (%) and F1 (%) of hu-
		man performance, the PSH-SJTU system as well
		as baselines on the development and test sets of
		task 2
	</Extractive Summary>
</Paper ID=ument1013>


<Paper ID=ument1013> <Table ID =5>
	<Abstractive Summary> =
		685
		Table 5: Performance of participating systems and baselines for task 1 on the 5 most common question types
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the systems’ performance on sin-
		gle question types for task 1
	</Extractive Summary>
</Paper ID=ument1013>


<Paper ID=ument1014> <Table ID =1>
	<Abstractive Summary> =
		Each passage is concatenated with
		Edge Relation
		Event Phrase
		RelatedTo
		A is related to B
		FormOf
		A is a form of B
		PartOf
		A is a part of B
		UsedFor
		A is used for B
		AtLocation
		A is at B
		Causes
		A causes B
		Synonym
		A is synonym of B
		Antonym
		A is antonym of B
		DerivedFrom
		A is derived from B
		Table 1: Event Phrases
		edge relation from ConceptNet
	</Abstractive Summary>
</Paper ID=ument1014>


<Paper ID=ument1014> <Table ID =2>
	<Abstractive Summary> =
		The model is trained on
		77
		Figure 1: Model Overview
		Paramter
		Value
		learningrate
		2e-5
		maxseqlength
		210
		batchsize
		4
		epochs
		3
		Optimizer
		ADAM
		Table 2: Hyperparameters
		Google Colab GPU for 2 epochs
	</Abstractive Summary>
</Paper ID=ument1014>


<Paper ID=ument1014> <Table ID =3>
	<Abstractive Summary> =
		1%
		-
		Table 3: Results
		• w/o Qand PA Rel :
		Model with context
		containing passage and relation between
		question and answer
	</Abstractive Summary>
</Paper ID=ument1014>


<Paper ID=ument1015> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Accuracy results for various models
	</Abstractive Summary>
</Paper ID=ument1015>


<Paper ID=ument1015> <Table ID =2>
	<Abstractive Summary> =
		3
		Yes
		(Jain and Singh, 2019)
		Table 2: Performance comparison among participants
		of the COIN Shared Task 1, depicting use of common-
		sense knowledge bases
	</Abstractive Summary>
	<Extractive Summary> =
		This is also evident from Table 2 as the
		top 3 systems (unofﬁcial leaderboard)3 do not use
		any kind of commonsense knowledge bases
	</Extractive Summary>
</Paper ID=ument1015>


<Paper ID=ument1016> <Table ID =1>
	<Abstractive Summary> =
		When did they wait for their train?
		a) before buying the ticket
		b) after buying a ticket
		Table 1: Example of a prompt from the shared task
		dataset, an everyday commonsense reasoning dataset
	</Abstractive Summary>
</Paper ID=ument1016>


<Paper ID=ument1016> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Results with B
	</Abstractive Summary>
</Paper ID=ument1016>


<Paper ID=ument1016> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Question type comparison between different
		models on the shared task: previous state-of-the-art
		TriAN (Wang, 2018), BERTLARGE, and B
	</Abstractive Summary>
</Paper ID=ument1016>


<Paper ID=ument1016> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4:
		Hyperparameters used throughout experi-
		ments
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Hyperparameters
		Seen in Table 4 is a list of hyperparameters for our
		experiments
	</Extractive Summary>
</Paper ID=ument1016>


<Paper ID=ument1017> <Table ID =1>
	<Abstractive Summary> =
		The intuition behind
		why this multi-stage ﬁne-tuning strategy works is
		95
		Dataset
		Options
		Sentence A
		Sentence B
		RACE
		4
		passage
		query+option
		SWAG
		4
		query
		option
		Task 1
		2
		passage
		query+option
		Table 1: Structure of inputs for the two supplementary
		tasks and the target task dataset
		Dataset
		Train
		Dev
		RACE
		87866
		4887
		SWAG
		73546
		20006
		Task 1
		14191
		2020
		Table 2: Basic statistics for the three datasets involved
		in solving task 1
		that (a) to let the pre-trained LMs to adopt to the
		similar contextual environment, (b) and make the
		model more suitable for this speciﬁc task forma-
		tion
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents the structure of the inputs for
		the three datasets, where k is the number of op-
		tions for each query
	</Extractive Summary>
</Paper ID=ument1017>


<Paper ID=ument1017> <Table ID =2>
	<Abstractive Summary> =
		The intuition behind
		why this multi-stage ﬁne-tuning strategy works is
		95
		Dataset
		Options
		Sentence A
		Sentence B
		RACE
		4
		passage
		query+option
		SWAG
		4
		query
		option
		Task 1
		2
		passage
		query+option
		Table 1: Structure of inputs for the two supplementary
		tasks and the target task dataset
		Dataset
		Train
		Dev
		RACE
		87866
		4887
		SWAG
		73546
		20006
		Task 1
		14191
		2020
		Table 2: Basic statistics for the three datasets involved
		in solving task 1
		that (a) to let the pre-trained LMs to adopt to the
		similar contextual environment, (b) and make the
		model more suitable for this speciﬁc task forma-
		tion
	</Abstractive Summary>
</Paper ID=ument1017>


<Paper ID=ument1017> <Table ID =3>
	<Abstractive Summary> =
		76%
		-
		Table 3: Main results on the task 1
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		The main experimental accuracy results for task
		1 are shown in Table 3, in which human perfor-
		mance is provided by task organizers
	</Extractive Summary>
	<Extractive Summary> =
		As a result, Table 3 illustrates that pre-training on
		RACE plays a signiﬁcant role in the system
	</Extractive Summary>
</Paper ID=ument1017>


<Paper ID=ument1017> <Table ID =4>
	<Abstractive Summary> =
		74 %
		Table 4: The main results on task 2
	</Abstractive Summary>
	<Extractive Summary> =
		For task 2, the results are presented on Table 4,
		where the bolded models are our submissions on
		the test leaderboard
	</Extractive Summary>
</Paper ID=ument1017>


<Paper ID=ument1018> <Table ID =1>
	<Abstractive Summary> =
		5K
		Table 1: Datasets used in this paper for ﬁne-tuning
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists datasets selected in this pa-
		per
	</Extractive Summary>
</Paper ID=ument1018>


<Paper ID=ument1018> <Table ID =2>
	<Abstractive Summary> =
		7
		-
		Table 2: Main results
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results and Analysis
		Table 2 demonstrates results of various trained
		models, consisting of three groups
	</Extractive Summary>
</Paper ID=ument1018>


<Paper ID=ument1018> <Table ID =3>
	<Abstractive Summary> =
		Description
		ﬁrst
		stage
		second
		stage
		t: tokens max length
		350
		300
		e: ﬁne-tune epoch
		{3,4}
		{3,4}
		α: learning rate
		3e-5
		1e-5
		b: batch size
		32
		64
		g:gradient accumulation step
		4
		8
		Table 3: Hyper-parameters settings used during train-
		ing
	</Abstractive Summary>
</Paper ID=ument1018>


<Paper ID=ument1019> <Table ID =1>
	<Abstractive Summary> =
		c) meaningless causal relations: break the ice by
		seeing it; killing each other by slashing the rate;
		110
		Figure 4: Flowchart of causal relations extraction from text
		Table 1: Most frequent 3-grams for extracted result verbs
		Table 2: Samples of extracted causal relations
		7
		Conclusion and Further Work
		Commonsense inferences allow us to equip and
		empower cognitive robots with an ability to under-
		stand high-level natural language commands (or
		instructions)
	</Abstractive Summary>
	<Extractive Summary> =
		Totally 60 3-grams were ex-
		tracted (see Table 1 for details)
	</Extractive Summary>
</Paper ID=ument1019>


<Paper ID=ument1019> <Table ID =2>
	<Abstractive Summary> =
		c) meaningless causal relations: break the ice by
		seeing it; killing each other by slashing the rate;
		110
		Figure 4: Flowchart of causal relations extraction from text
		Table 1: Most frequent 3-grams for extracted result verbs
		Table 2: Samples of extracted causal relations
		7
		Conclusion and Further Work
		Commonsense inferences allow us to equip and
		empower cognitive robots with an ability to under-
		stand high-level natural language commands (or
		instructions)
	</Abstractive Summary>
</Paper ID=ument1019>


<Paper ID=ument1019> <Table ID =3>
	<Abstractive Summary> =
		We present a method for acquir-
		Table 3: Examples of causal relations for the 3-gram
		“open the window”
		ing the knowledge needed to transform high-level
		result-verb commands into action-verb commands
		for further implementation into primitive actions
	</Abstractive Summary>
</Paper ID=ument1019>


<Paper ID=ument102> <Table ID =1>
	<Abstractive Summary> =
		test
		Finnish
		fi_tdt
		14981
		1875
		1555
		North Sámi
		sme_giella
		1128
		1129
		865
		Portuguese
		pt_bosque
		8329
		560
		477
		Galician
		gl_treegal
		300
		300
		400
		Turkish
		tr_imst
		3685
		975
		975
		Kazakh
		kk_ktb
		15
		16
		1047
		Table 1: Train/dev split used for each treebank
	</Abstractive Summary>
	<Extractive Summary> =
		None of our target
		treebanks have a development set, so we generate
		new train/dev splits by 50:50 (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 pro-
		vides the statistics of the augmented data
	</Extractive Summary>
</Paper ID=ument102>


<Paper ID=ument102> <Table ID =2>
	<Abstractive Summary> =
		4
		Parsing North Sámi
		North Sámi is our largest low-resource treebank,
		so we use it for a full evaluation and analysis of
		original
		+Morph
		+Nonce
		T100
		1128
		7636
		4934
		T50
		564
		3838
		2700
		T10
		141
		854
		661
		Table 2: Number of North Sámi training sentences
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 reports the number of training
		sentences after we augment the data using meth-
		ods described in Section 2
	</Extractive Summary>
</Paper ID=ument102>


<Paper ID=ument102> <Table ID =3>
	<Abstractive Summary> =
		9)
		Table 3: LAS results on North Sámi development data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 reports our results, and we
		review our motivating scenarios below
	</Extractive Summary>
</Paper ID=ument102>


<Paper ID=ument102> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Results for the monolingual POS predictions,
		ordered by the frequency of each tag in the dev split
		(%dev)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 reports the POS prediction accuracy
	</Extractive Summary>
</Paper ID=ument102>


<Paper ID=ument102> <Table ID =5>
	<Abstractive Summary> =
		In parsing
		this effect is less apparent, mainly because mono-
		lingual NONCE has the poorest POS representation
		for infrequent tags (%dev), and better representa-
		1110
		Top nearest Finnish words
		North Sámi
		char-level
		word-level
		borrat (VERB; eat)
		herrat (NOUN; gentleman)
		k¨ayd¨a (VERB; go)
		kerrat (NOUN; time)
		otan (VERB; take)
		naurat (VERB; laugh)
		sain (VERB; get)
		veahki (NOUN; help)
		nuuhki (VERB; sniff)
		tyhj¨aksi (ADJ; empty)
		v¨aki (NOUN; power)
		johonki (PRON; something)
		avarsi (VERB; expand)
		l¨aht¨okohdaksi (NOUN; basis)
		divrras (ADJ; expensive)
		harras (ADJ; devout)
		v¨altt¨am¨at¨ont¨a (ADJ; essential)
		reipas (ADJ; brave)
		mahdollista (ADJ; possible)
		sarjaporras (NOUN; series)
		kilpailukykyisempi (ADJ; competitive)
		Table 5: Most similar Finnish words for each North Sámi word based on cosine similarity
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows examples of top three closest
		Finnish training words for a given North Sámi
		word
	</Extractive Summary>
</Paper ID=ument102>


<Paper ID=ument102> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: # of North Sámi tokens for which the most
		similar Finnish word has the same POS
	</Abstractive Summary>
	<Extractive Summary> =
		In fact, when we compare
		the most similar Finnish word (Table 6) quantita-
		tively, we find that the word-level representations
		of North Sámi are often similar to Finnish word
		with the same POS; the same trend does not hold
		for character-level representations
	</Extractive Summary>
</Paper ID=ument102>


<Paper ID=ument102> <Table ID =7>
	<Abstractive Summary> =
		7
		Table 7: LAS results on development sets
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Experimental results
		Table 7 reports the LAS performance on the devel-
		opment sets
	</Extractive Summary>
	<Extractive Summary> =
		) in Table 7)
	</Extractive Summary>
</Paper ID=ument102>


<Paper ID=ument102> <Table ID =8>
	<Abstractive Summary> =
		23
		2/27
		Table 8: Comparison to CoNLL 2018 UD Shared Task on test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 presents the overall comparison on the
		test sets
	</Extractive Summary>
</Paper ID=ument102>


<Paper ID=ument1020> <Table ID =1>
	<Abstractive Summary> =
		2%
		Table 1: The result of crowdsourcing
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the ratio of event pairs with each
		label
	</Extractive Summary>
	<Extractive Summary> =
		2
		The Quality of Original Datasets
		As shown in Table 1, only 84
	</Extractive Summary>
</Paper ID=ument1020>


<Paper ID=ument1020> <Table ID =2>
	<Abstractive Summary> =
		Train
		Dev
		Test
		New Test
		Wikihow
		1,287,360
		26,820
		26,820
		858 (174)
		Descript
		23,320
		2,915
		2,915
		2,203 (199)
		Table 2: Statistics of the datasets
	</Abstractive Summary>
</Paper ID=ument1020>


<Paper ID=ument1020> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Event prediction performance evaluated by automatic evaluation metrics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 lists the evaluation results
	</Extractive Summary>
</Paper ID=ument1020>


<Paper ID=ument1020> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Next events generated by the deterministic and probabilistic models
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Qualitative Analysis
		Table 4 shows next events generated by the deter-
		ministic and probabilistic models, with Table 4a
		being an example from Wikihow
	</Extractive Summary>
	<Extractive Summary> =
		Table 4b shows an example from Descript
	</Extractive Summary>
	<Extractive Summary> =
		For the reference
		next events of “board bus” (Table 4b), “pay for the
		bus” and its variants dominate, but we are unsure
		if they are truly more typical than “place your lug-
		gage overhead or beneath seat”
	</Extractive Summary>
</Paper ID=ument1020>


<Paper ID=ument1021> <Table ID =1>
	<Abstractive Summary> =
		,
		2016), paragraph reconstruction (Li and Juraf-
		sky, 2017), and hard coreference resolution (Peng
		Event
		Plausible?
		bird-construct-nest
		�
		bottle-contain-elephant
		�
		gorilla-ride-camel
		�
		lake-fuse-tie
		�
		Table 1:
		Example events from Wang et al
	</Abstractive Summary>
	<Extractive Summary> =
		This is the problem of determining if a
		given event, represented as an s-v-o triple, is phys-
		ically plausible (Table 1)
	</Extractive Summary>
</Paper ID=ument1021>


<Paper ID=ument1021> <Table ID =2>
	<Abstractive Summary> =
		We follow
		the same evaluation procedure as previous work
		125
		Wikipedia
		male-have-income
		village-have-population
		event-take-place
		NELL
		login-post-comment
		use-constitute-acceptance
		modules-have-options
		Table 2: Most frequent s-v-o triples for each corpus
	</Abstractive Summary>
</Paper ID=ument1021>


<Paper ID=ument1021> <Table ID =3>
	<Abstractive Summary> =
		89
		Table 3: Mean accuracy of classifying plausible events
		for models trained in a supervised setting
	</Abstractive Summary>
	<Extractive Summary> =
		(2018): we
		perform 10-fold cross validation on the dataset of
		3,062 s-v-o triples, and report the mean accuracy
		of running this procedure 20 times all with the
		same model initialization (Table 3)
	</Extractive Summary>
</Paper ID=ument1021>


<Paper ID=ument1021> <Table ID =4>
	<Abstractive Summary> =
		Event
		Plausible?
		BERT
		GT
		dentist-capsize-canoe
		�
		�
		stove-heat-air
		�
		�
		sun-cool-water
		�
		�
		chair-crush-water
		�
		�
		Table 4: Interpreting log-likelihood as conﬁdence, ex-
		ample events for which BERT was highly conﬁdent and
		either correct or incorrect with respect to the ground
		truth (GT) label
	</Abstractive Summary>
</Paper ID=ument1021>


<Paper ID=ument1021> <Table ID =5>
	<Abstractive Summary> =
		56
		Table 5: Accuracy of classifying plausible events for
		models trained on a corpus in a self-supervised manner
	</Abstractive Summary>
	<Extractive Summary> =
		2), we report
		both the validation and test accuracies of classify-
		ing physically plausible events (Table 5)
	</Extractive Summary>
</Paper ID=ument1021>


<Paper ID=ument1022> <Table ID =1>
	<Abstractive Summary> =
		75
		Table 1: Accuracy of the probing model compared with the baselines including previous approaches on the at-
		tributes in the Verb Physics dataset
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results and Discussion
		The probing model (Figure 1) with pre-trained
		representations has better accuracy than previous
		approaches which use extra information in addi-
		tion to the words being compared (Table 1)
	</Extractive Summary>
</Paper ID=ument1022>


<Paper ID=ument1022> <Table ID =2>
	<Abstractive Summary> =
		893
		Table 2: The probing model trained on the Verb Physics
		size dataset and evaluated on (Bagherinezhad et al
	</Abstractive Summary>
	<Extractive Summary> =
		Yet the probing model achieves at
		least 4% higher accuracy (Table 2)
	</Extractive Summary>
</Paper ID=ument1022>


<Paper ID=ument1022> <Table ID =3>
	<Abstractive Summary> =
		52
		Table 3: Accuracy of probing models (averaged across
		the ﬁve attributes) on the Verb Physics dev sets
	</Abstractive Summary>
	<Extractive Summary> =
		jority class baseline for this dataset), but the drop
		in accuracy is higher than 10% for GloVe and
		ELMo (Table 3): Our model is not simply rely-
		ing on lexical memorization
	</Extractive Summary>
</Paper ID=ument1022>


<Paper ID=ument1022> <Table ID =4>
	<Abstractive Summary> =
		68
		Table 4: Evaluation on Bagherinezhad et al
	</Abstractive Summary>
	<Extractive Summary> =
		, 2016): Using just one word when training
		and evaluating sees a drop of about 12 to 15% in
		accuracy (Table 4)
	</Extractive Summary>
</Paper ID=ument1022>


<Paper ID=ument1022> <Table ID =5>
	<Abstractive Summary> =
		Examples of Orders Formed Around a Word
		head < knee < meal < chair < back < place <
		street < world < gate < air < ﬂoor < room
		eye < chair < child < king < daughter < wife <
		boy < messenger < father < coach < horse < door <
		house < gate < train < room < sun
		Table 5: Two examples for orderings formed around
		the words chair and gate for the size attribute using
		GloVe
	</Abstractive Summary>
</Paper ID=ument1022>


<Paper ID=ument1022> <Table ID =6>
	<Abstractive Summary> =
		75
		Table 6: The best accuracies obtained by a Linear
		Model compared with the best accuracies obtained by a
		shallow Neural Network
	</Abstractive Summary>
	<Extractive Summary> =
		Per Table 6, linear modela are
		almost at par (accuracy within 1%) with shal-
		low fully connected neural networks on the Verb
		Physics dev set
	</Extractive Summary>
</Paper ID=ument1022>


<Paper ID=ument1022> <Table ID =7>
	<Abstractive Summary> =
		12
		Table 7: Label-Wise Accuracy: The GloVe, ELMo, and
		BERT representations (fed to a linear model) struggle
		to capture the relationship word1 ≈ word2 (label 2)
	</Abstractive Summary>
	<Extractive Summary> =
		This might explain the relative
		struggles GloVe, ELMo, and BERT face classify-
		ing comparisons labeled 2 (Table 7)
	</Extractive Summary>
</Paper ID=ument1022>


<Paper ID=ument1023> <Table ID =1>
	<Abstractive Summary> =
		22)
		Table 1: IC accuracy on SNIPS and Facebook dataset
		with all training data, reported as mean (SD)80
		Table 1: XNLI accuracy scores
		languages9
		487
		Table 1: Task description360
		Table 1: Results for DN and Chimera tasks5%
		3
		10,001
		Hypothesis
		Premise
		Natural language inference
		ABSA-L
		3
		2,618
		Aspect
		Review
		Aspect-based sentiment analysis, laptop domain
		ABSA-R
		3
		2,256
		Aspect
		Review
		Aspect-based sentiment analysis, restaurant domain
		Target
		3
		5,623
		Target
		Text
		Target-dependent sentiment analysis
		Stance
		3
		3,209
		Target
		Tweet
		Stance detection
		Topic-2
		2
		5,177
		Topic
		Tweet
		Topic-based sentiment analysis, binary
		Topic-5
		5
		7,236
		Topic
		Tweet
		Topic-based sentiment analysis, ﬁne-grained
		FNC-1
		4
		39,741
		Headline
		Document
		Fake News Detection
		Table 1: Size of label set, number of training examples, content of sequences, and task description of each dataset
		Table 1: Mention disambiguation features47
		Table 1: Examples of verb mappings for compound-
		disease relation
		Task Category
		Dataset
		Train Size
		Dev Size
		Natural Language Inference
		SNLI
		510,711
		9,831
		Natural Language Inference
		MNLI
		392,702
		9,815
		Answer Sentence Selection
		QNLI
		108,436
		5,732
		Paraphrase Detection
		Quora
		363,847
		40,430
		Table 1: Summary of the datasets
		SNLI The Stanford Natural Language Inference
		(SNLI) Corpus (Bowman et al
		As illustrated in Figure 2,
		these computationally inexpensive methods can
		consistently outperform the random baseline by a
		89
		Table 1: Results for Model-Driven and Data-Driven approaches
		98
		Benchmark
		Original benchmark query q∗
		OOV substituted query p∗
		Logical form l(q∗)
		OOV dataset
		ATIS
		list all ﬂights departing from ap0
		list all ﬂights taking off from ap0
		(λ $0 (and (ﬂight $0) (from $0 ap0)))
		PARA-ATIS
		ATIS
		i need a ﬂight from ci0 to ci1
		i require a ﬂight from ci0 to ci1
		(λ $0 (and (ﬂight $0) (from $0 ci0) (to $0 ci1)))
		PARA-ATIS
		GEO
		how many big cities are in s0
		how many large cities are in s0
		(count (λ $0 (and (major $0) (city $0) (loc $0 s0))))
		PARA-GEO
		GEO
		which state has the highest elevation
		which
		state
		has
		the
		highest
		natural elevation
		(argmax (λ $0 (state $0)) (λ $1 (elevation $1)))
		PARA-GEO
		Table 1: Table showing examples from OOV datasets PARA-GEO and PARA-ATIS which were constructed from
		the benchmark GEO and ATIS datasets (Ray et al
		104
		Dataset/Language
		Tokens (Dev)
		DEA
		German (Anselm)
		45,996
		DER
		German (RIDGES)
		9,712
		EN
		English
		16,334
		ES
		Spanish
		11,650
		HU
		Hungarian
		16,707
		IS
		Icelandic
		6,109
		PT
		Portuguese
		26,749
		SLB
		Slovene (Bohoriˇc)
		5,841
		SLG
		Slovene (Gaj)
		20,878
		SV
		Swedish
		2,245
		Table 1: Historical datasets used in our experiments
		and the size of their development sets
		117
		Table 1: We compare the existing baselines with MCT-MSR using LASER, BERT and XLM as PLRMs without
		ﬁne-tuning on “Leave Dataset”0
		Table 1: (Results) A comparison of model size and accuracy on 3 text classiﬁcation datasets
		135
		Dataset
		Train
		Dev
		Test
		SNLI
		549,367
		9842
		9842
		MNLI
		392,702
		9815
		-
		Table 1: Statistics of training datasets5
		Table 1: Test accuracy for each dataset and method999
		Table 1: Chosen hyperparameters for our POS tagging
		and parsing models770, which is comparable to
		178
		Table 1: Performance across different sizes of training data9100
		Table 1: Classiﬁcation accuracies and F1-Scores for news arcticle classiﬁcations for different source and target
		domains
		Train
		Test
		Languages
		311
		507
		Words
		631,828
		25,894
		Scripts
		42
		45
		Table 1: Corpus statics for Wiktionary dataset
		24
		Table 1: GLUE test results for our models, along with previous comparison points
		211
		Arabic
		Chinese
		English
		German
		vocab size
		21,902
		23,714
		15,617
		10,367
		train
		18,087
		57,251
		43,738
		18,598
		dev
		2,422
		6,736
		1,699
		1,000
		test
		2,556
		7,075
		2,416
		1,000
		Table 1: Dataset statistics4644)
		Table 1: Our best STL/MTL on a more realistic span based evaluation indicates (top) a more realistic but lower
		performance vs04
		Table 1: NER models comparison326%
		Table 1: Experiment Results of Question Classiﬁcation
		239
		We can notice that LSTM outperforms all the
		other algorithmsmil/program/low-resource-
		languages-for-emergent-incidents
		246
		Symbol
		Feature Name
		Equation
		Resource
		f 1
		l (ei,j)
		Mention-entity prior score
		log(max(p(ei,j|mi), ϵ))
		Variable
		f 2
		l (ei,j)
		Entity prior
		log(max(
		c(ei,j)
		�
		e∈EKB c(e), ϵ))
		Weng
		f 3
		l (ei,j)
		Related mention number
		�
		mk∈MD\mi 1(anyek,m∈Ekf 1
		g (ei,j, ek,m) > 0)
		-
		f 4
		l (ei,j)
		Exact match number
		�
		mk∈MD\mi 1(e ∈ Ek)
		-
		f 1
		g (ei,j, ek,w)
		Co-occurrence probability
		log(max(
		c(ei,j,ek,w)
		c(ei,j)
		), ϵ)
		Weng
		f 2
		g ei,j, ek,w)
		Positive Pointwise Mutual Information (PPMI)
		max(log2(
		p(ei,j,ek,w)
		p′(ei,j)p′(ek,w)), 0)
		Weng
		f 3
		g (ei,j, ek,w)
		Entity embedding similarity
		cosine(Vei,j, Vek,w)
		Weng
		f 4
		g (ei,j, ek,w)
		Hyperlink count
		log(max(
		�
		ek∈Hei,j 1(ei,j=ek,w)
		|Hei,j |
		, ϵ)
		Weng
		Table 1: Unary features (top half) and binary features (bottom half)58%
		Table 1: Macro-averaged changes in accuracy from the
		MTL baseline for DepRel – POS (top), SemTag – POS
		(bottom)536
		Table 1:
		Retrieval performance (MAP scores) of
		all models on Swahili and Tagalog CLIR evaluation
		datasets
		Table 1: Examples from our dataset of the same question-context pairs across all the languages with the correct
		answers highlighted in boldface209
		(b) soundex-based baseline
		Table 1: Performance statistics for our model (top) and baseline approaches (bottom)16
		Table 1: LAS scores of our parser in the raw text setup
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that augmenting data in
		feature space provides only minor improvements
		in classiﬁcation accuracy
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		shows an example of this mapping process
	</Extractive Summary>
	<Extractive Summary> =
		1
		Model-Driven Approaches
		Figure 1 and Table 1 compares the results of
		model-driven approaches and random sampling
		baseline
	</Extractive Summary>
	<Extractive Summary> =
		The above trends are inline with the results re-
		ported in Table 1 as well
	</Extractive Summary>
	<Extractive Summary> =
		, 2018) referred as PARA-
		ATIS and PARA-GEO datasets respectively (ex-
		amples in Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Table 1 gives an overview of the languages and
		the size of the development set, which we use for
		evaluation
	</Extractive Summary>
	<Extractive Summary> =
		According
		to Table 1, on Leave dataset, for LASER, MCT-
		MSR perform slightly better than other baseline
		approaches
	</Extractive Summary>
	<Extractive Summary> =
		Results:
		Table 1 shows the accuracy of the ﬁne
		tuned models for all three methods
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 further shows that the hyperbolic model
		outperforms its Euclidean counterpart in the low
		148
		Dataset
		Model
		nfine = 5
		nfine = 10
		nfine = 20
		nfine = 100
		MLP
		37
	</Extractive Summary>
	<Extractive Summary> =
		Results: Table 1 shows the details results of our
		approach for all the datasets
	</Extractive Summary>
	<Extractive Summary> =
		In
		Table 1, we have shown improvements by a high
		margin for all datasets
	</Extractive Summary>
	<Extractive Summary> =
		1
		Quality and Efﬁciency
		Of the transfer set construction approaches,
		our principled generation methods consistently
		achieve the highest results (see Table 1, rows 6 and
		7), followed by the rule-based TSMP and the man-
		ually curated TSIMDb (rows 8 and 9)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows
		statistics for all languages
	</Extractive Summary>
	<Extractive Summary> =
		2
		Embedding Ablations
		We found that in general as we increase the com-
		plexity of pre-trained embeddings, from character-
		based learned CNN embeddings to pre-trained
		GloVe, ELMo, and BERT, we see improved per-
		formance (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		5
		Performance Comparison
		The ﬁrst two rows of Table 1 depicts the compar-
		ison of the proposed model to the state-of-the-art
		NER models on the English datasets
	</Extractive Summary>
	<Extractive Summary> =
		The top half of Table 1 shows unary fea-
		ture functions, which take one argument ei,j and
		return a value that represents some property of this
		entity
	</Extractive Summary>
	<Extractive Summary> =
		The
		formulation and resource requirements of unary
		and binary features are shown in the top and bot-
		tom halves of Table 1 respectively
	</Extractive Summary>
	<Extractive Summary> =
		We name our proposed feature set that includes
		all features listed in Table 1 as FEAT
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results and Discussion
		Table 1 contains results of the experiments macro-
		averaged across all languages and treebanks in the
		UD and across all languages in the PMB
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the same
		example across ﬁve languages
	</Extractive Summary>
</Paper ID=ument1023>


<Paper ID=ument1023> <Table ID =2>
	<Abstractive Summary> =
		12)
		Table 2: Average IC accuracy for all intents’ FSI simu-
		lations on SNIPS and FBDialog dataset50
		Table 2: Sentiment Classiﬁcation accuracy scores
		anced dataset)4
		Table 2: Results of the experiment (i), accuracy per-
		centage on test data for the English and Portuguese cor-
		pora
		943
		Table 2: Results on both CRW tasks5%, ABSA-L, Target
		Table 2: Main tasks and their corresponding auxiliary
		tasks as used here and by Augenstein et al7
		Table 2: Linear entity mapping statistics and perfor-
		mance (Precision (%) at K) (en: English, es: Spanish,
		it: Italian, ru: Russian, so: Somali, tr: Turkish, uk:
		Ukrainian, zh: Chinese)2m)
		-
		-
		Table 2: Data sets used in our experiments3 %
		Table 2: Transfer performance (accuracy) of different
		domain adaptation methods31
		Table 2: Comparison of best 10 fold accuracy of all
		models on benchmark JOB dataset80
		(b) Comparison to previous work
		Table 2: Normalization accuracy on dev sets after training on 1,000 tokens8
		Table 2: We compare the existing baselines with MCT-FT using Watt, BERT and XLM as PLRMs with ﬁne-
		tuning on “Leave Dataset”13
		Table 2: Above, we show a comparison of hard labeling and distillation for labeling the synthetic examples pro-
		duced by our generator network4
		Table 2: Accuracy of biased classiﬁers on SNLI test set
		and MNLI development set7
		(b) NEWS
		Table 2: Test accuracy for each dataset and method30
		Table 2: Source model LAS scores on the development
		treebanks using silver POS tags Both
		the two answers are graded incorrectly as 0-points
		179
		Table 2: Instance 1-point answers
		Dataset
		Train Size
		Test Size
		#Classes
		News20
		18000
		2000
		20
		BBC
		2000
		225
		5
		BBC Sports
		660
		77
		5
		Table 2: Dataset Speciﬁcations
		(News20)1 (Lichman, 2013) (b) BBC2 (Greene
		and Cunningham, 2006), (c) BBC Sports2 (Greene
		and Cunningham, 2006)29
		Table 2: MCD scores for Wilderness languages2
		Verses
		Words
		Length (min)
		Train
		10,000
		139,796
		1060
		Dev
		1,000
		13,937
		106
		ID Test
		1,000
		13,815
		104
		OoD Test
		1,000
		15,418
		107
		Table 3: Statistics for Wilderness-based corpus
		3
		Baseline
		Multilingual neural machine translation tech-
		niques have recently been applied to the g2p prob-
		lem (Peters et al07
		Table 2: Diversity and generation statistics878
		Table 2: F1 scores for grammar induction38
		Table 2: Result with different setting of the distantly
		supervised NER model343
		Table 2: Human Evaluation Results
		Lucian Galescu, Choh Man Teng, James Allen, and
		Ian Perera1K
		Table 2: Gold candidate recall of WIKIMENTION over seven languages, accuracy (%) of selecting the highest
		score entity, and accuracy after end-to-end EL using the BASE+GREEDY method2M
		Table 2: The number of positive and negative triples for
		each language with (*) and without templates45
		Table 2: LAS scores when transferring between Ko-
		rean and Japanese in two tokenization conditions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows average accuracy for all intents’
		FSI simulations
	</Extractive Summary>
	<Extractive Summary> =
		The results presented both in Table 2 and Fig-
		ure 3 seem to conﬁrm our initial hypothesis on the
		effectiveness of transfer learning in a cross-lingual
		fashion
	</Extractive Summary>
	<Extractive Summary> =
		For each of our main tasks, we use the best-
		performing set of auxiliary tasks found by ARS
		(Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows
		the statistics and the performance
	</Extractive Summary>
	<Extractive Summary> =
		We should
		note that for our approach we map each verb to the
		respective relation class that is depicted in Table 2
		in parentheses
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 com-
		pares the best 10 fold accuracy achieved by all
		models in JOB dataset, while Table 3 compares the
		same in GEO dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 2a shows the accuracy scores for all
		datasets and models
	</Extractive Summary>
	<Extractive Summary> =
		7 These results are shown in
		Table 2b
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2, we compare two different meth-
		ods of labeling the synthetic examples produced
		by our generator network (GPT-2): hard labeling
		and distillation
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 compares the accuracy of two types
		of pretrained prototypical models provided with
		a variable number of new examples
	</Extractive Summary>
	<Extractive Summary> =
		It also has the highest source language
		parsing accuracy (Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		On the source side,
		the monolingual Norwegian Nynorsk model also
		performed slightly better than the polyglot model
		(Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		We show two typical exam-
		ples of 1-point answers in Table 2, where each an-
		swer is graded (a) correctly and (b) incorrectly by
		the system trained with 12
	</Extractive Summary>
	<Extractive Summary> =
		We chose lan-
		guages with MCD scores less than 6 for our exper-
		iments; see Table 2 for more on these languages
	</Extractive Summary>
	<Extractive Summary> =
		• Comparing Figure 4 to Table 2, we see a
		gap between the PRPN’s performances for
		12,500 examples and for the entire training
		sets for all languages
	</Extractive Summary>
	<Extractive Summary> =
		We further investigate the impact of the differ-
		ent components of the model (Table 2) in the two
		English datasets via ablation experiments, where
		we contrast the use of partial annotation (PA) and
		reinforcement-based denoising RL, with and with-
		out the high-quality phrases (%)
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows
		the ratings given by the evaluators in various as-
		pects for judging the overall performance of the
		system
	</Extractive Summary>
	<Extractive Summary> =
		1
		Results
		Table 2 shows various statistics for the baseline
		system on English, two high-resource, and four
		low-resource XEL languages
	</Extractive Summary>
	<Extractive Summary> =
		Dataset statistics
		Table 2 shows the number of
		positive and negative triples and examples (i
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, Japanese performance
		becomes even lower in this setup
	</Extractive Summary>
</Paper ID=ument1023>


<Paper ID=ument1023> <Table ID =3>
	<Abstractive Summary> =
		12)
		Table 3: IC accuracy on SNIPS dataset in the FSI set-
		ting, reported as mean (SD)958
		Table 3: Test results8
		Table 3: F1 (%) of the evaluation set in TAC KBP 2015
		Tri-lingual Entity Linking Track (Ji et al1
		Table 3: Results on relation classiﬁcation05
		Table 3: Comparison of best 10 fold accuracy of all
		models on benchmark GEO dataset96
		Table 3: Normalization accuracy on dev sets for zero-
		shot experiments0∗
		Table 3: Dataset description just let it go and move on, someone else will take
		care of the rest
		Table 3: Examples of captions generated by GPT-2 for the DBPedia and Yahoo Answers datasets4
		Table 3: Accuracy of models trained by MLE, DRiFt,
		and RM with different biased models
		SOURCE
		MONOLINGUAL POLYGLOT
		Danish
		13,950
		13,944
		Swedish
		10,894
		10,874
		Norwegian Nynorsk
		13,177
		13,194
		Norwegian Bokm˚al
		17,345
		17,378
		Multi-source
		6,716
		6,833
		Table 3: The number of valid sentences in the Faroese
		synthetic treebank for each source language after anno-
		tation projection and sentence ﬁltering01
		Word vector dim
		300
		300
		300
		Latent dim (m)
		50
		50
		50
		#Neighbours (k)
		5
		5
		5
		Scale factor (λ)
		10−4
		10−4
		10−4
		# Epochs
		30
		20
		20
		Table 3: Hyper-parameters which were used in experi-
		ments for News20, BBC & BBC-Sports
		The word embeddings were randomly initial-
		ized and trained along with the model29
		Table 2: MCD scores for Wilderness languages2
		Verses
		Words
		Length (min)
		Train
		10,000
		139,796
		1060
		Dev
		1,000
		13,937
		106
		ID Test
		1,000
		13,815
		104
		OoD Test
		1,000
		15,418
		107
		Table 3: Statistics for Wilderness-based corpus
		3
		Baseline
		Multilingual neural machine translation tech-
		niques have recently been applied to the g2p prob-
		lem (Peters et al5
		Table 3: Language modeling statistics06)
		Table 3: Monolingual and multilingual PRPN test re-
		sults for 2500 training sentences21
		Table 3: Unsupervised NER Performance Comparison769
		Table 3: Human-evaluation accuracy metrics
		10
		Conclusion
		This work mainly combines both deep learning
		techniques as well as rule-based computational
		techniques8
		Table 3: Accuracy (%) of different systems97
		Table 3: Precision, Recall, and F1-score results for all languages’ monolingual (Mono
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the mean accuracy of all
		7 intents FSI simulations results for different FDA
		techniques
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 3, with upsampling in
		text space, DELTAR performs better than any other
		FDA method
	</Extractive Summary>
	<Extractive Summary> =
		We explore multi-task UGR in this work because
		we found that single-task UGR can improve per-
		formance (see Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		We reimplement their baseline model as an
		additional comparison in our results (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		Our
		approach differs from Rei (2017) in three ways: we
		employ a conditional language model instead of an
		unconditional language model, allowing our model
		to learn in a supervised way from signal derived
		from the labels; we do not use semi-supervised
		learning; and we train in a multi-task setting in-
		volving both multiple datasets and a compound ob-
		jective, whereas Rei (2017) optimizes a compound
		objective on a single dataset for each task (similar
		to GSTL in Table 3 of this work)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 3 shows the results for the four data sets,
		reporting the average over ﬁve runs
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 com-
		pares the best 10 fold accuracy achieved by all
		models in JOB dataset, while Table 3 compares the
		same in GEO dataset
	</Extractive Summary>
	<Extractive Summary> =
		Results
		Table 3 shows the accuracy of zero-shot
		normalization compared to the naive identity base-
		line, i
	</Extractive Summary>
	<Extractive Summary> =
		In Supplementary Table 3, we show examples
		of synthetic training texts generated by sampling
		from the ﬁnetuned GPT-2 model, for both DBPe-
		dia and Yahoo Answers
	</Extractive Summary>
	<Extractive Summary> =
		Finally, to characterize the LMs, we report
		GPT-2’s and TXL’s word-level perplexity (PPL)
		and bits per character (BPC) on the development
		sets, as well as the percentage of OOV tokens on
		the dataset—see Table 3, where lower scores are
		better
	</Extractive Summary>
	<Extractive Summary> =
		We report the outcome together with the scores
		of the other state-of-the-art unsupervised meth-
		ods in Table 3, where we also compare to sim-
		ple dictionary matching
	</Extractive Summary>
	<Extractive Summary> =
		The Table 3 shows
		the accuracy metrics
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 3 compares models on the datasets we in-
		troduce in Section 4
	</Extractive Summary>
	<Extractive Summary> =
		In
		a set of initial trials (see Table 3), we found that
		this model far outperformed the bias-augmented
		BiDAF model (Seo et al
	</Extractive Summary>
	<Extractive Summary> =
		Results
		Table 3 shows the results of the mono-
		lingual baselines
	</Extractive Summary>
</Paper ID=ument1023>


<Paper ID=ument1023> <Table ID =4>
	<Abstractive Summary> =
		51)
		Table 4: IC accuracy on SNIPS’s AddToPlaylist and
		FBDialog’s GetDirections in the FSI setting, reported
		as mean (SD)900
		Table 4: Comparisons of mean training epoch time,
		number of trainable architecture parameters (i
		Table 4: Examples of mined parallel sentences from
		Wikipedia05
		Null (provide)
		Table 4: Examples of predictions from the three methods on the CDR data set37
		Table 4: Comparison of best test accuracy of all models
		on benchmark ATIS datasetcom/facebookresearch/
		LASER
		118
		Table 4: We compare the results of Fine-tuning vs Pre-training followed by Fine-tuning of various models on
		“Leave Dataset”
		Table 4: Above, we show two example sentences from DPedia along with their nearest neighbors from the training
		dataset (DBPedia)9
		Table 4: F1 scores of the entailment (E) and non-
		entailment (¬E) classes on HANS40
		Table 4:
		LAS on the target Faroese test treebank8931
		BBC Sports
		Table 4: Test Accuracy for proposed model using in-
		stances from the same target dataset
		niﬁcantly better than baseline models when vary-
		ing fractions of the training data is used00
		Table 4: Examples for SER and PER calculations
		194
		and SER are functionally identical for Wiktionary,
		which comprises single-word grapheme-phoneme
		pairs[EOS]
		Training the cinematography to the outstanding soundtrack and unconventional narrative [EOS]
		Table 4: Generation examples on SST-2
	</Abstractive Summary>
	<Extractive Summary> =
		This
		decrease in performance is only seen with BERT
		and not with the Bi-LSTM feature extractor (see
		Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows intent accuracy for SNIPS and
		Facebook datasets
	</Extractive Summary>
	<Extractive Summary> =
		Comparisons in training time, model size, and
		performance between the reimplemented ARS
		baseline model and the DAN model are given in
		Table 4 for MultiNLI2
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows some examples of mined par-
		allel sentences from Wikipedia, with word and en-
		tity alignment highlighted
	</Extractive Summary>
	<Extractive Summary> =
		Table 4
		shows some characteristic cases:
		• In the ﬁrst sentence, the syntactic pars-
		ing+verb mapping baseline (SP+VM) fails
		since the verb (developed) is not associated
		with cause
	</Extractive Summary>
	<Extractive Summary> =
		According to Table 4, MCT-PT-FT
		outperforms translate-train by a margin of 7
	</Extractive Summary>
	<Extractive Summary> =
		Does noisy translation affect MCT ?
		It is interesting to note, from Table 4, the gains
		obtained by MCT-PT-FT over Translate-train on
		Spanish (es) (86
	</Extractive Summary>
	<Extractive Summary> =
		In Supplementary Table 4, we show two syn-
		thetic training texts along with their nearest neigh-
		bors in the training set
	</Extractive Summary>
	<Extractive Summary> =
		5 Compared to results
		5 Since results of RM are similar to those in Table 4, we
		method
		Negation
		Overlap
		E
		C
		N
		E
		C
		N
		HYPO
		41
	</Extractive Summary>
	<Extractive Summary> =
		The second result column (MULTI) of Table 4
		shows the effect of training a multi-treebank POS
		tagger and parser on the Faroese treebanks created
		by each of the four source languages as well as
		the treebank which is produced by multi-source
		projection
	</Extractive Summary>
	<Extractive Summary> =
		We present a random ex-
		ample from each transfer set in Table 4 for SST-2
	</Extractive Summary>
</Paper ID=ument1023>


<Paper ID=ument1023> <Table ID =5>
	<Abstractive Summary> =
		04)
		Table 5: IC accuracy on SNIPS dataset in the FSI setting (k = 10), reported as mean (SD)3%
		Table 5: Quality of the mined parallel sentences (Per-
		fect and Partial stand for the percentage of perfect and
		partial respectively; Word and Entity stand for the Ac-
		curacy of word and entity alignments respectively)8∗
		Table 5: We compare the results of different approaches to bilingual co-training (BCT) on “Leave Dataset”9
		Table 5: F1 scores of each class on STRESS70
		Table 5: LAS scores between target models trained on
		the subset of sentences eligible for multi-source projec-
		tion (with annotations from the stated source)9100
		Table 5: Comparison of results using other learning schemes on News20, BBC and BBC Sports datasets1
		Training epochs
		14
		Beam size
		10
		Table 5: Multimodal Model Parameters
		ness data: the Wiktionary dataset uses a different
		and incompatible IPA character vocabulary, which
		prevents us from training a model on Wilder-
		ness and testing on Wiktionary
	</Abstractive Summary>
	<Extractive Summary> =
		Results on individual intent’s FSI
		simulations can be found in Appendix’s Table 5
		and Table 6
	</Extractive Summary>
	<Extractive Summary> =
		Refer to Ap-
		pendix’s Table 5 and Table 6 for individual intents’ re-
		sults
	</Extractive Summary>
	<Extractive Summary> =
		For exam-
		ple, for SNIPS’s RateBookIntent (column Book in
		Table 5), it yields 96
	</Extractive Summary>
	<Extractive Summary> =
		According to Table 5, BERT
		based MCT-PT-FT performs better for both lan-
		guage pairs i
	</Extractive Summary>
	<Extractive Summary> =
		In Table 5, we show the F1 scores of each class
		for all models on STRESS
	</Extractive Summary>
	<Extractive Summary> =
		Comparative Study:
		Table 5 gives the ex-
		perimental results for our proposed approach,
		baselines and other conventional learning tech-
		niques on the 20 Newsgroups, BBC and BBC
		Sports datasets
	</Extractive Summary>
	<Extractive Summary> =
		In Table 5, the models studied are Multino-
		mial Naive Bayes, k-nearest neighbour classiﬁer,
		Support Vector Machine (SVM) (Bishop, 2006)
		and Random Forests Classiﬁer
	</Extractive Summary>
</Paper ID=ument1023>


<Paper ID=ument1023> <Table ID =6>
	<Abstractive Summary> =
		12)
		Table 6: IC accuracy on FBDialog dataset in the FSI setting (k = 10), reported as mean (SD) Overall, LASER-based approaches perform
		better than BERT and XLM since, pre-training ob-
		jective of LASER, “machine translation using sin-
		gle encoder for 93 languages”, seems to explic-
		itly force alignment of sentence representations in
		119
		Table 6: We compare MCT-FT with the existing baselines on “MLDoc” (Schwenk and Li, 2018) Dataset5
		Table 6:
		Comparison to previous work8027
		Table 6: Test Accuracy using instances from multiple
		source datasets with 50% target dataset
		important point to note here is that although for
		News20 dataset we are not able to beat the state of
		the art(by less than 1%), by instance infusion we
		are able to improve the performance of the deep
		learning model by a signiﬁcant margin of 13%00
		Table 6: Comparison of Models on Wiktionary Dataset
		For the Wilderness data, we report results on
		two test sets (In-Domain and Out-of-Domain) to
		illustrate generalization to unseen languages
	</Abstractive Summary>
	<Extractive Summary> =
		Refer to Ap-
		pendix’s Table 5 and Table 6 for individual intents’ re-
		sults
	</Extractive Summary>
	<Extractive Summary> =
		We also tested the effect of
		training only the dependency parser using multiple treebanks
		170
		Table 6 places our systems in the context of pre-
		vious results on the same Faroese test set
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 compares the results, when single
		source and multiple source datasets are used for
		50% dataset fraction
	</Extractive Summary>
</Paper ID=ument1023>


<Paper ID=ument1023> <Table ID =7>
	<Abstractive Summary> =
		12)
		Table 7: IC accuracy on SNIPS dataset in the FSI setting, reported as mean (SD)2∗
		Table 7: We compare MCT-FT (Ours) with the existing baselines on “XNLI Dataset” (Conneau et al39
		Table 7: Comparison of Models on Wilderness Dataset
		we were able
	</Abstractive Summary>
	<Extractive Summary> =
		Refer
		to Appendix’s Table 7 for individual intents’ results
	</Extractive Summary>
	<Extractive Summary> =
		According
		to Table 7, for XLM, baseline translate-train per-
		forms better than the proposed approach by 0
	</Extractive Summary>
</Paper ID=ument1023>


<Paper ID=ument1023> <Table ID =8>
	<Abstractive Summary> =
		00
		Table 8: Multimodal Model Error Rates by Language
		197
		In-Domain
		Out-of-Domain
		Code
		Name
		Family
		Code
		Name
		Family
		SHIRBD
		Shilha
		Afro-Asiatic
		MYYWBT
		Macuna
		Tucanoan
		COKWBT
		Cora, Santa Teresa
		Uto-Aztecan
		SABWBT
		Buglere
		Chibchan
		LTNNVV
		Latin
		Indo-European
		LONBSM
		Elhomwe
		Niger-Congo
		XMMLAI
		Manadonese Malay
		Austronesian/Indo-Euro
	</Abstractive Summary>
	<Extractive Summary> =
		NHYTBL
		Nahuatl
		Uto-Aztecan
		TS1BSM
		Tsonga
		Niger-Congo
		ALJOMF
		Alangan
		Austronesian
		GAGIBT
		Gagauz
		Turkic
		BFABSS
		Bari
		Nilo-Saharan
		KNETBL
		Kankanaey
		Austronesian
		HUBWBT
		Huambisa
		Jivaroan
		TPPTBL
		Tepehua
		Totonacan
		TWBOMF
		Tawbuid
		Austronesian
		HAUCLV
		Hausa
		Afro-Asiatic
		ENXBSP
		Enxet
		Mascoyan
		ESSWYI
		Yupik
		Eskimo-Aleut
		POHPOC
		Pokomchi
		Mayan
		Table 9: More Information on Wilderness languages
		(Table 8)
	</Extractive Summary>
</Paper ID=ument1023>


<Paper ID=ument1023> <Table ID =9>
	<Abstractive Summary> =
		NHYTBL
		Nahuatl
		Uto-Aztecan
		TS1BSM
		Tsonga
		Niger-Congo
		ALJOMF
		Alangan
		Austronesian
		GAGIBT
		Gagauz
		Turkic
		BFABSS
		Bari
		Nilo-Saharan
		KNETBL
		Kankanaey
		Austronesian
		HUBWBT
		Huambisa
		Jivaroan
		TPPTBL
		Tepehua
		Totonacan
		TWBOMF
		Tawbuid
		Austronesian
		HAUCLV
		Hausa
		Afro-Asiatic
		ENXBSP
		Enxet
		Mascoyan
		ESSWYI
		Yupik
		Eskimo-Aleut
		POHPOC
		Pokomchi
		Mayan
		Table 9: More Information on Wilderness languages
		(Table 8)
	</Abstractive Summary>
	<Extractive Summary> =
		For more details on each of the
		languages, as well as expansions of the abbrevia-
		tions, see Table 9 at the end of the paper
	</Extractive Summary>
</Paper ID=ument1023>


<Paper ID=ument1025> <Table ID =1>
	<Abstractive Summary> =
		22)
		Table 1: IC accuracy on SNIPS and Facebook dataset
		with all training data, reported as mean (SD)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that augmenting data in
		feature space provides only minor improvements
		in classiﬁcation accuracy
	</Extractive Summary>
</Paper ID=ument1025>


<Paper ID=ument1025> <Table ID =2>
	<Abstractive Summary> =
		12)
		Table 2: Average IC accuracy for all intents’ FSI simu-
		lations on SNIPS and FBDialog dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows average accuracy for all intents’
		FSI simulations
	</Extractive Summary>
</Paper ID=ument1025>


<Paper ID=ument1025> <Table ID =3>
	<Abstractive Summary> =
		12)
		Table 3: IC accuracy on SNIPS dataset in the FSI set-
		ting, reported as mean (SD)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the mean accuracy of all
		7 intents FSI simulations results for different FDA
		techniques
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 3, with upsampling in
		text space, DELTAR performs better than any other
		FDA method
	</Extractive Summary>
</Paper ID=ument1025>


<Paper ID=ument1025> <Table ID =4>
	<Abstractive Summary> =
		51)
		Table 4: IC accuracy on SNIPS’s AddToPlaylist and
		FBDialog’s GetDirections in the FSI setting, reported
		as mean (SD)
	</Abstractive Summary>
	<Extractive Summary> =
		This
		decrease in performance is only seen with BERT
		and not with the Bi-LSTM feature extractor (see
		Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows intent accuracy for SNIPS and
		Facebook datasets
	</Extractive Summary>
</Paper ID=ument1025>


<Paper ID=ument1025> <Table ID =5>
	<Abstractive Summary> =
		04)
		Table 5: IC accuracy on SNIPS dataset in the FSI setting (k = 10), reported as mean (SD)
	</Abstractive Summary>
	<Extractive Summary> =
		Results on individual intent’s FSI
		simulations can be found in Appendix’s Table 5
		and Table 6
	</Extractive Summary>
	<Extractive Summary> =
		Refer to Ap-
		pendix’s Table 5 and Table 6 for individual intents’ re-
		sults
	</Extractive Summary>
	<Extractive Summary> =
		For exam-
		ple, for SNIPS’s RateBookIntent (column Book in
		Table 5), it yields 96
	</Extractive Summary>
</Paper ID=ument1025>


<Paper ID=ument1025> <Table ID =6>
	<Abstractive Summary> =
		12)
		Table 6: IC accuracy on FBDialog dataset in the FSI setting (k = 10), reported as mean (SD)
	</Abstractive Summary>
	<Extractive Summary> =
		Refer to Ap-
		pendix’s Table 5 and Table 6 for individual intents’ re-
		sults
	</Extractive Summary>
</Paper ID=ument1025>


<Paper ID=ument1025> <Table ID =7>
	<Abstractive Summary> =
		12)
		Table 7: IC accuracy on SNIPS dataset in the FSI setting, reported as mean (SD)
	</Abstractive Summary>
	<Extractive Summary> =
		Refer
		to Appendix’s Table 7 for individual intents’ results
	</Extractive Summary>
</Paper ID=ument1025>


<Paper ID=ument1026> <Table ID =1>
	<Abstractive Summary> =
		80
		Table 1: XNLI accuracy scores
		languages
	</Abstractive Summary>
</Paper ID=ument1026>


<Paper ID=ument1026> <Table ID =2>
	<Abstractive Summary> =
		50
		Table 2: Sentiment Classiﬁcation accuracy scores
		anced dataset)
	</Abstractive Summary>
</Paper ID=ument1026>


<Paper ID=ument1027> <Table ID =1>
	<Abstractive Summary> =
		9
		487
		Table 1: Task description
	</Abstractive Summary>
</Paper ID=ument1027>


<Paper ID=ument1027> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Results of the experiment (i), accuracy per-
		centage on test data for the English and Portuguese cor-
		pora
		9
	</Abstractive Summary>
	<Extractive Summary> =
		The results presented both in Table 2 and Fig-
		ure 3 seem to conﬁrm our initial hypothesis on the
		effectiveness of transfer learning in a cross-lingual
		fashion
	</Extractive Summary>
</Paper ID=ument1027>


<Paper ID=ument1028> <Table ID =1>
	<Abstractive Summary> =
		360
		Table 1: Results for DN and Chimera tasks
	</Abstractive Summary>
</Paper ID=ument1028>


<Paper ID=ument1028> <Table ID =2>
	<Abstractive Summary> =
		43
		Table 2: Results on both CRW tasks
	</Abstractive Summary>
</Paper ID=ument1028>


<Paper ID=ument1029> <Table ID =1>
	<Abstractive Summary> =
		5%
		3
		10,001
		Hypothesis
		Premise
		Natural language inference
		ABSA-L
		3
		2,618
		Aspect
		Review
		Aspect-based sentiment analysis, laptop domain
		ABSA-R
		3
		2,256
		Aspect
		Review
		Aspect-based sentiment analysis, restaurant domain
		Target
		3
		5,623
		Target
		Text
		Target-dependent sentiment analysis
		Stance
		3
		3,209
		Target
		Tweet
		Stance detection
		Topic-2
		2
		5,177
		Topic
		Tweet
		Topic-based sentiment analysis, binary
		Topic-5
		5
		7,236
		Topic
		Tweet
		Topic-based sentiment analysis, ﬁne-grained
		FNC-1
		4
		39,741
		Headline
		Document
		Fake News Detection
		Table 1: Size of label set, number of training examples, content of sequences, and task description of each dataset
	</Abstractive Summary>
</Paper ID=ument1029>


<Paper ID=ument1029> <Table ID =2>
	<Abstractive Summary> =
		5%, ABSA-L, Target
		Table 2: Main tasks and their corresponding auxiliary
		tasks as used here and by Augenstein et al
	</Abstractive Summary>
	<Extractive Summary> =
		For each of our main tasks, we use the best-
		performing set of auxiliary tasks found by ARS
		(Table 2)
	</Extractive Summary>
</Paper ID=ument1029>


<Paper ID=ument1029> <Table ID =3>
	<Abstractive Summary> =
		958
		Table 3: Test results
	</Abstractive Summary>
	<Extractive Summary> =
		We explore multi-task UGR in this work because
		we found that single-task UGR can improve per-
		formance (see Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		We reimplement their baseline model as an
		additional comparison in our results (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		Our
		approach differs from Rei (2017) in three ways: we
		employ a conditional language model instead of an
		unconditional language model, allowing our model
		to learn in a supervised way from signal derived
		from the labels; we do not use semi-supervised
		learning; and we train in a multi-task setting in-
		volving both multiple datasets and a compound ob-
		jective, whereas Rei (2017) optimizes a compound
		objective on a single dataset for each task (similar
		to GSTL in Table 3 of this work)
	</Extractive Summary>
</Paper ID=ument1029>


<Paper ID=ument1029> <Table ID =4>
	<Abstractive Summary> =
		900
		Table 4: Comparisons of mean training epoch time,
		number of trainable architecture parameters (i
	</Abstractive Summary>
	<Extractive Summary> =
		Comparisons in training time, model size, and
		performance between the reimplemented ARS
		baseline model and the DAN model are given in
		Table 4 for MultiNLI2
	</Extractive Summary>
</Paper ID=ument1029>


<Paper ID=ument103> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Cross-lingual transfer performances for dependency parsing on 19 languages from 13 different fami-
		lies, with the performance on the source language (English) as a reference
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 illustrates the results for the 19
		target languages we selected,6 along with the per-
		formance on the source language (English)
	</Extractive Summary>
	<Extractive Summary> =
		As is shown in Table 1, the improvements by
		our constrained inference algorithms are dramatic
		in a few languages that have very distinct word
		order features from the source language
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows that the performance of Hindi im-
		proves from 34% to over 51% per UAS for both
		inference algorithms
	</Extractive Summary>
</Paper ID=ument103>


<Paper ID=ument103> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Ablation study: average UAS of baseline
		model with different sets of constraints
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 2, Despite some languages have
		Model
		UAS
		coverage
		�
		baseline
		54
	</Extractive Summary>
	<Extractive Summary> =
		The improvement of constraints is com-
		puted same as Table 2
		non-projective dependencies, we observed perfor-
		mance improvements on almost all the languages
		when the projective constraint is enforced
	</Extractive Summary>
</Paper ID=ument103>


<Paper ID=ument103> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Contribution of individual constraints and
		their statistics in Hindi
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the re-
		sults
	</Extractive Summary>
</Paper ID=ument103>


<Paper ID=ument103> <Table ID =4>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		All the hyper-
		parameters are speciﬁed in Appendix Table 4 to-
		gether with hyper-parameters for the inference al-
		gorithms in Appendix Table 5
	</Extractive Summary>
</Paper ID=ument103>


<Paper ID=ument103> <Table ID =5>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument103>


<Paper ID=ument1031> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Mention disambiguation features
	</Abstractive Summary>
</Paper ID=ument1031>


<Paper ID=ument1031> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Linear entity mapping statistics and perfor-
		mance (Precision (%) at K) (en: English, es: Spanish,
		it: Italian, ru: Russian, so: Somali, tr: Turkish, uk:
		Ukrainian, zh: Chinese)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows
		the statistics and the performance
	</Extractive Summary>
</Paper ID=ument1031>


<Paper ID=ument1031> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: F1 (%) of the evaluation set in TAC KBP 2015
		Tri-lingual Entity Linking Track (Ji et al
	</Abstractive Summary>
</Paper ID=ument1031>


<Paper ID=ument1031> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Examples of mined parallel sentences from
		Wikipedia
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows some examples of mined par-
		allel sentences from Wikipedia, with word and en-
		tity alignment highlighted
	</Extractive Summary>
</Paper ID=ument1031>


<Paper ID=ument1031> <Table ID =5>
	<Abstractive Summary> =
		3%
		Table 5: Quality of the mined parallel sentences (Per-
		fect and Partial stand for the percentage of perfect and
		partial respectively; Word and Entity stand for the Ac-
		curacy of word and entity alignments respectively)
	</Abstractive Summary>
</Paper ID=ument1031>


<Paper ID=ument1032> <Table ID =1>
	<Abstractive Summary> =
		47
		Table 1: Examples of verb mappings for compound-
		disease relation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows an example of this mapping process
	</Extractive Summary>
</Paper ID=ument1032>


<Paper ID=ument1032> <Table ID =2>
	<Abstractive Summary> =
		2m)
		-
		-
		Table 2: Data sets used in our experiments
	</Abstractive Summary>
	<Extractive Summary> =
		We should
		note that for our approach we map each verb to the
		respective relation class that is depicted in Table 2
		in parentheses
	</Extractive Summary>
</Paper ID=ument1032>


<Paper ID=ument1032> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Results on relation classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 3 shows the results for the four data sets,
		reporting the average over ﬁve runs
	</Extractive Summary>
</Paper ID=ument1032>


<Paper ID=ument1032> <Table ID =4>
	<Abstractive Summary> =
		05
		Null (provide)
		Table 4: Examples of predictions from the three methods on the CDR data set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		shows some characteristic cases:
		• In the ﬁrst sentence, the syntactic pars-
		ing+verb mapping baseline (SP+VM) fails
		since the verb (developed) is not associated
		with cause
	</Extractive Summary>
</Paper ID=ument1032>


<Paper ID=ument1033> <Table ID =1>
	<Abstractive Summary> =
		Task Category
		Dataset
		Train Size
		Dev Size
		Natural Language Inference
		SNLI
		510,711
		9,831
		Natural Language Inference
		MNLI
		392,702
		9,815
		Answer Sentence Selection
		QNLI
		108,436
		5,732
		Paraphrase Detection
		Quora
		363,847
		40,430
		Table 1: Summary of the datasets
		SNLI The Stanford Natural Language Inference
		(SNLI) Corpus (Bowman et al
	</Abstractive Summary>
</Paper ID=ument1033>


<Paper ID=ument1033> <Table ID =2>
	<Abstractive Summary> =
		3 %
		Table 2: Transfer performance (accuracy) of different
		domain adaptation methods
	</Abstractive Summary>
</Paper ID=ument1033>


<Paper ID=ument1034> <Table ID =1>
	<Abstractive Summary> =
		As illustrated in Figure 2,
		these computationally inexpensive methods can
		consistently outperform the random baseline by a
		90
		Table 1: Results for Model-Driven and Data-Driven approaches
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Model-Driven Approaches
		Figure 1 and Table 1 compares the results of
		model-driven approaches and random sampling
		baseline
	</Extractive Summary>
	<Extractive Summary> =
		The above trends are inline with the results re-
		ported in Table 1 as well
	</Extractive Summary>
</Paper ID=ument1034>


<Paper ID=ument1035> <Table ID =1>
	<Abstractive Summary> =
		99
		Benchmark
		Original benchmark query q∗
		OOV substituted query p∗
		Logical form l(q∗)
		OOV dataset
		ATIS
		list all ﬂights departing from ap0
		list all ﬂights taking off from ap0
		(λ $0 (and (ﬂight $0) (from $0 ap0)))
		PARA-ATIS
		ATIS
		i need a ﬂight from ci0 to ci1
		i require a ﬂight from ci0 to ci1
		(λ $0 (and (ﬂight $0) (from $0 ci0) (to $0 ci1)))
		PARA-ATIS
		GEO
		how many big cities are in s0
		how many large cities are in s0
		(count (λ $0 (and (major $0) (city $0) (loc $0 s0))))
		PARA-GEO
		GEO
		which state has the highest elevation
		which
		state
		has
		the
		highest
		natural elevation
		(argmax (λ $0 (state $0)) (λ $1 (elevation $1)))
		PARA-GEO
		Table 1: Table showing examples from OOV datasets PARA-GEO and PARA-ATIS which were constructed from
		the benchmark GEO and ATIS datasets (Ray et al
	</Abstractive Summary>
	<Extractive Summary> =
		, 2018) referred as PARA-
		ATIS and PARA-GEO datasets respectively (ex-
		amples in Table 1)
	</Extractive Summary>
</Paper ID=ument1035>


<Paper ID=ument1035> <Table ID =2>
	<Abstractive Summary> =
		31
		Table 2: Comparison of best 10 fold accuracy of all
		models on benchmark JOB dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 com-
		pares the best 10 fold accuracy achieved by all
		models in JOB dataset, while Table 3 compares the
		same in GEO dataset
	</Extractive Summary>
</Paper ID=ument1035>


<Paper ID=ument1035> <Table ID =3>
	<Abstractive Summary> =
		05
		Table 3: Comparison of best 10 fold accuracy of all
		models on benchmark GEO dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 com-
		pares the best 10 fold accuracy achieved by all
		models in JOB dataset, while Table 3 compares the
		same in GEO dataset
	</Extractive Summary>
</Paper ID=ument1035>


<Paper ID=ument1035> <Table ID =4>
	<Abstractive Summary> =
		37
		Table 4: Comparison of best test accuracy of all models
		on benchmark ATIS dataset
	</Abstractive Summary>
</Paper ID=ument1035>


<Paper ID=ument1036> <Table ID =1>
	<Abstractive Summary> =
		105
		Dataset/Language
		Tokens (Dev)
		DEA
		German (Anselm)
		45,996
		DER
		German (RIDGES)
		9,712
		EN
		English
		16,334
		ES
		Spanish
		11,650
		HU
		Hungarian
		16,707
		IS
		Icelandic
		6,109
		PT
		Portuguese
		26,749
		SLB
		Slovene (Bohoriˇc)
		5,841
		SLG
		Slovene (Gaj)
		20,878
		SV
		Swedish
		2,245
		Table 1: Historical datasets used in our experiments
		and the size of their development sets
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Table 1 gives an overview of the languages and
		the size of the development set, which we use for
		evaluation
	</Extractive Summary>
</Paper ID=ument1036>


<Paper ID=ument1036> <Table ID =2>
	<Abstractive Summary> =
		80
		(b) Comparison to previous work
		Table 2: Normalization accuracy on dev sets after training on 1,000 tokens
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2a shows the accuracy scores for all
		datasets and models
	</Extractive Summary>
	<Extractive Summary> =
		7 These results are shown in
		Table 2b
	</Extractive Summary>
</Paper ID=ument1036>


<Paper ID=ument1036> <Table ID =3>
	<Abstractive Summary> =
		96
		Table 3: Normalization accuracy on dev sets for zero-
		shot experiments
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 3 shows the accuracy of zero-shot
		normalization compared to the naive identity base-
		line, i
	</Extractive Summary>
</Paper ID=ument1036>


<Paper ID=ument1037> <Table ID =1>
	<Abstractive Summary> =
		118
		Table 1: We compare the existing baselines with MCT-MSR using LASER, BERT and XLM as PLRMs without
		ﬁne-tuning on “Leave Dataset”
	</Abstractive Summary>
	<Extractive Summary> =
		According
		to Table 1, on Leave dataset, for LASER, MCT-
		MSR perform slightly better than other baseline
		approaches
	</Extractive Summary>
</Paper ID=ument1037>


<Paper ID=ument1037> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: We compare the existing baselines with MCT-FT using Watt, BERT and XLM as PLRMs with ﬁne-
		tuning on “Leave Dataset”
	</Abstractive Summary>
</Paper ID=ument1037>


<Paper ID=ument1037> <Table ID =3>
	<Abstractive Summary> =
		0∗
		Table 3: Dataset description
	</Abstractive Summary>
</Paper ID=ument1037>


<Paper ID=ument1037> <Table ID =4>
	<Abstractive Summary> =
		com/facebookresearch/
		LASER
		119
		Table 4: We compare the results of Fine-tuning vs Pre-training followed by Fine-tuning of various models on
		“Leave Dataset”
	</Abstractive Summary>
	<Extractive Summary> =
		According to Table 4, MCT-PT-FT
		outperforms translate-train by a margin of 7
	</Extractive Summary>
	<Extractive Summary> =
		Does noisy translation affect MCT ?
		It is interesting to note, from Table 4, the gains
		obtained by MCT-PT-FT over Translate-train on
		Spanish (es) (86
	</Extractive Summary>
</Paper ID=ument1037>


<Paper ID=ument1037> <Table ID =5>
	<Abstractive Summary> =
		8∗
		Table 5: We compare the results of different approaches to bilingual co-training (BCT) on “Leave Dataset”
	</Abstractive Summary>
	<Extractive Summary> =
		According to Table 5, BERT
		based MCT-PT-FT performs better for both lan-
		guage pairs i
	</Extractive Summary>
</Paper ID=ument1037>


<Paper ID=ument1037> <Table ID =6>
	<Abstractive Summary> =
		Overall, LASER-based approaches perform
		better than BERT and XLM since, pre-training ob-
		jective of LASER, “machine translation using sin-
		gle encoder for 93 languages”, seems to explic-
		itly force alignment of sentence representations in
		120
		Table 6: We compare MCT-FT with the existing baselines on “MLDoc” (Schwenk and Li, 2018) Dataset
	</Abstractive Summary>
</Paper ID=ument1037>


<Paper ID=ument1037> <Table ID =7>
	<Abstractive Summary> =
		2∗
		Table 7: We compare MCT-FT (Ours) with the existing baselines on “XNLI Dataset” (Conneau et al
	</Abstractive Summary>
	<Extractive Summary> =
		According
		to Table 7, for XLM, baseline translate-train per-
		forms better than the proposed approach by 0
	</Extractive Summary>
</Paper ID=ument1037>


<Paper ID=ument1038> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: (Results) A comparison of model size and accuracy on 3 text classiﬁcation datasets
	</Abstractive Summary>
</Paper ID=ument1038>


<Paper ID=ument1038> <Table ID =2>
	<Abstractive Summary> =
		13
		Table 2: Above, we show a comparison of hard labeling and distillation for labeling the synthetic examples pro-
		duced by our generator network
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we compare two different meth-
		ods of labeling the synthetic examples produced
		by our generator network (GPT-2): hard labeling
		and distillation
	</Extractive Summary>
</Paper ID=ument1038>


<Paper ID=ument1038> <Table ID =3>
	<Abstractive Summary> =
		just let it go and move on, someone else will take
		care of the rest
		Table 3: Examples of captions generated by GPT-2 for the DBPedia and Yahoo Answers datasets
	</Abstractive Summary>
	<Extractive Summary> =
		In Supplementary Table 3, we show examples
		of synthetic training texts generated by sampling
		from the ﬁnetuned GPT-2 model, for both DBPe-
		dia and Yahoo Answers
	</Extractive Summary>
</Paper ID=ument1038>


<Paper ID=ument1038> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Above, we show two example sentences from DPedia along with their nearest neighbors from the training
		dataset (DBPedia)
	</Abstractive Summary>
	<Extractive Summary> =
		In Supplementary Table 4, we show two syn-
		thetic training texts along with their nearest neigh-
		bors in the training set
	</Extractive Summary>
</Paper ID=ument1038>


<Paper ID=ument1039> <Table ID =1>
	<Abstractive Summary> =
		136
		Dataset
		Train
		Dev
		Test
		SNLI
		549,367
		9842
		9842
		MNLI
		392,702
		9815
		-
		Table 1: Statistics of training datasets
	</Abstractive Summary>
</Paper ID=ument1039>


<Paper ID=ument1039> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Accuracy of biased classiﬁers on SNLI test set
		and MNLI development set
	</Abstractive Summary>
</Paper ID=ument1039>


<Paper ID=ument1039> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Accuracy of models trained by MLE, DRiFt,
		and RM with different biased models
	</Abstractive Summary>
</Paper ID=ument1039>


<Paper ID=ument1039> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: F1 scores of the entailment (E) and non-
		entailment (¬E) classes on HANS
	</Abstractive Summary>
	<Extractive Summary> =
		5 Compared to results
		5 Since results of RM are similar to those in Table 4, we
		method
		Negation
		Overlap
		E
		C
		N
		E
		C
		N
		HYPO
		41
	</Extractive Summary>
</Paper ID=ument1039>


<Paper ID=ument1039> <Table ID =5>
	<Abstractive Summary> =
		9
		Table 5: F1 scores of each class on STRESS
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 5, we show the F1 scores of each class
		for all models on STRESS
	</Extractive Summary>
</Paper ID=ument1039>


<Paper ID=ument104> <Table ID =1>
	<Abstractive Summary> =
		Utterance
		Logical Form
		John
		john
		Mary
		mary
		parents
		parent
		children
		child
		parents of John
		(field parent john)
		parents of Mary
		(field parent mary)
		children of John
		(field child john)
		children of Mary
		(field child mary)
		John ’s parents
		(field parent john)
		Mary ’s parents
		(field parent mary)
		John ’s children
		(field child john)
		Mary ’s children
		(field child mary)
		Table 1: Domain of the sample SCFG Gsample
	</Abstractive Summary>
	<Extractive Summary> =
		A
		sample SCFG Gsample with its domain are pro-
		vided in Tables 5 of Appendix A and Table 1 be-
		low, respectively
	</Extractive Summary>
</Paper ID=ument104>


<Paper ID=ument104> <Table ID =2>
	<Abstractive Summary> =
		The inputs to the encoder are GloVe
		1135
		Domain
		Utterance & Logical Form
		a1
		person
		the hometown ﬁeld of john
		(ﬁeld (relation hometown) (person john))
		a2
		person
		set her ’s parents with classmates
		(set (ﬁeld (relation parent) (person reference)) (person classmate))
		b1
		restaurant
		irish restaurant instances ’s price ﬁeld
		(ﬁeld (relation price) (restaurant irish))
		b2
		restaurant
		set address ﬁeld of all irish restaurant as indian restaurant ’s price
		(set (ﬁeld (relation address) (restaurant irish)) (ﬁeld (relation price) (restaurant indian)))
		c1
		event
		the start time of lectures
		(ﬁeld (relation start) (event lecture))
		c2
		event
		set the attendants of that event to organizers ﬁeld of receptions instances
		(set (ﬁeld (relation attendant) (event reference)) (ﬁeld (relation organizer) (event reception)))
		d1
		course
		size of my history course instances
		(ﬁeld (relation size) (course history))
		d2
		course
		set all history course instances ’s prerequisite ﬁeld as physics course
		(set (ﬁeld (relation prerequisite) (course history)) (course physics))
		e1
		animal
		life span of all lion instances
		(ﬁeld (relation span) (animal lion))
		e2
		animal
		set ﬁsh instances ’s family ﬁeld with dog
		(set (ﬁeld (relation family) (animal ﬁsh)) (animal dog))
		f1
		vehicle
		the source ﬁeld of all buses
		(ﬁeld (relation source) (vehicle bus))
		f2
		vehicle
		set operators of all subways as buses instances
		(set (ﬁeld (relation operator) (vehicle subway)) (vehicle bus))
		Table 2: Sample utterance-logical form pairs from each domain of discourse
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2
		1Our
		dataset
		can
		be
		accessed
		at
		https://github
	</Extractive Summary>
</Paper ID=ument104>


<Paper ID=ument104> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Test Accuracy on the extension Dataset
	</Abstractive Summary>
</Paper ID=ument104>


<Paper ID=ument104> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: Test Accuracy on the transfer Dataset
	</Abstractive Summary>
	<Extractive Summary> =
		As
		the numbers in row 3 of Table 4 show, using an
		oracle discriminator improves the model perfor-
		mance by more than 10% in the vehicle domain
		and course domain, and by smaller amounts in the
		other domains
	</Extractive Summary>
</Paper ID=ument104>


<Paper ID=ument1040> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Test accuracy for each dataset and method
	</Abstractive Summary>
	<Extractive Summary> =
		Results:
		Table 1 shows the accuracy of the ﬁne
		tuned models for all three methods
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 further shows that the hyperbolic model
		outperforms its Euclidean counterpart in the low
		149
		Dataset
		Model
		nfine = 5
		nfine = 10
		nfine = 20
		nfine = 100
		MLP
		37
	</Extractive Summary>
</Paper ID=ument1040>


<Paper ID=ument1040> <Table ID =2>
	<Abstractive Summary> =
		7
		(b) NEWS
		Table 2: Test accuracy for each dataset and method
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 compares the accuracy of two types
		of pretrained prototypical models provided with
		a variable number of new examples
	</Extractive Summary>
</Paper ID=ument1040>


<Paper ID=ument1042> <Table ID =1>
	<Abstractive Summary> =
		999
		Table 1: Chosen hyperparameters for our POS tagging
		and parsing models
	</Abstractive Summary>
</Paper ID=ument1042>


<Paper ID=ument1042> <Table ID =2>
	<Abstractive Summary> =
		30
		Table 2: Source model LAS scores on the development
		treebanks using silver POS tags
	</Abstractive Summary>
	<Extractive Summary> =
		It also has the highest source language
		parsing accuracy (Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		On the source side,
		the monolingual Norwegian Nynorsk model also
		performed slightly better than the polyglot model
		(Table 2)
	</Extractive Summary>
</Paper ID=ument1042>


<Paper ID=ument1042> <Table ID =3>
	<Abstractive Summary> =
		SOURCE
		MONOLINGUAL POLYGLOT
		Danish
		13,950
		13,944
		Swedish
		10,894
		10,874
		Norwegian Nynorsk
		13,177
		13,194
		Norwegian Bokm˚al
		17,345
		17,378
		Multi-source
		6,716
		6,833
		Table 3: The number of valid sentences in the Faroese
		synthetic treebank for each source language after anno-
		tation projection and sentence ﬁltering
	</Abstractive Summary>
</Paper ID=ument1042>


<Paper ID=ument1042> <Table ID =4>
	<Abstractive Summary> =
		40
		Table 4:
		LAS on the target Faroese test treebank
	</Abstractive Summary>
	<Extractive Summary> =
		The second result column (MULTI) of Table 4
		shows the effect of training a multi-treebank POS
		tagger and parser on the Faroese treebanks created
		by each of the four source languages as well as
		the treebank which is produced by multi-source
		projection
	</Extractive Summary>
</Paper ID=ument1042>


<Paper ID=ument1042> <Table ID =5>
	<Abstractive Summary> =
		70
		Table 5: LAS scores between target models trained on
		the subset of sentences eligible for multi-source projec-
		tion (with annotations from the stated source)
	</Abstractive Summary>
</Paper ID=ument1042>


<Paper ID=ument1042> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6:
		Comparison to previous work
	</Abstractive Summary>
	<Extractive Summary> =
		We also tested the effect of
		training only the dependency parser using multiple treebanks
		171
		Table 6 places our systems in the context of pre-
		vious results on the same Faroese test set
	</Extractive Summary>
</Paper ID=ument1042>


<Paper ID=ument1043> <Table ID =1>
	<Abstractive Summary> =
		770, which is comparable to
		179
		Table 1: Performance across different sizes of training data
	</Abstractive Summary>
</Paper ID=ument1043>


<Paper ID=ument1043> <Table ID =2>
	<Abstractive Summary> =
		Both
		the two answers are graded incorrectly as 0-points
		180
		Table 2: Instance 1-point answers
	</Abstractive Summary>
	<Extractive Summary> =
		We show two typical exam-
		ples of 1-point answers in Table 2, where each an-
		swer is graded (a) correctly and (b) incorrectly by
		the system trained with 12
	</Extractive Summary>
</Paper ID=ument1043>


<Paper ID=ument1044> <Table ID =1>
	<Abstractive Summary> =
		9100
		Table 1: Classiﬁcation accuracies and F1-Scores for news arcticle classiﬁcations for different source and target
		domains
	</Abstractive Summary>
	<Extractive Summary> =
		Results: Table 1 shows the details results of our
		approach for all the datasets
	</Extractive Summary>
	<Extractive Summary> =
		In
		Table 1, we have shown improvements by a high
		margin for all datasets
	</Extractive Summary>
</Paper ID=ument1044>


<Paper ID=ument1044> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		Train Size
		Test Size
		#Classes
		News20
		18000
		2000
		20
		BBC
		2000
		225
		5
		BBC Sports
		660
		77
		5
		Table 2: Dataset Speciﬁcations
		(News20)1 (Lichman, 2013) (b) BBC2 (Greene
		and Cunningham, 2006), (c) BBC Sports2 (Greene
		and Cunningham, 2006)
	</Abstractive Summary>
</Paper ID=ument1044>


<Paper ID=ument1044> <Table ID =3>
	<Abstractive Summary> =
		01
		Word vector dim
		300
		300
		300
		Latent dim (m)
		50
		50
		50
		#Neighbours (k)
		5
		5
		5
		Scale factor (λ)
		10−4
		10−4
		10−4
		# Epochs
		30
		20
		20
		Table 3: Hyper-parameters which were used in experi-
		ments for News20, BBC & BBC-Sports
		The word embeddings were randomly initial-
		ized and trained along with the model
	</Abstractive Summary>
</Paper ID=ument1044>


<Paper ID=ument1044> <Table ID =4>
	<Abstractive Summary> =
		8931
		BBC Sports
		Table 4: Test Accuracy for proposed model using in-
		stances from the same target dataset
		niﬁcantly better than baseline models when vary-
		ing fractions of the training data is used
	</Abstractive Summary>
</Paper ID=ument1044>


<Paper ID=ument1044> <Table ID =5>
	<Abstractive Summary> =
		9100
		Table 5: Comparison of results using other learning schemes on News20, BBC and BBC Sports datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Comparative Study:
		Table 5 gives the ex-
		perimental results for our proposed approach,
		baselines and other conventional learning tech-
		niques on the 20 Newsgroups, BBC and BBC
		Sports datasets
	</Extractive Summary>
	<Extractive Summary> =
		In Table 5, the models studied are Multino-
		mial Naive Bayes, k-nearest neighbour classiﬁer,
		Support Vector Machine (SVM) (Bishop, 2006)
		and Random Forests Classiﬁer
	</Extractive Summary>
</Paper ID=ument1044>


<Paper ID=ument1044> <Table ID =6>
	<Abstractive Summary> =
		8027
		Table 6: Test Accuracy using instances from multiple
		source datasets with 50% target dataset
		important point to note here is that although for
		News20 dataset we are not able to beat the state of
		the art(by less than 1%), by instance infusion we
		are able to improve the performance of the deep
		learning model by a signiﬁcant margin of 13%
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 compares the results, when single
		source and multiple source datasets are used for
		50% dataset fraction
	</Extractive Summary>
</Paper ID=ument1044>


<Paper ID=ument1045> <Table ID =1>
	<Abstractive Summary> =
		Train
		Test
		Languages
		311
		507
		Words
		631,828
		25,894
		Scripts
		42
		45
		Table 1: Corpus statics for Wiktionary dataset
		2
	</Abstractive Summary>
</Paper ID=ument1045>


<Paper ID=ument1045> <Table ID =2>
	<Abstractive Summary> =
		29
		Table 2: MCD scores for Wilderness languages2
		Verses
		Words
		Length (min)
		Train
		10,000
		139,796
		1060
		Dev
		1,000
		13,937
		106
		ID Test
		1,000
		13,815
		104
		OoD Test
		1,000
		15,418
		107
		Table 3: Statistics for Wilderness-based corpus
		3
		Baseline
		Multilingual neural machine translation tech-
		niques have recently been applied to the g2p prob-
		lem (Peters et al
	</Abstractive Summary>
	<Extractive Summary> =
		We chose lan-
		guages with MCD scores less than 6 for our exper-
		iments; see Table 2 for more on these languages
	</Extractive Summary>
</Paper ID=ument1045>


<Paper ID=ument1045> <Table ID =3>
	<Abstractive Summary> =
		29
		Table 2: MCD scores for Wilderness languages2
		Verses
		Words
		Length (min)
		Train
		10,000
		139,796
		1060
		Dev
		1,000
		13,937
		106
		ID Test
		1,000
		13,815
		104
		OoD Test
		1,000
		15,418
		107
		Table 3: Statistics for Wilderness-based corpus
		3
		Baseline
		Multilingual neural machine translation tech-
		niques have recently been applied to the g2p prob-
		lem (Peters et al
	</Abstractive Summary>
</Paper ID=ument1045>


<Paper ID=ument1045> <Table ID =4>
	<Abstractive Summary> =
		00
		Table 4: Examples for SER and PER calculations
		195
		and SER are functionally identical for Wiktionary,
		which comprises single-word grapheme-phoneme
		pairs
	</Abstractive Summary>
</Paper ID=ument1045>


<Paper ID=ument1045> <Table ID =5>
	<Abstractive Summary> =
		1
		Training epochs
		14
		Beam size
		10
		Table 5: Multimodal Model Parameters
		ness data: the Wiktionary dataset uses a different
		and incompatible IPA character vocabulary, which
		prevents us from training a model on Wilder-
		ness and testing on Wiktionary
	</Abstractive Summary>
</Paper ID=ument1045>


<Paper ID=ument1045> <Table ID =6>
	<Abstractive Summary> =
		00
		Table 6: Comparison of Models on Wiktionary Dataset
		For the Wilderness data, we report results on
		two test sets (In-Domain and Out-of-Domain) to
		illustrate generalization to unseen languages
	</Abstractive Summary>
</Paper ID=ument1045>


<Paper ID=ument1045> <Table ID =7>
	<Abstractive Summary> =
		39
		Table 7: Comparison of Models on Wilderness Dataset
		we were able
	</Abstractive Summary>
</Paper ID=ument1045>


<Paper ID=ument1045> <Table ID =8>
	<Abstractive Summary> =
		00
		Table 8: Multimodal Model Error Rates by Language
		198
		In-Domain
		Out-of-Domain
		Code
		Name
		Family
		Code
		Name
		Family
		SHIRBD
		Shilha
		Afro-Asiatic
		MYYWBT
		Macuna
		Tucanoan
		COKWBT
		Cora, Santa Teresa
		Uto-Aztecan
		SABWBT
		Buglere
		Chibchan
		LTNNVV
		Latin
		Indo-European
		LONBSM
		Elhomwe
		Niger-Congo
		XMMLAI
		Manadonese Malay
		Austronesian/Indo-Euro
	</Abstractive Summary>
	<Extractive Summary> =
		NHYTBL
		Nahuatl
		Uto-Aztecan
		TS1BSM
		Tsonga
		Niger-Congo
		ALJOMF
		Alangan
		Austronesian
		GAGIBT
		Gagauz
		Turkic
		BFABSS
		Bari
		Nilo-Saharan
		KNETBL
		Kankanaey
		Austronesian
		HUBWBT
		Huambisa
		Jivaroan
		TPPTBL
		Tepehua
		Totonacan
		TWBOMF
		Tawbuid
		Austronesian
		HAUCLV
		Hausa
		Afro-Asiatic
		ENXBSP
		Enxet
		Mascoyan
		ESSWYI
		Yupik
		Eskimo-Aleut
		POHPOC
		Pokomchi
		Mayan
		Table 9: More Information on Wilderness languages
		(Table 8)
	</Extractive Summary>
</Paper ID=ument1045>


<Paper ID=ument1045> <Table ID =9>
	<Abstractive Summary> =
		NHYTBL
		Nahuatl
		Uto-Aztecan
		TS1BSM
		Tsonga
		Niger-Congo
		ALJOMF
		Alangan
		Austronesian
		GAGIBT
		Gagauz
		Turkic
		BFABSS
		Bari
		Nilo-Saharan
		KNETBL
		Kankanaey
		Austronesian
		HUBWBT
		Huambisa
		Jivaroan
		TPPTBL
		Tepehua
		Totonacan
		TWBOMF
		Tawbuid
		Austronesian
		HAUCLV
		Hausa
		Afro-Asiatic
		ENXBSP
		Enxet
		Mascoyan
		ESSWYI
		Yupik
		Eskimo-Aleut
		POHPOC
		Pokomchi
		Mayan
		Table 9: More Information on Wilderness languages
		(Table 8)
	</Abstractive Summary>
	<Extractive Summary> =
		For more details on each of the
		languages, as well as expansions of the abbrevia-
		tions, see Table 9 at the end of the paper
	</Extractive Summary>
</Paper ID=ument1045>


<Paper ID=ument1046> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: GLUE test results for our models, along with previous comparison points
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Quality and Efﬁciency
		Of the transfer set construction approaches,
		our principled generation methods consistently
		achieve the highest results (see Table 1, rows 6 and
		7), followed by the rule-based TSMP and the man-
		ually curated TSIMDb (rows 8 and 9)
	</Extractive Summary>
</Paper ID=ument1046>


<Paper ID=ument1046> <Table ID =2>
	<Abstractive Summary> =
		07
		Table 2: Diversity and generation statistics
	</Abstractive Summary>
</Paper ID=ument1046>


<Paper ID=ument1046> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Language modeling statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Finally, to characterize the LMs, we report
		GPT-2’s and TXL’s word-level perplexity (PPL)
		and bits per character (BPC) on the development
		sets, as well as the percentage of OOV tokens on
		the dataset—see Table 3, where lower scores are
		better
	</Extractive Summary>
</Paper ID=ument1046>


<Paper ID=ument1046> <Table ID =4>
	<Abstractive Summary> =
		[EOS]
		Training the cinematography to the outstanding soundtrack and unconventional narrative [EOS]
		Table 4: Generation examples on SST-2
	</Abstractive Summary>
	<Extractive Summary> =
		We present a random ex-
		ample from each transfer set in Table 4 for SST-2
	</Extractive Summary>
</Paper ID=ument1046>


<Paper ID=ument1047> <Table ID =1>
	<Abstractive Summary> =
		212
		Arabic
		Chinese
		English
		German
		vocab size
		21,902
		23,714
		15,617
		10,367
		train
		18,087
		57,251
		43,738
		18,598
		dev
		2,422
		6,736
		1,699
		1,000
		test
		2,556
		7,075
		2,416
		1,000
		Table 1: Dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		statistics for all languages
	</Extractive Summary>
</Paper ID=ument1047>


<Paper ID=ument1047> <Table ID =2>
	<Abstractive Summary> =
		878
		Table 2: F1 scores for grammar induction
	</Abstractive Summary>
	<Extractive Summary> =
		• Comparing Figure 4 to Table 2, we see a
		gap between the PRPN’s performances for
		12,500 examples and for the entire training
		sets for all languages
	</Extractive Summary>
</Paper ID=ument1047>


<Paper ID=ument1047> <Table ID =3>
	<Abstractive Summary> =
		06)
		Table 3: Monolingual and multilingual PRPN test re-
		sults for 2500 training sentences
	</Abstractive Summary>
</Paper ID=ument1047>


<Paper ID=ument1048> <Table ID =1>
	<Abstractive Summary> =
		4644)
		Table 1: Our best STL/MTL on a more realistic span based evaluation indicates (top) a more realistic but lower
		performance vs
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Embedding Ablations
		We found that in general as we increase the com-
		plexity of pre-trained embeddings, from character-
		based learned CNN embeddings to pre-trained
		GloVe, ELMo, and BERT, we see improved per-
		formance (see Table 1)
	</Extractive Summary>
</Paper ID=ument1048>


<Paper ID=ument1049> <Table ID =1>
	<Abstractive Summary> =
		04
		Table 1: NER models comparison
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Performance Comparison
		The ﬁrst two rows of Table 1 depicts the compar-
		ison of the proposed model to the state-of-the-art
		NER models on the English datasets
	</Extractive Summary>
</Paper ID=ument1049>


<Paper ID=ument1049> <Table ID =2>
	<Abstractive Summary> =
		38
		Table 2: Result with different setting of the distantly
		supervised NER model
	</Abstractive Summary>
	<Extractive Summary> =
		We further investigate the impact of the differ-
		ent components of the model (Table 2) in the two
		English datasets via ablation experiments, where
		we contrast the use of partial annotation (PA) and
		reinforcement-based denoising RL, with and with-
		out the high-quality phrases (%)
	</Extractive Summary>
</Paper ID=ument1049>


<Paper ID=ument1049> <Table ID =3>
	<Abstractive Summary> =
		21
		Table 3: Unsupervised NER Performance Comparison
	</Abstractive Summary>
	<Extractive Summary> =
		We report the outcome together with the scores
		of the other state-of-the-art unsupervised meth-
		ods in Table 3, where we also compare to sim-
		ple dictionary matching
	</Extractive Summary>
</Paper ID=ument1049>


<Paper ID=ument105> <Table ID =1>
	<Abstractive Summary> =
		0
		#tok
		1088503
		147724
		152728
		#ent
		81828
		11066
		11257
		WNUT 2017
		#tok
		62729
		15733
		23394
		#ent
		3160
		1250
		1589
		Table 1: Statistics of these three datasets, #tok denotes
		tokens and #ent denotes entities
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents
		some statistics of the 3 datasets
	</Extractive Summary>
	<Extractive Summary> =
		Table 10: Three examples, identiﬁed entities are highlighted
		�������������
		���
		���
		���������
		�����
		��
		������
		����
		�����
		�����
		����
		����
		��
		������
		����
		����
		�����
		����������
		���
		������� ����������
		������
		���
		��������
		��������
		����
		�����
		0
	</Extractive Summary>
	<Extractive Summary> =
		Table 10 presents three
		cases from 3 datasets, respectively
	</Extractive Summary>
</Paper ID=ument105>


<Paper ID=ument105> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Hyper-parameters
		structure
		Accuracy
		F1(max)
		BiLSTM
		96
	</Abstractive Summary>
	<Extractive Summary> =
		All hyper-parameters, most of which
		are empirical parameters, are listed in Table 2 and
		an early stopping strategy was adopted
	</Extractive Summary>
</Paper ID=ument105>


<Paper ID=ument105> <Table ID =3>
	<Abstractive Summary> =
		65
		Table 3: Comparing different structure of SAC
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we can observe that the max-
		imum F1 value of the entire model is 91
	</Extractive Summary>
</Paper ID=ument105>


<Paper ID=ument105> <Table ID =4>
	<Abstractive Summary> =
		65
		Table 4: Compare SAC with TextCNN
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 demonstrates that NeuralCRF+SAC ob-
		tains a maximum F1 score of 91
	</Extractive Summary>
</Paper ID=ument105>


<Paper ID=ument105> <Table ID =5>
	<Abstractive Summary> =
		65
		Table 5: Results on CoNLL 2003 NER task without
		external resources
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 presents the experimental
		results4 on the CoNLL 2003 datasets without ex-
		ternal resources
	</Extractive Summary>
</Paper ID=ument105>


<Paper ID=ument105> <Table ID =6>
	<Abstractive Summary> =
		07)
		Table 6: Results of using the CoNLL 2003 NER task
		with ELMo representations
	</Abstractive Summary>
</Paper ID=ument105>


<Paper ID=ument105> <Table ID =7>
	<Abstractive Summary> =
		18)
		Table 7: Results on Ontonotes 5
	</Abstractive Summary>
</Paper ID=ument105>


<Paper ID=ument105> <Table ID =8>
	<Abstractive Summary> =
		29
		Table 8: Results of the WNUT 2017 shared task
		Length
		Ye and Ling (2018)
		NeuralCRF+SAC
		∆
		1
		91
	</Abstractive Summary>
</Paper ID=ument105>


<Paper ID=ument105> <Table ID =9>
	<Abstractive Summary> =
		10
		Table 9: Comparing performance with Ye and Ling
		(2018)
	</Abstractive Summary>
</Paper ID=ument105>


<Paper ID=ument105> <Table ID =10>
	<Abstractive Summary> =
		Table 10: Three examples, identiﬁed entities are highlighted
		�������������
		���
		���
		���������
		�����
		��
		������
		����
		�����
		�����
		����
		����
		��
		������
		����
		����
		�����
		����������
		���
		������� ����������
		������
		���
		��������
		��������
		����
		�����
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10 presents three
		cases from 3 datasets, respectively
	</Extractive Summary>
</Paper ID=ument105>


<Paper ID=ument1050> <Table ID =1>
	<Abstractive Summary> =
		326%
		Table 1: Experiment Results of Question Classiﬁcation
		240
		We can notice that LSTM outperforms all the
		other algorithms
	</Abstractive Summary>
</Paper ID=ument1050>


<Paper ID=ument1050> <Table ID =2>
	<Abstractive Summary> =
		343
		Table 2: Human Evaluation Results
		Lucian Galescu, Choh Man Teng, James Allen, and
		Ian Perera
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows
		the ratings given by the evaluators in various as-
		pects for judging the overall performance of the
		system
	</Extractive Summary>
</Paper ID=ument1050>


<Paper ID=ument1050> <Table ID =3>
	<Abstractive Summary> =
		769
		Table 3: Human-evaluation accuracy metrics
		10
		Conclusion
		This work mainly combines both deep learning
		techniques as well as rule-based computational
		techniques
	</Abstractive Summary>
	<Extractive Summary> =
		The Table 3 shows
		the accuracy metrics
	</Extractive Summary>
</Paper ID=ument1050>


<Paper ID=ument1051> <Table ID =1>
	<Abstractive Summary> =
		mil/program/low-resource-
		languages-for-emergent-incidents
		247
		Symbol
		Feature Name
		Equation
		Resource
		f 1
		l (ei,j)
		Mention-entity prior score
		log(max(p(ei,j|mi), ϵ))
		Variable
		f 2
		l (ei,j)
		Entity prior
		log(max(
		c(ei,j)
		�
		e∈EKB c(e), ϵ))
		Weng
		f 3
		l (ei,j)
		Related mention number
		�
		mk∈MD\mi 1(anyek,m∈Ekf 1
		g (ei,j, ek,m) > 0)
		-
		f 4
		l (ei,j)
		Exact match number
		�
		mk∈MD\mi 1(e ∈ Ek)
		-
		f 1
		g (ei,j, ek,w)
		Co-occurrence probability
		log(max(
		c(ei,j,ek,w)
		c(ei,j)
		), ϵ)
		Weng
		f 2
		g ei,j, ek,w)
		Positive Pointwise Mutual Information (PPMI)
		max(log2(
		p(ei,j,ek,w)
		p′(ei,j)p′(ek,w)), 0)
		Weng
		f 3
		g (ei,j, ek,w)
		Entity embedding similarity
		cosine(Vei,j, Vek,w)
		Weng
		f 4
		g (ei,j, ek,w)
		Hyperlink count
		log(max(
		�
		ek∈Hei,j 1(ei,j=ek,w)
		|Hei,j |
		, ϵ)
		Weng
		Table 1: Unary features (top half) and binary features (bottom half)
	</Abstractive Summary>
	<Extractive Summary> =
		The top half of Table 1 shows unary fea-
		ture functions, which take one argument ei,j and
		return a value that represents some property of this
		entity
	</Extractive Summary>
	<Extractive Summary> =
		The
		formulation and resource requirements of unary
		and binary features are shown in the top and bot-
		tom halves of Table 1 respectively
	</Extractive Summary>
	<Extractive Summary> =
		We name our proposed feature set that includes
		all features listed in Table 1 as FEAT
	</Extractive Summary>
</Paper ID=ument1051>


<Paper ID=ument1051> <Table ID =2>
	<Abstractive Summary> =
		1K
		Table 2: Gold candidate recall of WIKIMENTION over seven languages, accuracy (%) of selecting the highest
		score entity, and accuracy after end-to-end EL using the BASE+GREEDY method
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		Table 2 shows various statistics for the baseline
		system on English, two high-resource, and four
		low-resource XEL languages
	</Extractive Summary>
</Paper ID=ument1051>


<Paper ID=ument1051> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Accuracy (%) of different systems
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 3 compares models on the datasets we in-
		troduce in Section 4
	</Extractive Summary>
</Paper ID=ument1051>


<Paper ID=ument1052> <Table ID =1>
	<Abstractive Summary> =
		58%
		Table 1: Macro-averaged changes in accuracy from the
		MTL baseline for DepRel – POS (top), SemTag – POS
		(bottom)
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results and Discussion
		Table 1 contains results of the experiments macro-
		averaged across all languages and treebanks in the
		UD and across all languages in the PMB
	</Extractive Summary>
</Paper ID=ument1052>


<Paper ID=ument1053> <Table ID =1>
	<Abstractive Summary> =
		536
		Table 1:
		Retrieval performance (MAP scores) of
		all models on Swahili and Tagalog CLIR evaluation
		datasets
	</Abstractive Summary>
</Paper ID=ument1053>


<Paper ID=ument1054> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples from our dataset of the same question-context pairs across all the languages with the correct
		answers highlighted in boldface
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the same
		example across ﬁve languages
	</Extractive Summary>
</Paper ID=ument1054>


<Paper ID=ument1054> <Table ID =2>
	<Abstractive Summary> =
		2M
		Table 2: The number of positive and negative triples for
		each language with (*) and without templates
	</Abstractive Summary>
	<Extractive Summary> =
		Dataset statistics
		Table 2 shows the number of
		positive and negative triples and examples (i
	</Extractive Summary>
</Paper ID=ument1054>


<Paper ID=ument1054> <Table ID =3>
	<Abstractive Summary> =
		97
		Table 3: Precision, Recall, and F1-score results for all languages’ monolingual (Mono
	</Abstractive Summary>
	<Extractive Summary> =
		In
		a set of initial trials (see Table 3), we found that
		this model far outperformed the bias-augmented
		BiDAF model (Seo et al
	</Extractive Summary>
	<Extractive Summary> =
		Results
		Table 3 shows the results of the mono-
		lingual baselines
	</Extractive Summary>
</Paper ID=ument1054>


<Paper ID=ument1055> <Table ID =1>
	<Abstractive Summary> =
		209
		(b) soundex-based baseline
		Table 1: Performance statistics for our model (top) and baseline approaches (bottom)
	</Abstractive Summary>
</Paper ID=ument1055>


<Paper ID=ument1056> <Table ID =1>
	<Abstractive Summary> =
		16
		Table 1: LAS scores of our parser in the raw text setup
	</Abstractive Summary>
</Paper ID=ument1056>


<Paper ID=ument1056> <Table ID =2>
	<Abstractive Summary> =
		45
		Table 2: LAS scores when transferring between Ko-
		rean and Japanese in two tokenization conditions
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, Japanese performance
		becomes even lower in this setup
	</Extractive Summary>
</Paper ID=ument1056>


<Paper ID=ument1057> <Table ID =1>
	<Abstractive Summary> =
		[M1]WHOLE - 
		[M2]PART 
		 
		Table 1: Gold-annotated cross-document relation types in the THYME colon cancer corpus7m
		health insurance claims
		clinical narratives
		full journal texts
		Table 1: Characteristics of the embeddings compared, including the name referred, the embedding dimensions, the
		number of embeddings in the dataset, and the type of data used to train them8
		Table 1: Results on DDI 2013
		From the tables, we have the following observa-
		tions about the effectiveness of the pooling meth-
		ods for RE with deep learning:
		164
		Table 1: Precision (Prec), recall (Rec) and micro F-
		score (F1) results on DDI 2013 corpus
		1158
		163
		294
		Table 1: Description and statistics of the HealthUnlocked dataset used for the experiments7
		Table 1: Dataset comparison and statistics
		Variations
		All
		Possible Meanings in Test Data
		ﬁz
		FIZ, ﬁz, Fiz,
		15
		ﬁzjologiczny, ﬁzycznie, ﬁzyczny, ﬁzykalnie, ﬁzykalny, ﬁzykoterapia
		Physiological, Physically, Physical, Physical, Physical Therapy
		cz
		CZ, Cz
		44
		czerwie´n, czołowy, czynnik, czynno´s´c, cz˛esto´s´c, cz˛e´s´c
		redness, frontal, factor, activity, frequency, part
		gł
		gł
		22
		głowa, główkowy, głównie, główny, gł˛eboki
		head, head(adj), mainly, main, deep
		op
		OP Op
		30
		opak, opakowanie, opatrunek, opera, operacja, operacyjnie, operacyjny, oko prawy
		operowany, opieka, opis, opuszek, opór
		awry, package, dressing, opera, operation, operationally, operational, right eye
		operated, care, description, pad, resistance
		Table 1: Four from the list of 15 abbreviations with variations, the number of all different longer words found in
		the training data
		Emotion
		Train
		Test
		Dev
		Total
		Anger
		857
		760
		84
		1701
		Fear
		1147
		995
		110
		2252
		Joy
		823
		714
		79
		1616
		Sadness
		786
		673
		74
		1533
		Total
		3613
		3142
		347
		7102
		Table 1: The number of tweets under each emotion cat-
		egory
		The dataset contains 194 tweets that belong to
		multiple emotion categories
		3
		Dataset
		Primary diagnosis
		Dyads
		ADHD
		100
		Depression
		100
		COPD
		101
		Inﬂuenza
		100
		Osteoporosis
		87
		Type II diabetes
		86
		Other
		226
		Table 1: Data distribution (ADHD: Attention Deﬁcit
		Hyperactivity Disorder; COPD: Chronic Obstructive
		Pulmonary Disorder)
		For training and testing our models, we use a
		dataset of 800 patient-clinician dialogues (dyads)
		purchased from Verilogue Inc This is concatenated with the ﬁxed size sen-
		tence representation from Fs and
		Fdi, together
		76
		Datasets
		Entity
		Pair
		# of
		Sent
		# of
		Positive
		# of
		Negative
		AiMed
		1995
		1000
		4834
		BioInfer
		1100
		2534
		7132
		LLL
		PPI
		77
		164
		166
		HPRD50
		145
		163
		270
		IEPA
		486
		355
		482
		Table 1: Protein Protein Interaction Dataset statistics1
		Structured Features
		Structure features are features that were identiﬁed
		on the EHR using regular expression matching and
		include rating scores that have been reported in the
		psychiatric literature as correlated with increased
		readmission risk, such as Global Assessment of
		Functioning, Insight and Compliance:
		Sociodemographics
		Age
		Gender
		Race
		Marital status
		Veteran
		Past medical history
		History of Suicidality
		Number of past admissions
		Average length of stay (previous)
		Average # days between admissions
		Previous 30-day readmission (Y/N)
		Number of past readmissions
		Readmission ratio
		Average GAF at admission
		Average GAF at discharge
		Mode of past insight values
		Mode of past medication compliance
		Current admission
		Structured features
		Number of notes
		Number of tokens
		Number of tokens in discharge summary
		Average note length
		GAF at admission
		GAF at discharge
		GAF admission/discharge difference
		Mean GAF (all notes for visit)
		Insight (good, fair, poor)
		Medication Compliance
		Estimated length of stay
		Actual length of stay
		Difference b/w Estimated & Actual LOS
		Is ﬁrst admission (Y/N)
		Unstructured features
		Number of sentences (Appearance)
		Number of sentences (Mood)
		Number of sentences (Thought Content)
		Number of sentences (Thought Process)
		Number of sentences (Substance Use)
		Number of sentences (Interpersonal)
		Number of sentences (Occupation)
		Clinical sentiment (Appearance)
		Clinical sentiment (Mood)
		Clinical sentiment (Thought Content)
		Clinical sentiment (Thought Process)
		Clinical sentiment (Substance Use)
		Clinical sentiment (Interpersonal)
		Clinical sentiment (Occupation)
		Table 1: Extracted features by categoryhtml
		88
		Term
		# of tweets
		# of tweets localized in US
		#dinner
		5,455,890
		1,367,745
		#breakfast
		5,125,014
		1,183,462
		#lunch
		4,969,679
		1,094,681
		#brunch
		1,910,950
		681,978
		#snack
		797,676
		220,697
		#meal
		495,073
		101,976
		#supper
		124,979
		22,154
		Total
		24,493,223
		4,362,940
		Table 1: Seven meal-related hashtags and their corre-
		sponding number of tweets ﬁltered from Twittercom/praw-dev/praw
		98
		Domain
		Subreddit Name
		Total Posts
		Avg Tokens/Post
		Labeled Segments
		abuse
		r/domesticviolence
		1,529
		365
		388
		r/survivorsofabuse
		1,372
		444
		315
		Total
		2,901
		402
		703
		anxiety
		r/anxiety
		58,130
		193
		650
		r/stress
		1,078
		107
		78
		Total
		59,208
		191
		728
		ﬁnancial
		r/almosthomeless
		547
		261
		99
		r/assistance
		9,243
		209
		355
		r/food pantry
		343
		187
		43
		r/homeless
		2,384
		143
		220
		Total
		12,517
		198
		717
		PTSD
		r/ptsd
		4,910
		265
		711
		social
		r/relationships
		107,908
		578
		694
		All
		187,444
		420
		3,553
		Table 1: Data Statistics
		Table 1: An example of the GPT-2 n = 0 model generating a false negative conclusion (Varenicline did reduce
		general craving), while the GPT-2 n = 1 model generated a better true negative one1
		Data
		Two different data sets for de-identiﬁcation were
		used:
		Stockholm EPR PHI Psuedo Corpus
		(Pseudo) as well as the Stockholm EPR PHI Cor-
		119
		Table 1: Results from (Dalianis and Velupillai, 2010) using the Stanford CRF76
		Table 1: Content-point inter-annotator agreement (per-
		cent identity/kappa)51
		Table 1: LIWC Dimension Analysis
		Function Words and Content Words
		Next, we
		looked at selected function words and grammat-
		ical differences, which can be split into two cat-
		egories called Function Words (see Table 2), re-
		ﬂecting how humans communicate and Content
		words (see Table 2), demonstrating what humans
		say (Tausczik and Pennebaker, 2010) As methods for
		automated analysis of large-scale data sets have
		improved, more studies have investigated lexical
		and semantic characteristics, such as usage pat-
		terns of different verbs and semantic categories
		146
		Type
		Docs
		Lines
		Tokens
		Matches
		Concepts
		High Conﬁdence
		Concepts
		Consistency (%)
		Case Management
		967
		20,106
		165,608
		45,306
		557
		111
		75
		Consult
		98
		15,514
		96,515
		26,109
		812
		0
		–
		Discharge Summary
		59,652
		14,480,154
		104,027,364
		30,840,589
		6,381
		1,599
		67
		ECG
		209,051
		1,022,023
		7,307,381
		2,163,682
		540
		14
		56
		Echo
		45,794
		2,892,069
		19,752,879
		6,070,772
		1,233
		157
		65
		General
		8,301
		307,330
		2,191,618
		552,789
		2,559
		0
		–
		Nursing
		223,586
		9,839,274
		73,426,426
		18,903,892
		4,912
		2
		58
		Nursing/Other
		822,497
		10,839,123
		140,164,545
		31,135,584
		5,049
		83
		60
		Nutrition
		9,418
		868,102
		3,843,963
		1,147,918
		1,911
		198
		73
		Pharmacy
		103
		4,887
		39,163
		8,935
		376
		0
		–
		Physician
		141,624
		26,659,749
		148,306,543
		39,239,425
		5,538
		122
		57
		Radiology
		522,279
		17,811,429
		211,901,548
		34,433,338
		4,126
		599
		63
		Rehab Services
		5,431
		585,779
		2,936,022
		869,485
		2,239
		9
		62
		Respiratory
		31,739
		1,323,495
		6,358,924
		2,255,725
		1,039
		5
		63
		Social Work
		2,670
		100,124
		930,674
		195,417
		1,282
		0
		–
		Table 1: Document type subcorpora in MIMIC-III The
		semantic content of the documents is also differ-
		ent, with the discharge summaries focused exclu-
		sively on a single patient and their history, dis-
		ease progression, treatment, and outcomes, while
		the MedMentions abstracts typically summarize
		159
		i2b2 2010
		MedMentions (full)
		MedMentions (st21pv)
		Train
		Test
		Train
		Test
		Train
		Test
		# entity types
		3
		3
		126
		123
		21
		21
		# documents
		170
		256
		3513
		879
		3513
		879
		# tokens
		149,743
		267,837
		936,247
		234,910
		936,247
		234,910
		# entities
		16,520
		31,161
		281,719
		70,305
		162,908
		40,101
		Table 1: Properties of the datasets
		168
		Dataset
		# Documents
		# Unique patients
		# ICD-9 Codes
		# Unique ICD-9 codes
		Training
		47,719
		36,997
		758,212
		8,692
		Development
		1,631
		1,374
		28,896
		3,012
		Test
		3,372
		2,755
		61,578
		4,085
		Total
		52,722
		41,126
		848,686
		8,929
		Table 1: Distribution of documents and codes in the MIMIC-III dataset3
		Hyperparameter Settings
		Feature-based Embeddings
		For the aforemen-
		tioned set of experiments, the following architec-
		ture parameters have been considered:
		181
		Table 1: Performance of the negation scope detection task on BioScope and NegPar corpora using different ap-
		proaches
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 1, our data, Internal-200, is more chal-
		lenging due to much shorter text inputs and higher
		OOV rate compared with the benchmark MIMIC-
		3 dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 presents the detailed statistics of the
		dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the distribution of diagnoses
		in the dataset
	</Extractive Summary>
	<Extractive Summary> =
		3
		Datasets
		The dataset statistics is summarized in Table 1
		and Table 2
	</Extractive Summary>
	<Extractive Summary> =
		These features can be grouped into
		three categories (See Table 1 for complete list of
		features):
		- Sociodemographics: gender, age, marital sta-
		tus, etc
	</Extractive Summary>
	<Extractive Summary> =
		Tweets have
		been ﬁltered by a set of seven hashtags to make the
		dataset more relevant to food (see distribution in
		Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		We include ten subreddits in the ﬁve domains of
		abuse, social, anxiety, PTSD, and ﬁnancial, as de-
		tailed in Table 1, and our analysis focuses on the
		domain level
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 illustrates
		109
		Source:
		(BACKGROUND) Varenicline is believed to work , in part , by reducing craving responses to smoking cues and by
		reducing general levels of craving ; however , these hypotheses have never been evaluated with craving assessed in the
		natural environments of treatment-seeking smokers
	</Extractive Summary>
	<Extractive Summary> =
		The
		agreement scores are shown in Table 1 and 2
	</Extractive Summary>
	<Extractive Summary> =
		Dimension Analysis
		Firstly, we looked at the
		word count and different dimensions of each
		dataset (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		69
		Table 2: LIWC Function and Content Words
		Affect Analysis
		The analysis of emotions in sui-
		cide notes and last statements has often been ad-
		dressed in research (Schoene and Dethlefs, 2018;
		Lester and Gunn III, 2013) The number of Affect
		words is highest in LS notes, whilst they are low-
		est in DL notes, this could be related to the emo-
		tional Tone of a note (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		retaining only those with a self-consistency of
		at least 50%; Table 1 includes the number of
		high-conﬁdence concepts identiﬁed and the mean
		consistency among this subset
	</Extractive Summary>
	<Extractive Summary> =
		(2018) (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		5
		Results
		The performance of different approaches on Bio-
		Scope and NegPar corpora for the negation scope
		detection and the speculating scope detection are
		shown in Table 1 and Table 2, respectively
	</Extractive Summary>
</Paper ID=ument1057>


<Paper ID=ument1057> <Table ID =2>
	<Abstractive Summary> =
		MRI 
		testsA SET-SUBSET MRIA 
		testsB SET-SUBSET MRIB 
		testsA IDENT testsB 
		MRIA IDENT MRIB 
		Table 2: For File Set 1, there is no cross-doc S-SS link between testsA and MRIB because this can be inferred 
		from the cross-doc IDENT link and the within-doc S-SS link shown67
		Table 2: Illustrative example showing how System Vector Accuracy (SysVec) would be calculated for the medical
		ﬁeld “Mental Disorders” if it contained only six drugs5
		Table 2: Results on BioNLP BB3
		the DDI-2013 dataset4
		Table 2: Performance of different embeddings tech-
		nique on NER, when trained and evaluated on the
		dataset described in Section 34
		Table 2: The results on MIMIC-3 data (%)
		AL
		SL
		CL
		train
		test
		cz
		51022
		46172
		25642
		96
		137
		ﬁz
		10769
		10684
		9895
		61
		59
		gł
		15591
		14460
		9988
		55
		48
		kr
		37381
		24349
		20053
		81
		224
		mies
		9021
		8949
		6874
		35
		206
		op
		24677
		21673
		9285
		410
		1785
		poj
		4386
		4035
		3293
		75
		147
		pow
		22517
		5037
		17271
		69
		65
		pr
		88312
		20386
		57809
		105
		100
		rodz
		6459
		6459
		4903
		26
		52
		´sr
		3894
		2922
		2316
		61
		65
		wz
		9942
		6914
		3345
		42
		31
		zab
		8670
		8085
		7755
		69
		90
		zaw
		3826
		1296
		2012
		28
		29
		zał
		1657
		1544
		717
		18
		31
		total
		298149
		182965
		181140
		1231
		3069
		Table 2: Number of occurrences in train and test data7
		Gender (female)
		distribution per class
		74%
		67%
		80%
		Table 2: CLPSych 2015 shared task dataset statistics
		Preprocessing:
		All
		the
		URLs,
		@mentions,
		#hashtags, RTweets, emoticons, emojis, and num-
		bers were removed71
		Table 2: Utterance type classiﬁcation results
		normalized medication names) 9
		Datasets
		Entity
		Pair
		# of
		Train
		# of
		Valid
		# of
		Test
		DDI
		Drug-Drug
		27779
		-
		5713
		CPR: 3
		768
		550
		665
		CPR: 4
		2254
		1094
		1661
		CPR: 5
		Chem-Prot
		173
		116
		195
		CPR: 6
		235
		199
		293
		CPR: 9
		727
		457
		644
		Table 2: Drug-Drug Interaction and Chemical-Protein
		Dataset statistics72
		Table 2: Results (in ascending order)
		018
		Table 2: Correlation between socioeconomic factors
		(education, income, poverty) and type 2 diabetes mel-
		litus (T2DM) in 2017
		ﬁnancial
		stress
		3/5 (60%)
		Table 2: Data Examples71
		Table 2: ROUGE scores of the PGNet baseline models
		and the GPT-2 ﬁne-tuned models on the development
		set
		PHI classes
		Pseudo Unique
		Real
		Unique
		First Name
		885
		24 %
		938
		79 %
		Last Name
		911
		15 %
		957
		86 %
		Age
		51
		80 %
		64
		97 %
		Phone Number
		310
		78 %
		327
		92 %
		Location
		159
		94 %
		229
		84 %
		Full Date
		726
		25 %
		770
		89 %
		Date Part
		1897
		6 %
		2079
		72 %
		Health Care Unit
		1278
		13 %
		2277
		73 %
		Total PHI instances
		6217
		20 %
		7647
		78 %
		Table 2: The distribution of PHI instances between the
		the Stockholm EPR PHI Psuedo Corpus, ’Pseudo’, and
		the Stockholm EPR PHI Corpus, ’Real’ based on the
		number of tokens76
		Table 2:
		Offset-level inter-annotator agreement
		(label/label-attribute)69
		Table 2: LIWC Function and Content Words
		Affect Analysis
		The analysis of emotions in sui-
		cide notes and last statements has often been ad-
		dressed in research (Schoene and Dethlefs, 2018;
		Lester and Gunn III, 2013) The number of Affect
		words is highest in LS notes, whilst they are low-
		est in DL notes, this could be related to the emo-
		tional Tone of a note (see Table 1) As shown in
		Figure 3, this yields considerably smaller concept
		150
		Query
		Discharge Summary
		Nursing/Other
		Radiology
		Diabetes Mellitus
		(C0011849)
		Diabetes (C0011847)
		Gestational
		Diabetes
		(C0085207)
		Poorly
		controlled
		(C3853134)
		Type 2 (C0441730)
		A2 immunologic symbol
		(C1443036)
		Insulin (C0021641)
		Type 1 (C0441729)
		Diabetes Mellitus, Insulin-
		Dependent (C0011854)
		Diabetes Mellitus, Insulin-
		Dependent (C0011854)
		Gestational
		Diabetes
		(C0085207)
		Factor V (C0015498)
		Diabetes
		Mellitus,
		Non-Insulin-Dependent
		(C0011860)
		Diabetes Mellitus, Insulin-
		Dependent (C0011854)
		A1 immunologic symbol
		(C1443035)
		Stage level 5 (C0441777)
		Discharge Summary
		Echo
		Radiology
		Mental state
		(C0278060)†
		Coherent (C4068804)
		Donor:Type:Point
		in
		time:ˆPatient:Nominal
		(C3263710)
		Mental
		status
		changes
		(C0856054)
		Confusion (C0009676)
		Donor person (C0013018)
		Abnormal
		mental
		state
		(C0278061)
		Respiratory
		status:-
		:Point
		in
		time:ˆPatient:-
		(C2598168)
		Respiratory
		arrest
		(C0162297)
		Level
		of
		consciousness
		(C0234425)
		Respiratory
		status
		(C1998827)
		Organ
		donor:Type:Point
		in
		time:ˆDonor:Nominal
		(C1716004)
		Level
		of
		conscious-
		ness:Find:Pt:ˆPatient:Ord
		(C4050479)
		Abnormal
		mental
		state
		(C0278061)
		Swallowing
		G-code
		(C4281783)
		Mississippi
		(state)
		(C0026221)
		Table 2: 5 nearest neighbor concepts to Diabetes Mellitus and Mental state from 3 high-conﬁdence document
		types, averaging cosine similarities across all replicate embedding sets within each document type60
		Table 2: Results of entity recognition for each dataset and model681
		Table 2: Benchmark results for the models trained with F1macro stopping criterion This is sup-
		ported by the fact that the performance on clini-
		183
		Table 2: Performance of speculation scope detection task on BioScope corpus using different approaches
	</Abstractive Summary>
	<Extractive Summary> =
		3 
		Cross-document annotation process 
		To manage the potentially vast number of 
		cross-document links, we established a set of 
		assumptions about inferable relations that guided 
		the following process and are further discussed in 
		Table 2 (note: “structural links” refers to links that 
		have 
		a 
		hierarchical 
		rather 
		than 
		identical 
		relationship: CON-SUB, S-SS, W-P): 
		(a) Link topmost mention to topmost mention
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2,
		there is a comparison of the numbers of considered
		abbreviations in simulated (depending on the cho-
		sen expansion list) and manually annotated train-
		ing and test data
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 presents
		the detailed statistics of this dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the classiﬁcation results by utter-
		ance type
	</Extractive Summary>
	<Extractive Summary> =
		Even though in general, the cor-
		relations are relatively low (see Table 2), we will
		show that the model strongly beneﬁts from the ad-
		ditional information leading to accuracy increases
		of 2–6% (see Section 5)
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2, we provide examples of labeled seg-
		ments from the various domains in our dataset
	</Extractive Summary>
	<Extractive Summary> =
		For example, posters in these domains
		discuss topics such as symptoms, medical care,
		and diagnoses (Figure 1, Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Automatic Evaluation
		Table 2 shows the best validation ROUGE scores
		of baselines and our models
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2, it can be observed that the
		distribution of PHI instances between the two data
		sets is somewhat similar, but there is a signiﬁcant
		difference when it comes to unique instances be-
		tween the two data sets
	</Extractive Summary>
	<Extractive Summary> =
		51
		Table 1: LIWC Dimension Analysis
		Function Words and Content Words
		Next, we
		looked at selected function words and grammat-
		ical differences, which can be split into two cat-
		egories called Function Words (see Table 2), re-
		ﬂecting how humans communicate and Content
		words (see Table 2), demonstrating what humans
		say (Tausczik and Pennebaker, 2010)
	</Extractive Summary>
	<Extractive Summary> =
		Diabetes Mellitus (C0011849) Diabetes Melli-
		tus (search strings: “diabetes mellitus” and “dia-
		betes mellitus dm”) was high-conﬁdence in Dis-
		charge Summary, Nursing/Other, and Radiology
		document types; Table 2 gives the top 5 neighbors
		from each type
	</Extractive Summary>
	<Extractive Summary> =
		For brevity, its nearest
		neighbors are omitted from Table 2, as there
		is little variation among the top 5
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 2 shows the results of the entity recognition
		experiments for each model and dataset
	</Extractive Summary>
	<Extractive Summary> =
		5
		Results
		The performance of different approaches on Bio-
		Scope and NegPar corpora for the negation scope
		detection and the speculating scope detection are
		shown in Table 1 and Table 2, respectively
	</Extractive Summary>
</Paper ID=ument1057>


<Paper ID=ument1057> <Table ID =3>
	<Abstractive Summary> =
		Markables  
		(594 documents) 
		143,147 
		total 
		Relations, within-doc and 
		cross-doc 
		(594 documents) 
		70,572 
		total 
		Cross-doc relations  
		(198 documents) 
		10,762 
		total 
		TIMEX3s 
		7,796 
		Temporal links 
		35,428 total 
		IDENTICAL 
		9,102 
		Entities* 
		47,355 
		     CONTAINS 
		14,037 
		SET-SUBSET 
		405 
		EVENTs 
		86,172 
		     CON-SUB** 
		4,718 
		WHOLE-PART 
		13 
		SECTIONTIME 
		1,230 
		     BEFORE 
		4,217 
		CON-SUB 
		1,242 
		DOCTIME 
		594 
		     OVERLAP 
		5,091 
		 
		 
		 
		 
		     BEGINS-ON 
		1,200 
		 
		 
		 
		 
		     ENDS-ON 
		557 
		 
		 
		 
		 
		     NOTED-ON 
		5,608 
		 
		 
		 
		 
		Aspectual links 
		873 total 
		 
		 
		 
		 
		     INITIATES 
		259 
		 
		 
		 
		 
		     CONTINUES 
		302 
		 
		 
		 
		 
		     TERMINATES 
		278 
		 
		 
		 
		 
		     REINITIATES 
		34 
		 
		 
		 
		 
		Coreference and bridging links 
		38,337 total 
		 
		 
		 
		 
		     IDENTICAL 
		23,827 
		 
		 
		 
		 
		     SET-SUBSET 
		5,907 
		 
		 
		 
		 
		     WHOLE-PART 
		3,885 
		 
		 
		 
		 
		     CON-SUB** 
		4,718 
		 
		 
		Table 3: Total gold markables and relations for the THYME colon cancer corpus90
		Table 3: Mean scores for embedding sets for each evaluation method9
		Table 3: Comparison with the state-of-the-art systems
		on the DDI-2013 test set
		Models
		P
		R
		F1
		(Lever and Jones, 2016)
		514
		Table 3: Performance on the NER task of the Flair-
		BioReddit on the HealthUnlocked dataset on the seven
		categories deﬁned in Section 37†
		Table 3: The results on NTUH data
		expansions
		cz
		137
		czerwie´n(1), czynnik(3), czynno´s´c(14), cz˛esto´s´c(14) cz˛e´s´c(102)
		redness, factor, activity, frequency, part
		ﬁz
		59
		ﬁzjologiczny(2), ﬁzycznie(1), ﬁzyczny(5),ﬁzykalnnie(45), ﬁzykalny(6)
		physiological, physically, physical, physically, physical
		gł
		48
		gładki(1), głowa(16), główkowy(4), głównie(17), główny(10)
		smooth, head, head, mainly, main
		kr
		224
		kraw˛ed´z(1), kreatynina(2), kropla(68), krople(4), kr˛egosłup(149)
		edge, creatinine, drop, drops, spine
		mies
		206
		miesi ˛ac(187), miesi ˛aczka(18), miesi˛eczny(1)
		month, menstruation, monthly
		op
		1785
		oko prawe (349), ostatni poród (1), opakowanie(1384), opatrunek(6), operacja(22), operacyjnie(3),
		operacyjny(10), operowa´c(1), opieka(7), opuszek(2)
		right eye, last delivery, package, dressing, surgery, surgically, surgical, operate, care, ﬁngertip
		poj
		147
		pojawi´c(2), pojedynczy(127), pojemnik(18)
		appear, single, container
		pow
		65
		powierzchnia(17), powi˛ekszony(6), powi˛ekszy´c(8), powleka´c(9), powy˙zej(24), powód(1)
		surface, enlarged, enlarge,coated, above, reason
		pr
		100
		Pogotowie Ratunkowe(5), public relations(3), prawa r˛eka(1), PR(1), per rectum(5), prawidłowo(17),
		prawidłowy(34), prawy(20), preparat(2), prostata(4), przewód(1), przychodnia(1), próba(6)
		Emergency Service, public relations, right hand, PR(in ECG), per rectum, properly, normal, right,
		preparation, prostate, tract, clinic, test
		rodz
		52
		rodze´nstwo(8), rodzice(3), rodzina(4), rodzinne(1), rodzinnie(20), rodzinny(16)
		sibling, parents, family, family, family, family
		wz
		31
		wziernik(9), wzrost(4)
		speculum, high
		´sr
		65
		´sredni(3), ´srednica(47), ´srednio(10), ´sroda(1), ´srodek(3), ´srodkowy(1)
		medium, diameter, medium, Wednesday, middle, middle
		zab
		90
		zabieg(14), zaburzenie(76)
		surgery, disorder
		zaw
		29
		zawiesina(23), zawód(6)
		suspension, profession
		zał
		31
		załamek(10), zało˙zy´c(5), zał ˛aczy´c(16)
		crinkle, put on, attach
		Table 3: Test set abbreviation expansions in numbers
		from all the possible word forms in our distribu-
		tional model, those whose similarity to a particu-
		lar abbreviation was higher than 097
		Table 3: Emotion multi-class, multi-label classiﬁcation
		results
		In Table 3, the recorded accuracy is based on
		the Keras API2 accuracy calculation on the multi-
		class, multi-label models where it takes into ac-
		count the individual label predictions rather than a
		complete match (i64
		Table 3: Entity extraction results
		501
		Table 3: F1 scores for SSST experiment on test set of target (RHS of →) 05
		Table 3: Results from using various feature sets to pre-
		dict state-level characteristics: whether a given state
		is above or below the national median for diabetes,
		poverty, income, and education62%
		81%
		Table 3: LIWC Analysis by Domain
		PGnet n = 1
		15
		3
		5
		3
		24
		36%
		GPT-2 n = 0
		24
		3
		4
		5
		14
		54%
		GPT-2 n = 1
		26
		6
		5
		3
		10
		64%
		Target
		32
		11
		0
		0
		7
		86%
		Table 3: Human evaluation results for text understand-
		ing on the annotation questions of 50 randomly selected
		source documents54
		Table 3: Entity-based evaluation for CRF with ten fold cross-validation If turned on, similarity features for n-grams
		129
		rubric item
		total
		correct
		incorrect missingitem
		incorrect contrary
		incorrect section
		cc frequent urination
		579
		535
		9
		35
		0
		checks blood sugar regularly 110s before meals
		152
		39
		86
		26
		1
		bp ﬂuctuates 150100
		365
		239
		91
		34
		1
		denies abdominal pain vomiting constipation
		270
		98
		169
		3
		0
		denies hematochezia
		211
		202
		1
		8
		0
		denies recent travel
		257
		245
		5
		4
		3
		duration of 1 week
		256
		221
		5
		30
		0
		feels loopy
		125
		110
		8
		6
		1
		feral cat in the house occasionally
		175
		56
		95
		22
		2
		frequent bm 3-4 times per day
		343
		249
		50
		44
		0
		frequent urination every 30 minutes
		347
		276
		31
		39
		1
		has not had ua
		190
		177
		7
		6
		0
		healthy diet
		178
		157
		21
		0
		0
		husband was sick with uti symptoms
		331
		257
		56
		18
		0
		hx of htn
		298
		254
		40
		4
		0
		initially thought she had respiratory infection
		204
		78
		33
		93
		0
		loose stools with mucous
		324
		205
		111
		8
		0
		losartan hctz every night with dinner
		325
		148
		154
		23
		0
		mild dysuria
		266
		185
		57
		24
		0
		no recent antibiotics
		279
		263
		10
		5
		1
		pe abdomen hyperactive bowel sounds at llq no pain with palp
		334
		97
		180
		51
		6
		pe cv normal
		315
		300
		10
		4
		1
		pe extremities no edema
		297
		282
		7
		2
		6
		pe heent normal no thyromegaly masses carotid bruit
		430
		173
		251
		4
		2
		pe resp normal
		331
		324
		6
		1
		0
		pe skin intact no clubbing cyanosis
		276
		84
		173
		6
		13
		plan advise brat diet
		312
		249
		42
		15
		6
		plan bp goal 13080
		155
		123
		11
		18
		3
		plan may notice leg swelling notify if unbearable
		216
		93
		114
		5
		4
		plan prescribed amlodipine 5mg
		268
		205
		40
		18
		5
		plan recommend 30 mins physical activity 4-5 times per week
		296
		128
		131
		34
		3
		plan reduce stress levels
		119
		100
		13
		1
		5
		plan rtn in 1 month with continued bp log
		280
		172
		85
		16
		7
		plan ua today to rule out infx
		302
		202
		91
		3
		6
		side effects of difﬁculty breathing with metoprolol
		164
		104
		38
		21
		1
		stress work related
		223
		215
		7
		1
		0
		takes blood pressure every morning
		222
		145
		66
		11
		0
		tried yogurt and turmeric no improvement
		176
		66
		106
		4
		0
		was seen by dr reynolds yesterday
		154
		44
		96
		13
		1
		weight normal
		61
		52
		5
		3
		1
		Table 3: Label frequencies for full corpus
		and umls concept grams (if the umls ﬂag is on) are
		also added53
		Table 3: LIWC Affect Analysis
		Social
		and
		Psychological
		Processes
		Social
		Processes highlights the social relationships of
		note writers, where it can be seen in Table 4 that
		the highest amount of social processes can be
		found in LS and the lowest in DL64*
		Table 3: Results of entity recognition using concatenated BERT models727
		Table 3: Benchmark results for the models trained with F1micro stopping criterion
		We also compute the error overlap for the Neg-
		Par test set performance for the top 3 performing
		models: almost half of the ELMo errors and about
		3/4 of BERT ﬁne-tuned and BERT ﬁne-tuned with
		184
		Table 3: Performance on percentage of correct span on BioScope Abstracts sub-corpus trained under different
		schemes
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3 we provide a per-category breakdown
		of the best performing embeddings, i
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 gives appropriate statistics
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 reports the emo-
		tion classiﬁcation results obtained using multi-
		channel CNN (MCCNN), and in addition, results
		from several other experiments: CNN with max-
		pooling (CNNMax) and bidirectional Long Short
		Term Memory module (biLSTM) were reported
		for comparison
	</Extractive Summary>
	<Extractive Summary> =
		,
		2013) whose results are discussed in Table 3
		4
	</Extractive Summary>
	<Extractive Summary> =
		5
		Results and Discussions
		In Table 3 we see that BioInfer generalizes well
		to AiMed and DDI corpora using vanilla LSTM
		or CNN architecture
	</Extractive Summary>
	<Extractive Summary> =
		1
		State-level characteristics prediction
		In Table 3, we show the results for predict-
		ing state-level socioeconomic characteristics using
		various sets of features
	</Extractive Summary>
	<Extractive Summary> =
		(2014) relies on
		hash-tags (see Table 3, lines 5 and 7) and the worst
		performing model use food words (lines 9 and 11)
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents the results from the annota-
		tion question, where the number of true posi-
		tive and true negative generations from the GPT-2
		ﬁne-tuned models increase when compared to the
		PGNet baseline
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents surface form neighbors identiﬁed
		152
		Query
		Discharge Summary
		Nutrition
		Case Management
		Community (C0009462)
		Community
		Dilute
		Substance
		Health center
		Social work
		Monitoring
		Acquired
		Surgical site
		Somewhat
		Residence
		In situ
		Hearing
		Nursing facility
		Nephritis
		Speech
		Discharge Summary
		Echo
		ECG
		ECG (C0013798)
		ECG
		ECG
		ECG
		EKG
		Exercise
		Physician
		Sinus tachycardia
		Stress
		Last
		Sinus bradycardia
		Fair
		No change
		Right bundle branch block
		Speciﬁc
		Abnormal
		Discharge Summary
		Echo
		Radiology
		Blood pressure (C0005823)
		Blood pressure
		Blood pressure
		Blood pressure
		Heart rate
		Heart rate
		Heart rate
		Pressure
		Rate
		Rate
		Systolic blood pressure
		Exercise
		Method
		Rate
		Stress
		Exercise
		Table 3: 5 nearest neighbor surface forms to three frequent clinical concepts, across document types for which they
		are high-conﬁdence
	</Extractive Summary>
	<Extractive Summary> =
		The results are presented in Table 3; our method
		achieves the highest F1micro score, as well as the
		highest P@8 score
	</Extractive Summary>
	<Extractive Summary> =
		As can be seen in Table 3, removing punctua-
		tion affects all models’ behaviour similarly: model
		performance degrades by losing 2-3 percent of
		PCS on average
	</Extractive Summary>
</Paper ID=ument1057>


<Paper ID=ument1057> <Table ID =4>
	<Abstractive Summary> =
		00% 
		 
		Table 4: Intra-document and cross-document inter-
		annotator agreement scores in terms of percentage 
		agreement75
		0
		0
		Table 4: Comparison of mean scores using different evaluation methods for the ﬁelds of medicine as represented
		by their ICD-9 system4
		Table 4: Comparison with the state-of-the-art systems
		on the BB3 test set
		The most important observation from the tables
		is that the BiLSTM model, once combined with the
		ENT-DEP1 pooling method, signiﬁcantly outper-
		forms the previous models on DDI-2013 and BB3,
		establishing new state-of-the-art performance for
		these datasets0
		Table 4: Performance of the Flair embeddings on the
		NER and Adverse Reaction Mention Detection on the
		PsyTAR and CADEC corpora491
		Table 4: Results for 10-fold cross validation for different bidirectional LSTM settings for one training set (a subset
		of randomly selected cluster elements) and a chosen extension list70
		Table 4: Mental illness detection using multi-task, multi-channel, multi-input architecture77
		Table 4: Modality classiﬁcation results
		Type
		Instances
		P
		R
		F1
		ADHD
		126
		092
		Table 4: F1 scores for MSST experiment on test set of
		target (RHS of →)59
		Table 4: Results for predicting T2DM rate using a ran-
		dom forest classiﬁer with our additional socioeconomic
		correlation features15%
		Table 4: LIWC Analysis by Label98
		Table 4: Human evaluation results for generation qual-
		ity on the rating questions70
		Table 4: Entity-based evaluation for LSTM with three fold cross-validation where 66% of the data were used for
		training and 33% for evaluation52
		Table 4: Feature-based system results for 5-fold cross-
		validation, varying tagsets73
		Table 4: LIWC Social Processes
		The term Cognitive processes encompasses a
		number of different aspects, where we have found
		that the highest amount of cognitive processes was
		in DL notes and the lowest in LS notes7
		Table 4: Ablation study of individual components of the ﬁnal method
	</Abstractive Summary>
	<Extractive Summary> =
		The total number of 
		gold markables and relations are shown in Table 
		3; IAA results are shown in Table 4 and are 
		averaged over all the documents (both tables 
		shown on following page)
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4 we see the results we obtain on the
		NER task on the PsyTAR and CADEC corpora
		while using Flair embeddings, where BioReddit
		embeddings always outperform general-purpose
		and PubMed trained ones
	</Extractive Summary>
	<Extractive Summary> =
		Input to a net consists of a subset of the follow-
		ing data (names given after features descriptions
		are used in Table 4 headings):
		• word vectors form the models trained on the
		entire dataset,
		• POS tags encoded as one-hot vector (pos) (31
		most frequent categories),
		a)
		b)
		Figure 1: Two neural net architectures tested
		• vector representing an abbreviation itself
		(padded with zeros if needed), (c),
		• all possible longer versions of the particu-
		lar abbreviations coded in a vector represent-
		ing all possible forms of all the abbreviations
		taken into account
	</Extractive Summary>
	<Extractive Summary> =
		The chosen set of features (second column in
		Table 4) was used for building models for all ab-
		breviation lists and four variants of training sets
		(only annotated data – ANOT, only simulated –
		SIM (only word abbreviations) and SIM-ac (word
		abbreviations and acronyms), and the sum of both
	</Extractive Summary>
	<Extractive Summary> =
		In comparison to the word frequency-
		based approach, using the vocabulary based on
		the TF-IDF scores has produced relatively better
		results for the recorded matrices (refer Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Even though our best results were obtained by in-
		stantiating the embedding layer weights with ran-
		dom numbers (refer Table 4), we conducted sev-
		eral preliminary experiments using word embed-
		dings trained on the fastText (Joulin et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 demonstrates the model performances
		according to different combinations of the multi-
		task, multi-channel, and multi-input architectures
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4, BioBERT is seen to perform well
		for ChemProt
	</Extractive Summary>
	<Extractive Summary> =
		The best multiplier for each model, according to
		SVM performance, is used in our Random Forest
		models (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		ﬁer (Table 5), but when using a RF classiﬁer we
		see up to 6% improvement (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		In the rating questions depicted in Table 4, the
		human written conclusions obtain a score nearly
		4 out of 5 on all three dimensions
	</Extractive Summary>
	<Extractive Summary> =
		2
		LSTM - Results
		The experimental results of the LSTM algorithm
		are shown in Table 4 and again, not presented in
		the table is the combination of training on real data
		and evaluation of pseudo data (Real-Pseudo)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Cross-validation and test results
		Table 4 shows performances for the feature-based
		system using different tagsets in cross-validation
	</Extractive Summary>
	<Extractive Summary> =
		53
		Table 3: LIWC Affect Analysis
		Social
		and
		Psychological
		Processes
		Social
		Processes highlights the social relationships of
		note writers, where it can be seen in Table 4 that
		the highest amount of social processes can be
		found in LS and the lowest in DL
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4 we present an ablation study
	</Extractive Summary>
</Paper ID=ument1057>


<Paper ID=ument1057> <Table ID =5>
	<Abstractive Summary> =
		968
		Table 5: Results for 10-fold cross validation of the selected net architecture for all extension lists and training set
		variants (notation explained in the text)62
		Table 5: Pertinence classiﬁcation results
		cases, the context of the conversation, as well as
		time information, is crucial to recording the pa-
		tient’s information accurately63
		Table 5: Results for predicting T2DM rate using our
		SVM classiﬁer, which is similar to that of Fried et al01
		Table 5: Complexity by Label
		Table 5: An example of GPT-2 models generating true positive conclusions23
		Table 5: LIWC Psychological Processes
		some of the most common reasons for a person to
		commit suicide686
		Table 5: Benchmark results for the models trained with F1macro stopping criterion
	</Abstractive Summary>
	<Extractive Summary> =
		The results on cross validation (Table 5) are the
		best for simulated data, probably because of their
		size and repetitiveness
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the re-
		sults of pertinence classiﬁcation
	</Extractive Summary>
	<Extractive Summary> =
		2
		Impact of socioeconomic factors
		In Table 5, we show the SVM results for pre-
		dicting T2DM rate from extending the feature ma-
		trix from 5
	</Extractive Summary>
	<Extractive Summary> =
		For each feature set, we use the
		best performing multiplier, as determined in the previ-
		ous experiment that used a SVM classiﬁer (Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		ﬁer (Table 5), but when using a RF classiﬁer we
		see up to 6% improvement (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		These scores are
		comparable for all splits of our data; however, as
		shown in Table 5, we do see non-signiﬁcant but
		persistent differences between stressful and non-
		stressful data, with stressful data being generally
		longer and more complex but also rated simpler
		by readability indices
	</Extractive Summary>
</Paper ID=ument1057>


<Paper ID=ument1057> <Table ID =6>
	<Abstractive Summary> =
		546
		Table 6: Results for the test set of the models trained on different datasets for all extension lists (notation explained
		in the text)60
		Table 6: SOAP classiﬁcation results
		Class
		P
		R
		F1
		ADHD
		058
		Table 6: The Cohen’s kappa inter-annotator agreement
		scores among the three annotators8065
		Table 6: Supervised Results
		Table 6: An example of GPT-2 n = 0 model generating a true negative conclusion, while the GPT-2 n = 1 model
		generated a false positive one59
		Table 6: Detailed results for the test set, tag gran=4lab64
		Table 6: LIWC Personal Concerns
		Time Orientation and Relativity
		Looking at
		the Time Orientation of a note can give interest-
		ing insight into the temporal focus of attention and
		differences in verb tenses can show psychological
		distance or to which extend disclosed events have
		been processed (Tausczik and Pennebaker, 2010)
	</Abstractive Summary>
	<Extractive Summary> =
		The results for the test set (Table 6) are better
		than those obtained for the cross validation on the
		training set in many cases
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows the results of SOAP classiﬁca-
		tion
	</Extractive Summary>
	<Extractive Summary> =
		The results are shown in
		Table 6 and 7
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows a false positive
		example by the GPT-2 n = 1 model
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows results
		for the test set for different systems
	</Extractive Summary>
</Paper ID=ument1057>


<Paper ID=ument1057> <Table ID =7>
	<Abstractive Summary> =
		ruchomo´sci kolana pr, przykurcz
		prawy
		prawy
		prawidłowy
		‘limitation of the right knee mobility, contracture’
		‘right’
		‘right’
		‘normal’
		Table 7: Examples of miss-interpretation of ‘pr’ for the CL list of potential expansions and for two training data:
		SIM and ANOT+SIM04
		Table 7: Primary diagnosis classiﬁcation results low-carb diets
		woman, vegetarian diet
		mint, saffron, fennel, squash,
		soup, tomato, eggplant
		man, vegetarian diet
		beet, onion, coconut, spinach,
		kale, carrot
		woman, low-carb diet
		hazelnut, nut, cupcake, pastry,
		grain, caramel, wheat
		man, low-carb diet
		cereal, spaghetti, buns, ham-
		burger, pepperoni, crunch
		Table 7: The 4 corners in the man vs48
		Table 7: Feature Sets and Data Sets
		Table 7: An example of GPT-2 n = 0 model generating a false negative conclusion, while the GPT-2 n = 1 model
		generated a true positive one85
		Table 7: Results for the test set, ngram+simfeats+umls
		tag gran=4lab51
		Table 7: LIWC Time orientation
		Overall it was noted that for most analysis GSN
		falls between the two extremes of LS and DL
	</Abstractive Summary>
	<Extractive Summary> =
		We list the 4 corners in the projection as 4 rows
		in Table 7, where the left column corresponds to
		the concepts and the right column contains the
		words
	</Extractive Summary>
	<Extractive Summary> =
		In Table 7, we examine the impact of differ-
		ent feature sets and levels of annotator agree-
		ment on our logistic regressor with domain-
		speciﬁc Word2Vec embeddings and ﬁnd consis-
		tent patterns supporting this model
	</Extractive Summary>
	<Extractive Summary> =
		The generated conclusions in Table 7 is also much
		shorter than the target conclusion written by hu-
		man
	</Extractive Summary>
	<Extractive Summary> =
		Table 7 shows that the focus of LS letters is pri-
		marily in the past whilst GSN and DL letters fo-
		cus on the present
	</Extractive Summary>
</Paper ID=ument1057>


<Paper ID=ument1057> <Table ID =8>
	<Abstractive Summary> =
		In the previous section, we observe that
		topic modeling can be helpful in identifying the
		Topic#
		Topic words
		0
		focus, sleeping, depressed, asleep,
		attention, mind, cymbalta, appetite,
		psychiatrist, energy
		1
		ache, h1n1, treat, asthma, temper-
		ature, diarrhea, anybody, mucinex,
		chill, allergic
		2
		period, knee, birth, heavy, ultrasound,
		iron, metoprolol, pregnancy, preg-
		nant, history,
		3
		meal, diabetic, lose, unit, mail, deal,
		crazy, card, swelling, pound
		4
		cymbalta, lantus, cool, cancer, crazy,
		allergy, sister, attack, nurse, wow
		5
		referral, trazodone, asked, shingle,
		woman, medicare, med, friend, clinic,
		form
		6
		breo, cream, puff, rash, smoking, al-
		buterol, skin, allergy, proair, allergic
		7
		fosamax, allergy, tramadol, covered,
		plan, calcium, bladder, kept, alcohol,
		ache
		8
		metformin, x-ray, nerve, knee, lasix,
		bottle, lantus, hurting, referral, switch
		9
		lantus,
		looked,
		injection,
		botox,
		changed, ﬂare, happening, cream,
		salt, sweating
		10
		generic, triumeq, cost, farxiga, phys-
		ical, therapy, gosh, fracture, increase,
		invokana
		11
		unit, list, appreciate, therapy, differ-
		ence, counter, report, lasix, lantus, en-
		docrinologist
		Table 8: Topic Modeling: Top 10 words for 12 topics
		Both models seem to have trouble with less ex-
		plicit expressions of stress, framing negative ex-
		104
		Gold
		0
		1
		LogReg
		0
		241
		105
		1
		49
		320
		Gold
		0
		1
		BERT
		0
		240
		106
		1
		48
		321
		BERT
		0
		1
		LogReg
		0
		237
		51
		1
		53
		374
		Table 8: Confusion Matrices87
		Table 8: Test accuracy and F1-score of different learning models in %
		Figure 4: Example of LS note correctly classiﬁed
	</Abstractive Summary>
	<Extractive Summary> =
		Although the dataset is
		nearly balanced, both BERT-base and our best lo-
		gistic regression model greatly overclassify stress,
		as shown in Table 8, and they broadly overlap but
		do differ in their predictions (disagreeing with one
		another on approximately 100 instances)
	</Extractive Summary>
	<Extractive Summary> =
		It can
		be seen in Table 8 that the dilated LSTM with an
		attention layer outperforms the BiLSTM with At-
		tention by 5
	</Extractive Summary>
</Paper ID=ument1057>


<Paper ID=ument1057> <Table ID =9>
	<Abstractive Summary> =
		Not Stress
		100%
		domesticviolence
		LogReg
		Table 9: Error Analysis Examples
	</Abstractive Summary>
</Paper ID=ument1057>


<Paper ID=ument1059> <Table ID =1>
	<Abstractive Summary> =
		[M1]WHOLE - 
		[M2]PART 
		 
		Table 1: Gold-annotated cross-document relation types in the THYME colon cancer corpus
	</Abstractive Summary>
</Paper ID=ument1059>


<Paper ID=ument1059> <Table ID =2>
	<Abstractive Summary> =
		MRI 
		testsA SET-SUBSET MRIA 
		testsB SET-SUBSET MRIB 
		testsA IDENT testsB 
		MRIA IDENT MRIB 
		Table 2: For File Set 1, there is no cross-doc S-SS link between testsA and MRIB because this can be inferred 
		from the cross-doc IDENT link and the within-doc S-SS link shown
	</Abstractive Summary>
	<Extractive Summary> =
		3 
		Cross-document annotation process 
		To manage the potentially vast number of 
		cross-document links, we established a set of 
		assumptions about inferable relations that guided 
		the following process and are further discussed in 
		Table 2 (note: “structural links” refers to links that 
		have 
		a 
		hierarchical 
		rather 
		than 
		identical 
		relationship: CON-SUB, S-SS, W-P): 
		(a) Link topmost mention to topmost mention
	</Extractive Summary>
</Paper ID=ument1059>


<Paper ID=ument1059> <Table ID =3>
	<Abstractive Summary> =
		Markables  
		(594 documents) 
		143,147 
		total 
		Relations, within-doc and 
		cross-doc 
		(594 documents) 
		70,572 
		total 
		Cross-doc relations  
		(198 documents) 
		10,762 
		total 
		TIMEX3s 
		7,796 
		Temporal links 
		35,428 total 
		IDENTICAL 
		9,102 
		Entities* 
		47,355 
		     CONTAINS 
		14,037 
		SET-SUBSET 
		405 
		EVENTs 
		86,172 
		     CON-SUB** 
		4,718 
		WHOLE-PART 
		13 
		SECTIONTIME 
		1,230 
		     BEFORE 
		4,217 
		CON-SUB 
		1,242 
		DOCTIME 
		594 
		     OVERLAP 
		5,091 
		 
		 
		 
		 
		     BEGINS-ON 
		1,200 
		 
		 
		 
		 
		     ENDS-ON 
		557 
		 
		 
		 
		 
		     NOTED-ON 
		5,608 
		 
		 
		 
		 
		Aspectual links 
		873 total 
		 
		 
		 
		 
		     INITIATES 
		259 
		 
		 
		 
		 
		     CONTINUES 
		302 
		 
		 
		 
		 
		     TERMINATES 
		278 
		 
		 
		 
		 
		     REINITIATES 
		34 
		 
		 
		 
		 
		Coreference and bridging links 
		38,337 total 
		 
		 
		 
		 
		     IDENTICAL 
		23,827 
		 
		 
		 
		 
		     SET-SUBSET 
		5,907 
		 
		 
		 
		 
		     WHOLE-PART 
		3,885 
		 
		 
		 
		 
		     CON-SUB** 
		4,718 
		 
		 
		Table 3: Total gold markables and relations for the THYME colon cancer corpus
	</Abstractive Summary>
</Paper ID=ument1059>


<Paper ID=ument1059> <Table ID =4>
	<Abstractive Summary> =
		00% 
		 
		Table 4: Intra-document and cross-document inter-
		annotator agreement scores in terms of percentage 
		agreement
	</Abstractive Summary>
	<Extractive Summary> =
		The total number of 
		gold markables and relations are shown in Table 
		3; IAA results are shown in Table 4 and are 
		averaged over all the documents (both tables 
		shown on following page)
	</Extractive Summary>
</Paper ID=ument1059>


<Paper ID=ument106> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Development scores
	</Abstractive Summary>
	<Extractive Summary> =
		To avoid this problem we
		used the alternative reweighting scheme:
		w′(xi+1, yi+1) =
		P(xi+1, yi+1)
		�
		y′∈ ˜Y(xi+1) P(xi+1, y′)
		that avoids this rounding problem and all the other
		results reported in Table 1 use instead this alterna-
		tive scheme
	</Extractive Summary>
</Paper ID=ument106>


<Paper ID=ument106> <Table ID =2>
	<Abstractive Summary> =
		The comparison remains
		indicative
		Table 2: Test results
	</Abstractive Summary>
	<Extractive Summary> =
		All these observations have practical con-
		sequences: the beam method takes hours to com-
		plete the parsing of the Wall Street Journal (wsj,
		see Table 2) development while the particle search
		takes minutes on a standard workstation
	</Extractive Summary>
	<Extractive Summary> =
		For the test, we observe again from Table 2 that
		our search method (K = 50000) gets better F-
		score and worse perplexity than the beam method
	</Extractive Summary>
	<Extractive Summary> =
		The perplexities re-
		ported as Prince in Table 2 are the perplexities
		resulting from processing the Little Prince cor-
		pus used later in the paper for analyzing neuro-
		imaging data
	</Extractive Summary>
</Paper ID=ument106>


<Paper ID=ument106> <Table ID =3>
	<Abstractive Summary> =
		3)
		Word rate
		tagging spoken words on the fMRI signal
		Word frequency
		word-by-word log-frequency in movie subtitles
		RMS amplitude
		an acoustic correlate of volume every 10ms
		Table 3: Predictors used in the fMRI Analysis
	</Abstractive Summary>
	<Extractive Summary> =
		In section 3 we motivate and
		study some properties of the method from a com-
		putational perspective and we explore in section 5
		the potential of the overall beam activity predictors
		for modelling and analyzing fMRI data (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		The
		predictors shown in Table 3 were convolved us-
		ing SPM’s (Friston et al
	</Extractive Summary>
	<Extractive Summary> =
		B400 and K50000) increases the cross-validated
		r2, from a baseline model that does not include
		them, see Table 3)
	</Extractive Summary>
</Paper ID=ument106>


<Paper ID=ument106> <Table ID =4>
	<Abstractive Summary> =
		66
		Table 4: Clusters showing a signiﬁcant better ﬁt for B400 ﬁx beam-size search method in the RNNG parser, p<
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4), shows the signiﬁcance (z-
		scores, p < 0
	</Extractive Summary>
</Paper ID=ument106>


<Paper ID=ument1060> <Table ID =1>
	<Abstractive Summary> =
		7m
		health insurance claims
		clinical narratives
		full journal texts
		Table 1: Characteristics of the embeddings compared, including the name referred, the embedding dimensions, the
		number of embeddings in the dataset, and the type of data used to train them
	</Abstractive Summary>
</Paper ID=ument1060>


<Paper ID=ument1060> <Table ID =2>
	<Abstractive Summary> =
		67
		Table 2: Illustrative example showing how System Vector Accuracy (SysVec) would be calculated for the medical
		ﬁeld “Mental Disorders” if it contained only six drugs
	</Abstractive Summary>
</Paper ID=ument1060>


<Paper ID=ument1060> <Table ID =3>
	<Abstractive Summary> =
		90
		Table 3: Mean scores for embedding sets for each evaluation method
	</Abstractive Summary>
</Paper ID=ument1060>


<Paper ID=ument1060> <Table ID =4>
	<Abstractive Summary> =
		75
		0
		0
		Table 4: Comparison of mean scores using different evaluation methods for the ﬁelds of medicine as represented
		by their ICD-9 system
	</Abstractive Summary>
</Paper ID=ument1060>


<Paper ID=ument1061> <Table ID =1>
	<Abstractive Summary> =
		8
		Table 1: Results on DDI 2013
		From the tables, we have the following observa-
		tions about the effectiveness of the pooling meth-
		ods for RE with deep learning:
		1
	</Abstractive Summary>
</Paper ID=ument1061>


<Paper ID=ument1061> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Results on BioNLP BB3
		the DDI-2013 dataset
	</Abstractive Summary>
</Paper ID=ument1061>


<Paper ID=ument1061> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: Comparison with the state-of-the-art systems
		on the DDI-2013 test set
		Models
		P
		R
		F1
		(Lever and Jones, 2016)
		51
	</Abstractive Summary>
</Paper ID=ument1061>


<Paper ID=ument1061> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4: Comparison with the state-of-the-art systems
		on the BB3 test set
		The most important observation from the tables
		is that the BiLSTM model, once combined with the
		ENT-DEP1 pooling method, signiﬁcantly outper-
		forms the previous models on DDI-2013 and BB3,
		establishing new state-of-the-art performance for
		these datasets
	</Abstractive Summary>
</Paper ID=ument1061>


<Paper ID=ument1062> <Table ID =1>
	<Abstractive Summary> =
		64
		Table 1: Precision (Prec), recall (Rec) and micro F-
		score (F1) results on DDI 2013 corpus
	</Abstractive Summary>
</Paper ID=ument1062>


<Paper ID=ument1063> <Table ID =1>
	<Abstractive Summary> =
		1158
		163
		294
		Table 1: Description and statistics of the HealthUnlocked dataset used for the experiments
	</Abstractive Summary>
</Paper ID=ument1063>


<Paper ID=ument1063> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Performance of different embeddings tech-
		nique on NER, when trained and evaluated on the
		dataset described in Section 3
	</Abstractive Summary>
</Paper ID=ument1063>


<Paper ID=ument1063> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Performance on the NER task of the Flair-
		BioReddit on the HealthUnlocked dataset on the seven
		categories deﬁned in Section 3
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3 we provide a per-category breakdown
		of the best performing embeddings, i
	</Extractive Summary>
</Paper ID=ument1063>


<Paper ID=ument1063> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Performance of the Flair embeddings on the
		NER and Adverse Reaction Mention Detection on the
		PsyTAR and CADEC corpora
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4 we see the results we obtain on the
		NER task on the PsyTAR and CADEC corpora
		while using Flair embeddings, where BioReddit
		embeddings always outperform general-purpose
		and PubMed trained ones
	</Extractive Summary>
</Paper ID=ument1063>


<Paper ID=ument1064> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Dataset comparison and statistics
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 1, our data, Internal-200, is more chal-
		lenging due to much shorter text inputs and higher
		OOV rate compared with the benchmark MIMIC-
		3 dataset
	</Extractive Summary>
</Paper ID=ument1064>


<Paper ID=ument1064> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: The results on MIMIC-3 data (%)
	</Abstractive Summary>
</Paper ID=ument1064>


<Paper ID=ument1064> <Table ID =3>
	<Abstractive Summary> =
		7†
		Table 3: The results on NTUH data
	</Abstractive Summary>
</Paper ID=ument1064>


<Paper ID=ument1065> <Table ID =1>
	<Abstractive Summary> =
		Variations
		All
		Possible Meanings in Test Data
		ﬁz
		FIZ, ﬁz, Fiz,
		15
		ﬁzjologiczny, ﬁzycznie, ﬁzyczny, ﬁzykalnie, ﬁzykalny, ﬁzykoterapia
		Physiological, Physically, Physical, Physical, Physical Therapy
		cz
		CZ, Cz
		44
		czerwie´n, czołowy, czynnik, czynno´s´c, cz˛esto´s´c, cz˛e´s´c
		redness, frontal, factor, activity, frequency, part
		gł
		gł
		22
		głowa, główkowy, głównie, główny, gł˛eboki
		head, head(adj), mainly, main, deep
		op
		OP Op
		30
		opak, opakowanie, opatrunek, opera, operacja, operacyjnie, operacyjny, oko prawy
		operowany, opieka, opis, opuszek, opór
		awry, package, dressing, opera, operation, operationally, operational, right eye
		operated, care, description, pad, resistance
		Table 1: Four from the list of 15 abbreviations with variations, the number of all different longer words found in
		the training data
	</Abstractive Summary>
</Paper ID=ument1065>


<Paper ID=ument1065> <Table ID =2>
	<Abstractive Summary> =
		AL
		SL
		CL
		train
		test
		cz
		51022
		46172
		25642
		96
		137
		ﬁz
		10769
		10684
		9895
		61
		59
		gł
		15591
		14460
		9988
		55
		48
		kr
		37381
		24349
		20053
		81
		224
		mies
		9021
		8949
		6874
		35
		206
		op
		24677
		21673
		9285
		410
		1785
		poj
		4386
		4035
		3293
		75
		147
		pow
		22517
		5037
		17271
		69
		65
		pr
		88312
		20386
		57809
		105
		100
		rodz
		6459
		6459
		4903
		26
		52
		´sr
		3894
		2922
		2316
		61
		65
		wz
		9942
		6914
		3345
		42
		31
		zab
		8670
		8085
		7755
		69
		90
		zaw
		3826
		1296
		2012
		28
		29
		zał
		1657
		1544
		717
		18
		31
		total
		298149
		182965
		181140
		1231
		3069
		Table 2: Number of occurrences in train and test data
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2,
		there is a comparison of the numbers of considered
		abbreviations in simulated (depending on the cho-
		sen expansion list) and manually annotated train-
		ing and test data
	</Extractive Summary>
</Paper ID=ument1065>


<Paper ID=ument1065> <Table ID =3>
	<Abstractive Summary> =
		expansions
		cz
		137
		czerwie´n(1), czynnik(3), czynno´s´c(14), cz˛esto´s´c(14) cz˛e´s´c(102)
		redness, factor, activity, frequency, part
		ﬁz
		59
		ﬁzjologiczny(2), ﬁzycznie(1), ﬁzyczny(5),ﬁzykalnnie(45), ﬁzykalny(6)
		physiological, physically, physical, physically, physical
		gł
		48
		gładki(1), głowa(16), główkowy(4), głównie(17), główny(10)
		smooth, head, head, mainly, main
		kr
		224
		kraw˛ed´z(1), kreatynina(2), kropla(68), krople(4), kr˛egosłup(149)
		edge, creatinine, drop, drops, spine
		mies
		206
		miesi ˛ac(187), miesi ˛aczka(18), miesi˛eczny(1)
		month, menstruation, monthly
		op
		1785
		oko prawe (349), ostatni poród (1), opakowanie(1384), opatrunek(6), operacja(22), operacyjnie(3),
		operacyjny(10), operowa´c(1), opieka(7), opuszek(2)
		right eye, last delivery, package, dressing, surgery, surgically, surgical, operate, care, ﬁngertip
		poj
		147
		pojawi´c(2), pojedynczy(127), pojemnik(18)
		appear, single, container
		pow
		65
		powierzchnia(17), powi˛ekszony(6), powi˛ekszy´c(8), powleka´c(9), powy˙zej(24), powód(1)
		surface, enlarged, enlarge,coated, above, reason
		pr
		100
		Pogotowie Ratunkowe(5), public relations(3), prawa r˛eka(1), PR(1), per rectum(5), prawidłowo(17),
		prawidłowy(34), prawy(20), preparat(2), prostata(4), przewód(1), przychodnia(1), próba(6)
		Emergency Service, public relations, right hand, PR(in ECG), per rectum, properly, normal, right,
		preparation, prostate, tract, clinic, test
		rodz
		52
		rodze´nstwo(8), rodzice(3), rodzina(4), rodzinne(1), rodzinnie(20), rodzinny(16)
		sibling, parents, family, family, family, family
		wz
		31
		wziernik(9), wzrost(4)
		speculum, high
		´sr
		65
		´sredni(3), ´srednica(47), ´srednio(10), ´sroda(1), ´srodek(3), ´srodkowy(1)
		medium, diameter, medium, Wednesday, middle, middle
		zab
		90
		zabieg(14), zaburzenie(76)
		surgery, disorder
		zaw
		29
		zawiesina(23), zawód(6)
		suspension, profession
		zał
		31
		załamek(10), zało˙zy´c(5), zał ˛aczy´c(16)
		crinkle, put on, attach
		Table 3: Test set abbreviation expansions in numbers
		from all the possible word forms in our distribu-
		tional model, those whose similarity to a particu-
		lar abbreviation was higher than 0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 gives appropriate statistics
	</Extractive Summary>
</Paper ID=ument1065>


<Paper ID=ument1065> <Table ID =4>
	<Abstractive Summary> =
		491
		Table 4: Results for 10-fold cross validation for different bidirectional LSTM settings for one training set (a subset
		of randomly selected cluster elements) and a chosen extension list
	</Abstractive Summary>
	<Extractive Summary> =
		Input to a net consists of a subset of the follow-
		ing data (names given after features descriptions
		are used in Table 4 headings):
		• word vectors form the models trained on the
		entire dataset,
		• POS tags encoded as one-hot vector (pos) (31
		most frequent categories),
		a)
		b)
		Figure 1: Two neural net architectures tested
		• vector representing an abbreviation itself
		(padded with zeros if needed), (c),
		• all possible longer versions of the particu-
		lar abbreviations coded in a vector represent-
		ing all possible forms of all the abbreviations
		taken into account
	</Extractive Summary>
	<Extractive Summary> =
		The chosen set of features (second column in
		Table 4) was used for building models for all ab-
		breviation lists and four variants of training sets
		(only annotated data – ANOT, only simulated –
		SIM (only word abbreviations) and SIM-ac (word
		abbreviations and acronyms), and the sum of both
	</Extractive Summary>
</Paper ID=ument1065>


<Paper ID=ument1065> <Table ID =5>
	<Abstractive Summary> =
		968
		Table 5: Results for 10-fold cross validation of the selected net architecture for all extension lists and training set
		variants (notation explained in the text)
	</Abstractive Summary>
	<Extractive Summary> =
		The results on cross validation (Table 5) are the
		best for simulated data, probably because of their
		size and repetitiveness
	</Extractive Summary>
</Paper ID=ument1065>


<Paper ID=ument1065> <Table ID =6>
	<Abstractive Summary> =
		546
		Table 6: Results for the test set of the models trained on different datasets for all extension lists (notation explained
		in the text)
	</Abstractive Summary>
	<Extractive Summary> =
		The results for the test set (Table 6) are better
		than those obtained for the cross validation on the
		training set in many cases
	</Extractive Summary>
</Paper ID=ument1065>


<Paper ID=ument1065> <Table ID =7>
	<Abstractive Summary> =
		ruchomo´sci kolana pr, przykurcz
		prawy
		prawy
		prawidłowy
		‘limitation of the right knee mobility, contracture’
		‘right’
		‘right’
		‘normal’
		Table 7: Examples of miss-interpretation of ‘pr’ for the CL list of potential expansions and for two training data:
		SIM and ANOT+SIM
	</Abstractive Summary>
</Paper ID=ument1065>


<Paper ID=ument1066> <Table ID =1>
	<Abstractive Summary> =
		Emotion
		Train
		Test
		Dev
		Total
		Anger
		857
		760
		84
		1701
		Fear
		1147
		995
		110
		2252
		Joy
		823
		714
		79
		1616
		Sadness
		786
		673
		74
		1533
		Total
		3613
		3142
		347
		7102
		Table 1: The number of tweets under each emotion cat-
		egory
		The dataset contains 194 tweets that belong to
		multiple emotion categories
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents the detailed statistics of the
		dataset
	</Extractive Summary>
</Paper ID=ument1066>


<Paper ID=ument1066> <Table ID =2>
	<Abstractive Summary> =
		7
		Gender (female)
		distribution per class
		74%
		67%
		80%
		Table 2: CLPSych 2015 shared task dataset statistics
		Preprocessing:
		All
		the
		URLs,
		@mentions,
		#hashtags, RTweets, emoticons, emojis, and num-
		bers were removed
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents
		the detailed statistics of this dataset
	</Extractive Summary>
</Paper ID=ument1066>


<Paper ID=ument1066> <Table ID =3>
	<Abstractive Summary> =
		97
		Table 3: Emotion multi-class, multi-label classiﬁcation
		results
		In Table 3, the recorded accuracy is based on
		the Keras API2 accuracy calculation on the multi-
		class, multi-label models where it takes into ac-
		count the individual label predictions rather than a
		complete match (i
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 reports the emo-
		tion classiﬁcation results obtained using multi-
		channel CNN (MCCNN), and in addition, results
		from several other experiments: CNN with max-
		pooling (CNNMax) and bidirectional Long Short
		Term Memory module (biLSTM) were reported
		for comparison
	</Extractive Summary>
</Paper ID=ument1066>


<Paper ID=ument1066> <Table ID =4>
	<Abstractive Summary> =
		70
		Table 4: Mental illness detection using multi-task, multi-channel, multi-input architecture
	</Abstractive Summary>
	<Extractive Summary> =
		In comparison to the word frequency-
		based approach, using the vocabulary based on
		the TF-IDF scores has produced relatively better
		results for the recorded matrices (refer Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Even though our best results were obtained by in-
		stantiating the embedding layer weights with ran-
		dom numbers (refer Table 4), we conducted sev-
		eral preliminary experiments using word embed-
		dings trained on the fastText (Joulin et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 demonstrates the model performances
		according to different combinations of the multi-
		task, multi-channel, and multi-input architectures
	</Extractive Summary>
</Paper ID=ument1066>


<Paper ID=ument1067> <Table ID =1>
	<Abstractive Summary> =
		3
		Dataset
		Primary diagnosis
		Dyads
		ADHD
		100
		Depression
		100
		COPD
		101
		Inﬂuenza
		100
		Osteoporosis
		87
		Type II diabetes
		86
		Other
		226
		Table 1: Data distribution (ADHD: Attention Deﬁcit
		Hyperactivity Disorder; COPD: Chronic Obstructive
		Pulmonary Disorder)
		For training and testing our models, we use a
		dataset of 800 patient-clinician dialogues (dyads)
		purchased from Verilogue Inc
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the distribution of diagnoses
		in the dataset
	</Extractive Summary>
</Paper ID=ument1067>


<Paper ID=ument1067> <Table ID =2>
	<Abstractive Summary> =
		71
		Table 2: Utterance type classiﬁcation results
		normalized medication names) 9
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the classiﬁcation results by utter-
		ance type
	</Extractive Summary>
</Paper ID=ument1067>


<Paper ID=ument1067> <Table ID =3>
	<Abstractive Summary> =
		64
		Table 3: Entity extraction results
		5
	</Abstractive Summary>
</Paper ID=ument1067>


<Paper ID=ument1067> <Table ID =4>
	<Abstractive Summary> =
		77
		Table 4: Modality classiﬁcation results
		Type
		Instances
		P
		R
		F1
		ADHD
		126
		0
	</Abstractive Summary>
</Paper ID=ument1067>


<Paper ID=ument1067> <Table ID =5>
	<Abstractive Summary> =
		62
		Table 5: Pertinence classiﬁcation results
		cases, the context of the conversation, as well as
		time information, is crucial to recording the pa-
		tient’s information accurately
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the re-
		sults of pertinence classiﬁcation
	</Extractive Summary>
</Paper ID=ument1067>


<Paper ID=ument1067> <Table ID =6>
	<Abstractive Summary> =
		60
		Table 6: SOAP classiﬁcation results
		Class
		P
		R
		F1
		ADHD
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the results of SOAP classiﬁca-
		tion
	</Extractive Summary>
	<Extractive Summary> =
		The results are shown in
		Table 6 and 7
	</Extractive Summary>
</Paper ID=ument1067>


<Paper ID=ument1067> <Table ID =7>
	<Abstractive Summary> =
		04
		Table 7: Primary diagnosis classiﬁcation results
	</Abstractive Summary>
</Paper ID=ument1067>


<Paper ID=ument1067> <Table ID =8>
	<Abstractive Summary> =
		In the previous section, we observe that
		topic modeling can be helpful in identifying the
		Topic#
		Topic words
		0
		focus, sleeping, depressed, asleep,
		attention, mind, cymbalta, appetite,
		psychiatrist, energy
		1
		ache, h1n1, treat, asthma, temper-
		ature, diarrhea, anybody, mucinex,
		chill, allergic
		2
		period, knee, birth, heavy, ultrasound,
		iron, metoprolol, pregnancy, preg-
		nant, history,
		3
		meal, diabetic, lose, unit, mail, deal,
		crazy, card, swelling, pound
		4
		cymbalta, lantus, cool, cancer, crazy,
		allergy, sister, attack, nurse, wow
		5
		referral, trazodone, asked, shingle,
		woman, medicare, med, friend, clinic,
		form
		6
		breo, cream, puff, rash, smoking, al-
		buterol, skin, allergy, proair, allergic
		7
		fosamax, allergy, tramadol, covered,
		plan, calcium, bladder, kept, alcohol,
		ache
		8
		metformin, x-ray, nerve, knee, lasix,
		bottle, lantus, hurting, referral, switch
		9
		lantus,
		looked,
		injection,
		botox,
		changed, ﬂare, happening, cream,
		salt, sweating
		10
		generic, triumeq, cost, farxiga, phys-
		ical, therapy, gosh, fracture, increase,
		invokana
		11
		unit, list, appreciate, therapy, differ-
		ence, counter, report, lasix, lantus, en-
		docrinologist
		Table 8: Topic Modeling: Top 10 words for 12 topics
	</Abstractive Summary>
</Paper ID=ument1067>


<Paper ID=ument1068> <Table ID =1>
	<Abstractive Summary> =
		This is concatenated with the ﬁxed size sen-
		tence representation from Fs and
		Fdi, together
		77
		Datasets
		Entity
		Pair
		# of
		Sent
		# of
		Positive
		# of
		Negative
		AiMed
		1995
		1000
		4834
		BioInfer
		1100
		2534
		7132
		LLL
		PPI
		77
		164
		166
		HPRD50
		145
		163
		270
		IEPA
		486
		355
		482
		Table 1: Protein Protein Interaction Dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Datasets
		The dataset statistics is summarized in Table 1
		and Table 2
	</Extractive Summary>
</Paper ID=ument1068>


<Paper ID=ument1068> <Table ID =2>
	<Abstractive Summary> =
		Datasets
		Entity
		Pair
		# of
		Train
		# of
		Valid
		# of
		Test
		DDI
		Drug-Drug
		27779
		-
		5713
		CPR: 3
		768
		550
		665
		CPR: 4
		2254
		1094
		1661
		CPR: 5
		Chem-Prot
		173
		116
		195
		CPR: 6
		235
		199
		293
		CPR: 9
		727
		457
		644
		Table 2: Drug-Drug Interaction and Chemical-Protein
		Dataset statistics
	</Abstractive Summary>
</Paper ID=ument1068>


<Paper ID=ument1068> <Table ID =3>
	<Abstractive Summary> =
		01
		Table 3: F1 scores for SSST experiment on test set of target (RHS of →)
	</Abstractive Summary>
	<Extractive Summary> =
		,
		2013) whose results are discussed in Table 3
		4
	</Extractive Summary>
	<Extractive Summary> =
		5
		Results and Discussions
		In Table 3 we see that BioInfer generalizes well
		to AiMed and DDI corpora using vanilla LSTM
		or CNN architecture
	</Extractive Summary>
</Paper ID=ument1068>


<Paper ID=ument1068> <Table ID =4>
	<Abstractive Summary> =
		92
		Table 4: F1 scores for MSST experiment on test set of
		target (RHS of →)
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, BioBERT is seen to perform well
		for ChemProt
	</Extractive Summary>
</Paper ID=ument1068>


<Paper ID=ument1069> <Table ID =1>
	<Abstractive Summary> =
		1
		Structured Features
		Structure features are features that were identiﬁed
		on the EHR using regular expression matching and
		include rating scores that have been reported in the
		psychiatric literature as correlated with increased
		readmission risk, such as Global Assessment of
		Functioning, Insight and Compliance:
		Sociodemographics
		Age
		Gender
		Race
		Marital status
		Veteran
		Past medical history
		History of Suicidality
		Number of past admissions
		Average length of stay (previous)
		Average # days between admissions
		Previous 30-day readmission (Y/N)
		Number of past readmissions
		Readmission ratio
		Average GAF at admission
		Average GAF at discharge
		Mode of past insight values
		Mode of past medication compliance
		Current admission
		Structured features
		Number of notes
		Number of tokens
		Number of tokens in discharge summary
		Average note length
		GAF at admission
		GAF at discharge
		GAF admission/discharge difference
		Mean GAF (all notes for visit)
		Insight (good, fair, poor)
		Medication Compliance
		Estimated length of stay
		Actual length of stay
		Difference b/w Estimated & Actual LOS
		Is ﬁrst admission (Y/N)
		Unstructured features
		Number of sentences (Appearance)
		Number of sentences (Mood)
		Number of sentences (Thought Content)
		Number of sentences (Thought Process)
		Number of sentences (Substance Use)
		Number of sentences (Interpersonal)
		Number of sentences (Occupation)
		Clinical sentiment (Appearance)
		Clinical sentiment (Mood)
		Clinical sentiment (Thought Content)
		Clinical sentiment (Thought Process)
		Clinical sentiment (Substance Use)
		Clinical sentiment (Interpersonal)
		Clinical sentiment (Occupation)
		Table 1: Extracted features by category
	</Abstractive Summary>
	<Extractive Summary> =
		These features can be grouped into
		three categories (See Table 1 for complete list of
		features):
		- Sociodemographics: gender, age, marital sta-
		tus, etc
	</Extractive Summary>
</Paper ID=ument1069>


<Paper ID=ument1069> <Table ID =2>
	<Abstractive Summary> =
		72
		Table 2: Results (in ascending order)
		0
	</Abstractive Summary>
</Paper ID=ument1069>


<Paper ID=ument107> <Table ID =1>
	<Abstractive Summary> =
		, 2017)
		# examples
		# annotators
		MNLI (matched)
		402,517
		380
		OPENBOOKQA
		5,457
		84
		COMMONSENSEQA
		11,096
		132
		Table 1: Statistics for datasets used in our experiments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the size and number of an-
		notators who worked on each dataset
	</Extractive Summary>
	<Extractive Summary> =
		The different results for MNLI com-
		pared to those observed for OPENBOOKQA and
		COMMONSENSEQA may be attributed to the less-
		skewed annotator distribution and large number of
		examples in MNLI (see Figure 1 and Table 1),
		which make the model more robust to small per-
		turbations in the data distribution
	</Extractive Summary>
</Paper ID=ument107>


<Paper ID=ument107> <Table ID =2>
	<Abstractive Summary> =
		13e−7
		Table 2: Model development performance, after train-
		ing with/without annotator IDs as additional inputs
	</Abstractive Summary>
	<Extractive Summary> =
		Adding the anno-
		tator ID improves model performance across all
		datasets (Table 2), showing that perfect annotator
		information is useful for prediction, and there is
		incentive for the model to capture this information
	</Extractive Summary>
</Paper ID=ument107>


<Paper ID=ument107> <Table ID =3>
	<Abstractive Summary> =
		5%
		Table 3: Performance difference (p
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the mean and standard de-
		viation of performance difference (p
	</Extractive Summary>
</Paper ID=ument107>


<Paper ID=ument1070> <Table ID =1>
	<Abstractive Summary> =
		html
		89
		Term
		# of tweets
		# of tweets localized in US
		#dinner
		5,455,890
		1,367,745
		#breakfast
		5,125,014
		1,183,462
		#lunch
		4,969,679
		1,094,681
		#brunch
		1,910,950
		681,978
		#snack
		797,676
		220,697
		#meal
		495,073
		101,976
		#supper
		124,979
		22,154
		Total
		24,493,223
		4,362,940
		Table 1: Seven meal-related hashtags and their corre-
		sponding number of tweets ﬁltered from Twitter
	</Abstractive Summary>
	<Extractive Summary> =
		Tweets have
		been ﬁltered by a set of seven hashtags to make the
		dataset more relevant to food (see distribution in
		Table 1)
	</Extractive Summary>
</Paper ID=ument1070>


<Paper ID=ument1070> <Table ID =2>
	<Abstractive Summary> =
		18
		Table 2: Correlation between socioeconomic factors
		(education, income, poverty) and type 2 diabetes mel-
		litus (T2DM) in 2017
	</Abstractive Summary>
	<Extractive Summary> =
		Even though in general, the cor-
		relations are relatively low (see Table 2), we will
		show that the model strongly beneﬁts from the ad-
		ditional information leading to accuracy increases
		of 2–6% (see Section 5)
	</Extractive Summary>
</Paper ID=ument1070>


<Paper ID=ument1070> <Table ID =3>
	<Abstractive Summary> =
		05
		Table 3: Results from using various feature sets to pre-
		dict state-level characteristics: whether a given state
		is above or below the national median for diabetes,
		poverty, income, and education
	</Abstractive Summary>
	<Extractive Summary> =
		1
		State-level characteristics prediction
		In Table 3, we show the results for predict-
		ing state-level socioeconomic characteristics using
		various sets of features
	</Extractive Summary>
	<Extractive Summary> =
		(2014) relies on
		hash-tags (see Table 3, lines 5 and 7) and the worst
		performing model use food words (lines 9 and 11)
	</Extractive Summary>
</Paper ID=ument1070>


<Paper ID=ument1070> <Table ID =4>
	<Abstractive Summary> =
		59
		Table 4: Results for predicting T2DM rate using a ran-
		dom forest classiﬁer with our additional socioeconomic
		correlation features
	</Abstractive Summary>
	<Extractive Summary> =
		The best multiplier for each model, according to
		SVM performance, is used in our Random Forest
		models (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		ﬁer (Table 5), but when using a RF classiﬁer we
		see up to 6% improvement (Table 4)
	</Extractive Summary>
</Paper ID=ument1070>


<Paper ID=ument1070> <Table ID =5>
	<Abstractive Summary> =
		63
		Table 5: Results for predicting T2DM rate using our
		SVM classiﬁer, which is similar to that of Fried et al
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Impact of socioeconomic factors
		In Table 5, we show the SVM results for pre-
		dicting T2DM rate from extending the feature ma-
		trix from 5
	</Extractive Summary>
	<Extractive Summary> =
		For each feature set, we use the
		best performing multiplier, as determined in the previ-
		ous experiment that used a SVM classiﬁer (Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		ﬁer (Table 5), but when using a RF classiﬁer we
		see up to 6% improvement (Table 4)
	</Extractive Summary>
</Paper ID=ument1070>


<Paper ID=ument1070> <Table ID =6>
	<Abstractive Summary> =
		58
		Table 6: The Cohen’s kappa inter-annotator agreement
		scores among the three annotators
	</Abstractive Summary>
</Paper ID=ument1070>


<Paper ID=ument1070> <Table ID =7>
	<Abstractive Summary> =
		low-carb diets
		woman, vegetarian diet
		mint, saffron, fennel, squash,
		soup, tomato, eggplant
		man, vegetarian diet
		beet, onion, coconut, spinach,
		kale, carrot
		woman, low-carb diet
		hazelnut, nut, cupcake, pastry,
		grain, caramel, wheat
		man, low-carb diet
		cereal, spaghetti, buns, ham-
		burger, pepperoni, crunch
		Table 7: The 4 corners in the man vs
	</Abstractive Summary>
	<Extractive Summary> =
		We list the 4 corners in the projection as 4 rows
		in Table 7, where the left column corresponds to
		the concepts and the right column contains the
		words
	</Extractive Summary>
</Paper ID=ument1070>


<Paper ID=ument1071> <Table ID =1>
	<Abstractive Summary> =
		com/praw-dev/praw
		99
		Domain
		Subreddit Name
		Total Posts
		Avg Tokens/Post
		Labeled Segments
		abuse
		r/domesticviolence
		1,529
		365
		388
		r/survivorsofabuse
		1,372
		444
		315
		Total
		2,901
		402
		703
		anxiety
		r/anxiety
		58,130
		193
		650
		r/stress
		1,078
		107
		78
		Total
		59,208
		191
		728
		ﬁnancial
		r/almosthomeless
		547
		261
		99
		r/assistance
		9,243
		209
		355
		r/food pantry
		343
		187
		43
		r/homeless
		2,384
		143
		220
		Total
		12,517
		198
		717
		PTSD
		r/ptsd
		4,910
		265
		711
		social
		r/relationships
		107,908
		578
		694
		All
		187,444
		420
		3,553
		Table 1: Data Statistics
	</Abstractive Summary>
	<Extractive Summary> =
		We include ten subreddits in the ﬁve domains of
		abuse, social, anxiety, PTSD, and ﬁnancial, as de-
		tailed in Table 1, and our analysis focuses on the
		domain level
	</Extractive Summary>
</Paper ID=ument1071>


<Paper ID=ument1071> <Table ID =2>
	<Abstractive Summary> =
		ﬁnancial
		stress
		3/5 (60%)
		Table 2: Data Examples
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we provide examples of labeled seg-
		ments from the various domains in our dataset
	</Extractive Summary>
	<Extractive Summary> =
		For example, posters in these domains
		discuss topics such as symptoms, medical care,
		and diagnoses (Figure 1, Table 2)
	</Extractive Summary>
</Paper ID=ument1071>


<Paper ID=ument1071> <Table ID =3>
	<Abstractive Summary> =
		62%
		81%
		Table 3: LIWC Analysis by Domain
	</Abstractive Summary>
</Paper ID=ument1071>


<Paper ID=ument1071> <Table ID =4>
	<Abstractive Summary> =
		15%
		Table 4: LIWC Analysis by Label
	</Abstractive Summary>
</Paper ID=ument1071>


<Paper ID=ument1071> <Table ID =5>
	<Abstractive Summary> =
		01
		Table 5: Complexity by Label
	</Abstractive Summary>
	<Extractive Summary> =
		These scores are
		comparable for all splits of our data; however, as
		shown in Table 5, we do see non-signiﬁcant but
		persistent differences between stressful and non-
		stressful data, with stressful data being generally
		longer and more complex but also rated simpler
		by readability indices
	</Extractive Summary>
</Paper ID=ument1071>


<Paper ID=ument1071> <Table ID =6>
	<Abstractive Summary> =
		8065
		Table 6: Supervised Results
	</Abstractive Summary>
</Paper ID=ument1071>


<Paper ID=ument1071> <Table ID =7>
	<Abstractive Summary> =
		48
		Table 7: Feature Sets and Data Sets
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 7, we examine the impact of differ-
		ent feature sets and levels of annotator agree-
		ment on our logistic regressor with domain-
		speciﬁc Word2Vec embeddings and ﬁnd consis-
		tent patterns supporting this model
	</Extractive Summary>
</Paper ID=ument1071>


<Paper ID=ument1071> <Table ID =8>
	<Abstractive Summary> =
		Both models seem to have trouble with less ex-
		plicit expressions of stress, framing negative ex-
		105
		Gold
		0
		1
		LogReg
		0
		241
		105
		1
		49
		320
		Gold
		0
		1
		BERT
		0
		240
		106
		1
		48
		321
		BERT
		0
		1
		LogReg
		0
		237
		51
		1
		53
		374
		Table 8: Confusion Matrices
	</Abstractive Summary>
	<Extractive Summary> =
		Although the dataset is
		nearly balanced, both BERT-base and our best lo-
		gistic regression model greatly overclassify stress,
		as shown in Table 8, and they broadly overlap but
		do differ in their predictions (disagreeing with one
		another on approximately 100 instances)
	</Extractive Summary>
</Paper ID=ument1071>


<Paper ID=ument1071> <Table ID =9>
	<Abstractive Summary> =
		Not Stress
		100%
		domesticviolence
		LogReg
		Table 9: Error Analysis Examples
	</Abstractive Summary>
</Paper ID=ument1071>


<Paper ID=ument1072> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example of the GPT-2 n = 0 model generating a false negative conclusion (Varenicline did reduce
		general craving), while the GPT-2 n = 1 model generated a better true negative one
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 illustrates
		110
		Source:
		(BACKGROUND) Varenicline is believed to work , in part , by reducing craving responses to smoking cues and by
		reducing general levels of craving ; however , these hypotheses have never been evaluated with craving assessed in the
		natural environments of treatment-seeking smokers
	</Extractive Summary>
</Paper ID=ument1072>


<Paper ID=ument1072> <Table ID =2>
	<Abstractive Summary> =
		71
		Table 2: ROUGE scores of the PGNet baseline models
		and the GPT-2 ﬁne-tuned models on the development
		set
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Automatic Evaluation
		Table 2 shows the best validation ROUGE scores
		of baselines and our models
	</Extractive Summary>
</Paper ID=ument1072>


<Paper ID=ument1072> <Table ID =3>
	<Abstractive Summary> =
		PGnet n = 1
		15
		3
		5
		3
		24
		36%
		GPT-2 n = 0
		24
		3
		4
		5
		14
		54%
		GPT-2 n = 1
		26
		6
		5
		3
		10
		64%
		Target
		32
		11
		0
		0
		7
		86%
		Table 3: Human evaluation results for text understand-
		ing on the annotation questions of 50 randomly selected
		source documents
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the results from the annota-
		tion question, where the number of true posi-
		tive and true negative generations from the GPT-2
		ﬁne-tuned models increase when compared to the
		PGNet baseline
	</Extractive Summary>
</Paper ID=ument1072>


<Paper ID=ument1072> <Table ID =4>
	<Abstractive Summary> =
		98
		Table 4: Human evaluation results for generation qual-
		ity on the rating questions
	</Abstractive Summary>
	<Extractive Summary> =
		In the rating questions depicted in Table 4, the
		human written conclusions obtain a score nearly
		4 out of 5 on all three dimensions
	</Extractive Summary>
</Paper ID=ument1072>


<Paper ID=ument1072> <Table ID =5>
	<Abstractive Summary> =
		Table 5: An example of GPT-2 models generating true positive conclusions
	</Abstractive Summary>
</Paper ID=ument1072>


<Paper ID=ument1072> <Table ID =6>
	<Abstractive Summary> =
		Table 6: An example of GPT-2 n = 0 model generating a true negative conclusion, while the GPT-2 n = 1 model
		generated a false positive one
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows a false positive
		example by the GPT-2 n = 1 model
	</Extractive Summary>
</Paper ID=ument1072>


<Paper ID=ument1072> <Table ID =7>
	<Abstractive Summary> =
		Table 7: An example of GPT-2 n = 0 model generating a false negative conclusion, while the GPT-2 n = 1 model
		generated a true positive one
	</Abstractive Summary>
	<Extractive Summary> =
		The generated conclusions in Table 7 is also much
		shorter than the target conclusion written by hu-
		man
	</Extractive Summary>
</Paper ID=ument1072>


<Paper ID=ument1073> <Table ID =1>
	<Abstractive Summary> =
		1
		Data
		Two different data sets for de-identiﬁcation were
		used:
		Stockholm EPR PHI Psuedo Corpus
		(Pseudo) as well as the Stockholm EPR PHI Cor-
		120
		Table 1: Results from (Dalianis and Velupillai, 2010) using the Stanford CRF
	</Abstractive Summary>
</Paper ID=ument1073>


<Paper ID=ument1073> <Table ID =2>
	<Abstractive Summary> =
		PHI classes
		Pseudo Unique
		Real
		Unique
		First Name
		885
		24 %
		938
		79 %
		Last Name
		911
		15 %
		957
		86 %
		Age
		51
		80 %
		64
		97 %
		Phone Number
		310
		78 %
		327
		92 %
		Location
		159
		94 %
		229
		84 %
		Full Date
		726
		25 %
		770
		89 %
		Date Part
		1897
		6 %
		2079
		72 %
		Health Care Unit
		1278
		13 %
		2277
		73 %
		Total PHI instances
		6217
		20 %
		7647
		78 %
		Table 2: The distribution of PHI instances between the
		the Stockholm EPR PHI Psuedo Corpus, ’Pseudo’, and
		the Stockholm EPR PHI Corpus, ’Real’ based on the
		number of tokens
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2, it can be observed that the
		distribution of PHI instances between the two data
		sets is somewhat similar, but there is a signiﬁcant
		difference when it comes to unique instances be-
		tween the two data sets
	</Extractive Summary>
</Paper ID=ument1073>


<Paper ID=ument1073> <Table ID =3>
	<Abstractive Summary> =
		54
		Table 3: Entity-based evaluation for CRF with ten fold cross-validation
	</Abstractive Summary>
</Paper ID=ument1073>


<Paper ID=ument1073> <Table ID =4>
	<Abstractive Summary> =
		70
		Table 4: Entity-based evaluation for LSTM with three fold cross-validation where 66% of the data were used for
		training and 33% for evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		2
		LSTM - Results
		The experimental results of the LSTM algorithm
		are shown in Table 4 and again, not presented in
		the table is the combination of training on real data
		and evaluation of pseudo data (Real-Pseudo)
	</Extractive Summary>
</Paper ID=ument1073>


<Paper ID=ument1074> <Table ID =1>
	<Abstractive Summary> =
		76
		Table 1: Content-point inter-annotator agreement (per-
		cent identity/kappa)
	</Abstractive Summary>
	<Extractive Summary> =
		The
		agreement scores are shown in Table 1 and 2
	</Extractive Summary>
</Paper ID=ument1074>


<Paper ID=ument1074> <Table ID =2>
	<Abstractive Summary> =
		76
		Table 2:
		Offset-level inter-annotator agreement
		(label/label-attribute)
	</Abstractive Summary>
</Paper ID=ument1074>


<Paper ID=ument1074> <Table ID =3>
	<Abstractive Summary> =
		If turned on, similarity features for n-grams
		130
		rubric item
		total
		correct
		incorrect missingitem
		incorrect contrary
		incorrect section
		cc frequent urination
		579
		535
		9
		35
		0
		checks blood sugar regularly 110s before meals
		152
		39
		86
		26
		1
		bp ﬂuctuates 150100
		365
		239
		91
		34
		1
		denies abdominal pain vomiting constipation
		270
		98
		169
		3
		0
		denies hematochezia
		211
		202
		1
		8
		0
		denies recent travel
		257
		245
		5
		4
		3
		duration of 1 week
		256
		221
		5
		30
		0
		feels loopy
		125
		110
		8
		6
		1
		feral cat in the house occasionally
		175
		56
		95
		22
		2
		frequent bm 3-4 times per day
		343
		249
		50
		44
		0
		frequent urination every 30 minutes
		347
		276
		31
		39
		1
		has not had ua
		190
		177
		7
		6
		0
		healthy diet
		178
		157
		21
		0
		0
		husband was sick with uti symptoms
		331
		257
		56
		18
		0
		hx of htn
		298
		254
		40
		4
		0
		initially thought she had respiratory infection
		204
		78
		33
		93
		0
		loose stools with mucous
		324
		205
		111
		8
		0
		losartan hctz every night with dinner
		325
		148
		154
		23
		0
		mild dysuria
		266
		185
		57
		24
		0
		no recent antibiotics
		279
		263
		10
		5
		1
		pe abdomen hyperactive bowel sounds at llq no pain with palp
		334
		97
		180
		51
		6
		pe cv normal
		315
		300
		10
		4
		1
		pe extremities no edema
		297
		282
		7
		2
		6
		pe heent normal no thyromegaly masses carotid bruit
		430
		173
		251
		4
		2
		pe resp normal
		331
		324
		6
		1
		0
		pe skin intact no clubbing cyanosis
		276
		84
		173
		6
		13
		plan advise brat diet
		312
		249
		42
		15
		6
		plan bp goal 13080
		155
		123
		11
		18
		3
		plan may notice leg swelling notify if unbearable
		216
		93
		114
		5
		4
		plan prescribed amlodipine 5mg
		268
		205
		40
		18
		5
		plan recommend 30 mins physical activity 4-5 times per week
		296
		128
		131
		34
		3
		plan reduce stress levels
		119
		100
		13
		1
		5
		plan rtn in 1 month with continued bp log
		280
		172
		85
		16
		7
		plan ua today to rule out infx
		302
		202
		91
		3
		6
		side effects of difﬁculty breathing with metoprolol
		164
		104
		38
		21
		1
		stress work related
		223
		215
		7
		1
		0
		takes blood pressure every morning
		222
		145
		66
		11
		0
		tried yogurt and turmeric no improvement
		176
		66
		106
		4
		0
		was seen by dr reynolds yesterday
		154
		44
		96
		13
		1
		weight normal
		61
		52
		5
		3
		1
		Table 3: Label frequencies for full corpus
		and umls concept grams (if the umls ﬂag is on) are
		also added
	</Abstractive Summary>
</Paper ID=ument1074>


<Paper ID=ument1074> <Table ID =4>
	<Abstractive Summary> =
		52
		Table 4: Feature-based system results for 5-fold cross-
		validation, varying tagsets
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Cross-validation and test results
		Table 4 shows performances for the feature-based
		system using different tagsets in cross-validation
	</Extractive Summary>
</Paper ID=ument1074>


<Paper ID=ument1075> <Table ID =1>
	<Abstractive Summary> =
		51
		Table 1: LIWC Dimension Analysis
		Function Words and Content Words
		Next, we
		looked at selected function words and grammat-
		ical differences, which can be split into two cat-
		egories called Function Words (see Table 2), re-
		ﬂecting how humans communicate and Content
		words (see Table 2), demonstrating what humans
		say (Tausczik and Pennebaker, 2010)
	</Abstractive Summary>
	<Extractive Summary> =
		Dimension Analysis
		Firstly, we looked at the
		word count and different dimensions of each
		dataset (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		69
		Table 2: LIWC Function and Content Words
		Affect Analysis
		The analysis of emotions in sui-
		cide notes and last statements has often been ad-
		dressed in research (Schoene and Dethlefs, 2018;
		Lester and Gunn III, 2013) The number of Affect
		words is highest in LS notes, whilst they are low-
		est in DL notes, this could be related to the emo-
		tional Tone of a note (see Table 1)
	</Extractive Summary>
</Paper ID=ument1075>


<Paper ID=ument1075> <Table ID =2>
	<Abstractive Summary> =
		69
		Table 2: LIWC Function and Content Words
		Affect Analysis
		The analysis of emotions in sui-
		cide notes and last statements has often been ad-
		dressed in research (Schoene and Dethlefs, 2018;
		Lester and Gunn III, 2013) The number of Affect
		words is highest in LS notes, whilst they are low-
		est in DL notes, this could be related to the emo-
		tional Tone of a note (see Table 1)
	</Abstractive Summary>
	<Extractive Summary> =
		51
		Table 1: LIWC Dimension Analysis
		Function Words and Content Words
		Next, we
		looked at selected function words and grammat-
		ical differences, which can be split into two cat-
		egories called Function Words (see Table 2), re-
		ﬂecting how humans communicate and Content
		words (see Table 2), demonstrating what humans
		say (Tausczik and Pennebaker, 2010)
	</Extractive Summary>
</Paper ID=ument1075>


<Paper ID=ument1075> <Table ID =3>
	<Abstractive Summary> =
		53
		Table 3: LIWC Affect Analysis
		Social
		and
		Psychological
		Processes
		Social
		Processes highlights the social relationships of
		note writers, where it can be seen in Table 4 that
		the highest amount of social processes can be
		found in LS and the lowest in DL
	</Abstractive Summary>
</Paper ID=ument1075>


<Paper ID=ument1075> <Table ID =4>
	<Abstractive Summary> =
		73
		Table 4: LIWC Social Processes
		The term Cognitive processes encompasses a
		number of different aspects, where we have found
		that the highest amount of cognitive processes was
		in DL notes and the lowest in LS notes
	</Abstractive Summary>
	<Extractive Summary> =
		53
		Table 3: LIWC Affect Analysis
		Social
		and
		Psychological
		Processes
		Social
		Processes highlights the social relationships of
		note writers, where it can be seen in Table 4 that
		the highest amount of social processes can be
		found in LS and the lowest in DL
	</Extractive Summary>
</Paper ID=ument1075>


<Paper ID=ument1075> <Table ID =5>
	<Abstractive Summary> =
		23
		Table 5: LIWC Psychological Processes
		some of the most common reasons for a person to
		commit suicide
	</Abstractive Summary>
</Paper ID=ument1075>


<Paper ID=ument1075> <Table ID =6>
	<Abstractive Summary> =
		64
		Table 6: LIWC Personal Concerns
		Time Orientation and Relativity
		Looking at
		the Time Orientation of a note can give interest-
		ing insight into the temporal focus of attention and
		differences in verb tenses can show psychological
		distance or to which extend disclosed events have
		been processed (Tausczik and Pennebaker, 2010)
	</Abstractive Summary>
</Paper ID=ument1075>


<Paper ID=ument1075> <Table ID =7>
	<Abstractive Summary> =
		51
		Table 7: LIWC Time orientation
		Overall it was noted that for most analysis GSN
		falls between the two extremes of LS and DL
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows that the focus of LS letters is pri-
		marily in the past whilst GSN and DL letters fo-
		cus on the present
	</Extractive Summary>
</Paper ID=ument1075>


<Paper ID=ument1075> <Table ID =8>
	<Abstractive Summary> =
		87
		Table 8: Test accuracy and F1-score of different learning models in %
		Figure 4: Example of LS note correctly classiﬁed
	</Abstractive Summary>
	<Extractive Summary> =
		It can
		be seen in Table 8 that the dilated LSTM with an
		attention layer outperforms the BiLSTM with At-
		tention by 5
	</Extractive Summary>
</Paper ID=ument1075>


<Paper ID=ument1076> <Table ID =1>
	<Abstractive Summary> =
		As methods for
		automated analysis of large-scale data sets have
		improved, more studies have investigated lexical
		and semantic characteristics, such as usage pat-
		terns of different verbs and semantic categories
		147
		Type
		Docs
		Lines
		Tokens
		Matches
		Concepts
		High Conﬁdence
		Concepts
		Consistency (%)
		Case Management
		967
		20,106
		165,608
		45,306
		557
		111
		75
		Consult
		98
		15,514
		96,515
		26,109
		812
		0
		–
		Discharge Summary
		59,652
		14,480,154
		104,027,364
		30,840,589
		6,381
		1,599
		67
		ECG
		209,051
		1,022,023
		7,307,381
		2,163,682
		540
		14
		56
		Echo
		45,794
		2,892,069
		19,752,879
		6,070,772
		1,233
		157
		65
		General
		8,301
		307,330
		2,191,618
		552,789
		2,559
		0
		–
		Nursing
		223,586
		9,839,274
		73,426,426
		18,903,892
		4,912
		2
		58
		Nursing/Other
		822,497
		10,839,123
		140,164,545
		31,135,584
		5,049
		83
		60
		Nutrition
		9,418
		868,102
		3,843,963
		1,147,918
		1,911
		198
		73
		Pharmacy
		103
		4,887
		39,163
		8,935
		376
		0
		–
		Physician
		141,624
		26,659,749
		148,306,543
		39,239,425
		5,538
		122
		57
		Radiology
		522,279
		17,811,429
		211,901,548
		34,433,338
		4,126
		599
		63
		Rehab Services
		5,431
		585,779
		2,936,022
		869,485
		2,239
		9
		62
		Respiratory
		31,739
		1,323,495
		6,358,924
		2,255,725
		1,039
		5
		63
		Social Work
		2,670
		100,124
		930,674
		195,417
		1,282
		0
		–
		Table 1: Document type subcorpora in MIMIC-III
	</Abstractive Summary>
	<Extractive Summary> =
		retaining only those with a self-consistency of
		at least 50%; Table 1 includes the number of
		high-conﬁdence concepts identiﬁed and the mean
		consistency among this subset
	</Extractive Summary>
</Paper ID=ument1076>


<Paper ID=ument1076> <Table ID =2>
	<Abstractive Summary> =
		As shown in
		Figure 3, this yields considerably smaller concept
		151
		Query
		Discharge Summary
		Nursing/Other
		Radiology
		Diabetes Mellitus
		(C0011849)
		Diabetes (C0011847)
		Gestational
		Diabetes
		(C0085207)
		Poorly
		controlled
		(C3853134)
		Type 2 (C0441730)
		A2 immunologic symbol
		(C1443036)
		Insulin (C0021641)
		Type 1 (C0441729)
		Diabetes Mellitus, Insulin-
		Dependent (C0011854)
		Diabetes Mellitus, Insulin-
		Dependent (C0011854)
		Gestational
		Diabetes
		(C0085207)
		Factor V (C0015498)
		Diabetes
		Mellitus,
		Non-Insulin-Dependent
		(C0011860)
		Diabetes Mellitus, Insulin-
		Dependent (C0011854)
		A1 immunologic symbol
		(C1443035)
		Stage level 5 (C0441777)
		Discharge Summary
		Echo
		Radiology
		Mental state
		(C0278060)†
		Coherent (C4068804)
		Donor:Type:Point
		in
		time:ˆPatient:Nominal
		(C3263710)
		Mental
		status
		changes
		(C0856054)
		Confusion (C0009676)
		Donor person (C0013018)
		Abnormal
		mental
		state
		(C0278061)
		Respiratory
		status:-
		:Point
		in
		time:ˆPatient:-
		(C2598168)
		Respiratory
		arrest
		(C0162297)
		Level
		of
		consciousness
		(C0234425)
		Respiratory
		status
		(C1998827)
		Organ
		donor:Type:Point
		in
		time:ˆDonor:Nominal
		(C1716004)
		Level
		of
		conscious-
		ness:Find:Pt:ˆPatient:Ord
		(C4050479)
		Abnormal
		mental
		state
		(C0278061)
		Swallowing
		G-code
		(C4281783)
		Mississippi
		(state)
		(C0026221)
		Table 2: 5 nearest neighbor concepts to Diabetes Mellitus and Mental state from 3 high-conﬁdence document
		types, averaging cosine similarities across all replicate embedding sets within each document type
	</Abstractive Summary>
	<Extractive Summary> =
		Diabetes Mellitus (C0011849) Diabetes Melli-
		tus (search strings: “diabetes mellitus” and “dia-
		betes mellitus dm”) was high-conﬁdence in Dis-
		charge Summary, Nursing/Other, and Radiology
		document types; Table 2 gives the top 5 neighbors
		from each type
	</Extractive Summary>
	<Extractive Summary> =
		For brevity, its nearest
		neighbors are omitted from Table 2, as there
		is little variation among the top 5
	</Extractive Summary>
</Paper ID=ument1076>


<Paper ID=ument1076> <Table ID =3>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents surface form neighbors identiﬁed
		153
		Query
		Discharge Summary
		Nutrition
		Case Management
		Community (C0009462)
		Community
		Dilute
		Substance
		Health center
		Social work
		Monitoring
		Acquired
		Surgical site
		Somewhat
		Residence
		In situ
		Hearing
		Nursing facility
		Nephritis
		Speech
		Discharge Summary
		Echo
		ECG
		ECG (C0013798)
		ECG
		ECG
		ECG
		EKG
		Exercise
		Physician
		Sinus tachycardia
		Stress
		Last
		Sinus bradycardia
		Fair
		No change
		Right bundle branch block
		Speciﬁc
		Abnormal
		Discharge Summary
		Echo
		Radiology
		Blood pressure (C0005823)
		Blood pressure
		Blood pressure
		Blood pressure
		Heart rate
		Heart rate
		Heart rate
		Pressure
		Rate
		Rate
		Systolic blood pressure
		Exercise
		Method
		Rate
		Stress
		Exercise
		Table 3: 5 nearest neighbor surface forms to three frequent clinical concepts, across document types for which they
		are high-conﬁdence
	</Extractive Summary>
</Paper ID=ument1076>


<Paper ID=ument1077> <Table ID =1>
	<Abstractive Summary> =
		The
		semantic content of the documents is also differ-
		ent, with the discharge summaries focused exclu-
		sively on a single patient and their history, dis-
		ease progression, treatment, and outcomes, while
		the MedMentions abstracts typically summarize
		160
		i2b2 2010
		MedMentions (full)
		MedMentions (st21pv)
		Train
		Test
		Train
		Test
		Train
		Test
		# entity types
		3
		3
		126
		123
		21
		21
		# documents
		170
		256
		3513
		879
		3513
		879
		# tokens
		149,743
		267,837
		936,247
		234,910
		936,247
		234,910
		# entities
		16,520
		31,161
		281,719
		70,305
		162,908
		40,101
		Table 1: Properties of the datasets
	</Abstractive Summary>
</Paper ID=ument1077>


<Paper ID=ument1077> <Table ID =2>
	<Abstractive Summary> =
		60
		Table 2: Results of entity recognition for each dataset and model
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 2 shows the results of the entity recognition
		experiments for each model and dataset
	</Extractive Summary>
</Paper ID=ument1077>


<Paper ID=ument1077> <Table ID =3>
	<Abstractive Summary> =
		64*
		Table 3: Results of entity recognition using concatenated BERT models
	</Abstractive Summary>
</Paper ID=ument1077>


<Paper ID=ument1078> <Table ID =1>
	<Abstractive Summary> =
		169
		Dataset
		# Documents
		# Unique patients
		# ICD-9 Codes
		# Unique ICD-9 codes
		Training
		47,719
		36,997
		758,212
		8,692
		Development
		1,631
		1,374
		28,896
		3,012
		Test
		3,372
		2,755
		61,578
		4,085
		Total
		52,722
		41,126
		848,686
		8,929
		Table 1: Distribution of documents and codes in the MIMIC-III dataset
	</Abstractive Summary>
	<Extractive Summary> =
		(2018) (see Table 1)
	</Extractive Summary>
</Paper ID=ument1078>


<Paper ID=ument1078> <Table ID =2>
	<Abstractive Summary> =
		681
		Table 2: Benchmark results for the models trained with F1macro stopping criterion
	</Abstractive Summary>
</Paper ID=ument1078>


<Paper ID=ument1078> <Table ID =3>
	<Abstractive Summary> =
		727
		Table 3: Benchmark results for the models trained with F1micro stopping criterion
	</Abstractive Summary>
	<Extractive Summary> =
		The results are presented in Table 3; our method
		achieves the highest F1micro score, as well as the
		highest P@8 score
	</Extractive Summary>
</Paper ID=ument1078>


<Paper ID=ument1078> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Ablation study of individual components of the ﬁnal method
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4 we present an ablation study
	</Extractive Summary>
</Paper ID=ument1078>


<Paper ID=ument1078> <Table ID =5>
	<Abstractive Summary> =
		686
		Table 5: Benchmark results for the models trained with F1macro stopping criterion
	</Abstractive Summary>
</Paper ID=ument1078>


<Paper ID=ument1079> <Table ID =1>
	<Abstractive Summary> =
		3
		Hyperparameter Settings
		Feature-based Embeddings
		For the aforemen-
		tioned set of experiments, the following architec-
		ture parameters have been considered:
		182
		Table 1: Performance of the negation scope detection task on BioScope and NegPar corpora using different ap-
		proaches
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		The performance of different approaches on Bio-
		Scope and NegPar corpora for the negation scope
		detection and the speculating scope detection are
		shown in Table 1 and Table 2, respectively
	</Extractive Summary>
</Paper ID=ument1079>


<Paper ID=ument1079> <Table ID =2>
	<Abstractive Summary> =
		This is sup-
		ported by the fact that the performance on clini-
		184
		Table 2: Performance of speculation scope detection task on BioScope corpus using different approaches
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		The performance of different approaches on Bio-
		Scope and NegPar corpora for the negation scope
		detection and the speculating scope detection are
		shown in Table 1 and Table 2, respectively
	</Extractive Summary>
</Paper ID=ument1079>


<Paper ID=ument1079> <Table ID =3>
	<Abstractive Summary> =
		We also compute the error overlap for the Neg-
		Par test set performance for the top 3 performing
		models: almost half of the ELMo errors and about
		3/4 of BERT ﬁne-tuned and BERT ﬁne-tuned with
		185
		Table 3: Performance on percentage of correct span on BioScope Abstracts sub-corpus trained under different
		schemes
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 3, removing punctua-
		tion affects all models’ behaviour similarly: model
		performance degrades by losing 2-3 percent of
		PCS on average
	</Extractive Summary>
</Paper ID=ument1079>


<Paper ID=ument108> <Table ID =1>
	<Abstractive Summary> =
		3
		0
		3448
		1x
		Table 1: Accuracy and speedup on the test datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Next, we show that our frame-
		work is generic and can incorporate with other
		different classiﬁers, such as BCN (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		On the other hand, test results are
		tabulated in Table 1 to focus attention primarily on accuracy
	</Extractive Summary>
</Paper ID=ument108>


<Paper ID=ument108> <Table ID =2>
	<Abstractive Summary> =
		Len
		SST
		2
		13,750
		6,920/872/1,821
		19
		IMDB
		2
		61,046
		21,143/3,857/25,000
		240
		AGNews 4
		60,088
		101,851/18,149/7,600
		43
		Yelp
		5
		1,001,485
		600k/50k/50k
		149
		Table 2: Dataset statistics
	</Abstractive Summary>
</Paper ID=ument108>


<Paper ID=ument108> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Examples of the WE selector output on AGNews
	</Abstractive Summary>
</Paper ID=ument108>


<Paper ID=ument1080> <Table ID =1>
	<Abstractive Summary> =
		html
		2
		Data type
		Dataset
		Track
		train
		dev
		test
		In-domain
		arabic padt (ar)
		T1
		6,075
		909
		680
		chinese gsd (zh)
		T1
		3,997
		500
		500
		english ewt (en)
		T1, T2
		12,543
		2,002
		2,077
		english gum (en)
		T1, T2
		2,914
		707
		778
		english lines (en)
		T1, T2
		2,738
		912
		914
		english partut (en)
		T1, T2
		1,781
		156
		153
		french gsd (fr)
		T1, T2
		14,450
		1,476
		416
		french partut (fr)
		T1, T2
		803
		107
		110
		french sequoia (fr)
		T1, T2
		2,231
		412
		456
		hindi hdtb (hi)
		T1
		13,304
		1,659
		1,684
		indonesian gsd (id)
		T1
		4,477
		559
		557
		japanese gsd (ja)
		T1
		7,133
		511
		551
		korean gsd (ko)
		T1
		4,400
		950
		989
		korean kaist (ko)
		T1
		23,010
		2,066
		2,287
		portuguese bosque (pt)
		T1
		8,328
		560
		477
		portuguese gsd (pt)
		T1
		9,664
		1,210
		1,204
		russian gsd (ru)
		T1
		3,850
		579
		601
		russian syntagrus (ru)
		T1
		48,814
		6,584
		6,491
		spanish ancora (es)
		T1, T2
		14,305
		1,654
		1,721
		spanish gsd (es)
		T1, T2
		14,187
		1,400
		426
		Out-of-domain
		english pud (en)
		T1, T2
		-
		-
		1,000
		japanese pud (ja)
		T1
		-
		-
		1,000
		russian pud (ru)
		T1
		-
		-
		1,000
		Automatically parsed
		english ewt-HIT (en)
		T1, T2
		-
		-
		1,795
		english pud-LAT (en)
		T1, T2
		-
		-
		1,032
		hindi hdtb-HIT (hi)
		T1
		-
		-
		1,675
		korean kaist-HIT (ko)
		T1
		-
		-
		2,287
		portuguese bosque-Sta (pt)
		T1
		-
		-
		471
		spanish ancora-HIT (es)
		T1, T2
		-
		-
		1,723
		Table 1: SR’19 dataset sizes for training, development and test sets (number of sentences)
		Our
		model uses the following attributes for graph at-
		21
		Table 1: Average performance on English dataset (dev)
		of shallow track with different hyperparameters*)(f)ar$
		galonfar
		triunf´o
		\1\2´o
		galonf´o
		Table 1: Regular expressions and substitutions for sim-
		ple past with nonces from Jabberwocky (Carroll, 1872)
		and Spanish translation El Fablistan´on (Pascual, 1977)81
		Table 1: Highest performing conﬁgurations for each language
		Table 1: Representations used in the transformation of the Universal Dependencies in CONLLU format in row 1
		to the sentence shown in row 664
		Table 1: Automatic Evaluation Results of the shallow
		track (T1) and the BLEU difference with the best sys-
		tem among other participants for each treebank5
		Table 1: BLEU, DIST and NIST scores of our approach in the original (non-tokenized) test sets
		64
		#
		Feature
		Feature description
		Embedding size
		1
		Token
		Lemma or stem of word form
		300
		2
		UPOS
		Universal part-of-speech tag
		10
		3
		XPOS
		Language-speciﬁc part-of-speech tag
		10
		4
		Deprel
		Universal dependency relation to the Head
		10
		5
		Head
		Head of the current word
		20
		6
		Index
		Word index
		20
		7
		Lin
		Relative linear order with respect to the governor
		20
		8
		Lin sign
		The sign of the Lin feature
		3
		All
		Concatenation of all features
		393
		Table 1: The 8 features used in our model with their corresponding embedding sizes
		ule uses the transformer architecture described
		in Vaswani et al9
		N/A
		Table 1: Morphological inflection results on the development set
		Table 1: Ablation study: results of models on the
		MSR’19 validation set of UD English EWT (enewt-
		ud-dev) corpus42
		Table 1: The ofﬁcial scores of the DipInfoUniTo system for English, French and Chinese datasets, in terms of the
		automatic metrics BLUE, NIST, and DIST72
		933
		zh
		NA
		0
		0
		Table 1: Accuracy of the morphological realisation
		component08
		Table 1: Statistics of all cuts performed in the AMR
		Corpus
		4
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 gives an overview of the variety
		and sizes of the datasets
	</Extractive Summary>
	<Extractive Summary> =
		3
		Correlation of metrics with human
		assessment
		Table 10 shows the Pearson correlation of BLEU,
		NIST and DIST scores with human assessment for
		systems in tasks for which we ran human evalua-
		tions this year
	</Extractive Summary>
	<Extractive Summary> =
		978
		Table 10: Pearson correlation of BLEU, NIST and DIST scores with human assessment of Readability (left) and
		Meaning Similarity (right)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows that given local neighborhood in-
		formation, the model learns more useful graph em-
		beddings than without
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows this process for a sample of
		simple past forms
	</Extractive Summary>
	<Extractive Summary> =
		This improves the perfor-
		mance in most, but not all languages (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 lists the best conﬁguration
		Input: <L> f a m i l y </L> <P> UPOS=
		�→ NOUN XPOS=NNS </P> <T> Number=
		�→ Plur original_id=2 </T>
		Output: f a m i l i e s
		Figure 7: Example input and output of the inﬂection
		component
	</Extractive Summary>
	<Extractive Summary> =
		The UD structures are provided in tab sep-
		arated ﬁles in a well-deﬁned format (see row 1 of
		Table 1 for a small example)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Representations
		Table 1 illustrates the transformation steps be-
		tween an input UD (row 1) and an English sen-
		tence using a short example from the train set
		(en_ewt-ud-train
	</Extractive Summary>
	<Extractive Summary> =
		Row 5 of Table 1 is the JSREALB structure that
		is realized as the bottom part of the table
	</Extractive Summary>
	<Extractive Summary> =
		4
		Evaluation
		The automatic evaluation results of our submis-
		sion to the shared task are shown in Table 1 and
		Table 2 for the shallow and deep tracks, respec-
		tively
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 lists the 8 features
		used by our model: 6 features of the UD structure,
		in addition to 2 features for Lin (the Lin feature
		divided into its absolute value and its sign)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 indicates the embedding size
		of each feature
	</Extractive Summary>
	<Extractive Summary> =
		We suspected that the Head and Index fea-
		tures (see Table 1) constituted valuable informa-
		tion regarding the dependency tree structure of the
		sentences
	</Extractive Summary>
	<Extractive Summary> =
		, yt − 1}, st, ct)
		(4)
		ci =
		Tx
		∑
		j=1
		αijhj
		(5)
		αij =
		exp(eij)
		∑Tx
		k=1 exp(eik)
		(6)
		eij = a(si−1, hj)
		(7)
		As seen in Table 1, uncased results are al-
		most always higher than cased
	</Extractive Summary>
	<Extractive Summary> =
		1
		Encoder Model Selection
		As indicated in Table 1, we compare R-GCN with
		different encoders
	</Extractive Summary>
	<Extractive Summary> =
		In Table 1 the ofﬁcial scores of the DipIn-
		foUniTo system for English, French and Chinese
		datasets are reported, computed in terms of the au-
		tomatic metrics BLUE, NIST, and DIST
	</Extractive Summary>
	<Extractive Summary> =
		Table 1, second column)
	</Extractive Summary>
	<Extractive Summary> =
		We also calculated a number of lemmas, which
		can have different word forms, given the same set
		of POS and morphological features (Table 1, third
		column)
	</Extractive Summary>
	<Extractive Summary> =
		In general, high accu-
		racies of MR alone (word level, Table 1) do not
		guarantee good performance while evaluating on
		the sentence level
	</Extractive Summary>
	<Extractive Summary> =
		Table 1) and
		contractions (We did not develop a module
		for handling contractions in Arabic
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		shows the mean and standard deviation (std)
		of each cut for each dataset (for quality mea-
		sure)
	</Extractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1080> <Table ID =2>
	<Abstractive Summary> =
		, A6INV
		nth inverted argument of a predicate
		the ball→ fall
		AM/AMINV
		(i) none of governor or dependent are argument of the other
		fall→ last night
		(ii) unknown argument slot
		LIST
		List of elements
		fall→ [and] bounce
		NAME
		Part of a name
		Tower→ Eiffel
		DEP
		Undeﬁned dependent
		N/A
		Table 2: Deep labelscom/wenchaodudu/MSR
		Table 2:
		Comparison between different learning
		paradigms on English dev sets3
		Table 2: The number of lemmas in each test corpus
		showing (1) the number and percentage for which mor-
		phological features were given and (2) the number and
		percentage of wordforms generated via regex11
		Table 2: Macro-average of the top four systems across
		all datasets
		ception of one Russian and two Korean datasets
		where we are outperformed by only one system
		(IMS)html
		44
		UD
		JSREALB
		[noun:"mother",number:sing]
		n("mother")*n("s")
		[verb:"be",mood:ind,number:sing,person:3,
		v("be")*t("p")*pe(3)
		tense:pres,verbform:fin]
		[pron:"we",case:nom,number:plur,person:1,
		pro("I")*n("p")*pe(1)
		prontype:prs]
		Table 2: Some examples of mapping between UD features and JSREALB options
		it would be preferable to do a more aggressive
		lemmatization to lower the number of base forms
		in order to help further NLP processing that is of-
		ten dependent on the number of different types68
		Table 2: Automatic Evaluation Results of the deep
		track (T2) and the BLEU difference with the best sys-
		tem among other participants for each treebank53
		Table 2: Results of our submission in the shallow track task of SR’19
		done on the remaining 30 epochs902
		Table 2: Dev set BLEU scores (calculated along
		with non-terminals), using gold inflected forms
		In the table, the BLEU scores are calculated
		with the non-terminals included in both input
		and output sequences, inflating them some-
		what relative to regular BLEU scores
		Table 2: Comparison of outputs from systems with encoder variants given the graph in Figure 1 as input33
		Table 2: The MR module applied to the gold word or-
		dering input
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the number and rate of lemmas at
		which morphological features are listed, as well
		as how many of those were generated by the regex
		pattern-substitution methodology
	</Extractive Summary>
	<Extractive Summary> =
		4
		Evaluation
		The automatic evaluation results of our submis-
		sion to the shared task are shown in Table 1 and
		Table 2 for the shallow and deep tracks, respec-
		tively
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the results of our model on
		the test data
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2,
		our
		model achieved its highest performance on the
		en ewt-ud-test dataset with a BLEU score
		of 22
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows a com-
		parison of the different models that we tried
	</Extractive Summary>
	<Extractive Summary> =
		One of such examples
		is shown in Table 2, where we observe ﬂuency im-
		provements going from BiLSTM encoder to GCN,
		and ﬁnally to R-GCN, where an overall improve-
		ment in ﬂuency is conspicuous
	</Extractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1080> <Table ID =3>
	<Abstractive Summary> =
		93
		Table 3: The 6 combinations of dataset and parser out-
		puts selected for the automatically parsed test set It seems that the morphol-
		ogy of English and Spanish are relatively easy,
		22
		Table 3: Final performance on dev sets after word inﬂections
		Table 3: The T2 dependency given at the top is parsed into the nested list structure shown in the second line; the id
		and original id features are ignored as they are given for easing the training of learning algorithms29
		Table 3: Development results in the shallow track, including the linearization baselines8847
		Table 3: Preliminary BLEU scores for constrained decoding (calculated along with non-terminals), using
		gold inflected forms
		Dataset
		w/o reranking
		w/ reranking
		en_ewt
		02
		(*) denotes our submission to the shared task, which doesn’t use morphological features
		Table 3: Evaluation of our submissions to MSR’19 Deep Task across all corpora on both Test and Dev sets23
		Table 3: WO component performance on the develop-
		ment set
	</Abstractive Summary>
	<Extractive Summary> =
		, 2019)
		had the best scores for the remaining two;12 see
		Table 3 for an overview
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the T2 structure for the exam-
		ple shown in Table 1
	</Extractive Summary>
	<Extractive Summary> =
		We do not use any external resources for our
		system, except that we concatenate the training
		treebanks for some languages (see Table 3 and
		4)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Pipeline Performance
		Table 3 and 4 show the results on the develop-
		ment sets of the in-domain treebanks for the shal-
		low track and deep track, respectively
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, columns 1-4 are the BLEU scores of
		linearization evaluated on the lemmata, column 5
		is the accuracy of inﬂection, column 6 is BLEU
		score on the contracted word form (empty cells
		means contraction is not applied), column 7 is the
		ﬁnal BLEU score of the full pipeline
	</Extractive Summary>
	<Extractive Summary> =
		Similar to Table 3, the models
		marked with + are trained with concatenated tree-
		banks
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 compares the constrained and un-
		constrained versions of the Seq2Seq-Copy
		(again with non-terminals in the output and
		gold inflected forms)
	</Extractive Summary>
	<Extractive Summary> =
		1
		WO Evaluation
		Table 3 shows the results of WO
	</Extractive Summary>
	<Extractive Summary> =
		show a drop compared to the WO component per-
		formance (Table 3), which is consistent with the
		errors of the MR+CG module, described in Sec-
		tion 4
	</Extractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1080> <Table ID =4>
	<Abstractive Summary> =
		79
		Table 4: BLEU-4 scores for the 29 Shallow Track datasets
		–T1-NIST–
		ADA
		BME
		CLa
		CMU
		Dep
		Dip
		IMS
		LOR
		RAL
		OSU
		Til
		ar padt
		846
		Table 4: Automatic evaluation scores produced by the evaluation scripts of the SR’19 organizers on the train and
		dev sets70
		Table 4: Development results in the deep track7909
		Table 4: BLEU scores on dev sets before and after
		reranking, using gold inflected forms
		Test set
		BLEU
		NIST
		DIST
		en_ewt-ud-test
		6221
		Table 4: Automatic metrics on the development set
		(WO + MR)
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Training and development sets
		Table 4 shows the BLEU, NIST and DIST scores
		on the 2019 training and development sets for the
		four English corpora
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4, column 1 is the BLEU score on the
		lemmata of the given content words, columns 2
		is the BLEU score on the lemmata including gen-
		erated tokens, column 3 is the accuracy on word
		forms, column 4 is the BLEU score of contracted
		word forms, and column 5 is the BLEU score of
		the full pipeline
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the gains obtained by
		doing validity reranking (again with gold in-
		flected forms); here the scores shown are cal-
		culated without non-terminals
	</Extractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1080> <Table ID =5>
	<Abstractive Summary> =
		05
		Table 5: NIST scores for the 29 Shallow Track datasets
		9
		–T1-DIST–
		ADA
		BME
		CLa
		CMU
		Dep
		Dip
		IMS
		LOR
		RAL
		OSU
		Til
		ar padt
		4395
		Table 5: Automatic evaluation on the 2061 sentences of the 2018 test set compared with the scores obtained by
		systems participating in the 2018 shared task8
		Table 5: Test set results for English from the or-
		ganizers
		rors with the other test sets, errors with this
		particular set are much noisier
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Test sets
		Table 5 gives these scores for the test set used in
		the 2018 competition
	</Extractive Summary>
	<Extractive Summary> =
		Consistent with the human evaluation,
		the automatic scores for our system (Table 5)
		were also in the middle of the pack
	</Extractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1080> <Table ID =6>
	<Abstractive Summary> =
		32
		Table 6: DIST scores for the 29 Shallow Track datasets
		5
	</Abstractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1080> <Table ID =7>
	<Abstractive Summary> =
		57
		Table 7: BLEU-4, NIST and DIST scores for the 13 Deep Track datasets
		SynTagRus and Russian-PUD datasets both con-
		tain mostly news texts, so the structures to gener-
		ate are more similar; in this context, the impact of
		the change of domain becomes visible
	</Abstractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1080> <Table ID =8>
	<Abstractive Summary> =
		302
		625
		706
		LORIA
		Table 8: SR’19 human evaluation results for Meaning Similarity
	</Abstractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1080> <Table ID =9>
	<Abstractive Summary> =
		458
		506
		577
		LORIA
		Table 9: SR’19 human evaluation results for Readability
	</Abstractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1080> <Table ID =10>
	<Abstractive Summary> =
		978
		Table 10: Pearson correlation of BLEU, NIST and DIST scores with human assessment of Readability (left) and
		Meaning Similarity (right)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Correlation of metrics with human
		assessment
		Table 10 shows the Pearson correlation of BLEU,
		NIST and DIST scores with human assessment for
		systems in tasks for which we ran human evalua-
		tions this year
	</Extractive Summary>
</Paper ID=ument1080>


<Paper ID=ument1082> <Table ID =1>
	<Abstractive Summary> =
		html
		Data type
		Dataset
		Track
		train
		dev
		test
		In-domain
		arabic padt (ar)
		T1
		6,075
		909
		680
		chinese gsd (zh)
		T1
		3,997
		500
		500
		english ewt (en)
		T1, T2
		12,543
		2,002
		2,077
		english gum (en)
		T1, T2
		2,914
		707
		778
		english lines (en)
		T1, T2
		2,738
		912
		914
		english partut (en)
		T1, T2
		1,781
		156
		153
		french gsd (fr)
		T1, T2
		14,450
		1,476
		416
		french partut (fr)
		T1, T2
		803
		107
		110
		french sequoia (fr)
		T1, T2
		2,231
		412
		456
		hindi hdtb (hi)
		T1
		13,304
		1,659
		1,684
		indonesian gsd (id)
		T1
		4,477
		559
		557
		japanese gsd (ja)
		T1
		7,133
		511
		551
		korean gsd (ko)
		T1
		4,400
		950
		989
		korean kaist (ko)
		T1
		23,010
		2,066
		2,287
		portuguese bosque (pt)
		T1
		8,328
		560
		477
		portuguese gsd (pt)
		T1
		9,664
		1,210
		1,204
		russian gsd (ru)
		T1
		3,850
		579
		601
		russian syntagrus (ru)
		T1
		48,814
		6,584
		6,491
		spanish ancora (es)
		T1, T2
		14,305
		1,654
		1,721
		spanish gsd (es)
		T1, T2
		14,187
		1,400
		426
		Out-of-domain
		english pud (en)
		T1, T2
		-
		-
		1,000
		japanese pud (ja)
		T1
		-
		-
		1,000
		russian pud (ru)
		T1
		-
		-
		1,000
		Automatically parsed
		english ewt-HIT (en)
		T1, T2
		-
		-
		1,795
		english pud-LAT (en)
		T1, T2
		-
		-
		1,032
		hindi hdtb-HIT (hi)
		T1
		-
		-
		1,675
		korean kaist-HIT (ko)
		T1
		-
		-
		2,287
		portuguese bosque-Sta (pt)
		T1
		-
		-
		471
		spanish ancora-HIT (es)
		T1, T2
		-
		-
		1,723
		Table 1: SR’19 dataset sizes for training, development and test sets (number of sentences)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 gives an overview of the variety
		and sizes of the datasets
	</Extractive Summary>
	<Extractive Summary> =
		3
		Correlation of metrics with human
		assessment
		Table 10 shows the Pearson correlation of BLEU,
		NIST and DIST scores with human assessment for
		systems in tasks for which we ran human evalua-
		tions this year
	</Extractive Summary>
	<Extractive Summary> =
		978
		Table 10: Pearson correlation of BLEU, NIST and DIST scores with human assessment of Readability (left) and
		Meaning Similarity (right)
	</Extractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1082> <Table ID =2>
	<Abstractive Summary> =
		, A6INV
		nth inverted argument of a predicate
		the ball→ fall
		AM/AMINV
		(i) none of governor or dependent are argument of the other
		fall→ last night
		(ii) unknown argument slot
		LIST
		List of elements
		fall→ [and] bounce
		NAME
		Part of a name
		Tower→ Eiffel
		DEP
		Undeﬁned dependent
		N/A
		Table 2: Deep labels
	</Abstractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1082> <Table ID =3>
	<Abstractive Summary> =
		93
		Table 3: The 6 combinations of dataset and parser out-
		puts selected for the automatically parsed test set
	</Abstractive Summary>
	<Extractive Summary> =
		, 2019)
		had the best scores for the remaining two;12 see
		Table 3 for an overview
	</Extractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1082> <Table ID =4>
	<Abstractive Summary> =
		79
		Table 4: BLEU-4 scores for the 29 Shallow Track datasets
		–T1-NIST–
		ADA
		BME
		CLa
		CMU
		Dep
		Dip
		IMS
		LOR
		RAL
		OSU
		Til
		ar padt
		8
	</Abstractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1082> <Table ID =5>
	<Abstractive Summary> =
		05
		Table 5: NIST scores for the 29 Shallow Track datasets
		–T1-DIST–
		ADA
		BME
		CLa
		CMU
		Dep
		Dip
		IMS
		LOR
		RAL
		OSU
		Til
		ar padt
		43
	</Abstractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1082> <Table ID =6>
	<Abstractive Summary> =
		32
		Table 6: DIST scores for the 29 Shallow Track datasets
		5
	</Abstractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1082> <Table ID =7>
	<Abstractive Summary> =
		57
		Table 7: BLEU-4, NIST and DIST scores for the 13 Deep Track datasets
		SynTagRus and Russian-PUD datasets both con-
		tain mostly news texts, so the structures to gener-
		ate are more similar; in this context, the impact of
		the change of domain becomes visible
	</Abstractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1082> <Table ID =8>
	<Abstractive Summary> =
		302
		625
		706
		LORIA
		Table 8: SR’19 human evaluation results for Meaning Similarity
	</Abstractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1082> <Table ID =9>
	<Abstractive Summary> =
		458
		506
		577
		LORIA
		Table 9: SR’19 human evaluation results for Readability
	</Abstractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1082> <Table ID =10>
	<Abstractive Summary> =
		978
		Table 10: Pearson correlation of BLEU, NIST and DIST scores with human assessment of Readability (left) and
		Meaning Similarity (right)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Correlation of metrics with human
		assessment
		Table 10 shows the Pearson correlation of BLEU,
		NIST and DIST scores with human assessment for
		systems in tasks for which we ran human evalua-
		tions this year
	</Extractive Summary>
</Paper ID=ument1082>


<Paper ID=ument1083> <Table ID =1>
	<Abstractive Summary> =
		Our
		model uses the following attributes for graph at-
		22
		Table 1: Average performance on English dataset (dev)
		of shallow track with different hyperparameters
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that given local neighborhood in-
		formation, the model learns more useful graph em-
		beddings than without
	</Extractive Summary>
</Paper ID=ument1083>


<Paper ID=ument1083> <Table ID =2>
	<Abstractive Summary> =
		com/wenchaodudu/MSR
		Table 2:
		Comparison between different learning
		paradigms on English dev sets
	</Abstractive Summary>
</Paper ID=ument1083>


<Paper ID=ument1083> <Table ID =3>
	<Abstractive Summary> =
		It seems that the morphol-
		ogy of English and Spanish are relatively easy,
		23
		Table 3: Final performance on dev sets after word inﬂections
	</Abstractive Summary>
</Paper ID=ument1083>


<Paper ID=ument1084> <Table ID =1>
	<Abstractive Summary> =
		*)(f)ar$
		galonfar
		triunf´o
		\1\2´o
		galonf´o
		Table 1: Regular expressions and substitutions for sim-
		ple past with nonces from Jabberwocky (Carroll, 1872)
		and Spanish translation El Fablistan´on (Pascual, 1977)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows this process for a sample of
		simple past forms
	</Extractive Summary>
</Paper ID=ument1084>


<Paper ID=ument1084> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: The number of lemmas in each test corpus
		showing (1) the number and percentage for which mor-
		phological features were given and (2) the number and
		percentage of wordforms generated via regex
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the number and rate of lemmas at
		which morphological features are listed, as well
		as how many of those were generated by the regex
		pattern-substitution methodology
	</Extractive Summary>
</Paper ID=ument1084>


<Paper ID=ument1085> <Table ID =1>
	<Abstractive Summary> =
		81
		Table 1: Highest performing conﬁgurations for each language
	</Abstractive Summary>
	<Extractive Summary> =
		This improves the perfor-
		mance in most, but not all languages (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 lists the best conﬁguration
		Input: <L> f a m i l y </L> <P> UPOS=
		�→ NOUN XPOS=NNS </P> <T> Number=
		�→ Plur original_id=2 </T>
		Output: f a m i l i e s
		Figure 7: Example input and output of the inﬂection
		component
	</Extractive Summary>
</Paper ID=ument1085>


<Paper ID=ument1085> <Table ID =2>
	<Abstractive Summary> =
		11
		Table 2: Macro-average of the top four systems across
		all datasets
		ception of one Russian and two Korean datasets
		where we are outperformed by only one system
		(IMS)
	</Abstractive Summary>
</Paper ID=ument1085>


<Paper ID=ument1086> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Representations used in the transformation of the Universal Dependencies in CONLLU format in row 1
		to the sentence shown in row 6
	</Abstractive Summary>
	<Extractive Summary> =
		The UD structures are provided in tab sep-
		arated ﬁles in a well-deﬁned format (see row 1 of
		Table 1 for a small example)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Representations
		Table 1 illustrates the transformation steps be-
		tween an input UD (row 1) and an English sen-
		tence using a short example from the train set
		(en_ewt-ud-train
	</Extractive Summary>
	<Extractive Summary> =
		Row 5 of Table 1 is the JSREALB structure that
		is realized as the bottom part of the table
	</Extractive Summary>
</Paper ID=ument1086>


<Paper ID=ument1086> <Table ID =2>
	<Abstractive Summary> =
		html
		45
		UD
		JSREALB
		[noun:"mother",number:sing]
		n("mother")*n("s")
		[verb:"be",mood:ind,number:sing,person:3,
		v("be")*t("p")*pe(3)
		tense:pres,verbform:fin]
		[pron:"we",case:nom,number:plur,person:1,
		pro("I")*n("p")*pe(1)
		prontype:prs]
		Table 2: Some examples of mapping between UD features and JSREALB options
		it would be preferable to do a more aggressive
		lemmatization to lower the number of base forms
		in order to help further NLP processing that is of-
		ten dependent on the number of different types
	</Abstractive Summary>
</Paper ID=ument1086>


<Paper ID=ument1086> <Table ID =3>
	<Abstractive Summary> =
		Table 3: The T2 dependency given at the top is parsed into the nested list structure shown in the second line; the id
		and original id features are ignored as they are given for easing the training of learning algorithms
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the T2 structure for the exam-
		ple shown in Table 1
	</Extractive Summary>
</Paper ID=ument1086>


<Paper ID=ument1086> <Table ID =4>
	<Abstractive Summary> =
		46
		Table 4: Automatic evaluation scores produced by the evaluation scripts of the SR’19 organizers on the train and
		dev sets
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Training and development sets
		Table 4 shows the BLEU, NIST and DIST scores
		on the 2019 training and development sets for the
		four English corpora
	</Extractive Summary>
</Paper ID=ument1086>


<Paper ID=ument1086> <Table ID =5>
	<Abstractive Summary> =
		95
		Table 5: Automatic evaluation on the 2061 sentences of the 2018 test set compared with the scores obtained by
		systems participating in the 2018 shared task
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Test sets
		Table 5 gives these scores for the test set used in
		the 2018 competition
	</Extractive Summary>
</Paper ID=ument1086>


<Paper ID=ument1087> <Table ID =1>
	<Abstractive Summary> =
		64
		Table 1: Automatic Evaluation Results of the shallow
		track (T1) and the BLEU difference with the best sys-
		tem among other participants for each treebank
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Evaluation
		The automatic evaluation results of our submis-
		sion to the shared task are shown in Table 1 and
		Table 2 for the shallow and deep tracks, respec-
		tively
	</Extractive Summary>
</Paper ID=ument1087>


<Paper ID=ument1087> <Table ID =2>
	<Abstractive Summary> =
		68
		Table 2: Automatic Evaluation Results of the deep
		track (T2) and the BLEU difference with the best sys-
		tem among other participants for each treebank
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Evaluation
		The automatic evaluation results of our submis-
		sion to the shared task are shown in Table 1 and
		Table 2 for the shallow and deep tracks, respec-
		tively
	</Extractive Summary>
</Paper ID=ument1087>


<Paper ID=ument1087> <Table ID =3>
	<Abstractive Summary> =
		29
		Table 3: Development results in the shallow track, including the linearization baselines
	</Abstractive Summary>
	<Extractive Summary> =
		We do not use any external resources for our
		system, except that we concatenate the training
		treebanks for some languages (see Table 3 and
		4)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Pipeline Performance
		Table 3 and 4 show the results on the develop-
		ment sets of the in-domain treebanks for the shal-
		low track and deep track, respectively
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, columns 1-4 are the BLEU scores of
		linearization evaluated on the lemmata, column 5
		is the accuracy of inﬂection, column 6 is BLEU
		score on the contracted word form (empty cells
		means contraction is not applied), column 7 is the
		ﬁnal BLEU score of the full pipeline
	</Extractive Summary>
	<Extractive Summary> =
		Similar to Table 3, the models
		marked with + are trained with concatenated tree-
		banks
	</Extractive Summary>
</Paper ID=ument1087>


<Paper ID=ument1087> <Table ID =4>
	<Abstractive Summary> =
		70
		Table 4: Development results in the deep track
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, column 1 is the BLEU score on the
		lemmata of the given content words, columns 2
		is the BLEU score on the lemmata including gen-
		erated tokens, column 3 is the accuracy on word
		forms, column 4 is the BLEU score of contracted
		word forms, and column 5 is the BLEU score of
		the full pipeline
	</Extractive Summary>
</Paper ID=ument1087>


<Paper ID=ument1088> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: BLEU, DIST and NIST scores of our approach in the original (non-tokenized) test sets
	</Abstractive Summary>
</Paper ID=ument1088>


<Paper ID=ument1089> <Table ID =1>
	<Abstractive Summary> =
		65
		#
		Feature
		Feature description
		Embedding size
		1
		Token
		Lemma or stem of word form
		300
		2
		UPOS
		Universal part-of-speech tag
		10
		3
		XPOS
		Language-speciﬁc part-of-speech tag
		10
		4
		Deprel
		Universal dependency relation to the Head
		10
		5
		Head
		Head of the current word
		20
		6
		Index
		Word index
		20
		7
		Lin
		Relative linear order with respect to the governor
		20
		8
		Lin sign
		The sign of the Lin feature
		3
		All
		Concatenation of all features
		393
		Table 1: The 8 features used in our model with their corresponding embedding sizes
		ule uses the transformer architecture described
		in Vaswani et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists the 8 features
		used by our model: 6 features of the UD structure,
		in addition to 2 features for Lin (the Lin feature
		divided into its absolute value and its sign)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 indicates the embedding size
		of each feature
	</Extractive Summary>
	<Extractive Summary> =
		We suspected that the Head and Index fea-
		tures (see Table 1) constituted valuable informa-
		tion regarding the dependency tree structure of the
		sentences
	</Extractive Summary>
</Paper ID=ument1089>


<Paper ID=ument1089> <Table ID =2>
	<Abstractive Summary> =
		53
		Table 2: Results of our submission in the shallow track task of SR’19
		done on the remaining 30 epochs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results of our model on
		the test data
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2,
		our
		model achieved its highest performance on the
		en ewt-ud-test dataset with a BLEU score
		of 22
	</Extractive Summary>
</Paper ID=ument1089>


<Paper ID=ument109> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1:
		Example of generating candidate sen-
		tences
	</Abstractive Summary>
	<Extractive Summary> =
		See the supplementary
		materials for details and Table 1 for an exam-
		ple
	</Extractive Summary>
</Paper ID=ument109>


<Paper ID=ument109> <Table ID =2>
	<Abstractive Summary> =
		55
		Table 2:
		Main results for Task 1:
		Commonsense
		knowledge base completion (test F1 score) and Task
		2: Wikipedia mining (quality scores out of 4)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the full results
	</Extractive Summary>
	<Extractive Summary> =
		11 under the Coherency Rank-
		ing approach, well exceeding the performance of
		current supervised methods (Table 2)
	</Extractive Summary>
</Paper ID=ument109>


<Paper ID=ument109> <Table ID =3>
	<Abstractive Summary> =
		18
		Table 3:
		Test results examining the effect of sen-
		tence meaning and grammaticality on task perfor-
		mance
	</Abstractive Summary>
	<Extractive Summary> =
		In CKBC,
		we also ﬁnd that grammar has a high impact on the
		resulting F1 scores (Table 3)
	</Extractive Summary>
</Paper ID=ument109>


<Paper ID=ument1090> <Table ID =1>
	<Abstractive Summary> =
		9
		N/A
		Table 1: Morphological inflection results on the development set
	</Abstractive Summary>
	<Extractive Summary> =
		, yt − 1}, st, ct)
		(4)
		ci =
		Tx
		∑
		j=1
		αijhj
		(5)
		αij =
		exp(eij)
		∑Tx
		k=1 exp(eik)
		(6)
		eij = a(si−1, hj)
		(7)
		As seen in Table 1, uncased results are al-
		most always higher than cased
	</Extractive Summary>
</Paper ID=ument1090>


<Paper ID=ument1090> <Table ID =2>
	<Abstractive Summary> =
		902
		Table 2: Dev set BLEU scores (calculated along
		with non-terminals), using gold inflected forms
		In the table, the BLEU scores are calculated
		with the non-terminals included in both input
		and output sequences, inflating them some-
		what relative to regular BLEU scores
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows a com-
		parison of the different models that we tried
	</Extractive Summary>
</Paper ID=ument1090>


<Paper ID=ument1090> <Table ID =3>
	<Abstractive Summary> =
		8847
		Table 3: Preliminary BLEU scores for constrained decoding (calculated along with non-terminals), using
		gold inflected forms
		Dataset
		w/o reranking
		w/ reranking
		en_ewt
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 compares the constrained and un-
		constrained versions of the Seq2Seq-Copy
		(again with non-terminals in the output and
		gold inflected forms)
	</Extractive Summary>
</Paper ID=ument1090>


<Paper ID=ument1090> <Table ID =4>
	<Abstractive Summary> =
		7909
		Table 4: BLEU scores on dev sets before and after
		reranking, using gold inflected forms
		Test set
		BLEU
		NIST
		DIST
		en_ewt-ud-test
		62
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the gains obtained by
		doing validity reranking (again with gold in-
		flected forms); here the scores shown are cal-
		culated without non-terminals
	</Extractive Summary>
</Paper ID=ument1090>


<Paper ID=ument1090> <Table ID =5>
	<Abstractive Summary> =
		8
		Table 5: Test set results for English from the or-
		ganizers
		rors with the other test sets, errors with this
		particular set are much noisier
	</Abstractive Summary>
	<Extractive Summary> =
		Consistent with the human evaluation,
		the automatic scores for our system (Table 5)
		were also in the middle of the pack
	</Extractive Summary>
</Paper ID=ument1090>


<Paper ID=ument1091> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Ablation study: results of models on the
		MSR’19 validation set of UD English EWT (enewt-
		ud-dev) corpus
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Encoder Model Selection
		As indicated in Table 1, we compare R-GCN with
		different encoders
	</Extractive Summary>
</Paper ID=ument1091>


<Paper ID=ument1091> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Comparison of outputs from systems with encoder variants given the graph in Figure 1 as input
	</Abstractive Summary>
	<Extractive Summary> =
		One of such examples
		is shown in Table 2, where we observe ﬂuency im-
		provements going from BiLSTM encoder to GCN,
		and ﬁnally to R-GCN, where an overall improve-
		ment in ﬂuency is conspicuous
	</Extractive Summary>
</Paper ID=ument1091>


<Paper ID=ument1091> <Table ID =3>
	<Abstractive Summary> =
		2
		(*) denotes our submission to the shared task, which doesn’t use morphological features
		Table 3: Evaluation of our submissions to MSR’19 Deep Task across all corpora on both Test and Dev sets
	</Abstractive Summary>
</Paper ID=ument1091>


<Paper ID=ument1092> <Table ID =1>
	<Abstractive Summary> =
		42
		Table 1: The ofﬁcial scores of the DipInfoUniTo system for English, French and Chinese datasets, in terms of the
		automatic metrics BLUE, NIST, and DIST
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1 the ofﬁcial scores of the DipIn-
		foUniTo system for English, French and Chinese
		datasets are reported, computed in terms of the au-
		tomatic metrics BLUE, NIST, and DIST
	</Extractive Summary>
</Paper ID=ument1092>


<Paper ID=ument1093> <Table ID =1>
	<Abstractive Summary> =
		72
		933
		zh
		NA
		0
		0
		Table 1: Accuracy of the morphological realisation
		component
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1, second column)
	</Extractive Summary>
	<Extractive Summary> =
		We also calculated a number of lemmas, which
		can have different word forms, given the same set
		of POS and morphological features (Table 1, third
		column)
	</Extractive Summary>
	<Extractive Summary> =
		In general, high accu-
		racies of MR alone (word level, Table 1) do not
		guarantee good performance while evaluating on
		the sentence level
	</Extractive Summary>
	<Extractive Summary> =
		Table 1) and
		contractions (We did not develop a module
		for handling contractions in Arabic
	</Extractive Summary>
</Paper ID=ument1093>


<Paper ID=ument1093> <Table ID =2>
	<Abstractive Summary> =
		33
		Table 2: The MR module applied to the gold word or-
		dering input
	</Abstractive Summary>
</Paper ID=ument1093>


<Paper ID=ument1093> <Table ID =3>
	<Abstractive Summary> =
		23
		Table 3: WO component performance on the develop-
		ment set
	</Abstractive Summary>
	<Extractive Summary> =
		1
		WO Evaluation
		Table 3 shows the results of WO
	</Extractive Summary>
	<Extractive Summary> =
		show a drop compared to the WO component per-
		formance (Table 3), which is consistent with the
		errors of the MR+CG module, described in Sec-
		tion 4
	</Extractive Summary>
</Paper ID=ument1093>


<Paper ID=ument1093> <Table ID =4>
	<Abstractive Summary> =
		21
		Table 4: Automatic metrics on the development set
		(WO + MR)
	</Abstractive Summary>
</Paper ID=ument1093>


<Paper ID=ument1094> <Table ID =1>
	<Abstractive Summary> =
		08
		Table 1: Statistics of all cuts performed in the AMR
		Corpus
		4
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows the mean and standard deviation (std)
		of each cut for each dataset (for quality mea-
		sure)
	</Extractive Summary>
</Paper ID=ument1094>


<Paper ID=ument1095> <Table ID =1>
	<Abstractive Summary> =
		86
		Table 1: CLEVR and CLEVR-Humans Accuracy by baseline methods and our models10%
		Table 1: Image-caption ranking results for English (Multi30k)
		Image to Text
		Text to Image
		R@1
		R@5
		R@10
		Mr
		R@1
		R@5
		R@10
		Mr
		Alignment
		symmetric
		Parallel (Gella et al02
		Table 1: Average accuracy ± standard deviation by FiLM on 3 test sets in settings A-H (in brackets, proportion
		of unexpected cases seen in training)
		It provides unique advantages over
		27
		Seed
		Similar characters
		湖
		溯潮漏瑚澗
		語
		諮晤誤診譜
		頭
		頹頷頸頌頻
		⿓
		鶺廳籠麓韙
		東
		泉帛柬⾞蒐
		Table 1: Similar characters found with eigenchar-
		acter space3
		Table 1: Results on the full MS-COCO dataset3
		Table 1: The differences in Corpus-level Meteor scores for the English–German Multi30K Test 2017 data for the
		different adversaries compared to the systems evaluated on the Original text and imagesbe/downloads/geco/
		53
		GECO
		Dundee
		Genre:
		novel
		news
		Readers:
		14
		10
		Sents
		Tokens
		Sents
		Tokens
		Train
		4,200
		45,004
		1,896
		41,618
		Dev
		547
		5,614
		231
		5,176
		Test
		574
		5,792
		243
		5,206
		Types
		–
		11,084
		–
		8,608
		Table 1: Overview of the eye-tracking corpora
	</Abstractive Summary>
	<Extractive Summary> =
		The corresponding results are shown in
		Table 1 (see last column)
	</Extractive Summary>
	<Extractive Summary> =
		However, Table 1
		shows results for CLEVR test set (evaluated by the authors of
		CLEVR dataset)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Multi30k Results
		In Table 1 and 2, we show the results for English
		and German captions
	</Extractive Summary>
	<Extractive Summary> =
		Results
		In Table 1 we report, for each setting,
		average model accuracy and standard deviation
		(sd) over 3 runs (the same results are visualized
		in Figure 2)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 show examples of similar charac-
		ters identified with eigencharacters
	</Extractive Summary>
	<Extractive Summary> =
		1
		Quantitative Results and Analysis
		Table 1 shows the performances of our mod-
		els against baseline models whose architecture is
		based on Bottom-up Top-down Attention model
		(Anderson et al
	</Extractive Summary>
	<Extractive Summary> =
		1
		Results
		In Table 1 we present the corpus-level Meteor
		scores for the text-only and multimodal systems
		when evaluated on the original data and the differ-
		ence in performance when evaluating these mod-
		els using the different adversaries
	</Extractive Summary>
</Paper ID=ument1095>


<Paper ID=ument1095> <Table ID =2>
	<Abstractive Summary> =
		43
		Table 2: Model Ablations for LNMN (CLEVR Validation set performance)10%
		Table 2: Image-caption ranking results for German (Multi30k)
		Figure 1: The AME - model architecture
		4
		Experiment and Results
		45
		Table 2:
		Results on the MS-COCO-GT dataset1352
		Table 2:
		Type-to-token ratios of the system outputs
		given congruent and incongruent visual context93
		Table 2: F1 scores for binary chunking task training
		with an early or late gaze metric as an auxiliary task or
		as a type-level lexicon
	</Abstractive Summary>
	<Extractive Summary> =
		The results from the ab-
		lations in Table 2 show that a naive concatenation
		of all inputs to a module (or cell) results in a poor
		performance (around 47 %)
	</Extractive Summary>
	<Extractive Summary> =
		However, the results in Table 2 indicate that
		there is no meaningful difference in TTR when the
		models are evaluated with the congruent or incon-
		gruent visual inputs
	</Extractive Summary>
	<Extractive Summary> =
		1
		Binary Phrase Chunking Results
		Table 2 presents the results for the binary phrase
		chunking
	</Extractive Summary>
	<Extractive Summary> =
		Token vs type-level
		In more detail, for the
		chunking task the results show no beneﬁt from
		learning gaze at token level in a multi-task setup
		(left two columns in Table 2)
	</Extractive Summary>
</Paper ID=ument1095>


<Paper ID=ument1095> <Table ID =3>
	<Abstractive Summary> =
		modules
		(4 input)
		LNMN (9)
		4
		2
		1
		1
		LNMN (11)
		4
		2
		2
		2
		LNMN (14)
		5
		4
		2
		2
		Table 3: Number of modules of each type for different
		model ablations99%
		Table 3: Image-caption ranking results for English (MS-COCO)
		Image to Text
		Text to Image
		R@1
		R@5
		R@10
		Mr
		R@1
		R@5
		R@10
		Mr
		Alignment
		symmetric
		Mono
		4279
		Table 3: The 95% conﬁdence interval of the sentence-
		level perplexity of the original and each textual adver-
		sarial data samples, as estimated by GPT-226
		Table 3: Accuracy scores for POS tagging with an early
		or late gaze metric as type-level lexicon or as an auxil-
		iary task
	</Abstractive Summary>
	<Extractive Summary> =
		See
		Table 3 for details2
	</Extractive Summary>
	<Extractive Summary> =
		3
		MS-COCO Results4
		In Table 3 and 4, we show the performance of
		AME and baselines for English and Japanese cap-
		tions
	</Extractive Summary>
	<Extractive Summary> =
		From Table 3, we observe that the bound-
		aries of the intervals are not over-lapping, indi-
		cating statistically signiﬁcant differences in distri-
		bution between the adversarial categories and the
		original sample4
	</Extractive Summary>
	<Extractive Summary> =
		2
		Part-of-Speech Tagging Results
		Table 3 shows the results for Part-of-Speech tag-
		ging
	</Extractive Summary>
</Paper ID=ument1095>


<Paper ID=ument1095> <Table ID =4>
	<Abstractive Summary> =
		67
		Table 4: Test Accuracy on Natural Image VQA datasets
		over all examples) is deﬁned as:
		IG(αm
		i ) =
		N
		�
		j=1
		p
		�
		k=1
		�
		(αm,k
		i
		− (αm,k
		i
		)
		′)×
		� 1
		ξ=0
		∂F(Ij, qj, (1 − ξ) × (αm,k
		i
		)
		′ + ξ × αm,k
		i
		)
		∂αm,k
		i
		�
		Please note that attributions are deﬁned relative
		to an uninformative input called the baseline99%
		Table 4: Image-caption ranking results for Japanese (MS-COCO)
		EN → DE
		DE → EN
		R@1 R@5 R@10
		R@1 R@5 R@10
		FME 51
	</Abstractive Summary>
</Paper ID=ument1095>


<Paper ID=ument1095> <Table ID =5>
	<Abstractive Summary> =
		5 e5
		Table 5: Analysis of gradient attributions of α parameters corresponding to each module (LNMN (9 modules)),
		summed across all examples of CLEVR validation set5
		Table 5:
		Textual similarity scores (asymmetric,
		Multi30k)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results for this
		experiment
	</Extractive Summary>
	<Extractive Summary> =
		In
		Table 5, we show the performance on Multi30k
		dataset in asymmetric mode
	</Extractive Summary>
</Paper ID=ument1095>


<Paper ID=ument1095> <Table ID =6>
	<Abstractive Summary> =
		7
		Module type
		Module implementation
		Attention
		(3 inputs)
		O(img, a, ctxt) = conv2(choose2(conv1(I), a) ⊙ W1ctxt) = conv2(a ⊙ W1ctxt)
		O(img, a, ctxt) = conv2(choose2(choose1(conv1(I), a), W1ctxt)) = conv2(W1ctxt)
		O(img, a, ctxt) = conv2(choose2(min(conv1(I), a), W1ctxt)) = conv2(W1ctxt)
		O(img, a, ctxt) = conv2(max(conv1(I), a) + W1ctxt))
		Attention
		(4 inputs)
		O(img, a1, a2, ctxt) = conv2(choose1(max(a1, a2), conv1(I)) ⊙ W1ctxt))
		= conv2(max(a1, a2) ⊙ W1ctxt)
		O(img, a1, a2, ctxt) = conv2(max(choose2(a1, a2), conv1(I)) ⊙ W1ctxt))
		= conv2(max(a2, conv1(I)) ⊙ W1ctxt))
		Answer
		(3 inputs)
		O(img, a, ctxt) = W2[� min(conv1(I), a) ⊙ W1ctxt, W1ctxt, fmem]
		O(img, a, ctxt) = W2[� min((conv1(I) ⊙ a), W1ctxt), W1ctxt, fmem]
		Answer
		(4 inputs)
		O(img, a1, a2, ctxt) = W2[� min((min(a1, a2) ⊙ conv1(I)), W1ctxt), W1ctxt, fmem]
		O(img, a1, a2, ctxt) = W2[�((min(a1, a2) + conv1(I)) ⊙ W1ctxt), W1ctxt, fmem]
		Table 6: Analytical expression of modules learned by LNMN (11 modules)
	</Abstractive Summary>
</Paper ID=ument1095>


<Paper ID=ument1095> <Table ID =7>
	<Abstractive Summary> =
		84
		Table 7: Analysis of performance drop with removing operators from a trained model (LNMN 9 modules) on
		CLEVR validation set
	</Abstractive Summary>
</Paper ID=ument1095>


<Paper ID=ument1095> <Table ID =8>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		2 (see Table 8)
	</Extractive Summary>
</Paper ID=ument1095>


<Paper ID=ument1097> <Table ID =1>
	<Abstractive Summary> =
		86
		Table 1: CLEVR and CLEVR-Humans Accuracy by baseline methods and our models
	</Abstractive Summary>
	<Extractive Summary> =
		The corresponding results are shown in
		Table 1 (see last column)
	</Extractive Summary>
	<Extractive Summary> =
		However, Table 1
		shows results for CLEVR test set (evaluated by the authors of
		CLEVR dataset)
	</Extractive Summary>
</Paper ID=ument1097>


<Paper ID=ument1097> <Table ID =2>
	<Abstractive Summary> =
		43
		Table 2: Model Ablations for LNMN (CLEVR Validation set performance)
	</Abstractive Summary>
	<Extractive Summary> =
		The results from the ab-
		lations in Table 2 show that a naive concatenation
		of all inputs to a module (or cell) results in a poor
		performance (around 47 %)
	</Extractive Summary>
</Paper ID=ument1097>


<Paper ID=ument1097> <Table ID =3>
	<Abstractive Summary> =
		modules
		(4 input)
		LNMN (9)
		4
		2
		1
		1
		LNMN (11)
		4
		2
		2
		2
		LNMN (14)
		5
		4
		2
		2
		Table 3: Number of modules of each type for different
		model ablations
	</Abstractive Summary>
	<Extractive Summary> =
		See
		Table 3 for details2
	</Extractive Summary>
</Paper ID=ument1097>


<Paper ID=ument1097> <Table ID =4>
	<Abstractive Summary> =
		67
		Table 4: Test Accuracy on Natural Image VQA datasets
		over all examples) is deﬁned as:
		IG(αm
		i ) =
		N
		�
		j=1
		p
		�
		k=1
		�
		(αm,k
		i
		− (αm,k
		i
		)
		′)×
		� 1
		ξ=0
		∂F(Ij, qj, (1 − ξ) × (αm,k
		i
		)
		′ + ξ × αm,k
		i
		)
		∂αm,k
		i
		�
		Please note that attributions are deﬁned relative
		to an uninformative input called the baseline
	</Abstractive Summary>
</Paper ID=ument1097>


<Paper ID=ument1097> <Table ID =5>
	<Abstractive Summary> =
		5 e5
		Table 5: Analysis of gradient attributions of α parameters corresponding to each module (LNMN (9 modules)),
		summed across all examples of CLEVR validation set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results for this
		experiment
	</Extractive Summary>
</Paper ID=ument1097>


<Paper ID=ument1097> <Table ID =6>
	<Abstractive Summary> =
		8
		Module type
		Module implementation
		Attention
		(3 inputs)
		O(img, a, ctxt) = conv2(choose2(conv1(I), a) ⊙ W1ctxt) = conv2(a ⊙ W1ctxt)
		O(img, a, ctxt) = conv2(choose2(choose1(conv1(I), a), W1ctxt)) = conv2(W1ctxt)
		O(img, a, ctxt) = conv2(choose2(min(conv1(I), a), W1ctxt)) = conv2(W1ctxt)
		O(img, a, ctxt) = conv2(max(conv1(I), a) + W1ctxt))
		Attention
		(4 inputs)
		O(img, a1, a2, ctxt) = conv2(choose1(max(a1, a2), conv1(I)) ⊙ W1ctxt))
		= conv2(max(a1, a2) ⊙ W1ctxt)
		O(img, a1, a2, ctxt) = conv2(max(choose2(a1, a2), conv1(I)) ⊙ W1ctxt))
		= conv2(max(a2, conv1(I)) ⊙ W1ctxt))
		Answer
		(3 inputs)
		O(img, a, ctxt) = W2[� min(conv1(I), a) ⊙ W1ctxt, W1ctxt, fmem]
		O(img, a, ctxt) = W2[� min((conv1(I) ⊙ a), W1ctxt), W1ctxt, fmem]
		Answer
		(4 inputs)
		O(img, a1, a2, ctxt) = W2[� min((min(a1, a2) ⊙ conv1(I)), W1ctxt), W1ctxt, fmem]
		O(img, a1, a2, ctxt) = W2[�((min(a1, a2) + conv1(I)) ⊙ W1ctxt), W1ctxt, fmem]
		Table 6: Analytical expression of modules learned by LNMN (11 modules)
	</Abstractive Summary>
</Paper ID=ument1097>


<Paper ID=ument1097> <Table ID =7>
	<Abstractive Summary> =
		84
		Table 7: Analysis of performance drop with removing operators from a trained model (LNMN 9 modules) on
		CLEVR validation set
	</Abstractive Summary>
</Paper ID=ument1097>


<Paper ID=ument1097> <Table ID =8>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		2 (see Table 8)
	</Extractive Summary>
</Paper ID=ument1097>


<Paper ID=ument1098> <Table ID =1>
	<Abstractive Summary> =
		10%
		Table 1: Image-caption ranking results for English (Multi30k)
		Image to Text
		Text to Image
		R@1
		R@5
		R@10
		Mr
		R@1
		R@5
		R@10
		Mr
		Alignment
		symmetric
		Parallel (Gella et al
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Multi30k Results
		In Table 1 and 2, we show the results for English
		and German captions
	</Extractive Summary>
</Paper ID=ument1098>


<Paper ID=ument1098> <Table ID =2>
	<Abstractive Summary> =
		10%
		Table 2: Image-caption ranking results for German (Multi30k)
		Figure 1: The AME - model architecture
		4
		Experiment and Results
		4
	</Abstractive Summary>
</Paper ID=ument1098>


<Paper ID=ument1098> <Table ID =3>
	<Abstractive Summary> =
		99%
		Table 3: Image-caption ranking results for English (MS-COCO)
		Image to Text
		Text to Image
		R@1
		R@5
		R@10
		Mr
		R@1
		R@5
		R@10
		Mr
		Alignment
		symmetric
		Mono
		42
	</Abstractive Summary>
	<Extractive Summary> =
		3
		MS-COCO Results4
		In Table 3 and 4, we show the performance of
		AME and baselines for English and Japanese cap-
		tions
	</Extractive Summary>
</Paper ID=ument1098>


<Paper ID=ument1098> <Table ID =4>
	<Abstractive Summary> =
		99%
		Table 4: Image-caption ranking results for Japanese (MS-COCO)
		EN → DE
		DE → EN
		R@1 R@5 R@10
		R@1 R@5 R@10
		FME 51
	</Abstractive Summary>
</Paper ID=ument1098>


<Paper ID=ument1098> <Table ID =5>
	<Abstractive Summary> =
		5
		Table 5:
		Textual similarity scores (asymmetric,
		Multi30k)
	</Abstractive Summary>
	<Extractive Summary> =
		In
		Table 5, we show the performance on Multi30k
		dataset in asymmetric mode
	</Extractive Summary>
</Paper ID=ument1098>


<Paper ID=ument1099> <Table ID =1>
	<Abstractive Summary> =
		02
		Table 1: Average accuracy ± standard deviation by FiLM on 3 test sets in settings A-H (in brackets, proportion
		of unexpected cases seen in training)
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		In Table 1 we report, for each setting,
		average model accuracy and standard deviation
		(sd) over 3 runs (the same results are visualized
		in Figure 2)
	</Extractive Summary>
</Paper ID=ument1099>


<Paper ID=ument11> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An error case of SMN (Wu et al
	</Abstractive Summary>
</Paper ID=ument11>


<Paper ID=ument11> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Adversarial experimental results on Ubuntu
		Dialogue Corpus
	</Abstractive Summary>
</Paper ID=ument11>


<Paper ID=ument11> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Data statistics for Ubuntu, Douban and E-
		commerce datasets
	</Abstractive Summary>
</Paper ID=ument11>


<Paper ID=ument11> <Table ID =4>
	<Abstractive Summary> =
		com
		117
		Table 4: Experimental results on Ubuntu, Douban and E-commerce datasets
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Experiment Result
		Table 4 shows the results of MSN and all baseline
		models on the datasets
	</Extractive Summary>
</Paper ID=ument11>


<Paper ID=ument11> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Ablation study on E-commerce corpus
	</Abstractive Summary>
	<Extractive Summary> =
		0
		From experimental results in Table 5, we can
		observe that:
		(1) Compared with MSNbase, removing selectors
		leads to performance degradation, which shows
		that the multi-hop selectors are indeed help to im-
		prove the selection performance
	</Extractive Summary>
</Paper ID=ument11>


<Paper ID=ument110> <Table ID =1>
	<Abstractive Summary> =
		SL expensive
		Table 1: Visualization of a sparse rational RNN trained
		on original mix containing only 3 WFSAs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 visualizes a sparse rational RNN trained
		on original mix with only three WFSAs, (8 main-
		path transitions in total)
	</Extractive Summary>
</Paper ID=ument110>


<Paper ID=ument1100> <Table ID =1>
	<Abstractive Summary> =
		It provides unique advantages over
		28
		Seed
		Similar characters
		湖
		溯潮漏瑚澗
		語
		諮晤誤診譜
		頭
		頹頷頸頌頻
		⿓
		鶺廳籠麓韙
		東
		泉帛柬⾞蒐
		Table 1: Similar characters found with eigenchar-
		acter space
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 show examples of similar charac-
		ters identified with eigencharacters
	</Extractive Summary>
</Paper ID=ument1100>


<Paper ID=ument1101> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Results on the full MS-COCO dataset
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Quantitative Results and Analysis
		Table 1 shows the performances of our mod-
		els against baseline models whose architecture is
		based on Bottom-up Top-down Attention model
		(Anderson et al
	</Extractive Summary>
</Paper ID=ument1101>


<Paper ID=ument1101> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2:
		Results on the MS-COCO-GT dataset
	</Abstractive Summary>
</Paper ID=ument1101>


<Paper ID=ument1102> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: The differences in Corpus-level Meteor scores for the English–German Multi30K Test 2017 data for the
		different adversaries compared to the systems evaluated on the Original text and images
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		In Table 1 we present the corpus-level Meteor
		scores for the text-only and multimodal systems
		when evaluated on the original data and the differ-
		ence in performance when evaluating these mod-
		els using the different adversaries
	</Extractive Summary>
</Paper ID=ument1102>


<Paper ID=ument1102> <Table ID =2>
	<Abstractive Summary> =
		1352
		Table 2:
		Type-to-token ratios of the system outputs
		given congruent and incongruent visual context
	</Abstractive Summary>
	<Extractive Summary> =
		However, the results in Table 2 indicate that
		there is no meaningful difference in TTR when the
		models are evaluated with the congruent or incon-
		gruent visual inputs
	</Extractive Summary>
</Paper ID=ument1102>


<Paper ID=ument1102> <Table ID =3>
	<Abstractive Summary> =
		79
		Table 3: The 95% conﬁdence interval of the sentence-
		level perplexity of the original and each textual adver-
		sarial data samples, as estimated by GPT-2
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 3, we observe that the bound-
		aries of the intervals are not over-lapping, indi-
		cating statistically signiﬁcant differences in distri-
		bution between the adversarial categories and the
		original sample4
	</Extractive Summary>
</Paper ID=ument1102>


<Paper ID=ument1104> <Table ID =1>
	<Abstractive Summary> =
		be/downloads/geco/
		54
		GECO
		Dundee
		Genre:
		novel
		news
		Readers:
		14
		10
		Sents
		Tokens
		Sents
		Tokens
		Train
		4,200
		45,004
		1,896
		41,618
		Dev
		547
		5,614
		231
		5,176
		Test
		574
		5,792
		243
		5,206
		Types
		–
		11,084
		–
		8,608
		Table 1: Overview of the eye-tracking corpora
	</Abstractive Summary>
</Paper ID=ument1104>


<Paper ID=ument1104> <Table ID =2>
	<Abstractive Summary> =
		93
		Table 2: F1 scores for binary chunking task training
		with an early or late gaze metric as an auxiliary task or
		as a type-level lexicon
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Binary Phrase Chunking Results
		Table 2 presents the results for the binary phrase
		chunking
	</Extractive Summary>
	<Extractive Summary> =
		Token vs type-level
		In more detail, for the
		chunking task the results show no beneﬁt from
		learning gaze at token level in a multi-task setup
		(left two columns in Table 2)
	</Extractive Summary>
</Paper ID=ument1104>


<Paper ID=ument1104> <Table ID =3>
	<Abstractive Summary> =
		26
		Table 3: Accuracy scores for POS tagging with an early
		or late gaze metric as type-level lexicon or as an auxil-
		iary task
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Part-of-Speech Tagging Results
		Table 3 shows the results for Part-of-Speech tag-
		ging
	</Extractive Summary>
</Paper ID=ument1104>


<Paper ID=ument1106> <Table ID =1>
	<Abstractive Summary> =
		Recent NMT systems that include context deal
		with both phenomena, coreference and coherence,
		but usually context is limited to the previous sen-
		# lines
		S1, S3
		S2
		Common Crawl
		2,394,878
		x1
		x4
		Europarl
		1,775,445
		x1
		x4
		News Commentary
		328,059
		x4 x16
		Rapid
		1,105,651
		x1
		x4
		ParaCrawl Filtered
		12,424,790
		x0
		x1
		Table 1: Number of lines of the corpora used for train-
		ing the NMT systems under study2
		Table 1: BLEU and METEOR (MTR) scores obtained
		with the oracles deﬁned in Section 4
		Remove most frequent
		words
		recent correctly foresaw absence stronger ﬁscal stimulus forthcoming
		either States recovery Great Recession 2008 slow
		Retain named entities
		recent years Europe the United States the Great Recession 2008
		Retain speciﬁc POS
		years I foresaw the absence stimulus was forthcoming either Europe or
		the United States recovery the Great Recession 2008 would be
		Table 1: Examples for ﬁltering of words in the context (News Commentary v14 English→German)3
		Table 1: Statistics of IWSLT2017 corpora: the number of sentence pairs and the average length (number of tokens
		per sentence) for the train / dev / test portions81
		×
		Table 1: BLEU scores for each model9
		Table 1: General characteristics of the two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 and Figure 2
		show the results for ORACLE1
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 and Figure 2 present also
		the results for ORACLE2
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 and Figure 2 present also
		the results for ORACLE3, which show that there
		is some margin for improvement for the fused sys-
		tem with respect to the NMT working in isolation
	</Extractive Summary>
	<Extractive Summary> =
		Nevertheless, there is still
		room for further gains since, as seen in Table 1,
		ORACLE3 is able to increase +2
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 gives examples
		of the ﬁltering
	</Extractive Summary>
	<Extractive Summary> =
		The statistics of the
		datasets we used to train 1-to-1 models are shown
		in Table 1 and 2
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results and Analysis
		Table 1 shows the results7
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows that the two datasets are com-
		parable in terms of sentence numbers
	</Extractive Summary>
</Paper ID=ument1106>


<Paper ID=ument1106> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Statistics on coreference features for news and TED texts considered14
		4463
		Table 2: BLEU and METEOR scores obtained with the
		fused systems with λ = 0 #sent)
		111
		37
		Table 2: Training data statistics9
		Table 2: Statistics of the target-side monolingual cor-
		pora and their source-side counterparts obtained by
		back-translation: the number of sentences in the origi-
		nal corpora and the average length in the pseudo paral-
		lel data used to train 1-to-1 models4%
		Table 2: The percentage of sentences containing coref-
		erences in the test set7
		per thousand tokens
		per thousand lines
		Table 2: Discourse-level features in the OST and WMT datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows that, overall, all machine transla-
		tions contain a greater number of annotated men-
		tions in both news texts and TED talks than in the
		annotated source (src and srcCoreNLP) and refer-
		ence (ref) texts
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 and Figure 3 show the
		results of the automatic evaluation of the different
		variations of the presented fused system
	</Extractive Summary>
	<Extractive Summary> =
		We observe in Table 2 that the scores improve as
		long as we increase the value of N until it seems
		to stabilize for N ≥ 4
	</Extractive Summary>
	<Extractive Summary> =
		This discrepancy can be explained
		with document lengths in each dataset (Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, this is because
		our proposed models can also use inter-sentential
		coreference information for translation
	</Extractive Summary>
	<Extractive Summary> =
		The results in Table 2 show that about ev-
		ery 10th word of the OST corpus is a pronoun,
		4By sentences, we mean the lines obtained by the sentence
		alignment process
	</Extractive Summary>
</Paper ID=ument1106>


<Paper ID=ument1106> <Table ID =3>
	<Abstractive Summary> =
		5%)
		Table 3: BLEU and METEOR (MTR) scores for the 3 systems on our full test set (all) and the subset of sentences
		where coreference occurrs (coref)9
		Table 3: Comparison of document-level model architectures and complexity01
		Table 3: BLEU scores of the sentence-level and context-aware models with data augmentation: All the models are
		trained on the original parallel corpora and the pseudo parallel data generated by back-translation, while varying
		the size of pseudo training data from 0 (no pseudo training data) to 4000k3
		Table 3: Basic statistics of ﬁxed-size windows data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the scores in two
		cases: all, when the complete texts are evaluated
		and coref, when only the subset of sentences that
		have been augmented in S3 are considered – 265
		out of 494 for news and 239 out of 518 for TED
	</Extractive Summary>
	<Extractive Summary> =
		Table 3
		also reports the number of mistakes in the trans-
		lation of coreferent mentions
	</Extractive Summary>
	<Extractive Summary> =
		1
		Model Comparison
		Model Architecture
		Firstly, we compare the
		performance of existing single-encoder and multi-
		encoder approaches (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Impact of the size of pseudo training data
		Table 3 lists the BLEU scores of sentence-level and
		context-aware NMT models while varying the size
		of pseudo parallel data
	</Extractive Summary>
</Paper ID=ument1106>


<Paper ID=ument1106> <Table ID =4>
	<Abstractive Summary> =
		54
		Table 4: Percentage of erroneous mentions: antence-
		dent vs0
		59%
		Table 4: Comparison of context word ﬁltering methods
		40
		pseudo train
		train
		dev
		test
		en
		14 / 27 / 45
		13 / 20 / 32
		14 / 23 / 35
		13 / 20 / 31
		ja
		18 / 34 / 56
		13 / 21 / 33
		13 / 22 / 37
		12 / 20 / 32
		Table 4: The quartile of the number of tokens per sentence in each dataset: train, dev and test indicate the train,
		dev, and test sets of IWSLT2017 corpus2
		Table 4: BLEU (B) and METEOR (M) scores for EN → DE translation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows that there is
		a variation between the news texts and TED talks
		8
		ant
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 4, there is clearly a
		gap between the original and pseudo parallel cor-
		pora in terms of the number of tokens per sentence
	</Extractive Summary>
</Paper ID=ument1106>


<Paper ID=ument1106> <Table ID =5>
	<Abstractive Summary> =
		#cases
		Category
		en-it en-de
		Coreference
		21
		2
		Topic-aware lexical choice
		66
		33
		Not interpretable
		292 1,211
		Total TER improved
		379 1,246
		Total
		1,147 2,998
		Table 5: Causes of improvements by document-level
		context58
		Table 5: The BLEU scores of ja→en context-aware models trained with pseudo parallel data generated by 1-to-1
		and 2-to-2 back-translation: The scores of the models trained on pseudo data generated by 2-to-1 back-translation
		are excerpted from Table 33
		Table 5: BLEU (B) and METEOR (M) scores for DE → EN translation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the evaluation results in BLEU
	</Extractive Summary>
</Paper ID=ument1106>


<Paper ID=ument1106> <Table ID =6>
	<Abstractive Summary> =
		(c) Not interpretable
		Table 6: Example translation outputs for each analysis category (WMT English→German newstest2018)5
		Table 6: Results of 2-to-2 models on the en→fr spe-
		cialized test sets (accuracy in %)
		Total
		Examples:
		25
		25
		25
		25
		10
		10
		5
		15
		30
		30
		200
		OST Curr → Curr
		9
		7
		6
		7
		5
		3
		1
		5
		20
		28
		91
		OST 1Prev + Curr → 1Prev + Curr
		10
		6
		12
		9
		5
		6
		1
		2
		24
		25
		100
		WMT Curr → Curr
		14
		12
		9
		10
		5
		4
		0
		8
		20
		26
		108
		WMT 1Prev + Curr → 1Prev + Curr
		9
		11
		13
		12
		5
		5
		1
		5
		19
		28
		108
		Table 6: Absolute numbers of PROTEST EN → DE pronoun translations evaluated semi-automatically as correct
	</Abstractive Summary>
	<Extractive Summary> =
		One of them is shown in Table 6a
	</Extractive Summary>
	<Extractive Summary> =
		Another interpretable cause is topic-aware lexi-
		cal choice (Table 6b)
	</Extractive Summary>
	<Extractive Summary> =
		Table 6c shows
		such an example
	</Extractive Summary>
	<Extractive Summary> =
		3
		Evaluation of context-aware translation
		using specialized test sets
		Table 6 and 7 show results on the en→fr and
		ja→en specialized test sets, respectively
	</Extractive Summary>
	<Extractive Summary> =
		en→fr models
		Table 6 shows the results of 2-to-
		2 models with the data augmentation and the best
		performing models excerpted from (Bawden et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 reports PROTEST results for two se-
		lected systems, the Curr → Curr baseline and
		the best-performing variable-window concatena-
		tion model 1Prev+Curr → 1Prev+Curr
	</Extractive Summary>
</Paper ID=ument1106>


<Paper ID=ument1106> <Table ID =7>
	<Abstractive Summary> =
		9
		Table 7: Sentence-level vs
		train
		2-to-1 2-to-2
		2-to-1 2-to-2
		2-to-1 2-to-2
		0k
		78
		79
		(same to the left)
		500k
		87
		84
		85
		89
		83
		89
		1000k
		91
		89
		81
		89
		88
		88
		2000k
		86
		90
		88
		93
		87
		90
		4000k
		85
		93
		91
		93
		86
		89
		Table 7: Results of 2-to-X models on the ja→en spe-
		cialized test sets (accuracy in %)
		DE Pronouns:
		ich, es, das, wir, sich, Sie, er, du, sie, die, was, mir, mich, uns, der, man, dich, ihn, dir, dies, ihm,
		ihr, wer, ’s, Ihnen, dem, denen, euch, ihnen, den, Ihr, diese, dessen, deren, einen, dieser, wen, welche,
		einem, wem, dieses, jene, diesen, dasselbe, welches, einander
		Ambiguous:
		Sie, den, denen, der, die, diese, dieser, ihm, ihn, ihnen, ihr, man, mich, mir, sich, sie, uns
		EN Pronouns:
		I, you, it, we, he, what, me, they, who, she, him, them, us, her, himself, itself, themselves, one, yourself,
		myself, whom, ourselves, i, ’em, herself, mine, yours, ya
		Ambiguous:
		her, him, it, me, myself, one, she, them, they, us, who, whom, you, yourself
		EN Connectives:
		although, even though, since, though, meanwhile, while, yet, however
		DE Negations:
		nicht, nie, niemand, nichts, nirgends, nirgendwo, kein, weder
		EN Negations:
		no, not, never, nobody, noone, no-one, nothing, nowhere, none, neither, nor
		Table 7: List of words and lemmas used to detect discourse-level properties
	</Abstractive Summary>
	<Extractive Summary> =
		In our experiments, we obtain a much stron-
		ger sentence-level baseline by applying a sim-
		ple regularization (dropout), which the document-
		level model cannot outperform (Table 7)
	</Extractive Summary>
	<Extractive Summary> =
		The document-level
		training does not give any improvement in BLEU
		(last two rows of Table 7)
	</Extractive Summary>
	<Extractive Summary> =
		ja→en models
		Table 7 lists the results of
		context-aware models
	</Extractive Summary>
	<Extractive Summary> =
		Table 7) and count the number of sentences that
		contain at least one negation word
	</Extractive Summary>
</Paper ID=ument1106>


<Paper ID=ument1106> <Table ID =8>
	<Abstractive Summary> =
		Table 8: Example of translated sentences; zero pronoun Φ3 is successfully restored in by the 2-to-2 model trained
		using 2M pseudo data
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Qualitative Analysis
		Table 8 shows examples of ja→en translation
		where the use of contexts and additional pseudo
		training data help improve the translation quality
	</Extractive Summary>
</Paper ID=ument1106>


<Paper ID=ument1108> <Table ID =1>
	<Abstractive Summary> =
		Recent NMT systems that include context deal
		with both phenomena, coreference and coherence,
		but usually context is limited to the previous sen-
		# lines
		S1, S3
		S2
		Common Crawl
		2,394,878
		x1
		x4
		Europarl
		1,775,445
		x1
		x4
		News Commentary
		328,059
		x4 x16
		Rapid
		1,105,651
		x1
		x4
		ParaCrawl Filtered
		12,424,790
		x0
		x1
		Table 1: Number of lines of the corpora used for train-
		ing the NMT systems under study
	</Abstractive Summary>
</Paper ID=ument1108>


<Paper ID=ument1108> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Statistics on coreference features for news and TED texts considered
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows that, overall, all machine transla-
		tions contain a greater number of annotated men-
		tions in both news texts and TED talks than in the
		annotated source (src and srcCoreNLP) and refer-
		ence (ref) texts
	</Extractive Summary>
</Paper ID=ument1108>


<Paper ID=ument1108> <Table ID =3>
	<Abstractive Summary> =
		5%)
		Table 3: BLEU and METEOR (MTR) scores for the 3 systems on our full test set (all) and the subset of sentences
		where coreference occurrs (coref)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the scores in two
		cases: all, when the complete texts are evaluated
		and coref, when only the subset of sentences that
		have been augmented in S3 are considered – 265
		out of 494 for news and 239 out of 518 for TED
	</Extractive Summary>
	<Extractive Summary> =
		Table 3
		also reports the number of mistakes in the trans-
		lation of coreferent mentions
	</Extractive Summary>
</Paper ID=ument1108>


<Paper ID=ument1108> <Table ID =4>
	<Abstractive Summary> =
		54
		Table 4: Percentage of erroneous mentions: antence-
		dent vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows that there is
		a variation between the news texts and TED talks
		9
		ant
	</Extractive Summary>
</Paper ID=ument1108>


<Paper ID=ument111> <Table ID =1>
	<Abstractive Summary> =
		618
		Table 1: Results on lexicon induction
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows results
	</Extractive Summary>
	<Extractive Summary> =
		Performed on the English
		concreteness task (line 8 in Table 1)
	</Extractive Summary>
</Paper ID=ument111>


<Paper ID=ument111> <Table ID =2>
	<Abstractive Summary> =
		, 2014)
		Table 2: Overview of resources for lexicon induction
	</Abstractive Summary>
	<Extractive Summary> =
		For details on the resources
		see Table 2 and (Rothe et al
	</Extractive Summary>
</Paper ID=ument111>


<Paper ID=ument111> <Table ID =3>
	<Abstractive Summary> =
		17
		Table 3: Top 5 occupations that exhibit the greatest bias
		(measured by difference in cosine similarity)
	</Abstractive Summary>
	<Extractive Summary> =
		To identify occupa-
		tions with the greatest bias, Table 3 lists occupa-
		tions for which sim(ew, eman)−sim(ew, ewoman) is
		largest/smallest
	</Extractive Summary>
</Paper ID=ument111>


<Paper ID=ument111> <Table ID =4>
	<Abstractive Summary> =
		11
		Table 4: Left part shows mean cosine similarity
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results
	</Extractive Summary>
</Paper ID=ument111>


<Paper ID=ument112> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Results on GLUE test sets
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		We ﬁrst use the three meta-learning algorithms
		with PPS sampling and present in Table 1 the ex-
		perimental results on the GLUE test set
	</Extractive Summary>
</Paper ID=ument112>


<Paper ID=ument112> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Effect of task distributions
	</Abstractive Summary>
</Paper ID=ument112>


<Paper ID=ument112> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Effect of the number of update steps and the
		inner learning rate α
	</Abstractive Summary>
</Paper ID=ument112>


<Paper ID=ument113> <Table ID =1>
	<Abstractive Summary> =
		26
		Table 1: L2 and Cosine distances between embeddings
		of boldfaced words
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows two examples, where ELMo gen-
		erates very different representations for the bold-
		faced words under semantic equivalent contexts
	</Extractive Summary>
</Paper ID=ument113>


<Paper ID=ument113> <Table ID =2>
	<Abstractive Summary> =
		46
		Table 2: Performance on downstream applications
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Result Analysis
		The results reported in Table 2 show that PAR
		leads to 2% ∼ 4% improvement in accuracy on
		sentence classiﬁcation tasks and sentence infer-
		ence tasks
	</Extractive Summary>
</Paper ID=ument113>


<Paper ID=ument113> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Exact Match and F1 on Adversarial SQuAD
	</Abstractive Summary>
	<Extractive Summary> =
		Besides sentence-level tasks, Table 3 shows that
		the proposed PAR method notably improves the
		performance of a downstream question-answering
		task
	</Extractive Summary>
</Paper ID=ument113>


<Paper ID=ument113> <Table ID =4>
	<Abstractive Summary> =
		54
		Table 4: Averaging L2 distance for the shared word in paraphrased and non-paraphrased contexts
	</Abstractive Summary>
</Paper ID=ument113>


<Paper ID=ument113> <Table ID =5>
	<Abstractive Summary> =
		30
		Table 5: L2 and Cosine distance between embeddings
		of boldfaced words after retroﬁtting
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the ELMo-PAR embedding
		distance for the shared words in the examples in
		Table 1
	</Extractive Summary>
</Paper ID=ument113>


<Paper ID=ument114> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Test results on different datasets
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results and Analysis
		Table 1 shows the results of our models on dif-
		ferent datasets
	</Extractive Summary>
	<Extractive Summary> =
		Based on Table 1, mPWIMseq clearly outperforms
		SSE on Twitter, PIT-2015, STS-2014, WikiQA,
		and TrecQA
	</Extractive Summary>
</Paper ID=ument114>


<Paper ID=ument115> <Table ID =1>
	<Abstractive Summary> =
		though it’s been claimed by others that kim il sang ﬂed north korea in 1940 to escape the
		reprisals, even that is disputed by k
		Table 1: Steganography example
	</Abstractive Summary>
	<Extractive Summary> =
		This is demonstrated in Table 1, which
		shows two examples of natural language messages
		(Step 1) encoded in unrelated and innocuous cover
		text (Step 3)
	</Extractive Summary>
</Paper ID=ument115>


<Paper ID=ument116> <Table ID =1>
	<Abstractive Summary> =
		685∗
		Table 1: Correlation results on DUC2001/2002
	</Abstractive Summary>
	<Extractive Summary> =
		The results are shown in Table 11
	</Extractive Summary>
</Paper ID=ument116>


<Paper ID=ument116> <Table ID =2>
	<Abstractive Summary> =
		250
		Table 2: DUC2001 and DUC2002 dataset statistics
		To understand what causes the low perfor-
		mance on DUC’01, we examined statistics for
		both datasets, shown in Table 2
	</Abstractive Summary>
</Paper ID=ument116>


<Paper ID=ument116> <Table ID =3>
	<Abstractive Summary> =
		143
		Table 3:
		Spearman correlation on Newsroom data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3
		shows Spearman correlations and Pearson correla-
		tions are in the Appendix
	</Extractive Summary>
</Paper ID=ument116>


<Paper ID=ument116> <Table ID =4>
	<Abstractive Summary> =
		667
		Table 4: Average correlation rank of each system on
		DUC2001, DUC2002 and Newsroom 60 data
	</Abstractive Summary>
</Paper ID=ument116>


<Paper ID=ument117> <Table ID =1>
	<Abstractive Summary> =
		88
		Table 1: Performance on CNN/Daily Mail test dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Automatic Evaluation Result
		As shown in Table 1 (the performance of other
		models is collected from their papers), our model
		exceeds the PGN baseline by 3
	</Extractive Summary>
</Paper ID=ument117>


<Paper ID=ument117> <Table ID =2>
	<Abstractive Summary> =
		92
		Table 2: Human Evaluation: pairwise comparison be-
		tween our ﬁnal model and PGN model
	</Abstractive Summary>
	<Extractive Summary> =
		The
		result in Table 2 demonstrate that our model per-
		forms better under both criteria w
	</Extractive Summary>
</Paper ID=ument117>


<Paper ID=ument117> <Table ID =3>
	<Abstractive Summary> =
		Table 3: The bold words in article are salient parts contained in reference summary
	</Abstractive Summary>
</Paper ID=ument117>


<Paper ID=ument118> <Table ID =1>
	<Abstractive Summary> =
		88
		Table 1: Training data size, accuracy and F1 scores of
		the reward models on the 200,000 validation triples
	</Abstractive Summary>
</Paper ID=ument118>


<Paper ID=ument118> <Table ID =2>
	<Abstractive Summary> =
		14
		Table 2: Coreference results: average F1 scores on the
		OntoNotes and WikiCoref test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents the downstream effects of
		applying these reward functions to our baseline
		4https://catalog
	</Extractive Summary>
</Paper ID=ument118>


<Paper ID=ument119> <Table ID =1>
	<Abstractive Summary> =
		Split
		Scorer
		BEA-train
		561,410
		1
		train
		-
		BEA-valid
		2,377
		1
		valid
		ERRANT
		CoNLL-2014
		1,312
		2
		test
		ERRANT & M 2 scorer
		JFLEG
		1,951
		4
		test
		GLEU
		BEA-test
		4,477
		5
		test
		ERRANT
		SimpleWiki∗
		1,369,460
		-
		-
		-
		Wikipedia∗
		145,883,941
		-
		-
		-
		Gigaword∗
		131,864,979
		-
		-
		-
		Table 1: Summary of datasets used in our experiments
	</Abstractive Summary>
	<Extractive Summary> =
		First,
		SimpleWiki is too small for the experiment (b)
		(see Table 1)
	</Extractive Summary>
</Paper ID=ument119>


<Paper ID=ument119> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Performance of models on BEA-valid: a value
		in bold indicates the best result within the column
	</Abstractive Summary>
</Paper ID=ument119>


<Paper ID=ument119> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Performance on BEA-valid when changing the
		seed corpus T used for generating pseudo data (|Dp| =
		1
	</Abstractive Summary>
</Paper ID=ument119>


<Paper ID=ument119> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: Performance of the model with different opti-
		mization settings on BEA-valid
	</Abstractive Summary>
	<Extractive Summary> =
		(a) Joint Training or Pretraining
		Table 4
		presents the results
	</Extractive Summary>
</Paper ID=ument119>


<Paper ID=ument119> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Comparison of our best model and current top models: a bold value indicates the best result within the
		column
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows a remarkable result, that is,
		1240
		CoNLL-2014
		(M 2 scorer)
		CoNLL-2014
		(ERRANT)
		JFLEG
		BEA-test
		(ERRANT)
		Model
		Ensemble
		Prec
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 presents the results of applying SSE,
		13http://www
	</Extractive Summary>
</Paper ID=ument119>


<Paper ID=ument12> <Table ID =1>
	<Abstractive Summary> =
		Listener
		I am always scared about those
		airbags! I am so glad you are ok!
		Table 1: One conversation from empathetic dialogue, a
		speaker tells the situation he(she) is facing, and a lis-
		tener try to understand speaker’s feeling and respond
		accordingly
		tively less focus is emotional understanding and
		empathy (Rashkin et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows an conversation from the
		empathetic-dialogues dataset (Rashkin et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows an example from the training set
	</Extractive Summary>
</Paper ID=ument12>


<Paper ID=ument12> <Table ID =2>
	<Abstractive Summary> =
		47
		Table 2: Comparison between our proposed methods
		and baselines
	</Abstractive Summary>
</Paper ID=ument12>


<Paper ID=ument12> <Table ID =3>
	<Abstractive Summary> =
		7%
		Table 3: Result of human A/B test
	</Abstractive Summary>
</Paper ID=ument12>


<Paper ID=ument12> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Generated responses from TRS, Multi-TRS and MoEL in 2 different user emotion states (top) and com-
		paring generation from different listeners (bottom)
	</Abstractive Summary>
	<Extractive Summary> =
		Model response comparison
		The top part of
		Table 4 compares the generated responses from
		MoEL and the two baselines on two different
		speaker emotional states
	</Extractive Summary>
	<Extractive Summary> =
		From the genera-
		tion results in the bottom parts of Table 4 we can
		see that the corresponding listeners can produce
		empathetic and relevant responses when they rea-
		sonably match the speaker emotions
	</Extractive Summary>
</Paper ID=ument12>


<Paper ID=ument120> <Table ID =1>
	<Abstractive Summary> =
		09) character, series, star, game, trek, create
		Table 1: The Movies topics given by models
	</Abstractive Summary>
</Paper ID=ument120>


<Paper ID=ument120> <Table ID =2>
	<Abstractive Summary> =
		20) 音乐 (music), 乐团 (musical group), 艺术 (art),
		创作 (create), 奖 (award), 演出 (perform)
		Table 2: Topics are linked because they have overlap in
		topical words
	</Abstractive Summary>
</Paper ID=ument120>


<Paper ID=ument121> <Table ID =1>
	<Abstractive Summary> =
		Economic
		Policy
		Uncertainty (EPU)
		economic
		slowdown,
		institutional
		weaknesses, war, crisis, terrorist attack
		These policies and other institutional weaknesses con-
		tinue to undermine prospects for sustained economic de-
		velopment
		Table 1: Indicators, examples of event triggers (words or phrases) and sentences
		Ei (Ei is the set of events for indicator i), we
		then generate count Ne,t
	</Abstractive Summary>
</Paper ID=ument121>


<Paper ID=ument121> <Table ID =2>
	<Abstractive Summary> =
		0000
		Table 2: Correlation coefﬁcients between ECIMs and
		indicators
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows quantitative correlation analy-
		sis between ECIMs and their corresponding in-
		dicators
	</Extractive Summary>
</Paper ID=ument121>


<Paper ID=ument122> <Table ID =1>
	<Abstractive Summary> =
		AMT
		GCS
		Total
		# questionnaires
		156
		72
		228
		# sentences
		2,993
		1,311
		4,304
		# tokens
		30,700
		11,131
		41,831
		Table 1: Dataset size statistics
		2Our study was exempt from IRB because the contributors
		were anonymous
	</Abstractive Summary>
</Paper ID=ument122>


<Paper ID=ument122> <Table ID =2>
	<Abstractive Summary> =
		Type
		attributes
		FT
		ft id, parent id 1, parent id 2,
		deceased {T/F}
		Name
		ft id
		Gender
		ft id, sex: {male/female/unknown}
		Age
		ft id, age at diagnosis {T/F},
		age at death {T/F}
		Illness
		ft id, ill id, cause of death {T/F},
		cancer {T/F}, cancer type {23 types}
		Table 2: The annotation types included in our dataset
		with their corresponding attributes
	</Abstractive Summary>
	<Extractive Summary> =
		4 5 The
		set of labels used consist of the annotation labels
		(Table 2) combined with the IOB markers (Inside,
		Outside, Beginning)
	</Extractive Summary>
</Paper ID=ument122>


<Paper ID=ument122> <Table ID =3>
	<Abstractive Summary> =
		Simple sentences
		Complex sentences
		#sent
		#rels
		#sent
		#rels
		AMT
		2413
		7069
		520
		7374
		GCS
		453
		1409
		803
		4923
		Total
		3214
		8477
		973
		12,297
		Table 3: Sentence and relations statistics
		The two data subsets (AMT, GCS) were ob-
		tained using the same questionnaire, but they are
		slightly different
	</Abstractive Summary>
	<Extractive Summary> =
		(Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		Because the two subsections – AMT and GCS
		– are slightly different despite being obtained us-
		ing the same questionnaire (see Table 3), we test
		whether this difference inﬂuences the relation ex-
		traction models
	</Extractive Summary>
</Paper ID=ument122>


<Paper ID=ument122> <Table ID =4>
	<Abstractive Summary> =
		We split our dataset into
		1258
		Annotation type
		AMT
		GCS
		Total
		# FT
		2,633
		1,128
		3,761
		# Name
		1,640
		1,225
		2,865
		# Gender
		879
		601
		1,480
		# Age
		1,094
		914
		2,008
		# Illness
		1,636
		441
		2,077
		# cancer
		428
		346
		774
		# age at diagnosis
		645
		179
		824
		# age at death
		206
		156
		362
		# cause of death
		478
		227
		705
		# relations
		14,443
		6,322
		20,774
		Table 4: Annotation types, attributes, and relations
		statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the overall annotation statistics
	</Extractive Summary>
</Paper ID=ument122>


<Paper ID=ument122> <Table ID =5>
	<Abstractive Summary> =
		AMT
		GCS
		train
		val
		test
		train
		val
		test
		#questionnaires
		91
		15
		50
		45
		7
		20
		#sentences
		1574
		372
		1047
		893
		151
		267
		Table 5: Data distribution in each split
	</Abstractive Summary>
	<Extractive Summary> =
		train, validation, and test based on the question-
		naire identiﬁer (Table 5)
	</Extractive Summary>
</Paper ID=ument122>


<Paper ID=ument122> <Table ID =6>
	<Abstractive Summary> =
		933
		Table 6: The precision, recall, and F-score of our mod-
		els for entity and relation classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the performance of
		1259
		the tagger and relation classiﬁer on the test set
	</Extractive Summary>
</Paper ID=ument122>


<Paper ID=ument122> <Table ID =7>
	<Abstractive Summary> =
		95
		Table 7: Accuracy of models trained and evaluated on
		different parts of the dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the re-
		Test data
		Simple
		Complex
		All
		AMT training data
		AMT
		0
	</Extractive Summary>
</Paper ID=ument122>


<Paper ID=ument123> <Table ID =1>
	<Abstractive Summary> =
		795
		Table 1: BLEU scores on E2E-NLG (E), TV, Laptop
		(L), Hotel (H) and Restaurant (R) testset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Result
		We present our experimental results in Table 1 and
		2
	</Extractive Summary>
</Paper ID=ument123>


<Paper ID=ument123> <Table ID =2>
	<Abstractive Summary> =
		626
		Table 2: NIST scores on E2E-NLG testset
	</Abstractive Summary>
</Paper ID=ument123>


<Paper ID=ument123> <Table ID =3>
	<Abstractive Summary> =
		001
		Batch size
		20
		20
		20
		20
		20
		Dictionary dimension
		100
		50
		50
		200
		50
		RNN hidden size
		512
		128
		512
		256
		512
		Table 3: Hyperparameters of NLG-LM in experiments
	</Abstractive Summary>
</Paper ID=ument123>


<Paper ID=ument123> <Table ID =4>
	<Abstractive Summary> =
		64s
		Table 4: Running time per batch of NLG-LM and w/o
		LM on E2ENLG training set
	</Abstractive Summary>
</Paper ID=ument123>


<Paper ID=ument124> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Results of NLL on Ubuntu and Movie datasets
		with different models
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Negative Log-likelihood Evaluation
		Table 1 reports the comparisons of the per-word
		negative log-likelihood (NLL) composition of dif-
		ferent models
	</Extractive Summary>
	<Extractive Summary> =
		As can be seen from Table 1, with almost the
		same KL-divergence, Dir-VHRED achieves the
		lowest NLL on both datasets, implying better per-
		formance compared with VHRED
	</Extractive Summary>
</Paper ID=ument124>


<Paper ID=ument124> <Table ID =2>
	<Abstractive Summary> =
		Therefore,
		Table 2: Results of three embedding metrics on Ubuntu
		dataset with 1-turn and 3-turn responses
	</Abstractive Summary>
</Paper ID=ument124>


<Paper ID=ument124> <Table ID =3>
	<Abstractive Summary> =
		313
		Table 3: Results of three embedding metrics on Movie
		dataset with 1-turn and 3-turn responses
	</Abstractive Summary>
</Paper ID=ument124>


<Paper ID=ument124> <Table ID =4>
	<Abstractive Summary> =
		Volunteers are re-
		quired to rate the generated responses from differ-
		ent models with scores from 1 to 3 (3 is the best),
		in term of the sentence coherence and contextual
		Table 4: Responses generated by different models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 lists some examples of the generated
		sentences of different models
	</Extractive Summary>
</Paper ID=ument124>


<Paper ID=ument124> <Table ID =5>
	<Abstractive Summary> =
		The dimension 3 (dim3) focuses on generat-
		ing the longest and most comprehensive responses
		that are usually declarative sentences and are high-
		Table 5: Responses generated by ablation study on dif-
		ferent dimension of z in Dir-VHRED
	</Abstractive Summary>
</Paper ID=ument124>


<Paper ID=ument125> <Table ID =1>
	<Abstractive Summary> =
		72
		Table 1: The accuracy (%) of different classiﬁcation
		of slot-value pairs in terms of their number of training
		examples
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents the results, showing
		that the Π-model improves accuracy by 5% when
		the slot-value pair is rarely (1-10 times) seen dur-
		ing training
	</Extractive Summary>
</Paper ID=ument125>


<Paper ID=ument126> <Table ID =1>
	<Abstractive Summary> =
		The ﬁrst step is to train the base
		model M0 using the loss function L0:
		L0(θθθ0) ≜ − 1
		n
		|S|
		�
		j=1
		n
		�
		i=1
		yj(i) log P 0
		j (i)
		where θθθ0 are the parameters in M0; |S| is the
		number of semantic slots in IOB format; and n is
		Table 1: Grouping Statistics in All Datasets
		Dataset
		Domain
		Group
		Vocab Size
		#Slots
		Train Size
		ATIS
		-
		1
		706
		49
		3,666
		2
		301
		24
		423
		3
		271
		18
		232
		4
		312
		29
		266
		5
		309
		33
		391
		Snips
		Add
		To
		Playlist
		1
		2,561
		6
		1417
		2
		802
		5
		259
		3
		764
		5
		259
		Book
		Restaurant
		1
		1,588
		14
		995
		2
		1,044
		14
		487
		3
		990
		14
		486
		Get
		Weather
		1
		1,095
		9
		805
		2
		977
		9
		592
		3
		946
		9
		591
		Play
		Music
		1
		2,333
		9
		1,546
		2
		464
		9
		210
		3
		492
		9
		210
		Rate
		Book
		1
		916
		7
		822
		2
		708
		6
		539
		3
		722
		6
		538
		Search
		Creative
		Work
		1
		1,353
		2
		690
		2
		1,298
		2
		631
		3
		1,335
		2
		630
		Search
		Screening
		Event
		1
		863
		7
		883
		2
		687
		7
		472
		3
		668
		7
		471
		the sequence length
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the detailed data statistics
	</Extractive Summary>
</Paper ID=ument126>


<Paper ID=ument126> <Table ID =2>
	<Abstractive Summary> =
		Since each domain in
		1282
		Table 2: Performance (F1 Score) on ATIS Dataset
		Approach
		Batch
		0
		1
		2
		3
		4
		AttRNN (upper bound)
		92
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Main Results
		Table 2 and Table 3 show the F1 score of
		slot ﬁlling performance comparison results on
		ATIS dataset and each domain of Snips dataset
	</Extractive Summary>
</Paper ID=ument126>


<Paper ID=ument126> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Performance (F1 Score) on Snips Dataset
		Domain
		Approach
		Batch
		0
		1
		2
		Add
		To
		Playlist
		AttRNN (upper bound)
		79
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Main Results
		Table 2 and Table 3 show the F1 score of
		slot ﬁlling performance comparison results on
		ATIS dataset and each domain of Snips dataset
	</Extractive Summary>
</Paper ID=ument126>


<Paper ID=ument126> <Table ID =4>
	<Abstractive Summary> =
		This has two
		reasons: First, the utterances in different groups of
		ATIS are quite structurally similar such that c-IDE
		further enhances the correct slot label distribution
		Table 4: Average Model Size (MB)
		Domain
		Batch
		Approach
		FT-AttRNN
		FT-Cp-AttRNN
		ProgModel
		ATIS
		0
		18
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Model Size Results
		Table 4 reports the model size comparison
		between FT-AttRNN
		(FT-AttRNN
		and FT-Lr-
		AttRNN ), FT-Cp-AttRNN
		and ProgModel (t-
		ProgModel and c-ProgModel)
	</Extractive Summary>
</Paper ID=ument126>


<Paper ID=ument127> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1:
		Average IC accuracy scores (%) on non-
		contextual datasets
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results and Analysis
		Table 1 shows the performance of our non-
		contextual model on two datasets, ATIS and
		SNIPS
	</Extractive Summary>
</Paper ID=ument127>


<Paper ID=ument127> <Table ID =2>
	<Abstractive Summary> =
		97
		Table 2: IC accuracy and SL F1 scores (%) for the
		three models NC-NLU, CGRU-NLU, CASA-NLU on the
		2 contextual datasets - Booking and Cable
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, CASA-NLU model outper-
		forms non-contextual NLU on both the Booking
		and Cable datasets
	</Extractive Summary>
</Paper ID=ument127>


<Paper ID=ument127> <Table ID =3>
	<Abstractive Summary> =
		25
		Table 3:
		IC accuracy scores (%) on ﬁrst (Ft) and
		follow-up (FU) turns in contextual datasets - Booking
		and Cable
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 gives further breakdown of
		the results by showing performance on ﬁrst vs
	</Extractive Summary>
</Paper ID=ument127>


<Paper ID=ument127> <Table ID =4>
	<Abstractive Summary> =
		25
		Table 4: Impact of contextual signals on IC accuracy
		and SL F1 scores (%) on Booking validation set for
		CASA-NLU
		Qualitative Analysis: Using example conver-
		sation in Figure 1, we highlight the relevance
		of contextual information in making intent pre-
		dictions by visualizing attention weights P(T)
		(Equation 1) for different contextual signals as
		shown in Figure 3
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows impact of some of the contex-
		tual signals on model performance for the Booking
		validation dataset
	</Extractive Summary>
</Paper ID=ument127>


<Paper ID=ument128> <Table ID =1>
	<Abstractive Summary> =
		796
		Table 1: Evaluation results of different sampling strategies on the two data sets
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Evaluation Results
		Table 1 reports evaluation results on the two data
		sets
	</Extractive Summary>
</Paper ID=ument128>


<Paper ID=ument129> <Table ID =1>
	<Abstractive Summary> =
		43
		Table 1: Results on different models including baseline models, where N refers to the Gaussian noise injection
		and R refers to the cross-lingual embeddings reﬁnement
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results & Discussion
		From Table 1, in general, LVM outperforms CRF
		models
	</Extractive Summary>
</Paper ID=ument129>


<Paper ID=ument129> <Table ID =2>
	<Abstractive Summary> =
		35
		Table 2: Ablation Study on LVM models
	</Abstractive Summary>
	<Extractive Summary> =
		Finally, in Table 2, we ablate the usage of LVM
		to see whether the boost of performance comes
		simply from the increase of parameter size
	</Extractive Summary>
</Paper ID=ument129>


<Paper ID=ument13> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Comparison of our model with baselines
		In the second block of Table 1, we can see that
		our framework trained with both the distant super-
		vision and the Gumbel-Softmax beats all existing
		models on two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		In the ﬁrst block of Table 1, we show the Hu-
		man, rule-based and KV Net (with*) result which
		are reported from Eric et al
	</Extractive Summary>
</Paper ID=ument13>


<Paper ID=ument13> <Table ID =2>
	<Abstractive Summary> =
		69
		Table 2: The generation consistency and Human Eval-
		uation on navigation domain
	</Abstractive Summary>
</Paper ID=ument13>


<Paper ID=ument130> <Table ID =1>
	<Abstractive Summary> =
		Would you like to
		ﬁnd tickets for a showing for any of them?
		agent acts
		inform(moviename=The Witch, The Other Side of the Door, The Boy;
		genre=thriller) multiple choice(moviename)
		Table 1: Dialogue example
	</Abstractive Summary>
	<Extractive Summary> =
		An exam-
		ple in Table 1 shows how the agent can produce
		both an inform and a multiple choice act, reducing
		the need for additional turns
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows an example, with the genre
		slot provided by the user and the system repeat-
		ing it in its answer
	</Extractive Summary>
	<Extractive Summary> =
		Critical slots refers to slots
		that the system must provide like moviename in
		the Table 1 example
	</Extractive Summary>
</Paper ID=ument130>


<Paper ID=ument130> <Table ID =2>
	<Abstractive Summary> =
		(3)
		The overall loss is the sum of the losses of the
		three units: L = Lc + La + Ls
		1306
		annotation
		inform(moviename=The Witch, The Other Side of the Door, The Boy; genre=thriller) multiple choice(moviename)
		classiﬁcation
		inform+moviename, inform+genre, multiple choice+moviename
		sequence
		‘inform’ ‘(’ ‘moviename’ ‘=’ ‘;’ ‘genre’ ‘=’ ‘)’ ‘multiple choice’ ‘(’ ‘moviename’ ‘)’ ‘⟨eos⟩’
		cas sequence
		(⟨continue⟩, inform, {moviename, genre}) (⟨continue⟩, multiple choice, {moviename}) (⟨stop⟩, ⟨pad⟩, {})
		Table 2: Multiple dialogue act format in different architectures
	</Abstractive Summary>
	<Extractive Summary> =
		The most common failure case
		is on non-critical slots (like ‘genre’ in the example
		in Table 2): gCAS does not predict them, while it
		predicts the critical ones (like ‘moviename’ in the
		example in Table 2)
	</Extractive Summary>
</Paper ID=ument130>


<Paper ID=ument130> <Table ID =3>
	<Abstractive Summary> =
		domain
		total
		train
		valid
		test
		acts
		slots
		pairs
		movie
		2888
		1445
		433
		1010
		11
		29
		90
		taxi
		3093
		1548
		463
		1082
		11
		23
		63
		restaurant
		4101
		2051
		615
		1435
		11
		31
		91
		Table 3: Dataset: train, validation and test split, and the
		count of distinct acts, slots and act-slot pairs
	</Abstractive Summary>
	<Extractive Summary> =
		In the movie and restaurant domains, the inform
		act usually appears during the dialogue and there
		are many optional non-critical slots that can ap-
		pear (see Table 3, movie and restaurant domains
		have more slots and pairs than the taxi domain)
	</Extractive Summary>
</Paper ID=ument130>


<Paper ID=ument130> <Table ID =4>
	<Abstractive Summary> =
		domain & speaker
		1 act
		2 acts
		3 acts
		4 acts
		movie user
		9130
		1275
		106
		11
		movie agent
		5078
		4982
		427
		33
		taxi user
		10544
		762
		50
		8
		taxi agent
		7855
		3301
		200
		8
		restaurant user
		12726
		1672
		100
		3
		restaurant agent
		10333
		3755
		403
		10
		Table 4: Dialogue act counts by turn
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the
		count of turns with multiple act annotations, which
		amounts to 23% of the dataset
	</Extractive Summary>
</Paper ID=ument130>


<Paper ID=ument130> <Table ID =5>
	<Abstractive Summary> =
		52
		Table 5: Entity F1 and Success F1 at dialogue level
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Result and Error Analysis
		As shown in Table 5, gCAS outperforms all other
		methods on Entity F1 in all three domains
	</Extractive Summary>
</Paper ID=ument130>


<Paper ID=ument130> <Table ID =6>
	<Abstractive Summary> =
		49
		Table 6: Precision (P), Recall (R) and F1score (F1) of turn-level acts and frames
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the turn-level acts and frame pre-
		diction performance
	</Extractive Summary>
</Paper ID=ument130>


<Paper ID=ument130> <Table ID =7>
	<Abstractive Summary> =
		example 1
		example 2
		groundtruth
		request(date; starttime)
		inform(restaurantname=; starttime =) multiple choice(restaurantname)
		classiﬁcation
		request+date
		[]
		Seq2Seq
		‘request’ ‘(’ ‘date’ ‘;’ ‘starttime’ ‘)’
		‘inform’ ‘(’ ‘restaurantname’ ‘=’ ‘)’ ‘multiple choice’ ‘=’ ‘restaurantname’ ‘)’
		Copy Seq2Seq
		‘request’ ‘(’ ‘date’ ‘=’ ‘)’
		‘inform’ ‘(’ ‘restaurantname’ ‘=’ ‘;’ ‘;’, ‘;’, ‘=’, ‘;’ ‘starttime’ ‘=’ ‘)’
		CAS
		request {}
		inform {restaurantname}
		gCAS
		request {date; starttime}
		inform {restaurantname} multiple choice{restaurantname}
		Table 7: Examples of predicted dialogue acts in the restaurant domain
	</Abstractive Summary>
	<Extractive Summary> =
		The advantage of gCAS in the restaurant domain
		is much more evident: the agent’s inform act usu-
		ally has multiple slots (see example 2 in Table 7)
		and this makes classiﬁcation and sequence gener-
		ation harder, but gCAS multi-label slots decoder
		handles it easily
	</Extractive Summary>
	<Extractive Summary> =
		The main reason is that CAS and gCAS out-
		put a tuple at each recurrent step, which makes for
		shorter sequences that are easier to generate com-
		pared to the long sequences of Seq2Seq (example
		2 in Table 7)
	</Extractive Summary>
	<Extractive Summary> =
		The classiﬁcation method has a good
		precision score, but a lower recall score, suggest-
		ing it has problems making granular decisions (ex-
		ample 2 in Table 7)
	</Extractive Summary>
	<Extractive Summary> =
		Table 7 shows predictions of all methods from
		two emblematic examples
	</Extractive Summary>
</Paper ID=ument130>


<Paper ID=ument130> <Table ID =8>
	<Abstractive Summary> =
		41
		Table 8: P, R and F1 of turn-level inform all slots and non-critical slots
	</Abstractive Summary>
</Paper ID=ument130>


<Paper ID=ument130> <Table ID =9>
	<Abstractive Summary> =
		01
		Table 9: P, R and F1 of turn-level inform critical slots
	</Abstractive Summary>
</Paper ID=ument130>


<Paper ID=ument131> <Table ID =1>
	<Abstractive Summary> =
		1313
		Domain
		Intent
		Query
		BANKING
		TRANSFER
		move 100 dollars from my savings to my checking
		WORK
		PTO REQUEST
		let me know how to make a vacation request
		META
		CHANGE LANGUAGE
		switch the language setting over to german
		AUTO & COMMUTE
		DISTANCE
		tell the miles it will take to get to las vegas from san diego
		TRAVEL
		TRAVEL SUGGESTION
		what sites are there to see when in evans
		HOME
		TODO LIST UPDATE
		nuke all items on my todo list
		UTILITY
		TEXT
		send a text to mom saying i’m on my way
		KITCHEN & DINING
		FOOD EXPIRATION
		is rice ok after 3 days in the refrigerator
		SMALL TALK
		TELL JOKE
		can you tell me a joke about politicians
		CREDIT CARDS
		REWARDS BALANCE
		how high are the rewards on my discover card
		OUT-OF-SCOPE
		OUT-OF-SCOPE
		how are my sports teams doing
		OUT-OF-SCOPE
		OUT-OF-SCOPE
		create a contact labeled mom
		OUT-OF-SCOPE
		OUT-OF-SCOPE
		what’s the extended zipcode for my address
		Table 1: Sample queries from our dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows exam-
		ples of the data
	</Extractive Summary>
</Paper ID=ument131>


<Paper ID=ument131> <Table ID =2>
	<Abstractive Summary> =
		8
		—
		Table 2: Benchmark classiﬁer results under each data condition using the oos-train (top half) and oos-threshold
		(bottom half) prediction methods
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results with oos-train
		Table 2 presents results for all models across the
		four variations of the dataset
	</Extractive Summary>
</Paper ID=ument131>


<Paper ID=ument131> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Results of oos-binary experiments on OOS+,
		where we compare performance of undersampling (un-
		der) and augmentation using sentences from Wikipedia
		(wiki aug)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results with oos-binary
		Table 3 compares classiﬁer performance using the
		oos-binary scheme
	</Extractive Summary>
</Paper ID=ument131>


<Paper ID=ument131> <Table ID =4>
	<Abstractive Summary> =
		(2019)
		54
		25,716
		
		
		
		
		Table 4: Classiﬁcation dataset properties
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 compares our dataset with other short-
		query intent classiﬁcation datasets
	</Extractive Summary>
	<Extractive Summary> =
		Cru-
		cially, none of the other datasets summarized in
		Table 4 offer a feasible way to evaluate out-of-
		scope performance
	</Extractive Summary>
</Paper ID=ument131>


<Paper ID=ument132> <Table ID =1>
	<Abstractive Summary> =
		00
		Table 1: Activity, Entity F1 results reported by previous
		work (rows 1-4 from Serban et al
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results and Analysis
		Automatic Results: Table 1 shows that all data-
		augmentation approaches (last 3 rows) improve
		statistically signiﬁcantly (p < 0
	</Extractive Summary>
</Paper ID=ument132>


<Paper ID=ument132> <Table ID =2>
	<Abstractive Summary> =
		All-ops
		50
		13
		37
		13
		Table 2: Human evaluation results on comparisons
		among the baseline, All-operations, and the two Au-
		toAugment models
	</Abstractive Summary>
	<Extractive Summary> =
		11
		Human Evaluation: In Table 2, both AutoAug-
		9We do not follow Cubuk et al
	</Extractive Summary>
</Paper ID=ument132>


<Paper ID=ument132> <Table ID =3>
	<Abstractive Summary> =
		eou
		ah , it happened to you too ?
		Table 3: Comparisons of perturbed outputs among the three Augmentation models
	</Abstractive Summary>
</Paper ID=ument132>


<Paper ID=ument132> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Top 3 policies on the validation set and their
		test performances
	</Abstractive Summary>
	<Extractive Summary> =
		Policy Learned by the Input-Agnostic Con-
		troller: We present 3 best learned policies from
		the Ubuntu val set (Table 4)
	</Extractive Summary>
</Paper ID=ument132>


<Paper ID=ument133> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Test accuracies on STS
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 1, our method improves the test accuracy
		over the baseline by 0
	</Extractive Summary>
</Paper ID=ument133>


<Paper ID=ument133> <Table ID =2>
	<Abstractive Summary> =
		zh-en
		ru-en
		train
		26M
		25M
		valid
		2K
		3K
		test
		6K
		6K
		Table 2: Number of sentence pairs in WMT
	</Abstractive Summary>
</Paper ID=ument133>


<Paper ID=ument133> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Test perplexities on WMT
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, we see consis-
		tent improvements in the uniblock-ﬁltered se-
		tups of zh, ru and en under zh-en
	</Extractive Summary>
</Paper ID=ument133>


<Paper ID=ument133> <Table ID =4>
	<Abstractive Summary> =
		82
		Financial Policy and Development Unit
		Table 4: Several text samples from the zh-en parallel corpus with uniblock scores
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 4, a Chinese sentence could be corrupted
		with Japanese characters and an English sentence
		could be corrupted with French characters
	</Extractive Summary>
</Paper ID=ument133>


<Paper ID=ument133> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Case-sensitive BLEU(%) scores on four differ-
		ent translation tasks
	</Abstractive Summary>
</Paper ID=ument133>


<Paper ID=ument134> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Multilingual word translation results for English, German, French, Spanish, Italian and Portuguese
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents detailed results for all
		30 language pairs and the average results
	</Extractive Summary>
</Paper ID=ument134>


<Paper ID=ument134> <Table ID =2>
	<Abstractive Summary> =
		source word direct (BI)
		multilingual (CAT) auxiliary words
		pace (en)
		veloz (es)
		ritmo (es)
		ritmo (pt), tempo (de)
		Cornell (en) Harvard (it)
		Cornell (it)
		Cornell (de), Cornell (fr), Cornell (es)
		lens (en)
		pentaprisma (it) lente (it)
		lente (es), lente (pt), linse (de)
		Table 2: Examples of erroneous translation of BI corrected by CAT, and its relevant auxiliary words
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents three
		examples of erroneous bilingual translations that
		were corrected using auxiliary languages
	</Extractive Summary>
</Paper ID=ument134>


<Paper ID=ument134> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Multilingual word translation results for English, German, French, Spanish, Italian, Japanese, Chinese
		and Korean
	</Abstractive Summary>
	<Extractive Summary> =
		A similar behaviour is shown in the European-
		Asian languages (Table 3)
	</Extractive Summary>
</Paper ID=ument134>


<Paper ID=ument134> <Table ID =4>
	<Abstractive Summary> =
		→
		en
		de
		fr
		es
		it
		pt
		en
		es (29%) – (29%)
		– (29%) fr (28%) fr (29%)
		de – (36%)
		en (37%) en (34%) en (37%) en (37%)
		fr it (29%) it (27%)
		– (29%)
		– (32%)
		it (30%)
		es pt (44%) pt (40%) pt (39%)
		pt (43%) – (47%)
		it fr (33%) es (36%) es (35%) – (35%)
		es (34%)
		pt es (59%) es (55%) es (58%) – (62%) es (57%)
		Table 4: Auxiliary languages that are most frequently selected by the CNT method (’–’ is none), for every pair of
		source and target languages
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		shows auxiliary language that is most commonly
		selected, for each pair of source-target languages
	</Extractive Summary>
</Paper ID=ument134>


<Paper ID=ument135> <Table ID =1>
	<Abstractive Summary> =
		30⇑
		Table 1: Case-sensitive BLEU scores on the WMT14
		English⇒German translation task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists the results
	</Extractive Summary>
</Paper ID=ument135>


<Paper ID=ument135> <Table ID =2>
	<Abstractive Summary> =
		76
		Table 2: Results for encoder strategies
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2, to compare with the pro-
		posed hybrid model, we stack 3-layers ON-LSTM
		on the top of 3-layers SANs (Row 2)
	</Extractive Summary>
</Paper ID=ument135>


<Paper ID=ument135> <Table ID =3>
	<Abstractive Summary> =
		85
		Table 3: Performance on the linguistic probing tasks
		of evaluating linguistics embedded in the learned rep-
		resentations
	</Abstractive Summary>
</Paper ID=ument135>


<Paper ID=ument136> <Table ID =1>
	<Abstractive Summary> =
		90
		Table 1: De–Fr test precision (P), recall (R), and F1
	</Abstractive Summary>
</Paper ID=ument136>


<Paper ID=ument136> <Table ID =2>
	<Abstractive Summary> =
		7M
		Somali
		So
		37413
		85k
		Afrikaans
		Af
		37081
		67k
		Tagalog
		Tl
		34207
		36k
		Norwegian
		No
		37064
		0*
		Table 2: Bible statistics
	</Abstractive Summary>
</Paper ID=ument136>


<Paper ID=ument136> <Table ID =3>
	<Abstractive Summary> =
		247
		Table 3: Bible verse alignment results
	</Abstractive Summary>
</Paper ID=ument136>


<Paper ID=ument137> <Table ID =1>
	<Abstractive Summary> =
		However, previous policy-learning
		1350
		German
		Ich
		bin
		mit
		dem
		Bus
		nach
		Ulm
		gekommen
		Gloss
		I
		am
		with
		the
		bus
		to
		Ulm
		come
		Action
		R
		W R
		R
		R
		R
		W
		W
		W R
		R
		R
		W
		W
		W
		W
		Translation
		I
		took the bus
		to come to Ulm
		Table 1: An example for READ/WRITE action sequence
	</Abstractive Summary>
	<Extractive Summary> =
		Thus, an adaptive policy (see Table 1 as an
		example), which can decides on the ﬂy whether
		to take READ action or WRITE action, is more
		desirable for simultaneous translation
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 gives an example for such a sequence
	</Extractive Summary>
</Paper ID=ument137>


<Paper ID=ument137> <Table ID =2>
	<Abstractive Summary> =
		Table 2: German-to-English example from validation set
	</Abstractive Summary>
	<Extractive Summary> =
		We also provide a translation example in Table 2 to
		compare diﬀerent methods
	</Extractive Summary>
</Paper ID=ument137>


<Paper ID=ument137> <Table ID =3>
	<Abstractive Summary> =
		2
		665
		665
		630
		Table 3: Training time (in hours) of diﬀerent methods
	</Abstractive Summary>
</Paper ID=ument137>


<Paper ID=ument138> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Classiﬁcation accuracy on the MLDoc test sets
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1, we report the classiﬁcation accuracy
		for all of the languages in MLDoc
	</Extractive Summary>
</Paper ID=ument138>


<Paper ID=ument138> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: F1 scores on the CoNLL 2002/2003 NER test sets
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we report the F1 scores for all
		of the CoNLL NER languages
	</Extractive Summary>
</Paper ID=ument138>


<Paper ID=ument138> <Table ID =3>
	<Abstractive Summary> =
		91
		Table 3:
		Median cosine similarity between the
		mean-pooled BERT embeddings of MLDoc English
		documents and their translations, with and without
		language-adversarial training
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we observe that the median cosine
		similarity increases dramatically with adversarial
		training, which suggests that the embeddings be-
		came more language-independent
	</Extractive Summary>
</Paper ID=ument138>


<Paper ID=ument139> <Table ID =1>
	<Abstractive Summary> =
		1M
		Table 1: Results for EN-DE translation task
	</Abstractive Summary>
</Paper ID=ument139>


<Paper ID=ument139> <Table ID =2>
	<Abstractive Summary> =
		5M
		Table 2: Results for ZH-EN translation Task
	</Abstractive Summary>
	<Extractive Summary> =
		In addition, Table 2 shows that the proposed
		models also gave similar improvements over the
		baseline system and the compared methods on the
		NIST ZH-EN task
	</Extractive Summary>
</Paper ID=ument139>


<Paper ID=ument139> <Table ID =3>
	<Abstractive Summary> =
		7K
		Table 3: Ablation experiments of position information
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results of ablation experiments
		on the EN-DE newstest2014 test set:
		newstest2014
		#Speed
		Transformer (base)
		27
	</Extractive Summary>
</Paper ID=ument139>


<Paper ID=ument14> <Table ID =1>
	<Abstractive Summary> =
		Table 1: A dialog example with the ground truth caption: bunches of bananas hang on a wall and arranged for
		sale
	</Abstractive Summary>
</Paper ID=ument14>


<Paper ID=ument14> <Table ID =2>
	<Abstractive Summary> =
		35
		Table 2: RL-Q-IG-NA performs best in PMR and RL-
		Q-IG perform best in perplexity
		Although our framework achieved an improved
		image retrieval accuracy, we observed, however,
		that there is little improvement gained in PMR af-
		ter additional rounds of conversation
	</Abstractive Summary>
</Paper ID=ument14>


<Paper ID=ument14> <Table ID =3>
	<Abstractive Summary> =
		25
		Table 3: Evaluation results on the human-AI image guessing game initialized with ground truth captions
		Model
		Win
		Fluency
		Relevance
		Comprehension
		Diversity
		RL-Q
		29
	</Abstractive Summary>
</Paper ID=ument14>


<Paper ID=ument14> <Table ID =4>
	<Abstractive Summary> =
		06
		Table 4: Evaluation results on the human-AI image guessing game initialized with human generated captions
		cremental improvement over the full framework
		RL-Q-IG in terms of the PMR rate with less than
		0
	</Abstractive Summary>
	<Extractive Summary> =
		From the results summarized in Ta-
		ble 3 and Table 4, we observe a consistent op-
		timal performance of our method from conver-
		sations with AI agent to conversations with real
		human
	</Extractive Summary>
</Paper ID=ument14>


<Paper ID=ument14> <Table ID =5>
	<Abstractive Summary> =
		20
		Table 5: Results on comparative evaluation of rele-
		vance on the human-AI image guessing dialogs
		6
		Conclusion and Future Work
		We present a novel framework for building a task-
		oriented visual dialog system
	</Abstractive Summary>
</Paper ID=ument14>


<Paper ID=ument14> <Table ID =6>
	<Abstractive Summary> =
		153
		B
		Qualitative Examples
		Image
		RL-Q-ImGuess
		SL-Q-ImGuess
		RL-Q
		Q: what color is vase?
		Q: what color is vase?
		Q: what color is table?
		A: it is transparent
		A: it is brown
		A: it is orange
		Q: is it on table?
		Q: is it in vase?
		Q: is there anything else on table?
		A: yes it is
		yes it is
		A: yes there are ﬂowers
		Q: is it in kitchen?
		Q: is it in vase?
		Q: what color is table?
		A: I do not know
		A: yes it is
		A: it is orange
		Q: any people?
		Q: is it in vase?
		Q: is there anything else on table?
		A: no, there is no people
		A: yes it is
		A: yes there are
		Q: what color is the table?
		Q: is it in vase?
		Q: is there anything else on table?
		A: yes there is
		A: yes it is
		A: yes there is
		Table 6: A dialog example with the human generated caption: there is a plant in a vase and cookies
	</Abstractive Summary>
</Paper ID=ument14>


<Paper ID=ument14> <Table ID =7>
	<Abstractive Summary> =
		Image
		RL-Q-ImGuess
		SL-Q-ImGuess
		RL-Q
		Q: are men old?
		Q: how old are men?
		Q: what color is hat?
		A: No they are not
		A: 30 years old
		A: there is no hat
		Q: are they in uniform?
		Q: are they in city?
		Q: is it sunny?
		A: I’m not sure
		A: yes they are
		A: it seems yes
		Q: is it sunny?
		Q: is it sunny?
		Q: is this in city?
		A: yes it is
		A: yes it is
		A: it is in city
		Q: are they on road?
		Q: any other people?
		Q: are there any people in picture?
		A: yes they are
		A: no other people
		A: there are two people
		Q: are they in parking lot?
		Q: animals?
		Q: is this in home or restaurant?
		A: No they’re not
		A: no other animals
		A: it is outside
		Table 7: A dialog example with the human generated caption: two men in formal wear standing next to a
		monster truck
	</Abstractive Summary>
</Paper ID=ument14>


<Paper ID=ument14> <Table ID =8>
	<Abstractive Summary> =
		Image
		RL-Q-ImGuess
		SL-Q-ImGuess
		RL-Q
		Q: how many horses are there?
		Q: how many horses are there?
		Q: is this in park?
		A: there are no horses
		A: There are no horses, only bulls
		A: maybe it is in park
		Q: are they in wild?
		Q: what color are horses?
		Q: are there any people in picture?
		A: no, they are in village
		A: they are white and black
		A: yes there is one picture
		Q: is there fence?
		Q: are they in ﬁeld?
		Q: is this in home or restaurant?
		A: yes, there is fence
		A: Yes they are
		A: it is in neither of those
		Q: any people?
		Q: is it sunny?
		Q: are there any people in picture?
		A: yes, a person
		A: Yes it is
		A: yes there is one person
		Q: animals?
		Q: is it sunny?
		Q: is this in restaurant?
		A: yes, several bulls
		A: Yes it is
		A: no it is not
		Table 8: A dialog example with the human generated caption: an image of running with the bulls outside
	</Abstractive Summary>
</Paper ID=ument14>


<Paper ID=ument14> <Table ID =9>
	<Abstractive Summary> =
		Table 9: A dialog example with the human generated caption: a man holding a kite while a girl tries to ﬂy it
	</Abstractive Summary>
</Paper ID=ument14>


<Paper ID=ument140> <Table ID =1>
	<Abstractive Summary> =
		56M
		Table 1: Statistics of the parallel corpora used for train-
		ing the generic NMT systems
		To emulate both scarce and sufﬁcient training
		data conditions, MT systems are trained using 5%
		and 100% of the available parallel data
	</Abstractive Summary>
</Paper ID=ument140>


<Paper ID=ument140> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: Classiﬁcation results (F1) obtained with: i) au-
		tomatic English translations by three models (Generic,
		Reinforce, MO-Reinforce), and ii) gold-standard En-
		glish (English) and untranslated German/Italian (Orig-
		inal) tweets
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results and Discussion
		Table 2 shows our classiﬁcation results, presenting
		the F1 scores obtained by the different MT-based
		approaches in the two training conditions
	</Extractive Summary>
</Paper ID=ument140>


<Paper ID=ument141> <Table ID =1>
	<Abstractive Summary> =
		8K
		Table 1: Datasets characteristics
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Experiments
		We worked on the lowercased Kyoto Free Trans-
		lation Task (Neubig, 2011, KFTT) and IWSLT’14
		(en↔de), without removing any sentences (see
		Table 1)
	</Extractive Summary>
</Paper ID=ument141>


<Paper ID=ument142> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example Russian source sentence, lexicon en-
		try, and English target sentence
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows example lexi-
		con entries resulting from this annotation process
	</Extractive Summary>
</Paper ID=ument142>


<Paper ID=ument142> <Table ID =2>
	<Abstractive Summary> =
		1384
		Development
		Test
		Entries
		Sentences
		Entries
		Sentences
		Ru
		9040
		2412
		8001
		2142
		Ko
		5593
		1744
		5595
		1756
		Zh
		1773
		885
		2289
		1025
		Table 2: Number of bilingual lexicon entries and sen-
		tences containing at least one annotation for each lan-
		guage pair
	</Abstractive Summary>
</Paper ID=ument142>


<Paper ID=ument142> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: BLEU and recall % (rec
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results & Discussion
		Table 3 summarizes key exploratory results for the
		baseline, CT, and CD approaches on the develop-
		ment set
	</Extractive Summary>
	<Extractive Summary> =
		(See Figure 1 for example performance as
		the model is trained on the bilingual lexicon and
		Table 3 for results on all three language pairs
	</Extractive Summary>
</Paper ID=ument142>


<Paper ID=ument142> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Baseline and constrained decoding (random)
		on test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows baseline and random CD
		benchmarks on the test set, which we otherwise re-
		serve and release for future evaluation
	</Extractive Summary>
</Paper ID=ument142>


<Paper ID=ument143> <Table ID =1>
	<Abstractive Summary> =
		17
		Table 1: BLEU of our approach (Reorder) with differ-
		ent amount of parallel sentences of ja-en and ug-en
		translation
	</Abstractive Summary>
</Paper ID=ument143>


<Paper ID=ument143> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Translation examples on ja-en (reorder with 6000 supervised pairs) and ug-en (reorder) from our
		model and the supervised counterpart
	</Abstractive Summary>
</Paper ID=ument143>


<Paper ID=ument144> <Table ID =1>
	<Abstractive Summary> =
		, 2019b)
		Table 1: Recent advances in simultaneous translation
	</Abstractive Summary>
</Paper ID=ument144>


<Paper ID=ument144> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Zh→En wait-1 model BLEU improvement of
		SBS against greedy search (b = 1, w = 0) on dev-set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the BLEU score
		of different b and w over wait-1 model
	</Extractive Summary>
</Paper ID=ument144>


<Paper ID=ument145> <Table ID =1>
	<Abstractive Summary> =
		67
		Table 1: Impact of the position encoding components
		on Chinese⇒English NIST02 development dataset us-
		ing Transformer-Base model
	</Abstractive Summary>
</Paper ID=ument145>


<Paper ID=ument145> <Table ID =2>
	<Abstractive Summary> =
		19⇑
		Table 2: Evaluation of translation performance on NIST Zh⇒En and WMT14 En⇒De test sets
	</Abstractive Summary>
</Paper ID=ument145>


<Paper ID=ument145> <Table ID =3>
	<Abstractive Summary> =
		12
		Table 3: Performance on linguistic probing tasks
	</Abstractive Summary>
	<Extractive Summary> =
		The experimental results on probing
		tasks are shown in Table 3, and the BLEU scores
		of “Base”, “+ Rel
	</Extractive Summary>
</Paper ID=ument145>


<Paper ID=ument146> <Table ID =1>
	<Abstractive Summary> =
		06∗+†
		Table 1: BLEU scores for all the tested conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 1 gives the BLEU scores (Papineni et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows that some languages are better
		translated (En-Vi) than others (En-Bn)
	</Extractive Summary>
</Paper ID=ument146>


<Paper ID=ument147> <Table ID =1>
	<Abstractive Summary> =
		79
		Table 1: Translation accuracy (BLEU) under different settings
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in the
		ﬁrst 6 results columns of Table 1, the unadapted
		baseline model (row 1) performs poorly when
		adapting between domains
	</Extractive Summary>
</Paper ID=ument147>


<Paper ID=ument147> <Table ID =2>
	<Abstractive Summary> =
		44
		Table 2:
		Providing mismatched domain embeddings
		leads to degraded performance
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, feeding mismatched
		domain embeddings leads to worse performance
	</Extractive Summary>
</Paper ID=ument147>


<Paper ID=ument147> <Table ID =3>
	<Abstractive Summary> =
		Table 3:
		Controlling the output domain by provid-
		ing different domain embeddings
	</Abstractive Summary>
	<Extractive Summary> =
		Examples in Table 3 further suggest the model
		with medical embeddings as input can generate
		domain-speciﬁc words like “EMEA” (European
		Medicines Evaluation Agency) and “intramuscu-
		lar”, while IT embeddings encourage the model to
		generate words like “bug” and “developers”
	</Extractive Summary>
</Paper ID=ument147>


<Paper ID=ument148> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Directed dependency accuracy (DDA) on the
		universal treebanks with universal POS tags, on sen-
		tences of length ≤ 10
	</Abstractive Summary>
	<Extractive Summary> =
		We are moti-
		vated by our observation that learning the unsu-
		pervised Convex-MST model (Grave and Elhadad,
		2015) on the English corpus and then directly ap-
		plying it to parse other languages produces sur-
		prisingly good results (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		As suggested by our empirical observation in
		Table 1, the model of one language may provide
		a useful inductive bias in learning the model of
		another language
	</Extractive Summary>
</Paper ID=ument148>


<Paper ID=ument148> <Table ID =2>
	<Abstractive Summary> =
		, 2019)
		�, min
		negative (conditional) log likelihood
		-
		Table 2: Notations of several widely used models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows
		the choices of O, D and R for several widely used
		models
	</Extractive Summary>
</Paper ID=ument148>


<Paper ID=ument148> <Table ID =3>
	<Abstractive Summary> =
		37
		Table 3: Transfer grammar induction from English to
		the other languages
	</Abstractive Summary>
</Paper ID=ument148>


<Paper ID=ument148> <Table ID =4>
	<Abstractive Summary> =
		52
		Table 4: Results of bilingual grammar induction on
		test sentences no longer than 10 (except the Avg-All
		row which shows the average accuracies on all the sen-
		tences)
	</Abstractive Summary>
</Paper ID=ument148>


<Paper ID=ument149> <Table ID =1>
	<Abstractive Summary> =
		1M
		Word senses
		84
		71
		Lexical ambiguities
		7,359
		6,746
		Instances
		50,792
		43,268
		Table 1:
		Training data for NMT, and data extracted
		from ContraWSD: Word senses:
		total number of
		senses
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists the detailed statis-
		tics of the data
	</Extractive Summary>
</Paper ID=ument149>


<Paper ID=ument149> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: BLEU scores of NMT models, and WSD ac-
		curacy on the test set using word embeddings or hid-
		den states to represent ambiguous nouns
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		Table 2 provides the BLEU scores and the WSD
		accuracy on test sets, using different representa-
		tions to represent ambiguous nouns
	</Extractive Summary>
	<Extractive Summary> =
		The results of ENC using RNNS2S in Table 2
		are only based on hidden states from the last back-
		ward RNN
	</Extractive Summary>
	<Extractive Summary> =
		3
		Decoders
		As Table 2 shows, RNN decoder hidden states
		could further improve the classiﬁcation accuracy
		which accords with our hypothesis
	</Extractive Summary>
</Paper ID=ument149>


<Paper ID=ument15> <Table ID =1>
	<Abstractive Summary> =
		(9)
		Relation
		ps(ui), ps(uj)
		i < j
		(i, j)
		1
		p1, p1
		Yes
		(1,3), (1,5), (3,5)
		2
		p1, p1
		No
		(1,1), (3,1), (3,3)
		(5,1), (5,3), (5,5)
		3
		p2, p2
		Yes
		(2,4)
		4
		p2, p2
		No
		(2,2), (4,2), (4, 4)
		5
		p1, p2
		Yes
		(1,2), (1,4), (3,4)
		6
		p1, p2
		No
		(3,2), (5,2), (5,4)
		7
		p2, p1
		Yes
		(2,3), (2,5), (4,5)
		8
		p2, p1
		No
		(2,1), (4,1), (4,3)
		Table 1: ps(ui) and ps(uj) denotes the speaker of ut-
		terances ui and uj
	</Abstractive Summary>
</Paper ID=ument15>


<Paper ID=ument15> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		# dialogues
		# utterances
		train
		val
		test
		train
		val
		test
		IEMOCAP
		120
		31
		5810
		1623
		AVEC
		63
		32
		4368
		1430
		MELD
		1039
		114
		280
		9989
		1109
		2610
		Table 2: Training, validation and test data distribution
		in the datasets
	</Abstractive Summary>
</Paper ID=ument15>


<Paper ID=ument15> <Table ID =3>
	<Abstractive Summary> =
		18
		Table 3: Comparison with the baseline methods on IEMOCAP dataset; Acc
	</Abstractive Summary>
</Paper ID=ument15>


<Paper ID=ument15> <Table ID =4>
	<Abstractive Summary> =
		10
		Table 4: Comparison with the baseline methods on AVEC and MELD dataset; MAE and F1 metrics are user for
		AVEC and MELD, respectively
	</Abstractive Summary>
</Paper ID=ument15>


<Paper ID=ument15> <Table ID =5>
	<Abstractive Summary> =
		75
		Table 5: Ablation results w
	</Abstractive Summary>
</Paper ID=ument15>


<Paper ID=ument15> <Table ID =6>
	<Abstractive Summary> =
		11
		Table 6: Ablation results w
	</Abstractive Summary>
	<Extractive Summary> =
		The results
		of these tests in Table 6 show that having these
		different relational edges is indeed very impor-
		tant for modelling emotional dynamics
	</Extractive Summary>
</Paper ID=ument15>


<Paper ID=ument150> <Table ID =1>
	<Abstractive Summary> =
		86
		Table 1: Performances of Korean morphological analyzers
		Information
		Training
		Development
		Test
		Sentences
		197,508
		5,000
		50,631
		Eojeols
		2,674,571
		97,292
		694,524
		Morphemes
		5,952,985
		203,244
		1,542,483
		Avg
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results
		Table 1 shows the performances of the proposed
		model and the baselines
	</Extractive Summary>
</Paper ID=ument150>


<Paper ID=ument150> <Table ID =2>
	<Abstractive Summary> =
		of POS tags
		42
		Table 2: Simple statistics on the data set used
	</Abstractive Summary>
</Paper ID=ument150>


<Paper ID=ument150> <Table ID =3>
	<Abstractive Summary> =
		9%
		Table 3: The ratio of different error types
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows error types and their percentages
	</Extractive Summary>
</Paper ID=ument150>


<Paper ID=ument151> <Table ID =1>
	<Abstractive Summary> =
		6%
		Table 1: Number of word tokens as well as Out-Of-
		Vocabulary (OOV) rate
	</Abstractive Summary>
</Paper ID=ument151>


<Paper ID=ument151> <Table ID =2>
	<Abstractive Summary> =
		2%
		Table 2: Models’ performance on all words and OOV
		words per language
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the performance
		of the considered architectures across the sample
		languages
	</Extractive Summary>
	<Extractive Summary> =
		Comparison to Prior Work:
		Table 2 shows
		the performance of previous models trained on
		the same data
	</Extractive Summary>
</Paper ID=ument151>


<Paper ID=ument151> <Table ID =3>
	<Abstractive Summary> =
		0%
		+334%
		Table 3: Inference time in seconds for each architec-
		ture across languages
	</Abstractive Summary>
	<Extractive Summary> =
		Efﬁciency Comparison:
		We provide a compar-
		ative analysis of models’ training and inference
		runtime (Table 3)
	</Extractive Summary>
</Paper ID=ument151>


<Paper ID=ument152> <Table ID =1>
	<Abstractive Summary> =
		80
		Table 1: Q-BOT diversity and relevance on v1
	</Abstractive Summary>
	<Extractive Summary> =
		To evaluate Q-BOT’s diver-
		sity (Table 1), we generate Q-BOT-A-BOT dialogs
		(with beam size = 5) for 10 rounds on VisDial v1
	</Extractive Summary>
	<Extractive Summary> =
		6
		Results
		• Q-BOT’s diversity (Table 1):
		The question-
		repetition penalty consistently increases diver-
		sity (in both SL and RL) over the baseline! RL:
		Diverse-Q-bot asks ~1
	</Extractive Summary>
</Paper ID=ument152>


<Paper ID=ument152> <Table ID =2>
	<Abstractive Summary> =
		96
		Table 2: A-BOT performance on VisDial v1
	</Abstractive Summary>
	<Extractive Summary> =
		(2017a) (Table 2)
	</Extractive Summary>
</Paper ID=ument152>


<Paper ID=ument153> <Table ID =1>
	<Abstractive Summary> =
		1M, ES 114,702
		20K
		5K
		58K
		Table 1: Supervised data statistics
	</Abstractive Summary>
</Paper ID=ument153>


<Paper ID=ument153> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Performance of CLTL on large-scale data sets
	</Abstractive Summary>
	<Extractive Summary> =
		Is 100% better than 50%?
		Table 2 shows the
		performances of the different training strategies in
		our experiments
	</Extractive Summary>
	<Extractive Summary> =
		intent classiﬁcation
		As shown in
		Table 2source-language, our data selection method
		tends to bring higher gains to SF than to IC
	</Extractive Summary>
</Paper ID=ument153>


<Paper ID=ument153> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Performance of CLTL on ATIS
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the performance of CLTL on the
		small-scale ATIS data set
	</Extractive Summary>
</Paper ID=ument153>


<Paper ID=ument154> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Comparison of multilingual sentence-image retrieval/matching (German-Image) and (English-Image) re-
		sults on Multi30K
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents the results on the Multi30K
		testing set
	</Extractive Summary>
</Paper ID=ument154>


<Paper ID=ument154> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Results on the image and video datasets of
		Semantic Textual Similarity task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists the standard Pearson correlation
		coefﬁcients r between the system predictions and
		the STS gold-standard scores
	</Extractive Summary>
</Paper ID=ument154>


<Paper ID=ument155> <Table ID =1>
	<Abstractive Summary> =
		195
		Table 1: Automatic metric scores for the image cap-
		tioning task on Conceptual Captions
	</Abstractive Summary>
</Paper ID=ument155>


<Paper ID=ument155> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Accuracy (%) on the test-standard split for
		the VQA task on the VizWiz dataset
	</Abstractive Summary>
	<Extractive Summary> =
		For compar-
		ison, this number already surpasses all validation
		accuracy numbers in Table 2 for a strong model
		by Peng et al
	</Extractive Summary>
</Paper ID=ument155>


<Paper ID=ument156> <Table ID =1>
	<Abstractive Summary> =
		88
		Table 1: Caption-level correlation between metrics and human grading scores in Composite and PublicSys dataset
		by using Kendall tau (τ)
	</Abstractive Summary>
	<Extractive Summary> =
		rule-based metrics (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Do error-aware evaluation metrics help?
		The
		results of metric performance in Table 1 show
		that overall, using the three metrics proposed in
		REO, especially extraness and omission, led to a
		noticeable improvement in Kendall tau’s correla-
		tion compared to the best reported results based
		on prior metrics
	</Extractive Summary>
</Paper ID=ument156>


<Paper ID=ument157> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Comparison results based on R@1 on Activ-
		ityNet Captions
	</Abstractive Summary>
</Paper ID=ument157>


<Paper ID=ument157> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: R@1 results of our method on ActivityNet
		Captions when λ in Eq
	</Abstractive Summary>
</Paper ID=ument157>


<Paper ID=ument157> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Ablation study based on R@1 on ActivityNet
		Captions
	</Abstractive Summary>
</Paper ID=ument157>


<Paper ID=ument157> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4: Comparison results on DiDeMo
	</Abstractive Summary>
</Paper ID=ument157>


<Paper ID=ument158> <Table ID =1>
	<Abstractive Summary> =
		8
		Table 1: Average cosine similarity score and Delta-E distance over 5 runs
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 1 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, our
		models are able to predict target colors given seen
		modiﬁers but fail to make predictions for instances
		with unseen modiﬁers
	</Extractive Summary>
</Paper ID=ument158>


<Paper ID=ument159> <Table ID =1>
	<Abstractive Summary> =
		4%
		Table 1: N-grams instruction overlap statistics between
		validation seen and unseen environments
	</Abstractive Summary>
	<Extractive Summary> =
		This
		problem is common to many tasks where decod-
		ing is needed, including text generation, abstrac-
		tive summarization, and machine translation (Ben-
		1Table 1 shows n-gram overlap statistics between training
		seen and validation seen/unseen environments
	</Extractive Summary>
</Paper ID=ument159>


<Paper ID=ument159> <Table ID =2>
	<Abstractive Summary> =
		Validation Seen
		Validation Unseen
		Setting
		Agent
		SR ↑
		SPL ↑
		SR ↑
		SPL ↑
		S
		seq2seq
		51
		46
		32
		25
		PRESS
		47 (-4)
		43 (-3)
		43 (+11)
		38 (+13)
		M
		seq2seq
		49
		44
		33
		26
		PRESS
		56 (+7)
		53 (+9) 56 (+23)
		50 (+24)
		Table 2: Comparison of PRESS and seq2seq
	</Abstractive Summary>
</Paper ID=ument159>


<Paper ID=ument159> <Table ID =3>
	<Abstractive Summary> =
		61
		86
		76
		Table 3: Comparison with the state-of-the-art methods
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we com-
		pare the performance of our agent against all the
		published methods, our PRESS agent outperforms
		the existing models on nearly all the metrics
	</Extractive Summary>
</Paper ID=ument159>


<Paper ID=ument159> <Table ID =4>
	<Abstractive Summary> =
		31 59
		55
		Table 4: Ablation results of different language pretrain-
		ings and training strategies: Teacher Forcing (TF), Stu-
		dent Forcing (SF) and Stochastic Sampling (SS)
	</Abstractive Summary>
</Paper ID=ument159>


<Paper ID=ument16> <Table ID =1>
	<Abstractive Summary> =
		(Train/Val/Test)
		#Classes
		Evaluation
		EC
		Tweet
		30160/2755/5509
		90480/8265/16527
		4
		Micro-F1
		DailyDialog
		Daily Communication
		11118/1000/1000
		87170/8069/7740
		7
		Micro-F1
		MELD
		TV Show Scripts
		1038/114/280
		9989/1109/2610
		7
		Weighted-F1
		EmoryNLP
		TV Show Scripts
		659/89/79
		7551/954/984
		7
		Weighted-F1
		IEMOCAP
		Emotional Dialogues
		100/20/31
		4810/1000/1523
		6
		Weighted-F1
		Table 1: Dataset descriptions
	</Abstractive Summary>
</Paper ID=ument16>


<Paper ID=ument16> <Table ID =2>
	<Abstractive Summary> =
		5956
		Table 2: Performance comparisons on the ﬁve test sets
	</Abstractive Summary>
</Paper ID=ument16>


<Paper ID=ument16> <Table ID =3>
	<Abstractive Summary> =
		Dataset
		M
		m
		d
		p
		h
		EC
		2
		30
		200
		100
		4
		DailyDialog
		6
		30
		300
		400
		4
		MELD
		6
		30
		200
		100
		4
		EmoryNLP
		6
		30
		100
		200
		4
		IEMOCAP
		6
		30
		300
		400
		4
		Table 3: Hyper-parameter settings for KET
	</Abstractive Summary>
</Paper ID=ument16>


<Paper ID=ument16> <Table ID =4>
	<Abstractive Summary> =
		5251
		Table 4:
		Analysis of the relatedness-affectiveness
		tradeoff on the validation sets
	</Abstractive Summary>
</Paper ID=ument16>


<Paper ID=ument16> <Table ID =5>
	<Abstractive Summary> =
		5217
		Table 5: Ablation study for KET on the validation sets
	</Abstractive Summary>
</Paper ID=ument16>


<Paper ID=ument160> <Table ID =1>
	<Abstractive Summary> =
		24
		Table 1: Impact of various gaze features as auxiliary
		task(s) on the score (UAS/LAS) of dependency parsing
		as the main task evaluated on Dundee treebank (parallel
		setup)
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		Table 1 shows the results for Experiment 1 (on the
		Dundee treebank, i
	</Extractive Summary>
</Paper ID=ument160>


<Paper ID=ument160> <Table ID =2>
	<Abstractive Summary> =
		98
		Table 2: Results for dependency parsing evaluated on
		PTB treebank with gaze features as auxiliary task(s)
		learned from the disjoint dataset: Dundee treebank
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results for Experiment 2 (us-
		ing Dundee gaze data and PTB dependencies)
	</Extractive Summary>
</Paper ID=ument160>


<Paper ID=ument161> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Results on the full Wall Street Journal test set
	</Abstractive Summary>
</Paper ID=ument161>


<Paper ID=ument161> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: WSJ-10 unsupervised labeled constituency
		parsing with punctuation removed
	</Abstractive Summary>
</Paper ID=ument161>


<Paper ID=ument162> <Table ID =1>
	<Abstractive Summary> =
		1
		PRON
		2
		nsubj
		2
		got
		VERB
		0
		root
		3
		two
		NUM
		4
		nummod
		4
		dogs
		NOUN
		2
		obj
		Table 1: Example of omitted word node insertion
		annotate such occurrences in our dataset
	</Abstractive Summary>
	<Extractive Summary> =
		An example of using
		such a place-holder node is shown in Table 1,
		where the subject of the sentence in question has
		been omitted due to an ASR error
	</Extractive Summary>
</Paper ID=ument162>


<Paper ID=ument162> <Table ID =2>
	<Abstractive Summary> =
		2%
		Table 2: Comparison of common relation frequencies
	</Abstractive Summary>
</Paper ID=ument162>


<Paper ID=ument162> <Table ID =3>
	<Abstractive Summary> =
		82
		Table 3: Parsing Accuracy Results
		clear example is the correct use of the reparan-
		dum tag
	</Abstractive Summary>
</Paper ID=ument162>


<Paper ID=ument163> <Table ID =1>
	<Abstractive Summary> =
		63
		9m
		Table 1: The test exact match accuracy, labeled bracket
		F1, and training time per epoch of different methods
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the exact match accu-
		racy and bracket scores on the test data
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the average wall-clock time
		used to train the model for one epoch over the
		training data
	</Extractive Summary>
</Paper ID=ument163>


<Paper ID=ument163> <Table ID =2>
	<Abstractive Summary> =
		Error type
		no fe
		+ fe
		wrong top intent
		106
		99
		wrong label except top intent
		381
		367
		wrong non-terminal boundary
		133
		133
		missing a non-terminal
		153
		154
		spurious non-terminal
		198
		185
		joining two gold non-terminals
		20
		26
		splitting a gold non-terminal
		27
		36
		other errors
		6
		4
		Table 2: The error breakdown on the development data
		of our span-based models
	</Abstractive Summary>
</Paper ID=ument163>


<Paper ID=ument164> <Table ID =1>
	<Abstractive Summary> =
		26
		Table 1:
		The statistics of the training/development/test corpora in number of sentence pairs and the average
		document length (in sentences)
	</Abstractive Summary>
</Paper ID=ument164>


<Paper ID=ument164> <Table ID =2>
	<Abstractive Summary> =
		86
		Table 2: BLEU and Meteor scores of models
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results and Analysis
		Main Results
		Table 2 shows that our model surpasses all
		the context-agnostic(Vaswani et al
	</Extractive Summary>
</Paper ID=ument164>


<Paper ID=ument165> <Table ID =1>
	<Abstractive Summary> =
		48
		Table 1: Domain adaptation performance with differ-
		ent adaptation strategies
	</Abstractive Summary>
	<Extractive Summary> =
		Acquis, full model ﬁne-tuning (Full-FT columns
		in Table 1) on in-domain data signiﬁcantly im-
		proves translation performance compared to the
		base, non-adapted Transformer Big by a huge mar-
		gin, 3 BLEU points for IWSLT and 9 BLEU points
		for JRC
	</Extractive Summary>
</Paper ID=ument165>


<Paper ID=ument166> <Table ID =1>
	<Abstractive Summary> =
		Lexical sub-
		stitution
		Table 1: Cross-lingual Newsela examples: the Spanish text si of complexity, or reading grade level, ci is automat-
		ically aligned to English text so of co
	</Abstractive Summary>
	<Extractive Summary> =
		For instance, given input Span-
		ish sentences in Table 1, complexity controlled
		MT aims to produce English translations at a spe-
		ciﬁc level of complexity, which might differ from
		the complexity of the original Spanish
	</Extractive Summary>
	<Extractive Summary> =
		More examples are provided in
		the Appendix ( Table 13)
	</Extractive Summary>
	<Extractive Summary> =
		6A random sample of outputs from the best model conﬁg-
		uration are provided in the Appendix (Table 14)
	</Extractive Summary>
	<Extractive Summary> =
		1561
		A
		Supplemental Material
		Table 10 and 11 provides the statistics of grade pair distribution in the Newsela English and Newsela
		Spanish-English dataset
	</Extractive Summary>
	<Extractive Summary> =
		Src / Tgt
		2
		3
		4
		5
		6
		7
		8
		9
		10
		3
		2652
		0
		0
		0
		0
		0
		0
		0
		0
		4
		4984
		8212
		0
		0
		0
		0
		0
		0
		0
		5
		2287
		19589
		23775
		0
		0
		0
		0
		0
		0
		6
		1914
		7625
		21022
		21380
		0
		0
		0
		0
		0
		7
		608
		8897
		14249
		33466
		10944
		0
		0
		0
		0
		8
		623
		3710
		13267
		17347
		22745
		12006
		0
		0
		0
		9
		130
		5058
		5031
		19834
		4684
		30929
		2144
		0
		0
		10
		6
		40
		224
		320
		382
		289
		400
		142
		0
		11
		0
		0
		15
		19
		11
		16
		28
		0
		0
		12
		1069
		6818
		18430
		34232
		28532
		41561
		29836
		31327
		97
		Table 10: Number of text segments per grade level pair in our Newsela English Corpus
		Src / Tgt
		2
		3
		4
		5
		6
		7
		8
		9
		10
		3
		293
		0
		0
		0
		0
		0
		0
		0
		0
		4
		670
		1305
		0
		0
		0
		0
		0
		0
		0
		5
		251
		3383
		1957
		0
		0
		0
		0
		0
		0
		6
		223
		1124
		2090
		2833
		0
		0
		0
		0
		0
		7
		60
		1249
		926
		4986
		1244
		0
		0
		0
		0
		8
		96
		548
		1016
		1804
		3705
		1221
		0
		0
		0
		9
		16
		717
		211
		3074
		189
		6135
		263
		0
		0
		10
		0
		3
		5
		15
		26
		1
		46
		0
		0
		12
		189
		1288
		1902
		5312
		4708
		7796
		4995
		7077
		30
		Table 11: Number of text segments per grade level pair in our Newsela English-Spanish Corpus
		1562
		Model
		Bleu
		SARI
		Flesch
		Results copied from Scarton and Specia (2018)
		seq2seq w/ side-constraint
		62
	</Extractive Summary>
	<Extractive Summary> =
		98
		Table 12: Comparison with previously published results on Newsela English text simpliﬁcation
	</Extractive Summary>
	<Extractive Summary> =
		Table 13: Examples of simpliﬁcation operations observed when simplifying from a higher Grade level into different
		lower grade levels using the multitask model (All tasks)
	</Extractive Summary>
	<Extractive Summary> =
		Table 14: Example translations produced by our best multitask model
	</Extractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =2>
	<Abstractive Summary> =
		04
		49,589
		4,939,085
		Table 2: Comparisons of the English-Spanish Newsela corpus with machine translation corpora from OPUS drawn
		from Global Voices and News Commentary
	</Abstractive Summary>
	<Extractive Summary> =
		Newsela (Table 2): seg-
		ments of vastly different lengths can have the same
		ARI score (Equation 4), thus confusing the multi-
		task model
	</Extractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =3>
	<Abstractive Summary> =
		96
		-
		-
		-
		Table 3: Grade level Statistics of the Newsela Spanish-English corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 highlights the vo-
		cabulary differences among the different grade
		levels for the Newsela Spanish-English corpus
	</Extractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =4>
	<Abstractive Summary> =
		608
		Table 4: Compared to pipeline models, multitask mod-
		els produce complexity controlled translations that bet-
		ter match human references (BLEU), that are simpler
		(SARI), and whose resulting complexity correlates bet-
		ter with the target grade level (PCC)
	</Abstractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Example of multi-task model outputs when translating grade 12 Spanish into increasingly simpler English
	</Abstractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =6>
	<Abstractive Summary> =
		636
		Table 6: Adjacency ARI accuracy within grade level given by Adjacency level for the system output with respect
		to the target grade: Multitask model is able to better capture the target grade than the pipeline model when the
		difference between the source and the target grade is greater than 3
	</Abstractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =7>
	<Abstractive Summary> =
		521
		Table 7: Data ablation experiments showing the impact
		of different types of training examples on multi-task
		model
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Ablation Experiments
		Table 7 shows the impact of different training
		data types on the multitask model using ablation
		experiments
	</Extractive Summary>
	<Extractive Summary> =
		Refer Table 7 (Row 1)
	</Extractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =8>
	<Abstractive Summary> =
		658
		Table 8: Evaluation on auxiliary tasks: Multitask mod-
		els trained on both the translation and simpliﬁcation
		dataset improves the performance for the task of En-
		glish Simpliﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 summarizes the results: the multitask
		model slightly outperforms a dedicated simpliﬁca-
		tion model on English simpliﬁcation, showing the
		beneﬁts of the additional training data from other
		tasks
	</Extractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =9>
	<Abstractive Summary> =
		577
		Table 9: Complexity controlled MT with automatic vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 shows that ARI provides an adequate
		substitute for manually annotated reading grade
		levels, as BLEU and SARI score remain close
		when Newsela reading grade levels are replaced
		by ARI-based tags
	</Extractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =10>
	<Abstractive Summary> =
		Src / Tgt
		2
		3
		4
		5
		6
		7
		8
		9
		10
		3
		2652
		0
		0
		0
		0
		0
		0
		0
		0
		4
		4984
		8212
		0
		0
		0
		0
		0
		0
		0
		5
		2287
		19589
		23775
		0
		0
		0
		0
		0
		0
		6
		1914
		7625
		21022
		21380
		0
		0
		0
		0
		0
		7
		608
		8897
		14249
		33466
		10944
		0
		0
		0
		0
		8
		623
		3710
		13267
		17347
		22745
		12006
		0
		0
		0
		9
		130
		5058
		5031
		19834
		4684
		30929
		2144
		0
		0
		10
		6
		40
		224
		320
		382
		289
		400
		142
		0
		11
		0
		0
		15
		19
		11
		16
		28
		0
		0
		12
		1069
		6818
		18430
		34232
		28532
		41561
		29836
		31327
		97
		Table 10: Number of text segments per grade level pair in our Newsela English Corpus
		Src / Tgt
		2
		3
		4
		5
		6
		7
		8
		9
		10
		3
		293
		0
		0
		0
		0
		0
		0
		0
		0
		4
		670
		1305
		0
		0
		0
		0
		0
		0
		0
		5
		251
		3383
		1957
		0
		0
		0
		0
		0
		0
		6
		223
		1124
		2090
		2833
		0
		0
		0
		0
		0
		7
		60
		1249
		926
		4986
		1244
		0
		0
		0
		0
		8
		96
		548
		1016
		1804
		3705
		1221
		0
		0
		0
		9
		16
		717
		211
		3074
		189
		6135
		263
		0
		0
		10
		0
		3
		5
		15
		26
		1
		46
		0
		0
		12
		189
		1288
		1902
		5312
		4708
		7796
		4995
		7077
		30
		Table 11: Number of text segments per grade level pair in our Newsela English-Spanish Corpus
		1562
		Model
		Bleu
		SARI
		Flesch
		Results copied from Scarton and Specia (2018)
		seq2seq w/ side-constraint
		62
	</Abstractive Summary>
	<Extractive Summary> =
		1561
		A
		Supplemental Material
		Table 10 and 11 provides the statistics of grade pair distribution in the Newsela English and Newsela
		Spanish-English dataset
	</Extractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =11>
	<Abstractive Summary> =
		Src / Tgt
		2
		3
		4
		5
		6
		7
		8
		9
		10
		3
		2652
		0
		0
		0
		0
		0
		0
		0
		0
		4
		4984
		8212
		0
		0
		0
		0
		0
		0
		0
		5
		2287
		19589
		23775
		0
		0
		0
		0
		0
		0
		6
		1914
		7625
		21022
		21380
		0
		0
		0
		0
		0
		7
		608
		8897
		14249
		33466
		10944
		0
		0
		0
		0
		8
		623
		3710
		13267
		17347
		22745
		12006
		0
		0
		0
		9
		130
		5058
		5031
		19834
		4684
		30929
		2144
		0
		0
		10
		6
		40
		224
		320
		382
		289
		400
		142
		0
		11
		0
		0
		15
		19
		11
		16
		28
		0
		0
		12
		1069
		6818
		18430
		34232
		28532
		41561
		29836
		31327
		97
		Table 10: Number of text segments per grade level pair in our Newsela English Corpus
		Src / Tgt
		2
		3
		4
		5
		6
		7
		8
		9
		10
		3
		293
		0
		0
		0
		0
		0
		0
		0
		0
		4
		670
		1305
		0
		0
		0
		0
		0
		0
		0
		5
		251
		3383
		1957
		0
		0
		0
		0
		0
		0
		6
		223
		1124
		2090
		2833
		0
		0
		0
		0
		0
		7
		60
		1249
		926
		4986
		1244
		0
		0
		0
		0
		8
		96
		548
		1016
		1804
		3705
		1221
		0
		0
		0
		9
		16
		717
		211
		3074
		189
		6135
		263
		0
		0
		10
		0
		3
		5
		15
		26
		1
		46
		0
		0
		12
		189
		1288
		1902
		5312
		4708
		7796
		4995
		7077
		30
		Table 11: Number of text segments per grade level pair in our Newsela English-Spanish Corpus
		1562
		Model
		Bleu
		SARI
		Flesch
		Results copied from Scarton and Specia (2018)
		seq2seq w/ side-constraint
		62
	</Abstractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =12>
	<Abstractive Summary> =
		98
		Table 12: Comparison with previously published results on Newsela English text simpliﬁcation
	</Abstractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =13>
	<Abstractive Summary> =
		Table 13: Examples of simpliﬁcation operations observed when simplifying from a higher Grade level into different
		lower grade levels using the multitask model (All tasks)
	</Abstractive Summary>
	<Extractive Summary> =
		More examples are provided in
		the Appendix ( Table 13)
	</Extractive Summary>
</Paper ID=ument166>


<Paper ID=ument166> <Table ID =14>
	<Abstractive Summary> =
		Table 14: Example translations produced by our best multitask model
	</Abstractive Summary>
	<Extractive Summary> =
		6A random sample of outputs from the best model conﬁg-
		uration are provided in the Appendix (Table 14)
	</Extractive Summary>
</Paper ID=ument166>


<Paper ID=ument168> <Table ID =1>
	<Abstractive Summary> =
		63
		Table 1: Performance (BLEU scores) comparison with the four baseline models on Chinese-English document-
		level translation
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, we divide the results
		into two main groups, i
	</Extractive Summary>
</Paper ID=ument168>


<Paper ID=ument168> <Table ID =2>
	<Abstractive Summary> =
		58
		Table 2: The effect of integrating HM-GDC into Trans-
		former with respect to the layer number (N) of the self-
		attention in the document encoder
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 illustrates the effectiveness of integrating
		our proposed HM-GDC model into the encoder,
		N
		Encoder
		Decoder
		Both
		1
		17
	</Extractive Summary>
</Paper ID=ument168>


<Paper ID=ument168> <Table ID =3>
	<Abstractive Summary> =
		72
		Table 3: Comparison with the Transformer model on
		German-English document-level translation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		the performance of our model on German-English
		document-level translation and the baseline here
		refers to the Transformer model
	</Extractive Summary>
</Paper ID=ument168>


<Paper ID=ument168> <Table ID =4>
	<Abstractive Summary> =
		77
		Table 4: Results of our model with and without pre-training on Chinese-English document-level translation
	</Abstractive Summary>
	<Extractive Summary> =
		93 BLEU points
		(the ﬁrst two rows in Table 4) when the large-
		scale sentence-level parallel corpus is used for the
		pre-training process
	</Extractive Summary>
</Paper ID=ument168>


<Paper ID=ument168> <Table ID =5>
	<Abstractive Summary> =
		50
		Table 5: Evaluation on pronoun translation of Chinese-English and German-English document-level translation
	</Abstractive Summary>
</Paper ID=ument168>


<Paper ID=ument168> <Table ID =6>
	<Abstractive Summary> =
		Table 6: An example of pronoun translation in the Chinese-English document-level translation
	</Abstractive Summary>
	<Extractive Summary> =
		Correspondingly, we dis-
		play a translation example in Table 6 to further
		illustrate this
	</Extractive Summary>
</Paper ID=ument168>


<Paper ID=ument168> <Table ID =7>
	<Abstractive Summary> =
		Pre-context
		· · · renhe yige youxiu de chengxuyuan dou hui zuo de · · ·
		Rear-context
		· · · zai xiaweiyi de IT bumen shangban de ren kandao le · · ·
		Source
		ta xie le yige xiangduilaishuo bijiao xiao de chengxu
		Reference
		he wrote a modest little app
		Baseline
		he wrote a relatively small procedure
		Ours
		he wrote a relative smaller program
		Table 7: An example of noun translation in the Chinese-English document-level translation
	</Abstractive Summary>
</Paper ID=ument168>


<Paper ID=ument169> <Table ID =1>
	<Abstractive Summary> =
		Train
		Dev
		Test
		Challenge
		CMRC 2018
		Question #
		10,321
		3,219
		4,895
		504
		Answer #
		1
		3
		3
		3
		DRCD
		Question #
		26,936
		3,524
		3,493
		-
		Answer #
		1
		2
		2
		-
		Table 1: Statistics of CMRC 2018 and DRCD
	</Abstractive Summary>
</Paper ID=ument169>


<Paper ID=ument169> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: Experimental results on CMRC 2018 and DRCD
	</Abstractive Summary>
</Paper ID=ument169>


<Paper ID=ument169> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Zero-shot cross-lingual machine reading com-
		prehension results on Japanese and French SQuAD
		data
	</Abstractive Summary>
</Paper ID=ument169>


<Paper ID=ument169> <Table ID =4>
	<Abstractive Summary> =
		8)
		Table 4: Ablations of Dual BERT on the CMRC 2018
		development set
	</Abstractive Summary>
</Paper ID=ument169>


<Paper ID=ument17> <Table ID =1>
	<Abstractive Summary> =
		1
		All
		109,129
		Table 1: Statistics for the three corpora used in our ex-
		periments
	</Abstractive Summary>
</Paper ID=ument17>


<Paper ID=ument17> <Table ID =2>
	<Abstractive Summary> =
		One Error
		1
		n
		�n
		i=1 δ[argmax
		et
		gt(xi) /∈ Ri]
		Average Precision
		1
		n
		�n
		i=1
		1
		|Ri| ×
		�
		t:et∈Ri
		|{es∈Ri|gs(xi)>gt(xi)}|
		|{es|gs(xi)>gt(xi)}|
		Coverage
		1
		n
		�n
		i=1 maxt:et∈Ri |{es|gs(xi) > gt(xi)}|
		F1exam
		1
		n
		�n
		i=1
		2|Ri∩ ˆ
		Ri|
		(|Ri|+| ˆ
		Ri|)
		MicroF1
		F1(
		T�
		t=1
		TPt,
		T�
		t=1
		FPt,
		T�
		t=1
		TNt,
		T�
		t=1
		FNt)
		MacroF1
		1
		T
		T�
		t=1
		F1(TPt, FPt, TNt, FNt)
		Table 2: Evaluation criteria for the Multi-Label Learn-
		ing (MLL) methods
	</Abstractive Summary>
</Paper ID=ument17>


<Paper ID=ument17> <Table ID =3>
	<Abstractive Summary> =
		6841
		Table 3: Comparison with Emotion Detection Methods and Multi-label Methods
	</Abstractive Summary>
</Paper ID=ument17>


<Paper ID=ument17> <Table ID =4>
	<Abstractive Summary> =
		6841
		Table 4: Comparison of different IRER-EA components
	</Abstractive Summary>
</Paper ID=ument17>


<Paper ID=ument170> <Table ID =1>
	<Abstractive Summary> =
		98
		Table 1: The performance of MTMSN and other competing approaches on DROP dev and test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the performance of our model
		and other competitive approaches on the develop-
		1602
		Type
		(%)
		NABERT
		MTMSN
		EM
		F1
		EM
		F1
		Date
		1
	</Extractive Summary>
</Paper ID=ument170>


<Paper ID=ument170> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Ablation tests of base and large models on the
		DROP dev set
	</Abstractive Summary>
	<Extractive Summary> =
		As illustrated in
		Table 2, the use of addition and subtraction is ex-
		tremely crucial: the EM/F1 performance of both
		the base and large models drop drastically by more
		than 20 points if it is removed
	</Extractive Summary>
</Paper ID=ument170>


<Paper ID=ument170> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Ablation tests of different architecture choices
		using MTMSNLARGE
	</Abstractive Summary>
	<Extractive Summary> =
		Architecture ablation
		We further conduct a de-
		tailed ablation in Table 3 to evaluate our architec-
		ture designs
	</Extractive Summary>
</Paper ID=ument170>


<Paper ID=ument170> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Performance breakdown of NABERTLARGE
		and MTMSNLARGE by gold answer types
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows
		that our gains mainly come from the most frequent
		number type, which requires various types of sym-
		bolic, discrete reasoning operations
	</Extractive Summary>
</Paper ID=ument170>


<Paper ID=ument170> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: Performance breakdown of NABERTLARGE
		and MTMSNLARGE by predicted answer types
	</Abstractive Summary>
</Paper ID=ument170>


<Paper ID=ument170> <Table ID =6>
	<Abstractive Summary> =
		8
		Table 6:
		Annotation statistics under different combi-
		nations of answer types in the DROP train set
	</Abstractive Summary>
</Paper ID=ument170>


<Paper ID=ument171> <Table ID =1>
	<Abstractive Summary> =
		com/q/1393090
		1609
		Method
		Duplicates
		Answers
		Bodies
		Supervised
		5
		-
		(5)
		WS-QA
		-
		5
		(5)
		Domain Transfer
		5⇤
		-
		(5)
		DQG
		-
		-
		5
		WS-TB
		-
		-
		5
		Table 1: The different training methods and the data
		they use
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 gives an
		overview of the different training methods
	</Extractive Summary>
</Paper ID=ument171>


<Paper ID=ument171> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		Train / Dev / Test
		|Q|
		|A|
		AskUbuntu-Lei
		12,584 / 200 / 200
		288k
		84k
		AskUbuntu
		9106 / 1000 / 1000
		288k
		84k
		SuperUser
		9106 / 1000 / 1000
		377k
		142k
		Apple
		- / 1000 / 1000
		89k
		29k
		Android
		- / 1000 / 1000
		47k
		14k
		Table 2:
		The dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		An
		overview of the datasets is given in Table 2, which
		also shows that they considerably differ in the
		amounts of data that is available for the different
		training methods
	</Extractive Summary>
	<Extractive Summary> =
		1
		DQG for Small-Scale cQA Forums
		In our previous experiments, we assumed that there
		exist enough unlabeled questions to train the ques-
		tion generation model (at least 47k questions, see
		Table 2)
	</Extractive Summary>
</Paper ID=ument171>


<Paper ID=ument171> <Table ID =3>
	<Abstractive Summary> =
		898
		Table 3: Results of the models with different training strategies
	</Abstractive Summary>
</Paper ID=ument171>


<Paper ID=ument171> <Table ID =4>
	<Abstractive Summary> =
		039
		Table 4: The domain transfer performances
	</Abstractive Summary>
</Paper ID=ument171>


<Paper ID=ument171> <Table ID =5>
	<Abstractive Summary> =
		5
		Table 5:
		Answer selection performances (averaged
		over ﬁve datasets) when trained with question-answer
		pairs vs
	</Abstractive Summary>
	<Extractive Summary> =
		The results are given in Table 5 where we re-
		port the accuracy (P@1), averaged over the ﬁve
		datasets
	</Extractive Summary>
</Paper ID=ument171>


<Paper ID=ument171> <Table ID =6>
	<Abstractive Summary> =
		7
		Table 6: Results of ﬁne-tuned BERT models with different training strategies
	</Abstractive Summary>
</Paper ID=ument171>


<Paper ID=ument172> <Table ID =1>
	<Abstractive Summary> =
		Table 1: The comparison of our dataset with relevant datasets
	</Abstractive Summary>
	<Extractive Summary> =
		As
		Table 10 shows, the unstructured models perform
		better than the structured models, indicating that
		this task tends to be over-ﬁtting and small models
		have better the generalization ability
	</Extractive Summary>
	<Extractive Summary> =
		Here we present one
		example in Table 11
	</Extractive Summary>
	<Extractive Summary> =
		7
		Table 10:
		Results of clariﬁcation-based question an-
		swering models
	</Extractive Summary>
	<Extractive Summary> =
		Qh: Who directed Come Back, Little Sheba?
		Output: Which one do you mean, winning work visual
		artist Claudia Coleman or winning work Elmer Gantry,
		when you ask the directors?
		Table 11: A bad generated case
	</Extractive Summary>
	<Extractive Summary> =
		005
		Optimizer
		Adam
		Table 12: Settings of classiﬁcation models
	</Extractive Summary>
	<Extractive Summary> =
		Table 12 shows the detailed hyper-parameter
		settings of these classiﬁcation models
	</Extractive Summary>
	<Extractive Summary> =
		1
		Optimizer
		Adam
		Table 13: Settings of generation models
	</Extractive Summary>
	<Extractive Summary> =
		Table 13 shows the detailed hyper-parameter
		settings of generation models
	</Extractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =2>
	<Abstractive Summary> =
		Qa: What are the languages used to create the source
		code of Midori?
		Table 2: An ambiguous question annotation example in
		the single-turn case
	</Abstractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =3>
	<Abstractive Summary> =
		Qa
		What are the languages used to cre-
		ate the source code of Midori?
		Qc: When you say the source code language used in
		the program Midori, are you referring to [web browser
		Midori] or [operating system Midori]?
		Table 3: A clariﬁcation question annotation example in
		the single-turn case
	</Abstractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =4>
	<Abstractive Summary> =
		Qa: What is the biological classiﬁcation?
		Table 4: An ambiguous question annotation example in
		the multi-turn case
	</Abstractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =5>
	<Abstractive Summary> =
		Qa
		What is the biological classiﬁcation?
		Qc: Are you referring to [Lineodes caracasia] or Li-
		neodes], when you ask the biological classiﬁcation?
		Table 5: A clariﬁcation question annotation example in
		the multi-turn case
	</Abstractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =6>
	<Abstractive Summary> =
		1622
		Single-Turn
		Multi-Turn
		Positive
		Negative
		Positive
		Negative
		Train
		8,139
		6,841
		12,173
		8,289
		Dev
		487
		422
		372
		601
		Test
		637
		673
		384
		444
		Table 6: Statistics of the dataset
	</Abstractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =7>
	<Abstractive Summary> =
		5
		Table 7: Results of clariﬁcation identiﬁcation models
	</Abstractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =8>
	<Abstractive Summary> =
		02
		Table 8:
		Results of clariﬁcation question generation
		models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8
		shows that Seq2Seq achieves low BLEU scores,
		which indicates its tendency to generate irrelevant
		text
	</Extractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =9>
	<Abstractive Summary> =
		Qa
		Biologically speaking, what is the
		classiﬁcation?
		Output: Are you referring to Lineodes interrupta or
		Lineodes, when you ask the biological classiﬁcation?
		Table 9: An example of the generated clariﬁcation
		questions
	</Abstractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =10>
	<Abstractive Summary> =
		7
		Table 10:
		Results of clariﬁcation-based question an-
		swering models
	</Abstractive Summary>
	<Extractive Summary> =
		As
		Table 10 shows, the unstructured models perform
		better than the structured models, indicating that
		this task tends to be over-ﬁtting and small models
		have better the generalization ability
	</Extractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =11>
	<Abstractive Summary> =
		Qh: Who directed Come Back, Little Sheba?
		Output: Which one do you mean, winning work visual
		artist Claudia Coleman or winning work Elmer Gantry,
		when you ask the directors?
		Table 11: A bad generated case
	</Abstractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =12>
	<Abstractive Summary> =
		005
		Optimizer
		Adam
		Table 12: Settings of classiﬁcation models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 12 shows the detailed hyper-parameter
		settings of these classiﬁcation models
	</Extractive Summary>
</Paper ID=ument172>


<Paper ID=ument172> <Table ID =13>
	<Abstractive Summary> =
		1
		Optimizer
		Adam
		Table 13: Settings of generation models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 13 shows the detailed hyper-parameter
		settings of generation models
	</Extractive Summary>
</Paper ID=ument172>


<Paper ID=ument173> <Table ID =1>
	<Abstractive Summary> =
		This can result in low cov-
		erage or quality, as illustrated by the in-domain
		embedding neighbors of “tree” in the smallest fo-
		rum from our dataset:
		windowsphone: dreamspark, l535ds, generally
		1631
		generic
		domain-speciﬁc
		contextualized
		fG: GloVe
		fD: FastText
		(in-domain)
		noncontextualized
		fU: USE
		fB: BERT
		(domain-ﬁnetuned)
		Table 1: Ensemble used in our experiments
	</Abstractive Summary>
	<Extractive Summary> =
		2, we describe an ensemble of different sen-
		tence encoders: domain-speciﬁc and generic, con-
		textualized and noncontextualized (see Table 1)
	</Extractive Summary>
</Paper ID=ument173>


<Paper ID=ument173> <Table ID =2>
	<Abstractive Summary> =
		155
		Table 2: Mean Average Precision (MAP) averaged over
		heldout forums
	</Abstractive Summary>
	<Extractive Summary> =
		(2017)
		(see Table 2, top), hence this is what we use below
	</Extractive Summary>
	<Extractive Summary> =
		We ﬁnd that domain-ﬁnetuning BERT on
		S results in improvements over generic BERT (see
		Table 2, bottom)
	</Extractive Summary>
</Paper ID=ument173>


<Paper ID=ument173> <Table ID =3>
	<Abstractive Summary> =
		5M
		out
		outdoors
		4651
		124
		580K
		pro
		productivity
		2508
		127
		380K
		rev
		reverseengineering
		15619
		119
		790K
		sit
		sitecore
		5605
		130
		680K
		spo
		sports
		4531
		127
		430K
		sqa
		sqa
		8360
		166
		950K
		win
		windowsphone
		3490
		192
		290K
		Table 3: Forum statistics
	</Abstractive Summary>
	<Extractive Summary> =
		For hyperparam-
		eter choices, we hold out two high-resource and
		two low-resource forums (highlighted in Table 3)
	</Extractive Summary>
</Paper ID=ument173>


<Paper ID=ument173> <Table ID =4>
	<Abstractive Summary> =
		999
		Table 4: Main results
	</Abstractive Summary>
	<Extractive Summary> =
		There are two exceptions (out of 20
		test forums): On law and outdoors, fU (USE) per-
		forms slightly better on its own (Table 4, row 15)
	</Extractive Summary>
	<Extractive Summary> =
		(2018) outperforms fG and fD on
		their own (see Table 4, rows 10, 21), which vali-
		dates the approach
	</Extractive Summary>
</Paper ID=ument173>


<Paper ID=ument173> <Table ID =5>
	<Abstractive Summary> =
		05)
		1
		MV-DASE, ¬fG, ¬fD
		MV-DASE
		2
		¬fB, ¬(fG, fD)
		¬fB,
		¬fD,
		¬fG,
		¬(fG, fD)
		3
		BM25,
		avg,
		concat,
		ADA, word-level CCA,
		ELMo, ¬fU, fD, fG,
		fB, ¬(fB, fU), fU
		BM25,
		avg,
		concat,
		doc2vec, ADA, word-
		level
		CCA,
		ELMo,
		fB, fD, fG, ¬fU, fU,
		¬(fB, fU)
		4
		InferSent, doc2vec
		InferSent
		Table 5: Group rankings by transitive closure of paired
		t-tests
	</Abstractive Summary>
	<Extractive Summary> =
		every single view (see Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Supervised domain-adversarial ADA per-
		forms signiﬁcantly worse than unsupervised MV-
		DASE (see Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows that
		omitting either fG or fD from the ensemble does
		not lead to a signiﬁcant drop in MAP, but omitting
		both does
	</Extractive Summary>
	<Extractive Summary> =
		USE has the biggest positive effect on MV-
		DASE (Table 6, rows 3,11), also evidenced by the
		fact that omitting it is signiﬁcantly more harmful
		than omitting any other single view (Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		The naive concatenation or
		averaging of views is signiﬁcantly less effective
		than view correlation by GCCA (Table 6, rows
		7,8,15,16, and Table 5)
	</Extractive Summary>
</Paper ID=ument173>


<Paper ID=ument173> <Table ID =6>
	<Abstractive Summary> =
		082
		Table 6: Ablation study
	</Abstractive Summary>
	<Extractive Summary> =
		¬fj is MV-DASE without fj (see Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		, MV-DASE on
		generic and domain-speciﬁc averaged word em-
		beddings (see Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		On the low-resource forums,
		omitting fD has a beneﬁcial effect (Table 6, row
		10)
	</Extractive Summary>
	<Extractive Summary> =
		USE has the biggest positive effect on MV-
		DASE (Table 6, rows 3,11), also evidenced by the
		fact that omitting it is signiﬁcantly more harmful
		than omitting any other single view (Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		The naive concatenation or
		averaging of views is signiﬁcantly less effective
		than view correlation by GCCA (Table 6, rows
		7,8,15,16, and Table 5)
	</Extractive Summary>
</Paper ID=ument173>


<Paper ID=ument173> <Table ID =7>
	<Abstractive Summary> =
		02
		Table 7: MAP and MRR (percentages) on SemEval-
		2017 3B test set
	</Abstractive Summary>
	<Extractive Summary> =
		See Table 7 for results
	</Extractive Summary>
</Paper ID=ument173>


<Paper ID=ument173> <Table ID =8>
	<Abstractive Summary> =
		771
		Table 8: Pearson’s r (dev / test) on the unsupervised
		STS Benchmark, using different embeddings for fG
	</Abstractive Summary>
</Paper ID=ument173>


<Paper ID=ument174> <Table ID =1>
	<Abstractive Summary> =
		1645
		Table 1: Descriptions of the categories of sexism used in our dataset
		Category
		Description
		Role stereotyping
		Socially constructed false generalizations about certain roles being more appropriate for women; also applies to such
		misconceptions about men
		Attribute stereotyping
		Mistaken linkage of women with some physical, psychological, or behavioral qualities or likes/dislikes; also applies
		to such false notions about men
		Body shaming
		Objectionable comments or behaviour concerning appearance including the promotion of certain body types or stan-
		dards
		Hyper-sexualization (excluding
		body shaming)
		Unwarranted focus on physical aspects or sexual acts
		Internalized sexism
		The perpetration of sexism by women via comments or other actions
		Pay gap
		Unequal salaries for men and women for the same work proﬁle
		Hostile work environment (ex-
		cluding pay gap)
		Sexism encountered by an employee at the workplace; also applies when a sexist misdeed committed outside the
		workplace by a co-worker makes working uncomfortable for the victim
		Denial or trivialization of sexist
		misconduct
		Denial or downplaying of sexist wrongdoings
		Threats
		All threats including wishing for violence or joking about it, stalking, threatening gestures, or rape threats
		Rape
		FBI’s expanded deﬁnition of rape
		Sexual assault (excluding rape)
		Any sexual contact without consent; unwanted touching
		Sexual harassment (excluding
		assault)
		Any sexually objectionable behaviour
		Tone policing
		Comments or actions that cause or aggravate restrictions on how women communicate
		Moral policing (excluding tone
		policing)
		The promotion of discriminatory codes of conduct for women in the guise of morality; also applies to statements that
		feed into such codes and narratives
		Victim blaming
		The act of holding the victim responsible (fully or partially) for sexual harassment, violence, or other sexism perpe-
		trated against her
		Slut shaming
		Inappropriate comments made about women 1) deviating from conservative expectations relating to sex or 2) dressing
		in a certain way when it gets linked to sexual availability
		Motherhood-related discrimina-
		tion
		Shaming, prejudices, or other discrimination or misconduct related to the notion of motherhood; also applies to the
		violation of reproductive rights
		Menstruation-related
		discrimi-
		nation
		Shaming, prejudices, or other discrimination or wrongdoings related to periods
		Religion-based sexism
		Sexist discrimination or prejudices stemming from religious scriptures or constructs
		Physical
		violence
		(excluding
		sexual violence)
		Domestic abuse, murder, kidnapping, conﬁnement, or other physical acts of violence linked to sexism
		Mansplaining
		A woman being condescendingly talked down to by a man; also applies when a man gives an unsolicited advice or
		explanation to a woman related to something she knows well that she disapproves of
		Gaslighting
		Sexist manipulation of the victim through psychological means into doubting her own sanity
		Other
		Any type of sexism not covered by the above categories
		3
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 provides succinct
		descriptions for the categories
	</Extractive Summary>
</Paper ID=ument174>


<Paper ID=ument174> <Table ID =2>
	<Abstractive Summary> =
		For our framework,
		s() denotes sentence-level concatenation; wl() de-
		1649
		Table 2: Results with traditional machine learning (Label Powerset)
		Features−→
		Word n-grams
		Character n-grams
		Averaged ELMo vectors
		Composite features
		Classiﬁer↓
		FI
		Fmacro AccI
		Fmicro
		FI
		Fmacro AccI
		Fmicro
		FI
		Fmacro AccI
		Fmicro
		FI
		Fmacro AccI
		Fmicro
		SVM
		0
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 2 shows results produced using traditional
		ML methods (SVM, RF, and LR) across four dif-
		ferent feature sets (word n-grams, character n-
		grams, averaged ELMo vectors, and composite
		features)
	</Extractive Summary>
</Paper ID=ument174>


<Paper ID=ument174> <Table ID =3>
	<Abstractive Summary> =
		(7) The LSTM
		based processing of word embeddings produces
		Table 3: Results for the proposed methods and deep
		learning baselines (using ELMo embeddings) with the
		EBCE loss and Random
		Approach
		FI
		Fmacro AccI Fmicro
		Baselines
		Random
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 contains results for the random and deep
		learning baselines and different variants of the pro-
		posed framework
	</Extractive Summary>
</Paper ID=ument174>


<Paper ID=ument174> <Table ID =4>
	<Abstractive Summary> =
		com/google-research/
		bert
		1650
		Table 4: Attention analysis for some test samples
		Account of sexism
		Two most-attended-to words / sentence
		Labels
		am in social services
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 lists accounts of sexism from the test
		set for which our best method made the right pre-
		dictions but the best baseline did not, along with
		the labels
	</Extractive Summary>
</Paper ID=ument174>


<Paper ID=ument174> <Table ID =5>
	<Abstractive Summary> =
		(tight, trowsers)
		Moral policing and victim blam-
		ing,
		Hyper-sexualization (ex-
		cluding body shaming)
		Table 5: Tuned hyper-parameter values for the deep learning baselines (ELMo embeddings) and proposed methods
		with the EBCE loss
		Approach
		LSTM dimension
		Attention dimension
		CNN Filters of each kernel size
		Hierarchical-biLSTM-Attention
		300
		400
		N
	</Abstractive Summary>
</Paper ID=ument174>


<Paper ID=ument174> <Table ID =6>
	<Abstractive Summary> =
		s(wc(ELMo), wc(GloVe), tBERT)
		300
		600
		100
		s(wc(ELMo), wl(ELMo), wc(GloVe), wl(GloVe), tBERT)
		300
		500
		100
		Table 6: Performance variation across #labels per post
		#Labels per post
		FI
		Fmacro
		AccI
		Fmicro
		1
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows results for one run of our best
		method across different numbers of labels per post
		(1 to 5)
	</Extractive Summary>
</Paper ID=ument174>


<Paper ID=ument175> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Twitter celebrities in our dataset, with tweet
		counts before and after ﬁltering (Foll
	</Abstractive Summary>
</Paper ID=ument175>


<Paper ID=ument175> <Table ID =2>
	<Abstractive Summary> =
		37
		Table 2: Test accuracy (%) of ﬁve approaches to classify user vs
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results and Comparison
		Table 2 presents the user-wise test accuracy of the
		ﬁve approaches under the speciﬁed conﬁgurations
	</Extractive Summary>
</Paper ID=ument175>


<Paper ID=ument175> <Table ID =3>
	<Abstractive Summary> =
		25e-24
		19
		Table 3: MTurk user study results: For each of these
		15 celebrities, human evaluators support our represen-
		tativeness scores with a signiﬁcance level above 0
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 3 describes the results obtained in the user
		study: the mean and standard deviation of percent-
		age of answers in agreement with our model, the
		p-value, and the number of MTurk workers who
		completed each task
	</Extractive Summary>
</Paper ID=ument175>


<Paper ID=ument175> <Table ID =4>
	<Abstractive Summary> =
		75
		Table 4: Pearson correlation coefﬁcients between mean
		popularity measure and percentile, for each user (Coef-
		ﬁcients with p-value < 0
	</Abstractive Summary>
</Paper ID=ument175>


<Paper ID=ument176> <Table ID =1>
	<Abstractive Summary> =
		”
		Table 1: Sub-themes that we developed based on the data from www
	</Abstractive Summary>
	<Extractive Summary> =
		Four key themes were identiﬁed in our analy-
		sis of the MAS data: Attributive, Institutionalized,
		Teaming, and Othering (see Table 1)
	</Extractive Summary>
</Paper ID=ument176>


<Paper ID=ument176> <Table ID =2>
	<Abstractive Summary> =
		4641
		Table 2: Inter-annotator agreement across three annotators
		on the four themes of microaggressions
	</Abstractive Summary>
</Paper ID=ument176>


<Paper ID=ument176> <Table ID =3>
	<Abstractive Summary> =
		17
		Table 3: The ﬁnal microaggression typology after analyzing 1,300 self-reported microaggressions in SELFMA dataset
	</Abstractive Summary>
</Paper ID=ument176>


<Paper ID=ument176> <Table ID =4>
	<Abstractive Summary> =
		”
		Table 4: Examples of MAS surfaced by the discrepancy/offensiveness classiﬁers, with corresponding manually-annotated
		labels of their typology category
	</Abstractive Summary>
</Paper ID=ument176>


<Paper ID=ument176> <Table ID =5>
	<Abstractive Summary> =
		1)
		Reddit
		Attributive
		53
		3
		Institutionalized
		59
		79
		Teaming
		10
		21
		Othering
		7
		10
		Table 5: Gender-based MAS crawled in SELFMA (n = 59)
		and found in Reddit (n = 19) after the second round of anno-
		tation, according to their category from our proposed typol-
		ogy (§2
	</Abstractive Summary>
</Paper ID=ument176>


<Paper ID=ument177> <Table ID =1>
	<Abstractive Summary> =
		)
		Clothing, Shoes & Jewelry
		8,789
		52,402
		37,036
		964
		6,630
		4,668
		Electronics
		16,456
		215,512
		136,798
		1,858
		21,706
		13,845
		Grocery & Gourmet Food
		30,617
		231,131
		151,828
		3,392
		25,984
		16,982
		Health & Personal Care
		19,817
		216,839
		135,501
		2,198
		23,517
		14,565
		Home & Kitchen
		17,999
		202,549
		147,324
		1,978
		24,163
		17,896
		Movies & TV
		7,518
		316,235
		143,790
		847
		35,880
		15,973
		Pet Supplies
		18,189
		167,775
		123,287
		2,031
		21,170
		15,650
		Tools & Home Improvement
		30,105
		217,397
		150,483
		3,454
		24,971
		17,426
		TOTAL
		149,490
		1,619,840
		1,026,047
		16,722
		184,021
		117,005
		Table 1: The statistics of the Amazon dataset for helpfulness prediction of online reviews
	</Abstractive Summary>
	<Extractive Summary> =
		, Amazon
		and Yelp, are built, and the statistics of them are
		shown by Table 1 and Table 2, respectively
	</Extractive Summary>
	<Extractive Summary> =
		in Table 1 and Ta-
		ble 2, as the experimental samples
	</Extractive Summary>
</Paper ID=ument177>


<Paper ID=ument177> <Table ID =2>
	<Abstractive Summary> =
		)
		Beauty & Spas
		8,552
		101,383
		56,711
		949
		10,007
		5,659
		Health & Medical
		10,079
		90,045
		57,967
		1,132
		10,434
		6,758
		Home Services
		7,675
		71,247
		47,369
		876
		7,011
		4,575
		Restaurants
		3,432
		100,613
		41,139
		383
		11,668
		4,779
		Shopping
		11,706
		104,555
		49,833
		1,295
		12,892
		5,969
		TOTAL
		41,444
		467,843
		253,019
		4,635
		52,012
		27,740
		Table 2: The statistics of the Yelp dataset for helpfulness prediction of online reviews
	</Abstractive Summary>
	<Extractive Summary> =
		, Amazon
		and Yelp, are built, and the statistics of them are
		shown by Table 1 and Table 2, respectively
	</Extractive Summary>
	<Extractive Summary> =
		) in Ta-
		ble 1 and Table 2 as the training and test sets
	</Extractive Summary>
</Paper ID=ument177>


<Paper ID=ument177> <Table ID =3>
	<Abstractive Summary> =
		049)
		Table 3: Results of performance (AUROC) of up-to-date methods on identifying helpful reviews evaluated by
		the test sets of Amazon
	</Abstractive Summary>
	<Extractive Summary> =
		In line with Table 3 and Table 4,
		MTNL (Fan et al
	</Extractive Summary>
</Paper ID=ument177>


<Paper ID=ument177> <Table ID =4>
	<Abstractive Summary> =
		047)
		Table 4: Results of performance (AUROC) on identifying helpful reviews evaluated by the test sets of Yelp
	</Abstractive Summary>
	<Extractive Summary> =
		In line with Table 3 and Table 4,
		MTNL (Fan et al
	</Extractive Summary>
</Paper ID=ument177>


<Paper ID=ument177> <Table ID =5>
	<Abstractive Summary> =
		054)
		Table 5: Results of performance (R2-score) on helpfulness voting regression evaluated by the test sets of Amazon
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 and Table 6 show that MTNL (Fan
		et al
	</Extractive Summary>
</Paper ID=ument177>


<Paper ID=ument177> <Table ID =6>
	<Abstractive Summary> =
		058)
		Table 6: Results of performance (R2-score) on helpfulness voting regression evaluated by the test sets of Yelp
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 and Table 6 show that MTNL (Fan
		et al
	</Extractive Summary>
</Paper ID=ument177>


<Paper ID=ument178> <Table ID =1>
	<Abstractive Summary> =
		65
		Table 1: Reddit author ranking results with 111,396 possible targets
	</Abstractive Summary>
	<Extractive Summary> =
		As described
		above, the experiments presented in Table 1 in-
		volved ranking episodes by test authors, some of
		whom had been seen during training, and some
		new to the model
	</Extractive Summary>
	<Extractive Summary> =
		As described above,
		the experiments presented in Table 1 involved
		episodes of length exactly 16
	</Extractive Summary>
</Paper ID=ument178>


<Paper ID=ument178> <Table ID =2>
	<Abstractive Summary> =
		305
		Table 2: Twitter ranking results with 25,000 queries and with 169,663 possible targets
	</Abstractive Summary>
</Paper ID=ument178>


<Paper ID=ument178> <Table ID =3>
	<Abstractive Summary> =
		72
		Table 3: Validation and test accuracy for the Wikipedia
		sockpuppet task
	</Abstractive Summary>
</Paper ID=ument178>


<Paper ID=ument178> <Table ID =4>
	<Abstractive Summary> =
		84
		Table 4: Clustering performance on Reddit episodes
		using embeddings obtained with different methods
	</Abstractive Summary>
</Paper ID=ument178>


<Paper ID=ument179> <Table ID =1>
	<Abstractive Summary> =
		Table 1:
		Textual variation across different denotation
		settings
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Preliminary Study
		Table 1 shows output texts produced by annotators
		given different input denotation settings
	</Extractive Summary>
</Paper ID=ument179>


<Paper ID=ument179> <Table ID =2>
	<Abstractive Summary> =
		44
		Table 2: Denotation experiment to ﬁnd the best input setting (i
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows results of the two metrics across
		different input settings we deﬁne
	</Extractive Summary>
</Paper ID=ument179>


<Paper ID=ument179> <Table ID =3>
	<Abstractive Summary> =
		Number of Sentences
		Number of Stories
		Train
		33,240
		6,648
		Valid
		4,155
		831
		Test
		4,155
		831
		total
		41,550
		8,310
		Table 3: Data statistics of the PASTEL
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the ﬁnal
		number of stories and sentences in our dataset
	</Extractive Summary>
</Paper ID=ument179>


<Paper ID=ument179> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Two sentence-level (top, middle) and one story-level (bottom) annotations in PASTEL
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows few examples randomly cho-
		sen from our dataset: two at sentence level (top,
		middle) and one at story level (bottom)
	</Extractive Summary>
</Paper ID=ument179>


<Paper ID=ument179> <Table ID =5>
	<Abstractive Summary> =
		Gender:Male
		Gender:Female
		PROPN, ADJ, # ENTITY,
		went, party, SENT LEN
		happy, day, end, group, just,
		snow, NOUN
		Politics:LeftWing
		Politics:RightWing
		female, time, NOUN, ADP,
		VERB, porch, day, loved
		SENT LENGTH, PROPN,
		# ENTITY, n’t, ADJ, NUM
		Education:Bachelor
		Education:NoDegree
		food, went,
		# STOPWORDS, race, ADP
		!, just, came, love, lots, male,
		fun, n’t, friends, happy
		Age:18-24
		Age:35-44
		ADP, come, PROPN, day,
		ride, playing, sunset
		ADV, did, town, went,
		NOUN, # STOPWORDS
		Table 5: Most salient lexical (lower cased) and syntac-
		tic (upper cased) features on story-level classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the most salient features for clas-
		siﬁcation of each style
	</Extractive Summary>
</Paper ID=ument179>


<Paper ID=ument179> <Table ID =6>
	<Abstractive Summary> =
		529
		Table 6:
		Supervised style transfer
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows our results on style tran-
		fer
	</Extractive Summary>
</Paper ID=ument179>


<Paper ID=ument179> <Table ID =7>
	<Abstractive Summary> =
		Style (α): (Master, Centrist)
		S + α → ˆS: The woman had a great time with her husband
		¯Sα: Her husband shared a cake with her during reception
		Style (α): (Vocational, Right)
		S + α → ˆS: The food is ready for the reception
		¯Sα: The new husband shared the cake at the reception
		Table 7: Examples of style transferred text by our supervised model (S2S+GLOVE+PRETR
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows output text ˆS produced by our
		model given a source text S and a style α
	</Extractive Summary>
</Paper ID=ument179>


<Paper ID=ument179> <Table ID =8>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 in Appendix includes the cate-
		gories in other styles and their distributions
	</Extractive Summary>
</Paper ID=ument179>


<Paper ID=ument18> <Table ID =1>
	<Abstractive Summary> =
		Table 1: In contrast to reviews and tips, we seek to
		automatically generate recommendation justiﬁcations
		that are more concise, concrete, and helpful for deci-
		sion making
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows examples of re-
		views, tips and ideal justiﬁcations
	</Extractive Summary>
</Paper ID=ument18>


<Paper ID=ument18> <Table ID =2>
	<Abstractive Summary> =
		325
		Table 2: Performance for classifying review segments
		as good or bad for recommendation justiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents results for our binary classiﬁca-
		tion task
	</Extractive Summary>
</Paper ID=ument18>


<Paper ID=ument18> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Examples of justiﬁcations with ﬁne-grained
		aspects in our annotated dataset
	</Abstractive Summary>
	<Extractive Summary> =
		1 Table 3 presents a set of examples from
		our dataset
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 3, the trained classiﬁer works
		well on both datasets
	</Extractive Summary>
</Paper ID=ument18>


<Paper ID=ument18> <Table ID =4>
	<Abstractive Summary> =
		193
		Embedding Layer
		Fine-grained Aspects
		������������������������������������ = [������������������������������������
		1 , … , ������������������������������������
		������������′]
		Attention Layer
		and
		are
		[MASK]
		[MASK]
		top
		notch
		service
		are
		and
		top
		notch
		atmosphere
		BERT
		������������1
		������������2
		������������3
		������������4
		������������5
		������������6
		������������������������������������������������
		������������1
		������������2
		������������3
		������������4
		������������5
		������������6
		������������
		[CLS]
		Projection Layer
		P(atmosphere)
		P(service)
		Figure 2: Structure of the Aspect Conditional Masked Language Model
		Iter 0
		universe [MASK] is extremely friendly and per-
		sona ##ble
		Iter 5
		the [MASK] is extremely friendly and persona
		##ble
		Iter 10
		the [MASK] is extremely friendly and persona
		##ble
		Iter 15
		the staff are extremely cool and persona ##ble
		Iter 20
		the staff are extra kind , persona ##ble
		Table 4:
		Examples of the generation output of
		ACMLM at different iterations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows an example of the generation pro-
		cess
	</Extractive Summary>
</Paper ID=ument18>


<Paper ID=ument18> <Table ID =5>
	<Abstractive Summary> =
		edu/data/amazon
		194
		Dataset
		Train
		Dev
		Test
		# Users
		# Items
		# Aspects
		Yelp
		1,219,962
		115,907
		115,907
		115,907
		51,948
		2,041
		Amazon Clothing
		202,528
		57,947
		57,947
		57,947
		50,240
		581
		Table 5: Statistics of our datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5
		shows the statistics of our two datasets
	</Extractive Summary>
</Paper ID=ument18>


<Paper ID=ument18> <Table ID =6>
	<Abstractive Summary> =
		312
		Table 6: Performance on Automatic Evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6,
		our reference-based models achieve the highest
		BLEU scores on both datasets except for BLEU-
		3 on Yelp
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 6, both
		sampling-based methods Ref2Seq (Top-k) and
		ACMLM achieve higher Distinct-1 and Distinct-2,
		while their BLEU scores are lower than Seq2Seq
		based models using beam search
	</Extractive Summary>
</Paper ID=ument18>


<Paper ID=ument18> <Table ID =7>
	<Abstractive Summary> =
		42
		Table 7: Performance on Human Evaluation, where
		R,I,D
		represents
		Relevance,
		Informativeness
		and
		Diversity, respectively
	</Abstractive Summary>
</Paper ID=ument18>


<Paper ID=ument18> <Table ID =8>
	<Abstractive Summary> =
		Ref2Seq (Review)
		i love trader joe ’s , i love trader
		joe ’s
		the food was good and the ser-
		vice was great
		i love this place ! the food is
		always good and the service is
		always great
		Ref2Seq (Tip)
		this place is awesome
		love this place
		come here
		Ref2Seq
		this place has some of the best
		burgers
		the sushi is delicious
		the room was nice
		Ref2Seq (Top-k)
		the fries are amazing
		fresh and delicious sushi
		open hotel for hours
		ACMLM
		breakfast sandwiches are over-
		all very ﬁlling
		overall fun experience with half
		price sushi
		family style dinner , long time
		shopping trip to vegas, family
		dining , cheap lunch
		Table 8: Comparisons of the generated justiﬁcations from different models for three businesses on the Yelp dataset
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Qualitative Analysis
		Here we study the following two qualitative ques-
		tions:
		RQ1: How do training data and methods affect
		generation? As Table 8 shows, models trained on
		reviews and tips tend to generate generic phrases
		(such as ‘i love this place’) which often do not
		include information that helps users to make de-
		Dataset
		Aspects
		Generated Output
		Yelp
		dining
		the dining room is nice
		pastry
		the pastries were pretty good
		chicken
		the chicken fried rice is the best
		sandwich
		the pulled pork sandwich is the
		best thing on the menu
		product
		great product , fast shippong
		Amazon-
		price
		design is nice , good price
		Clothing
		leather
		comfortable leather sneakers
	</Extractive Summary>
</Paper ID=ument18>


<Paper ID=ument18> <Table ID =9>
	<Abstractive Summary> =
		classic
		walking
		sturdy , great city walking shoes
		Table 9: Generated justiﬁcations from AP-Ref2Seq
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 9, most planned aspects are present
		in the generated outputs of AP-Req2Seq
	</Extractive Summary>
</Paper ID=ument18>


<Paper ID=ument180> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Turning points and their deﬁnitions
	</Abstractive Summary>
	<Extractive Summary> =
		Since the sequence, num-
		ber, and labels of TPs are ﬁxed (see Table 1), we
		treat TP identiﬁcation as a binary classiﬁcation
		problem (where 1 indicates that the text is a TP
		and 0 otherwise)
	</Extractive Summary>
</Paper ID=ument180>


<Paper ID=ument180> <Table ID =2>
	<Abstractive Summary> =
		4)
		Table 2: Statistics of the TRIPOD dataset; all means
		are shown with standard deviation in brackets
	</Abstractive Summary>
	<Extractive Summary> =
		In
		Table 2, we present various statistics of the dataset
	</Extractive Summary>
</Paper ID=ument180>


<Paper ID=ument180> <Table ID =3>
	<Abstractive Summary> =
		74
		Table 3: Expected TP position based on screenwriting
		theory; mean position µ and standard deviation σ in
		goldstandard synopses of our training set
	</Abstractive Summary>
	<Extractive Summary> =
		The results are shown in Table 3, to-
		gether with the TP positions postulated by screen-
		writing theory
	</Extractive Summary>
	<Extractive Summary> =
		In experiments we calculate
		a window µ±σ based on our data (see Table 3)
	</Extractive Summary>
</Paper ID=ument180>


<Paper ID=ument180> <Table ID =4>
	<Abstractive Summary> =
		43)
		(b) Test set
		Table 4: Identiﬁcation of TPs in plot synopses; re-
		sults are shown in percent (TA: mean Total Agreement;
		D: annotation distance; standard deviation in brackets)
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results
		TP Identiﬁcation in Synopses Table 4a reports
		our results on the development set (we extracted
		20 movies from the original training set) which
		aim at comparing various model instantiations for
		the TP identiﬁcation task
	</Extractive Summary>
	<Extractive Summary> =
		Moreover, the addition
		of a context interaction layer (see TAM row in
		Table 4a) yields an absolute TA improvement of
		4
	</Extractive Summary>
	<Extractive Summary> =
		Table 4b shows our results on the test set
	</Extractive Summary>
</Paper ID=ument180>


<Paper ID=ument180> <Table ID =5>
	<Abstractive Summary> =
		53
		Table 5: Mean annotation distance D (test set); results
		are shown per TP on the synopsis identiﬁcation task
	</Abstractive Summary>
	<Extractive Summary> =
		The annota-
		tion distance per TP is presented in Table 5 (last
		line), where it is compared with the automatic TP
		identiﬁcation results (to be explained later)
	</Extractive Summary>
</Paper ID=ument180>


<Paper ID=ument180> <Table ID =6>
	<Abstractive Summary> =
		93)
		Table 6: Identiﬁcation of TPs in screenplays; results are
		shown in percent using ﬁve-fold cross validation (TA:
		mean Total Agreement; PA: Partial Agreement; D: an-
		notation distance D; standard deviation in brackets)
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 6, tf*idf approaches per-
		form worse than position-related baselines
	</Extractive Summary>
</Paper ID=ument180>


<Paper ID=ument181> <Table ID =1>
	<Abstractive Summary> =
		1
		Total number of posts from all users in a year
		252,901
		190,087
		Table 1: Statistics of suicidal users’ normal posts and hidden tree hole posts on Microblog based on 3652 users
		from May 1, 2018 to April 30, 2019
	</Abstractive Summary>
</Paper ID=ument181>


<Paper ID=ument181> <Table ID =2>
	<Abstractive Summary> =
		1721
		Category
		Suicide Ideation
		Suicide behavior
		Psychache
		Mental illness
		Hopeless
		Number
		586
		88
		403
		48
		188
		Words/phrases
		want to die
		escape
		seppuku
		hypnotics
		want to cry
		loneliness
		depression
		hallucination
		dead end
		despair
		Table 2: Representative words/phrases of Chinese suicide dictionary
		3
		Suicide-oriented Word Embeddings
		Although there are some good works on word em-
		beddings (Mikolov et al
	</Abstractive Summary>
</Paper ID=ument181>


<Paper ID=ument181> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Summary of user’s features
	</Abstractive Summary>
</Paper ID=ument181>


<Paper ID=ument181> <Table ID =4>
	<Abstractive Summary> =
		Finally, we apply a concatenate operation to
		jointly consider G and F, and through a fully con-
		nected layer to compute the possibility of suicide:
		Users
		Posts
		Posts with
		image
		suicide
		3,652
		252,901
		93,461
		non-suicide
		3,677
		491,130
		260,667
		Table 4: Statistic of suicide data set
	</Abstractive Summary>
</Paper ID=ument181>


<Paper ID=ument181> <Table ID =5>
	<Abstractive Summary> =
		92
		Table 5: Performance comparison for different word embedding and different detection model, where “So-W2v”,
		“So-Glove” and “So-FastText” represent suicide-oriented word embeddings based on Word2vec, GloVe and Fast-
		Text respectively
	</Abstractive Summary>
</Paper ID=ument181>


<Paper ID=ument181> <Table ID =6>
	<Abstractive Summary> =
		77
		Table 6: Performance comparison between different
		suicide risk detection model, where Acc and F1 rep-
		resent accuracy and F1-score respectively
	</Abstractive Summary>
</Paper ID=ument181>


<Paper ID=ument181> <Table ID =7>
	<Abstractive Summary> =
		92
		Table 7: Ablation test for SDM with different inputs
	</Abstractive Summary>
</Paper ID=ument181>


<Paper ID=ument182> <Table ID =1>
	<Abstractive Summary> =
		3
		Pledge Speciﬁcity Dataset
		We annotated 22 election manifestos from the
		Australian Labor and Liberal parties, covering
		eleven Australian federal election cycles from
		1731
		Category
		Deﬁnition
		Example
		#
		Not a pledge
		Provide facts; greetings; approval or
		criticism of policies
		We have modernised Australia’s industrial re-
		lations system, particularly through the 1996
		Workplace Relations Act
		1
		Rhetorical pledge
		Based on moral values and applies to all
		irrespective of the party
		We will put our country ﬁrst
		2
		General pledge
		Specify intangible goals, and also not
		the ways to achieve them
		Labor will build a stronger and more productive
		economy
		3
		Continuity pledge
		Commit to the maintenance of currently
		functioning policy
		We will retain the voluntary health insurance
		system which now covers the great majority of
		Australians
		4
		Goal pledge
		Provide tangible outcomes and goals,
		without providing the means to achieve
		them
		A Shorten Labor Government will create 2000
		jobs in Adelaide
		5
		Action pledge
		Provide means to achieve the objective,
		but don’t reveal speciﬁc details
		We pledge to support effective voluntary fam-
		ily planning, and to recognize ofﬁcially the link
		between social and economic development and
		the willingness of the individual to limit family
		size
		6
		Detailed pledge
		Provide
		clear
		details
		of
		action
		to
		achieve an objective
		A re-elected Coalition Government will invest
		$1 million to support the Exeter Community
		Precinct
		7
		Table 1: Pledge speciﬁcity category, deﬁnition, example manifesto sentence, and the corresponding speciﬁcity
		value (#)
	</Abstractive Summary>
	<Extractive Summary> =
		We use a class schema proposed by Pomper
		and Lederman (1980) as detailed in Table 1, which
		captures seven levels of speciﬁcity, forming a non-
		linear increasing order of commitment and speci-
		ﬁcity (Pomper and Lederman, 1980)
	</Extractive Summary>
	<Extractive Summary> =
		See Table 1 for class def-
		initions and an example of each class
	</Extractive Summary>
</Paper ID=ument182>


<Paper ID=ument182> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Distribution and length statistics across speci-
		ﬁcity categories
	</Abstractive Summary>
	<Extractive Summary> =
		The
		class distribution in the ﬁnal dataset is given in
		Table 2, alongside the average sentence length in
		tokens
	</Extractive Summary>
</Paper ID=ument182>


<Paper ID=ument182> <Table ID =3>
	<Abstractive Summary> =
		49
		Table 3: Speciﬁcity prediction performance; the best
		approach is given in bold
	</Abstractive Summary>
	<Extractive Summary> =
		From the results in Table 3, we can see that se-
		quential models with ELMo embeddings (biGRU)
		perform better than neural bag-of-words models
		(NNREG)
	</Extractive Summary>
</Paper ID=ument182>


<Paper ID=ument182> <Table ID =4>
	<Abstractive Summary> =
		1737
		Speciﬁcity Weight — Model I
		Manifesto(x) ∧ Policy(I) ∧ Specw(x, I) ∧ IdeologyMap(I, social left/right) → socpos(x)
		Manifesto(x) ∧ Policy(I) ∧ Specw(x, I) ∧ IdeologyMap(I, economic left/right) → econpos(x)
		Overall position — Model II
		Manifesto(x) ∧ socpos(x) → pos(x)
		Manifesto(x) ∧ econpos(x) → pos(x)
		Global signals — Model III
		Manifesto(x) ∧ Party(x, a) ∧ Manifesto(y) ∧ Party(y, b) ∧ Coalition(a, b) ∧ pos(x) → pos(y)
		Manifesto(x) ∧ Party(x, a) ∧ PreviousManifesto(x, t) ∧ Party(t, a) ∧ pos(t) → pos(x)
		Relative speciﬁcity — Model IV
		Manifesto(x) ∧ Policy(I) ∧ SpecScale(x, I) ∧ IdeologyMap(I, social left/right) → socpos(x)
		Manifesto(x) ∧ Policy(I) ∧ SpecScale(x, I) ∧ IdeologyMap(I, economic left/right) → econpos(x)
		Table 4: PSL Model: Representative rules
	</Abstractive Summary>
	<Extractive Summary> =
		We then use the PSL model (Table 4) to re-
		calibrate the scores based on speciﬁcity scores and
		the global signals
	</Extractive Summary>
</Paper ID=ument182>


<Paper ID=ument182> <Table ID =5>
	<Abstractive Summary> =
		45
		Table 5: Spearman’s ρ for prediction of party position
		based on the different models
	</Abstractive Summary>
</Paper ID=ument182>


<Paper ID=ument182> <Table ID =6>
	<Abstractive Summary> =
		Log-
		likelihood values are given in Table 6:
		across
		all policy areas, pledge speciﬁcity better captures
		salience than a count-based representation71
		Table 6: Log-likelihood with pledges speciﬁcity weight
		(S) and count of sentences (C) across 57 policy themes
		as independent variables
	</Abstractive Summary>
</Paper ID=ument182>


<Paper ID=ument183> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		8
		Results and discussion
		Our results are shown in Table 1 — our objective
		here is maximum accuracy with minimum training
		data required
	</Extractive Summary>
</Paper ID=ument183>


<Paper ID=ument183> <Table ID =2>
	<Abstractive Summary> =
		91
		Table 2: MetaLWOz dataset statistics
		We use the Stanford Multi-Domain (SMD)
		dialogue dataset (Eric et al
	</Abstractive Summary>
</Paper ID=ument183>


<Paper ID=ument183> <Table ID =3>
	<Abstractive Summary> =
		83
		Table 3: Stanford multi-domain dataset statistics (train-
		set)
		mains: appointment scheduling, city navigation,
		and weather information
	</Abstractive Summary>
</Paper ID=ument183>


<Paper ID=ument183> <Table ID =4>
	<Abstractive Summary> =
		weather
		<usr>
		What is the weather forecast
		For what city would you
		For what city would you like
		for the weekend?
		like to know that?
		the weekend forecast for?
		Table 4: DiKTNet’s selected responses
		Where can I go shopping?
		Where does my friend live?
		Where can I get Chinese food?
		Where can I go to eat?
		Can you please take me to a coffee house?
		I’d like to set a reminder for my meeting at 2pm later this month please
	</Abstractive Summary>
	<Extractive Summary> =
		“You’re welcome!”;
		see more examples in Table 4)
	</Extractive Summary>
</Paper ID=ument183>


<Paper ID=ument183> <Table ID =5>
	<Abstractive Summary> =
		center? Anything within 4 miles?
		Get the address to my friend’s house that i could get to the fastest
		Car I need to get to my friends house, it should be within 4 miles from here
		Table 5: Selected clusters of utterances sharing the same LAED codes
		the alternative models at the same percentage of
		seed data
	</Abstractive Summary>
	<Extractive Summary> =
		In
		order to have a glimpse into the LAED-produced
		clustering, in Table 5 we present a snippet of the
		utterance clusters sharing the same, most frequent
		latent codes throughout the dataset (the clustering
		is obtained with LAED model trained on every do-
		main but ‘Store details’, i
	</Extractive Summary>
</Paper ID=ument183>


<Paper ID=ument183> <Table ID =6>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		All the domains available in the Met-
		aLWOz dataset are listed in the Table 6 of the Ap-
		pendix A
	</Extractive Summary>
</Paper ID=ument183>


<Paper ID=ument184> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example dialog context from the training set
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Multi-Granularity Training Example
		Table 1 shows an example dialog context, ground-
		truth response and the negative candidate re-
		sponses sampled at several levels of cosine dis-
		tance, as per Equation 11
	</Extractive Summary>
</Paper ID=ument184>


<Paper ID=ument184> <Table ID =2>
	<Abstractive Summary> =
		18%
		Table 2: Performance on MultiWOZ
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 2 demonstrate the strong
		performance gains obtained with MGT
	</Extractive Summary>
</Paper ID=ument184>


<Paper ID=ument184> <Table ID =3>
	<Abstractive Summary> =
		45
		Table 3: Results for next utterance retrieval on the Ubuntu dialog corpus
	</Abstractive Summary>
	<Extractive Summary> =
		The re-
		sults shown in Table 3 show the performance of
		MGT using two different underlying architectures,
		as well as previous work
	</Extractive Summary>
</Paper ID=ument184>


<Paper ID=ument184> <Table ID =4>
	<Abstractive Summary> =
		46
		Table 4: Results of the granularity analysis experiment
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 4 conﬁrm the hypothe-
		sis that MGT results in models that learn differ-
		ent granularities of representation
	</Extractive Summary>
</Paper ID=ument184>


<Paper ID=ument184> <Table ID =5>
	<Abstractive Summary> =
		75
		Table 5: Experimental results demonstrating perfor-
		mance on two downstream tasks, without any ﬁne-
		tuning of the latent representations
	</Abstractive Summary>
	<Extractive Summary> =
		The results shown in Table 5 demonstrate
		Model Name
		BoW (F-1)
		DA (F-1)
		Dual Encoder
		60
	</Extractive Summary>
</Paper ID=ument184>


<Paper ID=ument184> <Table ID =6>
	<Abstractive Summary> =
		46
		Table 6: Experimental results demonstrating perfor-
		mance on the downstream task of dialog act predic-
		tion, when the model is ﬁne-tuned on all available
		data
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 6 demonstrate that MGT
		learns general representations which effectively
		transfer to downstream tasks, especially more dif-
		ﬁcult tasks such as dialog act prediction
	</Extractive Summary>
</Paper ID=ument184>


<Paper ID=ument185> <Table ID =1>
	<Abstractive Summary> =
		Applicant: Xiao Liu Fruit
		System: Which is the nearest supermarket to $School$?
		Applicant: Educational Supermarket
		System: When was $School$ founded?
		Applicant: 2002
		Decison: Fraud (Wrong)
		Decison: Non-Fraud (Correct)
		Table 1: Examples of the low-level policies in two systems
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows an example of verifying personal
		information about “School” in HP-S and Full-S
	</Extractive Summary>
</Paper ID=ument185>


<Paper ID=ument186> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Automatic Evaluation Result
	</Abstractive Summary>
</Paper ID=ument186>


<Paper ID=ument186> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Manual evaluation result
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the manual evaluation result
		where the content and expression are considered
		simultaneously
	</Extractive Summary>
</Paper ID=ument186>


<Paper ID=ument186> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Generated responses from kgCVAE and ECM in two examples
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Qualitative Analysis
		Table 3 shows the responses generated by kgC-
		VAE and ECM
	</Extractive Summary>
</Paper ID=ument186>


<Paper ID=ument187> <Table ID =1>
	<Abstractive Summary> =
		10393
		Table 1: The upper/lower two tables show statistics
		of the EMNLP/ICLR datasets and corresponding aug-
		mented knowledg graphs
	</Abstractive Summary>
</Paper ID=ument187>


<Paper ID=ument187> <Table ID =2>
	<Abstractive Summary> =
		24
		Table 2: Results of automatic evaluations on the two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Results:
		As shown in Table 2, AKGCM
		(or AKGCM-5) can obtain the highest score
		on test set in terms of Hit@1, and the second
		highest scores in terms of BLEU-4, ROUGE-
		2 and ROUGE-L, surpassing other models, ex-
		cept BiDAF, by a large margin
	</Extractive Summary>
</Paper ID=ument187>


<Paper ID=ument187> <Table ID =3>
	<Abstractive Summary> =
		78
		Table 3: Results of human evaluations on the two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Results: In Table 3, each score for win/tie/lose
		is the percentage of messages for which AKGCM
		(or
		AKGCM-5)
		can
		generate
		better/almost
		same/worse responses, in comparison with a
		baseline
	</Extractive Summary>
</Paper ID=ument187>


<Paper ID=ument187> <Table ID =4>
	<Abstractive Summary> =
		72
		Table 4: Results of ablation study for AKGCM on
		EMNLP dataset
	</Abstractive Summary>
</Paper ID=ument187>


<Paper ID=ument187> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Examples in which AKGCM performs better than other models on two dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 presents the examples
		in which AKGCM (AKGCM-5) performs better
		than other models on two dataset
	</Extractive Summary>
</Paper ID=ument187>


<Paper ID=ument188> <Table ID =1>
	<Abstractive Summary> =
		108
		Table 1: Quantitative evaluation results (%)
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Overall Performance
		Table 1 lists the performance of our system and
		the comparison systems
	</Extractive Summary>
</Paper ID=ument188>


<Paper ID=ument188> <Table ID =2>
	<Abstractive Summary> =
		53
		Table 2: The results of human evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 reveals the results of subjective evalu-
		ation
	</Extractive Summary>
</Paper ID=ument188>


<Paper ID=ument188> <Table ID =3>
	<Abstractive Summary> =
		407
		Table 3: Speed test
	</Abstractive Summary>
</Paper ID=ument188>


<Paper ID=ument188> <Table ID =4>
	<Abstractive Summary> =
		Movie
		Politics
		Ubuntu
		Food
		Network
		imax
		trump
		install
		food
		router
		movie
		people
		grub
		vegetarian
		wireless
		youtube
		hillary
		apt
		seafood
		ip
		scene
		vote
		kernel
		restaurants
		address
		marvel
		clinton
		nvidia
		cotto
		phone
		hulk
		election
		cd
		gourmet
		network
		avengers
		debate
		sudo
		serves
		card
		nolan
		donald
		ssh
		starbucks
		eth0
		comics
		support
		boot
		breakfast
		dhcp
		batman
		working
		ubuntu
		pizzeria
		wiﬁ
		Table 4: Topics by the words (β in Eq
	</Abstractive Summary>
</Paper ID=ument188>


<Paper ID=ument188> <Table ID =5>
	<Abstractive Summary> =
		Inferred topic distribution:
		Table 5: Test samples of our model (ADAND) and
		the baselines
	</Abstractive Summary>
</Paper ID=ument188>


<Paper ID=ument189> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example of the outputs of recommender
		dialog systems
	</Abstractive Summary>
</Paper ID=ument189>


<Paper ID=ument189> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Evaluation of the recommender system
	</Abstractive Summary>
</Paper ID=ument189>


<Paper ID=ument189> <Table ID =3>
	<Abstractive Summary> =
		99
		Table 3: Automatic and human evaluation of dialog
		generation
	</Abstractive Summary>
	<Extractive Summary> =
		Dialog
		Table 3 shows the results of the eval-
		uation of the baseline models and our proposed
		method in dialog generation
	</Extractive Summary>
</Paper ID=ument189>


<Paper ID=ument189> <Table ID =4>
	<Abstractive Summary> =
		1810
		Movie
		1
		2
		3
		4
		5
		6
		7
		8
		Star Wars
		space
		alien
		sci-ﬁ
		star
		sci
		robot
		smith
		harry
		The Shining
		creepy
		stephen
		gory
		horror
		scary
		psychological
		haunted
		thriller
		The Avengers (2012)
		marvel
		superhero
		super
		dc
		wait
		batman
		thor
		take
		Beauty and the Beast
		cute
		disney
		animated
		live
		music
		child
		robin
		kids
		Table 4: Examples of top 8 vocabulary bias
	</Abstractive Summary>
</Paper ID=ument189>


<Paper ID=ument19> <Table ID =1>
	<Abstractive Summary> =
		32
		# Segments
		123,242
		46,255
		# NG (negative)
		12,619
		6,130
		# NE (neutral)
		97,380
		33,158
		# PO (positive)
		13,243
		6,976
		Table 1: Statistics of the datasets we collected
	</Abstractive Summary>
	<Extractive Summary> =
		One rea-
		son is that the service dialogues in our datasets
		have more than 25 utterances in average (See the
		statistics in Table 1) and contain a large proportion
		Clothes
		Makeup
		Methods
		MacroF1
		Acc
	</Extractive Summary>
</Paper ID=ument19>


<Paper ID=ument19> <Table ID =2>
	<Abstractive Summary> =
		785
		Table 2: Results of different satisfaction classiﬁcation
		methods
	</Abstractive Summary>
</Paper ID=ument19>


<Paper ID=ument19> <Table ID =3>
	<Abstractive Summary> =
		785
		Table 3: Results of different model conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we can observe that Customer out-
		performs Server by a large margin, which indi-
		cates that service satisfaction is mostly related to
		205
		Dataset
		Class
		positive
		neutral
		negative
		Clothes
		WS
		0
	</Extractive Summary>
</Paper ID=ument19>


<Paper ID=ument19> <Table ID =4>
	<Abstractive Summary> =
		355
		Table 4: Sentiment distribution in satisfaction classes
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results on Sentiment Classiﬁcation
		Table 4 shows another statistics of our datasets,
		i
	</Extractive Summary>
</Paper ID=ument19>


<Paper ID=ument19> <Table ID =5>
	<Abstractive Summary> =
		647
		Table 5: Results of sentiment classiﬁcation by different
		models
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 5, we compare the sentiment pre-
		diction results of MILNET, CAMILr, CAMILs
		and CAMILfull
	</Extractive Summary>
</Paper ID=ument19>


<Paper ID=ument19> <Table ID =6>
	<Abstractive Summary> =
		785
		Table 6:
		Satisfaction classiﬁcation comparison be-
		tween our method and a heuristic mapping method
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 6, we can observe that the Mapping
		method is far worse than our model
	</Extractive Summary>
</Paper ID=ument19>


<Paper ID=ument190> <Table ID =1>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Tasks and datasets
		We experiments with two tasks: generating arXiv-
		like and Holmes-like responses, respectively, us-
		ing the datasets summarized in Table 1
		Task
		Training
		Testing
		Dconv
		Dstyle
		Dtest
		arXiv-like
		Reddit
		arXiv
		arXiv-like Reddit
		Holmes-like
		Reddit
		Holmes
		Holmes-like Reddit
		Table 1: Summary of tasks and datasets
		(i) Reddit is a conversation dataset constructed
		from posts and comments on Reddit
	</Extractive Summary>
</Paper ID=ument190>


<Paper ID=ument190> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Example STYLEFUSION outputs for arXiv-
		like response generation task at different distance ρ and
		direction (towards zAE of a given sentence)
		0
		1
		neural
		Reddit
		arXiv
		Reddit
		arXiv
		target = arXiv
		Reddit
		Holmes
		Reddit
		Holmes
		target = Holmes
		0
	</Abstractive Summary>
</Paper ID=ument190>


<Paper ID=ument190> <Table ID =3>
	<Abstractive Summary> =
		a sphere?
		Table 3: Example system outputs on arXiv-like re-
		sponse generation task at ρ = 1
		5
	</Abstractive Summary>
	<Extractive Summary> =
		Additional examples of
		the system outputs and human responses are pro-
		vided in Table 3 and Table 4
		5
	</Extractive Summary>
</Paper ID=ument190>


<Paper ID=ument190> <Table ID =4>
	<Abstractive Summary> =
		human
		Must have feared what he was packin’
		Table 4: Example system outputs on Holmes-like re-
		sponse generation task at ρ = 1
		target
		model
		appropr-
		iateness
		style-
		intensity
		harmonic
		mean
		arXiv
		STYLEFUSION 0
	</Abstractive Summary>
	<Extractive Summary> =
		Additional examples of
		the system outputs and human responses are pro-
		vided in Table 3 and Table 4
		5
	</Extractive Summary>
</Paper ID=ument190>


<Paper ID=ument190> <Table ID =5>
	<Abstractive Summary> =
		37
		Table 5: Human evaluation results
	</Abstractive Summary>
</Paper ID=ument190>


<Paper ID=ument190> <Table ID =6>
	<Abstractive Summary> =
		17
		Table 6: Performance of each model on automatic measures
	</Abstractive Summary>
</Paper ID=ument190>


<Paper ID=ument191> <Table ID =1>
	<Abstractive Summary> =
		But I haven’t bought it
		在一起不错的选择
		Dating is a good choice
		B2
		(在巴黎开甜品店)不错啊，我很喜欢甜点！
		(Opening a dessert shop in Paris) Sounds great, 
		I love dessert!
		我有
		No problem, I have
		赞
		Cool
		A3
		你最喜欢哪一种?
		Which kind matches your taste most?
		一起啊
		Let’s do it together
		人呢
		Where are you?
		Label
		1
		1
		0
		Reference
		你最喜欢哪一种甜品?
		Which kind of dessert matches your taste most?
		一起玩桌游啊
		Let’s play board game together
		人呢
		Where are you?
		Table 1: Examples from the dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Take Example 1 in Table 1 for instance, con-
		tents in parentheses are information omitted in the
		utterance
	</Extractive Summary>
</Paper ID=ument191>


<Paper ID=ument191> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2:
		Statistics of Restoration-200K
	</Abstractive Summary>
	<Extractive Summary> =
		According
		to our survey (details in Table 2), in about 60%
		conversations, fully comprehending current utter-
		ance depends on previous context
	</Extractive Summary>
	<Extractive Summary> =
		The statis-
		tics in Table 2 show that most words in the refer-
		ence overlap with words in the original utterance
	</Extractive Summary>
</Paper ID=ument191>


<Paper ID=ument191> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: p, r, f here represent the restoration precision, recall and F-score that we propose in Section 3
	</Abstractive Summary>
</Paper ID=ument191>


<Paper ID=ument191> <Table ID =4>
	<Abstractive Summary> =
		51
		Table 4: Human evaluation on the restoration quality
		and language ﬂuency
	</Abstractive Summary>
</Paper ID=ument191>


<Paper ID=ument191> <Table ID =5>
	<Abstractive Summary> =
		你确定吗我怎么觉得其实是没拍好
		Are you sure? Why do I feel it was just a mistake
		Table 5: Examples for incomplete utterance restoration
	</Abstractive Summary>
	<Extractive Summary> =
		The third con-
		versation in Table 5 is a typical example
	</Extractive Summary>
	<Extractive Summary> =
		4
		Case study
		Table 5 shows some examples of the incomplete
		utterance restoration among different models
	</Extractive Summary>
</Paper ID=ument191>


<Paper ID=ument191> <Table ID =6>
	<Abstractive Summary> =
		11
		Table 6: Human evaluation on response quality from
		the single-turn dialogue system after restoration
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Evaluation results of single-turn and multi-turn
		dialogue systems are shown in Table 6 and
		Table 7, respectively
	</Extractive Summary>
</Paper ID=ument191>


<Paper ID=ument191> <Table ID =7>
	<Abstractive Summary> =
		11
		Table 7: Human evaluation on response quality from
		the multi-turn dialogue system after restoration
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Evaluation results of single-turn and multi-turn
		dialogue systems are shown in Table 6 and
		Table 7, respectively
	</Extractive Summary>
</Paper ID=ument191>


<Paper ID=ument191> <Table ID =8>
	<Abstractive Summary> =
		Now I am home
		(不喝酒)你可以去吃东西
		(If no drinking,) You can go and eat something
		MMI
		你达不到？
		Are you not good enough?
		你在哪上班呢?
		Where do you work?
		*没东西吃
		*There is nothing to eat
		MMI+PAC
		*那就别拍了
		*Then try to take photos of something else
		*你在西安干嘛呢?
		*What are you doing in Xi’an?
		我不喝酒的
		I do not drink
		SMN
		不行呗
		Just cannot do it
		你现在好好的啊
		You look cool now
		*啥东西吃?
		*What to eat?
		SMN+PAC
		*不行也得行
		*Yes, you can do it
		*带你高校一日游外加兴庆公园赏雪哈哈
		*I will take you to enjoy the one day trip to colleges and 
		the beauty of snow in Park of Xingqing Palace
		不喝酒那喝什么?
		What to drink if not the beer?
		Table 8: A comparison between taking original and restored utterance as the input query to a response generation
		model
	</Abstractive Summary>
</Paper ID=ument191>


<Paper ID=ument192> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: The result of rewriting quality
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the experiment result, which
		indicates that our rewriting method outperforms
		heuristic methods
	</Extractive Summary>
</Paper ID=ument192>


<Paper ID=ument192> <Table ID =2>
	<Abstractive Summary> =
		10
		Table 2: Automatic evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Evaluation Results
		Table 2 presents the automatic evaluation results,
		showing that our models outperform baselines on
		relevance and diversity
	</Extractive Summary>
</Paper ID=ument192>


<Paper ID=ument192> <Table ID =3>
	<Abstractive Summary> =
		02
		Table 3: The distribution of human evaluation in response generation model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 gives the hu-
		man annotation results, which also demonstrates
		the superiority of our models
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents the distribution of score in hu-
		man evaluation, we can observe that most of the
		responses generated by HRED and S2SA get 1
		or 2 in human evaluation, while most of the re-
		sponses generated by our model can get 2 or 3
	</Extractive Summary>
</Paper ID=ument192>


<Paper ID=ument192> <Table ID =4>
	<Abstractive Summary> =
		729
		Table 4: Evaluation results on multi-turn response selection
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Evaluation Results
		Table 4 shows the response selection perfor-
		mances of different methods
	</Extractive Summary>
</Paper ID=ument192>


<Paper ID=ument192> <Table ID =5>
	<Abstractive Summary> =
		5%
		Table 5: The result of end-to-end response selection
		subjective evaluation
	</Abstractive Summary>
</Paper ID=ument192>


<Paper ID=ument192> <Table ID =6>
	<Abstractive Summary> =
		Do you go off?
		Our Model+RL
		厦门欢迎你
		无聊到爆
		我也是坐班车
		Welcome to Xiamen
		I‘m bored to death
		I am taking the bus, too
		Table 6: The examples of end-to-end response generation
	</Abstractive Summary>
	<Extractive Summary> =
		1
		End-to-end Generation Chatbots
		Table 6 presents the generated examples of our
		models and baselines, our model can extract
		the keywords from the context which is help-
		ful to generate an informative response, but the
		HRED model often generates safe responses like
		“Metoo” or “Y es”
	</Extractive Summary>
</Paper ID=ument192>


<Paper ID=ument193> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example dialogue from the PERSONA-CHAT dataset
	</Abstractive Summary>
	<Extractive Summary> =
		An example dialogue
		conditioned on given proﬁles from this dataset is
		given in Table 1 for illustration
	</Extractive Summary>
</Paper ID=ument193>


<Paper ID=ument193> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2:
		Evaluation results of the IMN model
		and previous methods on PERSONA-CHAT dataset
		without using personas
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Experimental Results
		Table 2 presents the evaluation results of our
		reproduced IMN model (Gu et al
	</Extractive Summary>
</Paper ID=ument193>


<Paper ID=ument193> <Table ID =3>
	<Abstractive Summary> =
		2)
		Table 3: Performance of the proposed and previous methods on the PERSONA-CHAT under various persona
		conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the evaluation results of our
		proposed and previous methods on PERSONA-
		CHAT under various persona conﬁgurations
	</Extractive Summary>
	<Extractive Summary> =
		Improvement of Using Personas
		Examining
		the numbers which indicate the gains or losses
		after adding persona conditions in Table 3, we
		can see that the context-level persona fusion
		improves the performance of previous models
		signiﬁcantly when original self personas are
		used
	</Extractive Summary>
	<Extractive Summary> =
		Their
		As shown in Table 3, no signif-
		icant gains can be obtained when the models are
		conditioned on the personas of dialogue partners
	</Extractive Summary>
</Paper ID=ument193>


<Paper ID=ument193> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: Ablation tests of removing either persona-
		response matching or context-response matching in the
		DIM model conditioned on original self personas
	</Abstractive Summary>
</Paper ID=ument193>


<Paper ID=ument193> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: hits@1 results of transfer tests on the DIM
		model
	</Abstractive Summary>
</Paper ID=ument193>


<Paper ID=ument194> <Table ID =1>
	<Abstractive Summary> =
		So does he have a hump? A hump and a hairpiece?
		Table 1: Examples of DyKgChat corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows some examples from DyKgChat
	</Extractive Summary>
</Paper ID=ument194>


<Paper ID=ument194> <Table ID =2>
	<Abstractive Summary> =
		52
		Total unique tokens
		3,624
		19,762
		# KG entities
		174
		281
		# KG relation types
		9
		7
		total # KG entities appear
		46,059
		176,550
		# Dialogues w/ KG entities
		1,166
		2,373
		# turns w/ KG entities
		10,110
		9,199
		Table 2: The details of collected DyKgChat
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, the turns
		with knowledge graph entities are about 58
	</Extractive Summary>
</Paper ID=ument194>


<Paper ID=ument194> <Table ID =3>
	<Abstractive Summary> =
		Relation Type (Percentage)
		HGZHZ
		IsAliasOf (25%), IsChildOf (5%),
		IsLoverOf (6%), IsParentOf (5%),
		IsResidenceOf (16%), IsSiblingOf (2%),
		IsTitleOf (30%), IsEnemyOf (8%),
		IsServantOrMasterOf (3%)
		Friends
		IsLoverOf (12%), IsWorkplaceOf (2%),
		IsOccupationOf (8%), IsNameOf (47%),
		IsRelativeOf (8%), IsFriendOf (4%),
		IsNicknameOf (19%)
		Table 3: The included relation types in the collect
		knowledge graphs, and their percentages
	</Abstractive Summary>
	<Extractive Summary> =
		The relation types r ∈ L of each knowledge graph
		and their percentages are listed in Table 3, and
		the knowledge graph entities are plotted as word
		clouds in Figure 2
	</Extractive Summary>
</Paper ID=ument194>


<Paper ID=ument194> <Table ID =4>
	<Abstractive Summary> =
		68
		Table 4: The results of change rate and accurate change rate
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		As shown in Table 4, MemNet, TAware and
		KAware signiﬁcantly change when the knowl-
		edge graphs are largely updated (All) and can also
		achieve good accurate change rate
	</Extractive Summary>
</Paper ID=ument194>


<Paper ID=ument194> <Table ID =5>
	<Abstractive Summary> =
		56
		Table 5: The results of knowledge graph entities prediction
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results of the proposed met-
		rics for correctly predicting knowledge graph enti-
		ties
	</Extractive Summary>
	<Extractive Summary> =
		First, Qadpt wins Mem-
		Net and TAware less times than winning Seq2Seq
		and KAware, which aligns with Table 5 and Ta-
		ble 6
	</Extractive Summary>
</Paper ID=ument194>


<Paper ID=ument194> <Table ID =6>
	<Abstractive Summary> =
		024
		Table 6: The results of responses generation with BLEU, perplexity (PPL), distinct scores (1-gram to 4-gram)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 presents the BLEU-2 scores (as rec-
		ommended in the prior work (Liu et al
	</Extractive Summary>
</Paper ID=ument194>


<Paper ID=ument194> <Table ID =7>
	<Abstractive Summary> =
		68
		Table 7: The results of human evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		In
		most cases of Table 7, Qadpt outperforms other
		four models
	</Extractive Summary>
</Paper ID=ument194>


<Paper ID=ument195> <Table ID =1>
	<Abstractive Summary> =
		36
		Table 1: Human scores on response quality, depicted in three aspects: informativeness, relevance, and ﬂuency,
		with standard deviation in parentheses
	</Abstractive Summary>
</Paper ID=ument195>


<Paper ID=ument195> <Table ID =2>
	<Abstractive Summary> =
		02
		Table 2: Ablation study on the skeleton extractor
	</Abstractive Summary>
</Paper ID=ument195>


<Paper ID=ument195> <Table ID =3>
	<Abstractive Summary> =
		99
		Table 3: A systematic comparison of different compo-
		nent combinations, where C19’s SE and C19’s RG are
		short for Cai et al
	</Abstractive Summary>
</Paper ID=ument195>


<Paper ID=ument195> <Table ID =4>
	<Abstractive Summary> =
		14
		Table 4: Ablation study on the ranker
	</Abstractive Summary>
</Paper ID=ument195>


<Paper ID=ument195> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Example responses from different models, as well as a visualization of the skeleton extraction in our
		model
	</Abstractive Summary>
</Paper ID=ument195>


<Paper ID=ument196> <Table ID =1>
	<Abstractive Summary> =
		, 2019)
		O(n)
		Table 1: The Inference Time Complexity (ITC) of pre-
		vious DST models
	</Abstractive Summary>
</Paper ID=ument196>


<Paper ID=ument196> <Table ID =2>
	<Abstractive Summary> =
		18
		Number of Slots, n
		3
		35
		Number of Values, m
		99
		4510
		Training set size
		600
		8438
		Validation set size
		200
		1000
		Test set size
		400
		1000
		Table 2: The statistics of the WoZ2
	</Abstractive Summary>
</Paper ID=ument196>


<Paper ID=ument196> <Table ID =3>
	<Abstractive Summary> =
		72%
		O(1)
		Table 3: The joint goal accuracy of the DST models on the WoZ2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 compares our model with the previous
		state-of-the-art on both the WoZ2
	</Extractive Summary>
</Paper ID=ument196>


<Paper ID=ument196> <Table ID =4>
	<Abstractive Summary> =
		24%
		Table 4:
		The ablation study on the WoZ2
	</Abstractive Summary>
</Paper ID=ument196>


<Paper ID=ument196> <Table ID =5>
	<Abstractive Summary> =
		15%
		Table 5:
		The ablation study on the MultiWoZ
		dataset with the joint domain accuracy (JD Acc
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 5, We can further calcu-
		late that given the correct slot prediction COMER
		has 83
	</Extractive Summary>
</Paper ID=ument196>


<Paper ID=ument197> <Table ID =1>
	<Abstractive Summary> =
		469
		Table 1: Automatic evaluation results for the task of
		question response generation
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Evaluation Results
		Table 1 and Table 2 report the results of auto-
		matic evaluation on the two tasks
	</Extractive Summary>
</Paper ID=ument197>


<Paper ID=ument197> <Table ID =2>
	<Abstractive Summary> =
		603
		Table 2: Automatic evaluation results for the task of
		sentiment response generation
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Evaluation Results
		Table 1 and Table 2 report the results of auto-
		matic evaluation on the two tasks
	</Extractive Summary>
</Paper ID=ument197>


<Paper ID=ument197> <Table ID =3>
	<Abstractive Summary> =
		78
		Table 3: Human annotation results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results of human evaluation
	</Extractive Summary>
</Paper ID=ument197>


<Paper ID=ument197> <Table ID =4>
	<Abstractive Summary> =
		6 pounds
		Responses:
		[你]36
		[瘦 了]31
		[吗 ?]13
		[You]36
		[lost weight]31 [?]13
		[你 是 怎么]14
		[做到 的]42
		[?]26
		[How do you]14
		[make it]42
		[?]26
		[真的 吗 ?]48
		[我]36
		[不 信]32
		[Really ?]48
		[I]36
		[don’t believe it]32
		Table 4: Question response generation with various
		templates
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Case Study
		To further understand how S2S-Temp leverages
		templates for response generation, we show two
		examples with the test data, one for question re-
		sponse generation in Table 4 and the other for sen-
		timent response generation in Table 5, where sub-
		scripts refer to states of the NHSMMs
	</Extractive Summary>
</Paper ID=ument197>


<Paper ID=ument197> <Table ID =5>
	<Abstractive Summary> =
		]30
		[i ’m]23
		[so pumped]14
		[to watch it]31
		[yeah ,]47 [i was watching]48
		Table 5: Sentiment response generation with various
		templates
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Case Study
		To further understand how S2S-Temp leverages
		templates for response generation, we show two
		examples with the test data, one for question re-
		sponse generation in Table 4 and the other for sen-
		timent response generation in Table 5, where sub-
		scripts refer to states of the NHSMMs
	</Extractive Summary>
</Paper ID=ument197>


<Paper ID=ument198> <Table ID =1>
	<Abstractive Summary> =
		34
		73%
		30%
		Table 1: The automatic and human evaluation results of all compared methods
	</Abstractive Summary>
	<Extractive Summary> =
		For the human evaluation results on the right-
		hand side of Table 1, we show the mean and stan-
		dard deviation of all test results as well as the per-
		centage of acceptable responses (2 or 3 points) and
		good responses (3 points only)
	</Extractive Summary>
</Paper ID=ument198>


<Paper ID=ument199> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example of the multi-party conversation in
		the IRC dataset
	</Abstractive Summary>
</Paper ID=ument199>


<Paper ID=ument199> <Table ID =2>
	<Abstractive Summary> =
		We ﬁrst fuse the speaker em-
		bedding and the utterance embedding into a query
		representation as q, then measures the embedding
		similarity s between the query and each listener:
		q = tanh(WsaSPR + Wu˜u)
		s = σ(aLSRWmqT)
		(9)
		1914
		Table 2: Data statistics: sample size of datasets with
		different session lengths (i
	</Abstractive Summary>
</Paper ID=ument199>


<Paper ID=ument199> <Table ID =3>
	<Abstractive Summary> =
		For fair com-
		parison, we choose the hyper-parameters spec-
		1915
		Table 3: Addressee identiﬁcation results (in %) on datasets (Len-5, Len-10 and Len-15) where ⋆ denotes p-value
		< 0
	</Abstractive Summary>
	<Extractive Summary> =
		For automatic evalua-
		tion shown in Table 3, end-to-end deep learning
		approaches outperform heuristic ones, which in-
		dicates that simple intuition is far from satisfac-
		tion and further conﬁrm the value of this work
	</Extractive Summary>
</Paper ID=ument199>


<Paper ID=ument199> <Table ID =4>
	<Abstractive Summary> =
		23⋆
		Table 4: Consistency comparison between human in-
		ference and model predictions on overlapping rate (%)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the consistency between human
		inference and model predictions
	</Extractive Summary>
</Paper ID=ument199>


<Paper ID=ument199> <Table ID =5>
	<Abstractive Summary> =
		Symmetri-
		cally, we ﬁx the user representations in the session
		by removing the SGRU and LGRU cell and main-
		tain only the interaction affect from the users to
		Table 5: Ablation test on the effectiveness of W2W
		model parts in dataset Len-15
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results of
		ablation test
	</Extractive Summary>
	<Extractive Summary> =
		To investigate the effectiveness of interactive
		representation learning module, we ﬁrst remove
		the UGRU cell and ﬁx the utterance representa-
		tions in the state tracking procedure (referred as
		w/o Utterance Interaction in Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		89
		the utterances (referred as w/o User Interaction in
		Table 5)
	</Extractive Summary>
</Paper ID=ument199>


<Paper ID=ument199> <Table ID =6>
	<Abstractive Summary> =
		Considering that
		unlabeled sets consist of NN ones as well as NP
		ones on which W2W has much larger variance
		Table 6: Variance of matching scores on labeled and
		unlabeled utterances in the set Len-15
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 demonstrates
		the variance comparison on labeled and unlabeled
		cases in the test set Len-15
	</Extractive Summary>
</Paper ID=ument199>


<Paper ID=ument2> <Table ID =1>
	<Abstractive Summary> =
		Length
		Train Size
		Test Size
		(tokens)
		(neg/pos)
		(neg/pos)
		Diabetes
		1858
		6381/1353
		1295/319
		Anemia
		2188
		1847/3251
		460/802
		IMDb
		179
		12500/12500
		2184/2172
		SST
		19
		3034/3321
		863/862
		AgNews
		36
		30000/30000
		1900/1900
		20News
		115
		716/710
		151/183
		Table 1: Dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		This can be explained via the data distribution, which
		is heavily skewed towards positive examples (see Table 1 in
		the Appendix), together with the fact (conceded in Jain and
		Wallace (2019)’s section 4
	</Extractive Summary>
</Paper ID=ument2>


<Paper ID=ument2> <Table ID =2>
	<Abstractive Summary> =
		934
		Table 2: Classiﬁcation F1 scores (1-class) on attention
		models, both as reported by Jain and Wallace and in
		our reproduction, and on models forced to use uniform
		attention over hidden states
	</Abstractive Summary>
	<Extractive Summary> =
		All F1 scores are on par with the
		original model results reported in Table 2, indicat-
		ing the effectiveness of our adversarial models at
		imitating base model scores on the test sets
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 illustrates the dif-
		ference between inconsistently-achieved adversar-
		ial heatmaps and consistently trained ones
	</Extractive Summary>
</Paper ID=ument2>


<Paper ID=ument2> <Table ID =3>
	<Abstractive Summary> =
		700
		Table 3: F1 scores on the positive class for an MLP
		model trained on various weighting guides
	</Abstractive Summary>
	<Extractive Summary> =
		The results, reported in
		the bottom line of Table 3, show that despite
		Figure 5: Averaged per-instance test set JSD and TVD
		from base model for each model variant
	</Extractive Summary>
</Paper ID=ument2>


<Paper ID=ument2> <Table ID =4>
	<Abstractive Summary> =
		405
		Table 4:
		Best-performing adversarial models with
		instance-average JSD > 0
	</Abstractive Summary>
	<Extractive Summary> =
		4 in JSD, as
		well as their � setting and corresponding compar-
		ison metrics, in Table 4 (full results available in
		Appendix B)
	</Extractive Summary>
</Paper ID=ument2>


<Paper ID=ument20> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Statistics on forests generated with various �
		(upper half) and K (lower half) on the development set
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Analyses of generated forests
		Table 1 demonstrates several characteristics of
		the generated forests of both the EDGEWISE and
		KBESTEISNER algorithms in Section 5
	</Extractive Summary>
</Paper ID=ument20>


<Paper ID=ument20> <Table ID =2>
	<Abstractive Summary> =
		4**
		Table 2: Test results of Biocreative VI CPR
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Main results on BioCreative VI CPR
		Table 2 shows the main comparison results on
		the BioCreative CPR testset, with comparisons
		to the previous state-of-the-art and our baselines
	</Extractive Summary>
</Paper ID=ument20>


<Paper ID=ument20> <Table ID =3>
	<Abstractive Summary> =
		7**
		Table 3: Main results on PGR testest
	</Abstractive Summary>
	<Extractive Summary> =
		8
		Main results on PGR
		Table 3 shows the comparison with previous work
		on the PGR testset, where our models are sig-
		niﬁcantly better than the existing models
	</Extractive Summary>
</Paper ID=ument20>


<Paper ID=ument20> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Main results on SemEval-2010 task 8 testest
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, we conduct a prelim-
		inary study on SemEval-2010 task 8 (Hendrickx
		et al
	</Extractive Summary>
</Paper ID=ument20>


<Paper ID=ument200> <Table ID =1>
	<Abstractive Summary> =
		12%
		Table 1: Statistical information including number of dialogs in training, validation and test sets, average number
		of utterances (turns) per dialog, words per utterance, vocabulary size and its proportion in corpus
	</Abstractive Summary>
	<Extractive Summary> =
		The reason is that
		Ubuntu dataset has more words averagely per ut-
		terance than Cornell data (as the statistical details
		shown in Table 1), which forces the models to pro-
		duce longer responses
	</Extractive Summary>
</Paper ID=ument200>


<Paper ID=ument200> <Table ID =2>
	<Abstractive Summary> =
		1955
		Table 2: Automatic evaluation results on two dialogue datasets
	</Abstractive Summary>
</Paper ID=ument200>


<Paper ID=ument200> <Table ID =3>
	<Abstractive Summary> =
		2450
		Table 3: Performances of model ablation on Cornell dataset
	</Abstractive Summary>
	<Extractive Summary> =
		From
		the results listed in Table 3, we can observe that:
		(1) Removing the extractor (denoted as SVN)
		makes the distinct ratios and numbers drop
		dramatically, while the embedding-based met-
		ric scores are only slightly lower than that of
		SSVN
	</Extractive Summary>
</Paper ID=ument200>


<Paper ID=ument200> <Table ID =4>
	<Abstractive Summary> =
		45
		Table 4: Human evaluation results on Cornell dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Human Evaluation The human evaluation results
		on Cornell data are shown in Table 4, in which the
		score distribution values represent the percentages
		of responses belonging to each grade, and Fleiss’
		kappa (Fleiss and Cohen, 1973) is employed to
		evaluate the inter-annotator agreement
	</Extractive Summary>
</Paper ID=ument200>


<Paper ID=ument200> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Generated responses of all the models on Cornell Movie Dialogs Corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Case Study Besides the quantitative analysis, we
		also organize some examples (seen in Table 5)
		from different models to analyze the performances
		of the methods qualitatively
	</Extractive Summary>
</Paper ID=ument200>


<Paper ID=ument201> <Table ID =1>
	<Abstractive Summary> =
		com/
		Readability
		Is the response grammatically formed and smooth ?
		Informativeness
		Does the response contains informative words ?
		Personalization
		Does the response resembles with any user history?
		Table 1: Criteria of human evaluation
	</Abstractive Summary>
</Paper ID=ument201>


<Paper ID=ument201> <Table ID =2>
	<Abstractive Summary> =
		86
		Table 2: The results of both automatic evaluations and human evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents the
		results of automatic and human evaluation
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2,
		Persona model only achieves comparable results
		with Seq2Seq in terms of BLEU scores and BOW
		scores
	</Extractive Summary>
</Paper ID=ument201>


<Paper ID=ument201> <Table ID =3>
	<Abstractive Summary> =
		2480
		Table 3: The results of different settings of k in per-
		sonalization GMD, where k denotes number of distri-
		butions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the ablation
		results of the inﬂuence of k value in personaliza-
		1938
		tion Gaussian mixture distribution
	</Extractive Summary>
</Paper ID=ument201>


<Paper ID=ument201> <Table ID =4>
	<Abstractive Summary> =
		2244
		Table 4: The results of Ablation Experiments
	</Abstractive Summary>
	<Extractive Summary> =
		Besides, human evaluation re-
		sults in Table 4 further illustrate that DiaWAE-
		GMD fails to model personalization of different
		users insomuch as DiaWAE-GMD lacks user-level
		information learning
	</Extractive Summary>
</Paper ID=ument201>


<Paper ID=ument201> <Table ID =5>
	<Abstractive Summary> =
		)
		Table 5: Responses generated by baselines and our
		model
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Case Study
		Table 5 illustrates the generated response of dif-
		ferent models for a given context
	</Extractive Summary>
</Paper ID=ument201>


<Paper ID=ument201> <Table ID =6>
	<Abstractive Summary> =
		)
		Table 6: Responses generated by our model for four
		users
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows a few example responses generated
		by altering the user personalization information
	</Extractive Summary>
</Paper ID=ument201>


<Paper ID=ument202> <Table ID =1>
	<Abstractive Summary> =
		,
		Users
		Dyads
		Conv’s
		Utterances
		27,152
		107,611
		770,739
		6,109,469
		Table 1: Basic statistics of Twitter conversations corpus
		2017a; Park et al
	</Abstractive Summary>
</Paper ID=ument202>


<Paper ID=ument202> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Perplexity per word of the generated re-
		sponses
	</Abstractive Summary>
</Paper ID=ument202>


<Paper ID=ument202> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Comparison of various models using BLEU, Embedding, and ROUGE-L scores, which measure the
		quality of the generated responses with respect to the ground truth
		We then compute the following metrics, with
		the results in Table 3:
		• BLEU (Papineni et al
	</Abstractive Summary>
	<Extractive Summary> =
		Results As shown in Table 3, VHUCM-PUE
		outperforms the other methods on most metrics
	</Extractive Summary>
</Paper ID=ument202>


<Paper ID=ument202> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: Human evaluation of the appropriateness of
		the generated response
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the re-
		sults, in which VHUCM-PUE competes with three
		baselines and outperforms all of them
	</Extractive Summary>
</Paper ID=ument202>


<Paper ID=ument202> <Table ID =5>
	<Abstractive Summary> =
		how was your exam ? x → think it
		was okay actually :) x → thats good
		then :) x
		you !
		:d :d :d :d :d :d
		:d i miss you too
		<3 <3 <3
		hahah ! ! ! im so sorry
		to tell you when I get
		home or something lol
		you are still the best
		person who’s there ?
		x
		Table 5: Examples of generated responses from SpeakAddr, VHCR, DialogWAE, and VHUCM-PUE
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows an example of the generated
		responses from SpeakAddr, VHCR, DialogWAE,
		and VHUCM-PUE for the same three-turn con-
		text
	</Extractive Summary>
</Paper ID=ument202>


<Paper ID=ument202> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Responses to users’ personal information questions from VHUCM-PUE
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows
		examples of these questions and responses
	</Extractive Summary>
</Paper ID=ument202>


<Paper ID=ument202> <Table ID =7>
	<Abstractive Summary> =
		001
		Table 7: Distance between users in the user embed-
		ding spaces from VHUCM or VHUCM-PUE
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the results
	</Extractive Summary>
</Paper ID=ument202>


<Paper ID=ument203> <Table ID =1>
	<Abstractive Summary> =
		4
		engagingness scores & feedback collected
		18,308
		Table 1:
		Data statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows detailed statistics of our dataset
		regarding the movie sets, the annotated dialogues,
		actions made by expert and seeker, dialogue
		1954
		Dialogue statistics
		Number of dialogues
		9,125
		Number of utterances
		170,904
		Number of unique utterances
		85,208
		Avg length of a dialogue
		23
	</Extractive Summary>
</Paper ID=ument203>


<Paper ID=ument203> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: Evaluation on supervised models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows performance compari-
		son on the test set
	</Extractive Summary>
</Paper ID=ument203>


<Paper ID=ument203> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3:
		Evaluation on dialogue recommendation
		games:
		bot-bot (top three rows) and {bot,human}-
		human (bottom three rows)
	</Abstractive Summary>
</Paper ID=ument203>


<Paper ID=ument204> <Table ID =1>
	<Abstractive Summary> =
		34
		Total # unique tokens
		986
		2,142
		2,842
		24,071
		7,502
		# databases
		1
		1
		1
		7
		140
		# Slots #
		8
		4
		13
		25
		3,696
		# Values #
		212
		99
		1,363
		4,510
		>1,000,000
		Table 1: Comparison of CoSQL to some commonly used task-oriented dialogue datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Data statistics
		Table 1 and 2 summarize the
		statistics of CoSQL
	</Extractive Summary>
	<Extractive Summary> =
		Semantic complexity
		As shown in Table 1, if
		we consider the column names of the 200 DBs
		of CoSQL as slots and their entries as values, the
		number of slot-value pairs far exceed those de-
		ﬁned in other task-oriented dialogues
	</Extractive Summary>
</Paper ID=ument204>


<Paper ID=ument204> <Table ID =2>
	<Abstractive Summary> =
		0
		Unanswerable Q
		✓
		
		
		User intent
		✓
		
		
		System response
		✓
		
		
		Table 2: Comparison of CoSQL with other context-
		dependent text-to-SQL datasets
	</Abstractive Summary>
</Paper ID=ument204>


<Paper ID=ument204> <Table ID =3>
	<Abstractive Summary> =
		Train
		Dev
		Test
		# Dialogs
		2164
		292
		551
		# Databases
		140
		20
		40
		Table 3: Dataset Split Statistics
		5
		Tasks and Models
		CoSQL is meant to be used as the ﬁrst bench-
		mark for building general-purpose DB querying
		dialogue systems in arbitrary domains
	</Abstractive Summary>
	<Extractive Summary> =
		Cross domain
		As shown in Table 3, the dia-
		logues in CoSQL are randomly split into train, de-
		velopment and test sets by DB with a ratio of 7:1:2
		(the same split as SParC and Spider)
	</Extractive Summary>
</Paper ID=ument204>


<Paper ID=ument204> <Table ID =4>
	<Abstractive Summary> =
		1970
		Groups
		Dialog acts
		DB user
		inform sql,
		infer sql,
		ambiguous,
		affirm,
		negate,
		not related,
		cannot understand,
		cannot answer,
		greeting,
		goodbye,
		thank you
		DB expert
		conform sql, clarify, reject, request more, greeting, sorry, welcome, goodbye
		Table 4: Dialog acts in CoSQL
	</Abstractive Summary>
	<Extractive Summary> =
		For each dia-
		logue turn, the expert ﬁrst checks the user ques-
		tion and labels it using a set of pre-deﬁned user
		dialog action types (DATs, see Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		We deﬁne a series of dialogue acts for the DB user
		and the SQL expert (Table 4)
	</Extractive Summary>
</Paper ID=ument204>


<Paper ID=ument204> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Performance of various methods over all ques-
		tions (question match) and all interactions (interaction
		match)
	</Abstractive Summary>
</Paper ID=ument204>


<Paper ID=ument204> <Table ID =6>
	<Abstractive Summary> =
		6
		Table 6: BLEU scores on the development and test
		sets, and human evaluations of logic correctness rate
		(LCR) and grammar check on the 100 examples ran-
		domly sampled from the test set
	</Abstractive Summary>
	<Extractive Summary> =
		Response generation
		Table 6 shows the re-
		sults of three different baselines on three metrics:
		BLEU score (Papineni et al
	</Extractive Summary>
</Paper ID=ument204>


<Paper ID=ument204> <Table ID =7>
	<Abstractive Summary> =
		9
		Table 7: Accuracy of user dialog act prediction on the
		development and test sets
	</Abstractive Summary>
	<Extractive Summary> =
		User dialog act prediction
		Table 7 shows the
		accuracy of the two baselines on predicting user
		dialog acts
	</Extractive Summary>
</Paper ID=ument204>


<Paper ID=ument205> <Table ID =1>
	<Abstractive Summary> =
		738
		100
		Table 1: Results for DailyDialog and SwDA Datasets, PDA is for Predicted Dialogue Act, ADA is for Actual
		Dialogue Act, ST is Single-Task Learning, and MT is for Multi-Task Learning
		this data preparation exercise, the total number of
		conversations in train, test, and validation for Dai-
		lyDialog are 61030, 2849, and 2695, respectively
	</Abstractive Summary>
</Paper ID=ument205>


<Paper ID=ument205> <Table ID =2>
	<Abstractive Summary> =
		719
		Table 2: Comparison of MRR when using dialogue acts of Context-only, Response-only and Crossway fashion
		Multi-Task vs Single-Task Modelling:
		In Ta-
		ble 1, we report and compare the results of our
		proposed method with the baselines, for both
		datasets, i
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we report the
		MRR numbers of several models that use the dia-
		logue acts in different settings
	</Extractive Summary>
	<Extractive Summary> =
		Results in Table 2
		indicate that the Crossway always outperforms
		the Context-DA or the Response-DA, for both
		datasets
	</Extractive Summary>
</Paper ID=ument205>


<Paper ID=ument206> <Table ID =1>
	<Abstractive Summary> =
		64
		Table 1: Automatic metrics and human evaluation scores of different user simulators
	</Abstractive Summary>
	<Extractive Summary> =
		SL-End2End
		in Table 1 trains the NLU, DM and NLG jointly,
		and therefore, the vocabulary size is slightly larger
		than the template-based methods
	</Extractive Summary>
	<Extractive Summary> =
		The
		positive correlation between the “Solved Ratio”
		and “Satisfaction” in Table 1 also indicates auto-
		matic task completion rate is a good estimator for
		user satisfaction
	</Extractive Summary>
</Paper ID=ument206>


<Paper ID=ument206> <Table ID =2>
	<Abstractive Summary> =
		03
		Table 2: Human evaluation of RL systems trained with different simulators on AMT with 95% conﬁdence intervals
	</Abstractive Summary>
	<Extractive Summary> =
		We also listed two common automatic metrics
		in Table 2 to compare
	</Extractive Summary>
	<Extractive Summary> =
		However, according to Table 2, we ﬁnd that the
		naturalness score seems to correlate with the over-
		all system performance
	</Extractive Summary>
	<Extractive Summary> =
		4
		Cross Study of Simulators and Systems
		From the last column in Table 2, we ﬁnd that al-
		though the automatic success rates claimed by the
		user simulator used to train the system are all rela-
		tively high, the high automatic success rate doesn’t
		transfer to real human satisfaction
	</Extractive Summary>
	<Extractive Summary> =
		The diagonal should reﬂect the “Auto Success”
		column in Table 2, but since the 200 episodes are
		random and the “Auto Success” is the convergent
		success rate, the exact number won’t be the same
	</Extractive Summary>
</Paper ID=ument206>


<Paper ID=ument206> <Table ID =3>
	<Abstractive Summary> =
		461
		Table 3: Cross study results
	</Abstractive Summary>
	<Extractive Summary> =
		The last row in Table 3 shows the average
		success rate of each system across user simula-
		tors
	</Extractive Summary>
</Paper ID=ument206>


<Paper ID=ument207> <Table ID =1>
	<Abstractive Summary> =
		2006
		Table 1: Evaluation results of our proposed models
		Method
		MSVD
		MSR-VTT
		BLEU4 ROUGE METEOR CIDEr BLEU4 ROUGE METEOR CIDEr
		HOCA-U
		50
	</Abstractive Summary>
</Paper ID=ument207>


<Paper ID=ument207> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Evaluation results of video captioning, where
		B, R, M, C denote BLEU4, ROUGE, METEOR,
		CIDEr, respectively, and Ours denotes L-HOCA-UBT
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Impact of Cross-Modal Attention
		Table 2 shows the results of different variants of
		HOCA and Low-Rank HOCA
	</Extractive Summary>
	<Extractive Summary> =
		2
		Compared with the State-of-the-art
		Table 2 shows the results of different meth-
		ods on MSR-VTT and MSVD, including ours (L-
		HOCA-UBT), and some state-of-the-art methods,
		such as LSTM-TSA (Pan et al
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2, we ﬁnd that Ours(L-HOCA-
		UBT) shows competitive performances compared
		with the state-of-the-art methods
	</Extractive Summary>
</Paper ID=ument207>


<Paper ID=ument207> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Computing cost of different methods, where
		the “space” denotes the memory space requirement and
		the “training time” denotes the total time for training
	</Abstractive Summary>
</Paper ID=ument207>


<Paper ID=ument208> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1: Captioning performance comparison on MS
		COCO testset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the re-
		sults on the scarcely-paired COCO dataset
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, in a scarce data regime,
		utilizing the unpaired data improves the captioning
		performance in terms of all metrics by noticeable
		margins
	</Extractive Summary>
</Paper ID=ument208>


<Paper ID=ument208> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Comparison with the semi-supervised image captioning method, “Self-Retrieval” [Liu et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the comparison with Self-
		Retrieval
	</Extractive Summary>
</Paper ID=ument208>


<Paper ID=ument208> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Evaluation of our method with different backbone architectures
	</Abstractive Summary>
	<Extractive Summary> =
		, 2017] in Table 3, we use vi-
		sual features with size 2048 × 7 × 7
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the comparison
	</Extractive Summary>
</Paper ID=ument208>


<Paper ID=ument208> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Performance comparison with web-crawled
		data (Shutterstock)
	</Abstractive Summary>
	<Extractive Summary> =
		The results are shown in
		Table 4 with the performance obtained by Feng et
		1https://www
	</Extractive Summary>
</Paper ID=ument208>


<Paper ID=ument209> <Table ID =1>
	<Abstractive Summary> =
		04
		Table 1: Retrieval performance on VisDial v1
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 1, DAN signiﬁcantly outperforms
		all other approaches on NDCG, MRR, and R@1,
		including the previous state-of-the-art method,
		Synergistic (Guo et al
	</Extractive Summary>
</Paper ID=ument209>


<Paper ID=ument209> <Table ID =2>
	<Abstractive Summary> =
		92
		Table 2: Test-std performance of ensemble model on
		VisDial v1
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, our model sig-
		niﬁcantly outperforms all three challenge entries,
		including the challenge winner model, Synergis-
		tic (Guo et al
	</Extractive Summary>
</Paper ID=ument209>


<Paper ID=ument209> <Table ID =3>
	<Abstractive Summary> =
		66
		Table 3: VisDial v1
	</Abstractive Summary>
	<Extractive Summary> =
		From the Table 3, we
		draw three observations: (1) DAN shows signif-
		icantly better results than the No REFER model
		for SC questions
	</Extractive Summary>
</Paper ID=ument209>


<Paper ID=ument209> <Table ID =4>
	<Abstractive Summary> =
		88
		Table 4: Ablation studies on VisDial v1
	</Abstractive Summary>
	<Extractive Summary> =
		The ﬁrst four rows in Table 4
		show the performance of a single module
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4, RPN denotes the use of the re-
		gion proposal networks (Ren et al
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 4, the use of the resid-
		ual connection (i
	</Extractive Summary>
</Paper ID=ument209>


<Paper ID=ument21> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Precision, recall and F1 results (%) for differ-
		ent models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the experimental results, from which we can ob-
		serve that:
		(1) RSN models outperform all baseline models
		on precision, recall, and F1-score, among which
		Weakly-supervised RSN (SN-L+CV) achieves
		state-of-the-art performances
	</Extractive Summary>
</Paper ID=ument21>


<Paper ID=ument210> <Table ID =1>
	<Abstractive Summary> =
		0001 for
		train/val/test
		ni/mi
		# imgs density
		(median) (unique)
		MSCOCO 25K/2K/2K
		10/10
		83K
		5%
		Story-DII
		22K/3K/3K
		5/5
		47K
		20%
		Story-SIS
		37K/5K/5K
		5/5
		76K
		20%
		DII-Stress
		22K/3K/3K
		50/5
		47K
		2%
		DIY
		7K/1K/1K
		15/16
		154K
		8%
		RQA
		7K/1K/1K
		6/8
		88K
		17%
		WIKI
		14K/1K/1K
		86/5
		92K
		N/A
		Table 1:
		Dataset statistics:
		top half = crowdla-
		beled datasets; bottom half = organically-multimodal
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		8 Statistics of these
		datasets are given in the top half of Table 1, and
		example documents are given in Figure 2
	</Extractive Summary>
</Paper ID=ument210>


<Paper ID=ument210> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Results for crowdlabeled datasets (similar results for other settings
		are included in the supplementary material)
	</Abstractive Summary>
	<Extractive Summary> =
		10
		Table 2 shows test-set prediction results for
		b = 10 (results for b ∈ {20, 30} are similar)
	</Extractive Summary>
</Paper ID=ument210>


<Paper ID=ument210> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Performance on the organically-multimodal
		data; values within 1% of best-in-column are bolded
	</Abstractive Summary>
	<Extractive Summary> =
		We adopt the same experimental protocols as
		in §4, but increase the maximum sentence token-
		length from 20 to 50; Table 3 shows the test-set
		results
	</Extractive Summary>
</Paper ID=ument210>


<Paper ID=ument211> <Table ID =1>
	<Abstractive Summary> =
		2
		Background
		The dataset and experiments in this paper are con-
		nected to the following areas:
		Humor Analysis:
		Humor analysis has been
		Dataset
		#Pos
		#Neg
		Mod
		type
		#spk
		16000 One-Liners 16000 16000
		{t}
		joke
		-
		Pun of the Day
		2423
		2423
		{t}
		pun
		-
		PTT Jokes
		1425
		2551
		{t}
		political
		-
		Ted Laughter
		4726
		4726
		{t}
		speech
		1192
		Big Bang Theory
		18691 24981
		{t,a}
		tv show
		<50
		UR-FUNNY
		8257
		8257
		{t,a,v}
		speech
		1741
		Table 1: Comparison between UR-FUNNY and no-
		table humor detection datasets in the NLP community
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows a comparison between previously
		proposed datasets and UR-FUNNY dataset
	</Extractive Summary>
</Paper ID=ument211>


<Paper ID=ument211> <Table ID =2>
	<Abstractive Summary> =
		52
		Table 2: Summary of the UR-FUNNY dataset statis-
		tics
	</Abstractive Summary>
</Paper ID=ument211>


<Paper ID=ument211> <Table ID =3>
	<Abstractive Summary> =
		54
		Table 3: Statistics of train, validation & test folds of
		UR-FUNNY dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the standard train, validation and
		test folds of the UR-FUNNY dataset
	</Extractive Summary>
</Paper ID=ument211>


<Paper ID=ument211> <Table ID =4>
	<Abstractive Summary> =
		23
		Table 4: Binary accuracy for different variants of C-
		MFN and training scenarios outlined in Section 5
	</Abstractive Summary>
	<Extractive Summary> =
		The results from Table 4 demonstrate that while
		a state-of-the-art model can achieve a reason-
		able level of success in modeling humor, there is
		still a large gap between human-level performance
		with state of the art
	</Extractive Summary>
</Paper ID=ument211>


<Paper ID=ument212> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Precision (pr), recall (re) and F1 scores for
		detecting the minority class (perpetrator mentioned)
		on the held-out dataset
	</Abstractive Summary>
</Paper ID=ument212>


<Paper ID=ument212> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Ablation experiment assessing the contribu-
		tion of each modality for our multi-view model
	</Abstractive Summary>
</Paper ID=ument212>


<Paper ID=ument212> <Table ID =3>
	<Abstractive Summary> =
		88
		Table 3: Comparing the performance of early fusion
		(EF) and multi-view (MV) models with attentive early
		fusion models
	</Abstractive Summary>
</Paper ID=ument212>


<Paper ID=ument212> <Table ID =4>
	<Abstractive Summary> =
		72
		Table 4: Macro-average scores for accuracy (acc), precision (pr), recall (re) and F1 scores for early fusion (EF)
		and multi-view (MV) models on speaker type and case tagging
	</Abstractive Summary>
	<Extractive Summary> =
		The results for speaker type and case tagging
		can be found in Table 4, where our model (MV) is
		compared with an LSTM early fusion model
	</Extractive Summary>
</Paper ID=ument212>


<Paper ID=ument213> <Table ID =1>
	<Abstractive Summary> =
		(11)
		2073
		Table 1: Ablation studies of our proposed video captioning model on Youtube2Text and MSR-VTT datasets
	</Abstractive Summary>
</Paper ID=ument213>


<Paper ID=ument213> <Table ID =2>
	<Abstractive Summary> =
		, 2017), CIDEnt-RL
		2074
		Table 2: Performance compared with state-of-the-art
		methods on Youtube2Text dataset
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Quantitative Analysis
		In Table 2 and Table 3,
		we compare our
		ASGN+LNA model with the state-of-the-art mod-
		els on the Youtube2Text and MSR-VTT datasets
	</Extractive Summary>
</Paper ID=ument213>


<Paper ID=ument213> <Table ID =3>
	<Abstractive Summary> =
		813
		Table 3: Performance compared with state-of-the-art
		methods on MSR-VTT dataset
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Quantitative Analysis
		In Table 2 and Table 3,
		we compare our
		ASGN+LNA model with the state-of-the-art mod-
		els on the Youtube2Text and MSR-VTT datasets
	</Extractive Summary>
</Paper ID=ument213>


<Paper ID=ument213> <Table ID =4>
	<Abstractive Summary> =
		Table 4:
		Human evaluation between ASGN and
		ASGN+L models
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, the results of 150 samples from the
		Youtube2Text test set are studied
	</Extractive Summary>
</Paper ID=ument213>


<Paper ID=ument213> <Table ID =5>
	<Abstractive Summary> =
		253
		Table 5: The results under different supervisions of
		POS tags on Youtube2Text dataset
	</Abstractive Summary>
</Paper ID=ument213>


<Paper ID=ument214> <Table ID =1>
	<Abstractive Summary> =
		Sentence
		watch
		action
		movie
		Gold Slots
		O
		B-movie name
		I-movie name
		Gold Intent
		WatchMovie
		Table 1: An example with intent and slot annotation
		(BIO format), which indicates the slot of movie name
		from an utterance with an intent WatchMovie
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 1, given a movie-related utterance “watch
		action movie”, there are different slot labels for
		each token and an intent for the whole utterance
	</Extractive Summary>
</Paper ID=ument214>


<Paper ID=ument214> <Table ID =2>
	<Abstractive Summary> =
		0
		-
		-
		Table 2: Slot ﬁlling and intent detection results on two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the experiment re-
		sults of the proposed models on SNIPS and ATIS
		datasets
	</Extractive Summary>
</Paper ID=ument214>


<Paper ID=ument214> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: The SLU performance on baseline models compared with our Stack-Propagation model on two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 gives the result of the comparison ex-
		periment
	</Extractive Summary>
</Paper ID=ument214>


<Paper ID=ument214> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: The SLU performance on BERT-based model on two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 gives the results of BERT model on
		ATIS and SNIPS datasets
	</Extractive Summary>
</Paper ID=ument214>


<Paper ID=ument215> <Table ID =1>
	<Abstractive Summary> =
		01
		Table 1: Statistics of and comparison with existing datasets for object referral
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Quantitative Evaluation of Talk2Car
		Table 1 compares the Talk2Car dataset with
		prior object referral datasets
	</Extractive Summary>
</Paper ID=ument215>


<Paper ID=ument215> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Performance (IoU0
	</Abstractive Summary>
</Paper ID=ument215>


<Paper ID=ument216> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Accuracy and Average Precision for individual feature types, calculated using 10-fold cross-validation
		using the Snopes dataset (S), and the Snopes+Reuters dataset (S+R)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 illustrates the importance of each fea-
		ture type in isolation
	</Extractive Summary>
	<Extractive Summary> =
		Note that these top fea-
		tures for the two experiments are different: we use
		the scores in the AP(S) column in Table 1 for the
		Snopes dataset, and the AP(S+R) column for the
		Snopes+Reuters dataset
	</Extractive Summary>
	<Extractive Summary> =
		The best-performing features across the exper-
		iments differ, but as Table 1 shows, the URL do-
		mains are top-1 in three out of four experiments,
		and claim text is top-2 in two out of four experi-
		ments
	</Extractive Summary>
</Paper ID=ument216>


<Paper ID=ument216> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Accuracy and Average Precision on the New
		test dataset
	</Abstractive Summary>
</Paper ID=ument216>


<Paper ID=ument217> <Table ID =1>
	<Abstractive Summary> =
		, 2017a) by
		2115
		Table 1: Experimental results of answer generation on TACoS-MultiLevel and YoutubeClip datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the experimental results of
		answer generation on TACoS-MultiLevel and
		YoutubeClip datasets, and Table 2 shows the ques-
		tion generation results on same datasets
	</Extractive Summary>
</Paper ID=ument217>


<Paper ID=ument217> <Table ID =2>
	<Abstractive Summary> =
		104
		Table 2: Experimental results of question generation on TACoS-MultiLevel and YoutubeClip datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the experimental results of
		answer generation on TACoS-MultiLevel and
		YoutubeClip datasets, and Table 2 shows the ques-
		tion generation results on same datasets
	</Extractive Summary>
</Paper ID=ument217>


<Paper ID=ument217> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Ablation study results on TACoS-MultiLevel
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		The
		experimental results are listed in Table 3, where
		RICT(lstm), RICT(wo
	</Extractive Summary>
</Paper ID=ument217>


<Paper ID=ument218> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Development and test results on all systems, including ablation results
	</Abstractive Summary>
	<Extractive Summary> =
		8
		Results
		Table 1 shows development and test results, in-
		cluding ablations
	</Extractive Summary>
</Paper ID=ument218>


<Paper ID=ument219> <Table ID =1>
	<Abstractive Summary> =
		, bm] is a list of bounding boxes
		referring to regions of I, where each bi is
		Symbol
		Type
		Description
		m
		N
		number of extracted bounding boxes
		n
		N
		number of tokens input to BERT
		k
		N
		number of positional embeddings for im-
		age coordinates, usually 56
		d
		N
		visual features dimension, usually 2048
		h
		N
		hidden dimension of BERT, usually 1024
		l
		{0, 1}
		a binary label
		I
		R·×·×3
		an image
		B
		Rm×4
		rectangular bounding boxes on I, as co-
		ordinates of opposite corners
		R
		{0, 1}m×n
		matrix encoding which bounding boxes
		in B correspond to which tokens in T
		T
		Nn×2
		input tokens, each expressed as word
		piece id and token type
		Φ
		R·×·×3 → Rd
		a function to extract visual feature vec-
		tors from an image
		π
		R4 → Rd
		a function to embed the position and
		shape of a bounding box
		Ψ
		Rn×h → Rh
		a function to compute a passage embed-
		ding from per-token embeddings
		E
		Nn×2 → Rn×h non-contextualized token embeddings,
		encoding word piece ids, token types and
		positions
		Table 1: Glossary of mathematical symbols used in this
		work
	</Abstractive Summary>
	<Extractive Summary> =
		We refer the reader to Table 1 for an overview of
		the notation used in this work
	</Extractive Summary>
</Paper ID=ument219>


<Paper ID=ument219> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Experimental results on VCR, incorporating
		those reported by Zellers et al
	</Abstractive Summary>
</Paper ID=ument219>


<Paper ID=ument219> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: Ablations for B2T2 on VCR dev
	</Abstractive Summary>
</Paper ID=ument219>


<Paper ID=ument219> <Table ID =4>
	<Abstractive Summary> =
		(text-only)
		Table 4: Examples of the Q → A task from the VCR dev set
	</Abstractive Summary>
	<Extractive Summary> =
		Example 1 in Table 4 shows
		an example that our models can only get right if
		bounding box 5 is available
	</Extractive Summary>
	<Extractive Summary> =
		3
		Error Analysis
		We picked some examples, shown in Table 4, to il-
		lustrate the kinds of correct and incorrect choices
		that B2T2 is making, compared to our dual en-
		coder and to a text only model
	</Extractive Summary>
</Paper ID=ument219>


<Paper ID=ument22> <Table ID =1>
	<Abstractive Summary> =
		7*
		Table 1: Micro-averaged precision (P), recall (R) and
		F1 score on TACRED dataset
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results on TACRED dataset
		Table 1 shows the results of baseline as well as
		our proposed models on TACRED dataset
	</Extractive Summary>
</Paper ID=ument22>


<Paper ID=ument22> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Results of adding NER embeddings and entity
		categorical embeddings to the multi-channel attention
		(MCA) integrated model
	</Abstractive Summary>
</Paper ID=ument22>


<Paper ID=ument22> <Table ID =3>
	<Abstractive Summary> =
		0*
		Table 3: Macro-averaged F1 score on SemEval2010-
		Task8 dataset
	</Abstractive Summary>
</Paper ID=ument22>


<Paper ID=ument22> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Ablation study on knowledge-attention en-
		coder
	</Abstractive Summary>
</Paper ID=ument22>


<Paper ID=ument22> <Table ID =5>
	<Abstractive Summary> =
		correct
		Table 5: Attention visualization for knowledge-attention encoder (ﬁrst) and self-attention encoder (second)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5
		presents the attention visualization results on sam-
		ple sentences
	</Extractive Summary>
</Paper ID=ument22>


<Paper ID=ument220> <Table ID =1>
	<Abstractive Summary> =
		606
		Table 1: Caption-level correlation between metrics and
		human grading scores in Composite and Flickr 8K
		dataset by using Kendall tau and Spearman rho
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Result on Composite & Flickr 8K
		Metric Performance
		Table 1 displays the cor-
		relation between metrics and human judgments in
		terms of τ and ρ
	</Extractive Summary>
</Paper ID=ument220>


<Paper ID=ument220> <Table ID =2>
	<Abstractive Summary> =
		70
		Table 2: Accuracy of metrics at matching human judg-
		ments on PASCAL-50S with 5 reference captions
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Result on Pascal-50S
		Fixed Reference Number
		Table 2 reports the
		evaluation accuracy of metrics on PASCAL-50S
		with ﬁve reference captions per image
	</Extractive Summary>
</Paper ID=ument220>


<Paper ID=ument220> <Table ID =3>
	<Abstractive Summary> =
		2152
		Figure 8: Distribution of group score differences between TIGEr and human evaluation
		Table 3: Examples of scores given by TIGEr
	</Abstractive Summary>
</Paper ID=ument220>


<Paper ID=ument221> <Table ID =1>
	<Abstractive Summary> =
		Table 1: We create token sequences that commonly trigger a speciﬁc target prediction when concatenated to any
		input from a dataset
	</Abstractive Summary>
	<Extractive Summary> =
		, top of Table 1)
		and natural language inference models
	</Extractive Summary>
	<Extractive Summary> =
		, middle of Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		, bottom of Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Moreover, a trigger generated for
		the GPT-2 117M model also works for the 345M
		model: the ﬁrst language model sample in Table 1
		shows the larger model ranting on the “evil genes”
		of Black, Jewish, Chinese, and Indian people
	</Extractive Summary>
	<Extractive Summary> =
		The last two examples of Table 1
		show samples from the GPT-2 117M model when
		given different user inputs, and the reader can try
		their own inputs in the online GPT-2 demo
	</Extractive Summary>
	<Extractive Summary> =
		The ﬁrst language model sample in
		Table 1 is generated using the 345M model and
		further samples are shown in Figure 2
	</Extractive Summary>
	<Extractive Summary> =
		The resulting PMI value shows how much
		a word before/after the answer span is indicative of
		a particular answer type (Table 12 in Appendix C)
	</Extractive Summary>
	<Extractive Summary> =
		We
		randomly sample from the top PMI tokens to gen-
		erate twenty different triggers for each question
		type (Table 13 in Appendix C)
	</Extractive Summary>
	<Extractive Summary> =
		The average attack success
		rate over different shufﬂes is low, however, the
		best success rate comes close to the original trig-
		ger (Table 10 in Appendix C)
	</Extractive Summary>
	<Extractive Summary> =
		4
		when placed at the end (Table 11 in Appendix C)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =2>
	<Abstractive Summary> =
		42
		Table 2: We prepend a single word (Trigger) to SNLI
		hypotheses
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we show the top-5 trigger words for
		each ground-truth SNLI class and the correspond-
		ing accuracy for the three models
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: We prepend the trigger sequence to the paragraph of every SQuAD example of a certain type (e
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		The resulting triggers for each target an-
		swer are shown in Table 3, along with their attack
		success rate
	</Extractive Summary>
	<Extractive Summary> =
		2
		Table 4: We replace the target answer span from the
		triggers in Table 3 without changing the rest of the trig-
		ger
	</Extractive Summary>
	<Extractive Summary> =
		Ex-
		cept on “why” questions, this improves transfer-
		ability (second row for each type in Table 3)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: We replace the target answer span from the
		triggers in Table 3 without changing the rest of the trig-
		ger
	</Abstractive Summary>
	<Extractive Summary> =
		, the trigger is relatively
		agnostic to the target answer (Table 4)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: By removing tokens such as punctuation from
		the trigger generated for BiDAF, we can increase the
		attack success rate when transferred to the black-box
		ELMo model
	</Abstractive Summary>
	<Extractive Summary> =
		The resulting
		triggers are shorter but signiﬁcantly more effective
		(Table 5)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =6>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 in Ap-
		pendix B shows the prediction distribution for the
		DA model—targeted attacks are successful, e
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =7>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		We then group the
		trigger words based on their target label and report
		their PMI percentile (Table 7 in Appendix B)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =8>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		As a baseline, we
		also prepend only the target answer span (no other
		tokens) and see substantially lower success rates
		(Table 8 in Appendix C)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =9>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		We report the accuracy
		of these models in Table 9 in Appendix C
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =10>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		The average attack success
		rate over different shufﬂes is low, however, the
		best success rate comes close to the original trig-
		ger (Table 10 in Appendix C)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =11>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		4
		when placed at the end (Table 11 in Appendix C)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =12>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		The resulting PMI value shows how much
		a word before/after the answer span is indicative of
		a particular answer type (Table 12 in Appendix C)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument221> <Table ID =13>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		We
		randomly sample from the top PMI tokens to gen-
		erate twenty different triggers for each question
		type (Table 13 in Appendix C)
	</Extractive Summary>
</Paper ID=ument221>


<Paper ID=ument222> <Table ID =1>
	<Abstractive Summary> =
		78
		Table 1: Mean absolute error (MAE) and max error
		(Max) of the performance drop prediction for the task
		of sentiment analysis (left) and POS tagging (right)
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Error in Performance Drop Prediction
		Table 1 shows the mean absolute and maximum
		error values of the regression process that predicts
		the performance drop of a model trained over ds
		and evaluated on dt
	</Extractive Summary>
</Paper ID=ument222>


<Paper ID=ument222> <Table ID =2>
	<Abstractive Summary> =
		88
		Table 2: Mean absolute error and Max error of per-
		formance drop prediction for sentiment analysis under
		adversarial shift evaluation (The lower the better)
	</Abstractive Summary>
</Paper ID=ument222>


<Paper ID=ument222> <Table ID =3>
	<Abstractive Summary> =
		Acknowledgments
		This work was partially funded through the Eu-
		ropean Unions Horizon 2020 research and in-
		Measure
		Robust to
		Black box
		Good with small
		co-variate shift
		Target Data
		PAD
		X
		✓
		X
		PAD*
		(X)
		X
		X
		RCA
		✓
		✓
		X
		RCA*
		✓
		✓
		X
		CONF
		✓
		X
		✓
		CONF CALIB
		✓
		X
		✓
		Table 3: Summary of domain similarity metrics dis-
		cussed in this work and their different characteristics
	</Abstractive Summary>
	<Extractive Summary> =
		These conclusions are
		summarized in Table 3, which we recommend as
		guideline when deciding what measure to use
	</Extractive Summary>
</Paper ID=ument222>


<Paper ID=ument223> <Table ID =1>
	<Abstractive Summary> =
		93
		Table 1: Machine translation tokenized BLEU test results on IWSLT 2017 DE�EN, KFTT JA�EN, WMT 2016
		RO�EN and WMT 2014 EN�DE, respectively
	</Abstractive Summary>
</Paper ID=ument223>


<Paper ID=ument224> <Table ID =1>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		We include the full
		results of this analysis in Table 1 in the Appendix
	</Extractive Summary>
</Paper ID=ument224>


<Paper ID=ument225> <Table ID =1>
	<Abstractive Summary> =
		58
		Table 1: L2 reconstruction per glyph by number of observed characters
	</Abstractive Summary>
</Paper ID=ument225>


<Paper ID=ument226> <Table ID =1>
	<Abstractive Summary> =
		507
		Table 1: Spearman’s ρ correlation scores for 8 languages on datasets for intrinsic evaluation of true semantic
		similarity
	</Abstractive Summary>
</Paper ID=ument226>


<Paper ID=ument226> <Table ID =2>
	<Abstractive Summary> =
		782
		Table 2: Joint goal accuracy scores in the DST task
	</Abstractive Summary>
</Paper ID=ument226>


<Paper ID=ument226> <Table ID =3>
	<Abstractive Summary> =
		70
		Table 3: Lexical Simpliﬁcation (LS) performance for
		Italian, Spanish, and Portuguese; Semantic Textual
		Similarity (STS) performance for Arabic
	</Abstractive Summary>
</Paper ID=ument226>


<Paper ID=ument227> <Table ID =1>
	<Abstractive Summary> =
		27
		Table 1: EmoBank examples with normalised scores,
		illustrating the differences among the dimensions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists
		examples exhibiting a range of VAD values
	</Extractive Summary>
</Paper ID=ument227>


<Paper ID=ument227> <Table ID =2>
	<Abstractive Summary> =
		614
		Table 2:
		System performance for the word- and
		sentence-level metaphor tasks using the F1-score and
		Pearson’s r respectively
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 2 presents the results for the two metaphor
		tasks
	</Extractive Summary>
</Paper ID=ument227>


<Paper ID=ument227> <Table ID =3>
	<Abstractive Summary> =
		417
		Table 3: System performance for emotion regression
		tasks according to Pearson’s r
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the results for the emotion
		regression tasks
	</Extractive Summary>
</Paper ID=ument227>


<Paper ID=ument227> <Table ID =4>
	<Abstractive Summary> =
		L
		M
		M
		Table 4: Examples of how MTL improves over STL on metaphor identiﬁcation, with gold metaphors underlined
		and corrections by MTL bolded
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 illustrates model decisions corrected
		through joint learning
	</Extractive Summary>
</Paper ID=ument227>


<Paper ID=ument227> <Table ID =5>
	<Abstractive Summary> =
		30
		Table 5: Examples of how MTL using metaphor identiﬁcation improves over STL emotion prediction, with pre-
		dicted metaphors underlined
	</Abstractive Summary>
	<Extractive Summary> =
		Emotion prediction
		Table 5 lists examples for
		which including metaphor identiﬁcation improved
		performance on emotion regression
	</Extractive Summary>
</Paper ID=ument227>


<Paper ID=ument228> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of several verb signatures and illustrative contexts for each
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 provides
		several examples
	</Extractive Summary>
	<Extractive Summary> =
		The
		examples shown in Table 1 illustrate ⟨p, h⟩ pairs
		drawn from our dataset, generated this way
	</Extractive Summary>
</Paper ID=ument228>


<Paper ID=ument228> <Table ID =2>
	<Abstractive Summary> =
		+/+ realize that (34) know that (32) remember that (17) ﬁnd that (12)
		notice that (12) reveal that (12) acknowledge that (11) admit that (11) learn
		that (11) observe that (11) see that (11) note that (10) recognize that (10)
		understand that (10) discover that (8) +/− manage to (30) begin to (12)
		serve to (11) start to (11) dare to (8) use to (7) get to (6) come to (5) −/+
		forget to (15) fail to (10) ◦/+ suspect that (11) explain that (10) mean
		to (10) predict that (10) ◦/− attempt to (28) −/◦ refuse to (36) decline
		to (12) remain to (7) +/◦ show that (12) conﬁrm that (11) demonstrate
		that (10) ensure that (9) help to (9) tend to (8) ◦/◦ try to (34) hope that
		(20) hope to (18) mention that (14) like to (12) continue to (12) expect that
		(12) agree that (12) love to (12) reply that (12) conclude that (12) say that
		(12) complain that (12) speculate that (12) state that (12) suggest that (12)
		worry that (12) mean that (12) intend to (11) insist that (11) imply that (11)
		indicate that (11) plan to (11) promise to (11) prove to (11) saw that (11)
		seem that (11) tell that (11) think that (11) felt that (11) write that (11)
		decide to (11) assume that (11) believe that (11) assert that (11) concern
		that (11) estimate that (11) convince that (11) decide that (11) appear that
		(11) argue that (11) aim to (11) cease to (10) strive to (10) proceed to (10)
		choose to (10) seem to (10) prove that (10) provide that (10) seek to (10)
		appear to (10) comment that (10) contend that (10) want to (10) doubt that
		(10) feel that (10) fear that (10) agree to (10) announce that (9) claim that
		(9) struggle to (9) hear that (9) propose to (9) wish to (9) say to (9) turn to
		(8) wish that (8) work to (8) advise that (8) move to (8) claim to (8) expect
		to (8) report that (8) happen to (8) propose that (8) hold that (8) declare that
		(8) prefer to (8) need to (8) give that (7) deserve to (7) threaten to (7) exist
		to (7) be that (7) prepare to (6) wait to (6) pretend to (6) ask to (6) return to
		(6) request that (5) demand that (4) recommend that (4) require that (4)
		Table 2: 137 verbs belonging to 8 signatures
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists all of the signatures
		and the corresponding verbs we consider
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 lists the verbs in-
		cluded in our dataset and the number of sentences
		in which each appears
	</Extractive Summary>
</Paper ID=ument228>


<Paper ID=ument228> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Examples of verbs which are expected to be
		◦/◦, but which behave like +/− in context
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		some examples for which this is the case
	</Extractive Summary>
</Paper ID=ument228>


<Paper ID=ument228> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Examples of how the factive verb “know that”
		can exhibit different signatures, depending on context
	</Abstractive Summary>
</Paper ID=ument228>


<Paper ID=ument228> <Table ID =5>
	<Abstractive Summary> =
		57
		Table 5: Accuracy and Spearman correlation of BERT MNLI model predictions against human judgements
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows per-
		formance evaluated against human judgements in
		terms of both (discrete) classiﬁcation accuracy
		and (continuous) Spearman correlation
	</Extractive Summary>
</Paper ID=ument228>


<Paper ID=ument228> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Examples of counterfactual manipulations with the target verb construction “try to”
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows exam-
		ples
	</Extractive Summary>
</Paper ID=ument228>


<Paper ID=ument228> <Table ID =7>
	<Abstractive Summary> =
		02
		Table 7:
		Comparison (KL divergence) of post-
		manipulation prediction distribution to target verb dis-
		tribution (Dvt, top row) and baseline distribution (D,
		bottom row)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows, for verbs within each
		signature, the KL divergence between the post-
		manipulation prediction distribution (D∗) and 1)
		the baseline distribution (D) and 2) the target dis-
		tribution (Dvt)
	</Extractive Summary>
</Paper ID=ument228>


<Paper ID=ument228> <Table ID =8>
	<Abstractive Summary> =
		46
		Table 8: KL divergence between D∗ and Dvt for com-
		plement type manipulations (“to” vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows the KL divergence between D∗
		and the target verb distribution Dvt in the matched
		and mismatched cases
	</Extractive Summary>
</Paper ID=ument228>


<Paper ID=ument229> <Table ID =1>
	<Abstractive Summary> =
		While
		Language
		Color Word
		Literal Gloss
		Welsh
		brown
		brown
		Italian
		marrone
		chestnut
		Persianیا هﻮﻬﻗ
		coffee + of
		Cantonese
		ar
		coffee + color
		Table 1: Examples of terms representing brown, aris-
		ing from four processes: borrowing (Welsh; from En-
		glish), null afﬁxing (Italian), derivational afﬁxing (Per-
		sian), and compounding (Cantonese)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		6
		Morphology
		In this section, we ask whether there are afﬁxes that
		are highly correlated with color; these can be either
		general derivational afﬁxes or sequences speciﬁc to
		color terms, as in Table 1 and Table 4
	</Extractive Summary>
</Paper ID=ument229>


<Paper ID=ument229> <Table ID =2>
	<Abstractive Summary> =
		99
		Adj
		Table 2: Top back-translations and their concreteness
		for each of the 11 basic color categories
	</Abstractive Summary>
	<Extractive Summary> =
		Keehoon Trevor
		Lee prepared part of the data for Table 2 as an
		undergraduate research assistant
	</Extractive Summary>
</Paper ID=ument229>


<Paper ID=ument229> <Table ID =3>
	<Abstractive Summary> =
		This appears
		even with black and white—the ﬁrst basic color
		Language
		Afﬁxes
		nci
		-tic† (40%), tla- (27%), xox- (27%)
		aqc
		-тут⇤ (36%)
		dbu
		-Ngó (29%)
		mpj
		-lpa⇤ (26%), -ku (15%)
		ciw
		-aan-† (23%), -zo⇤ (23%)
		Table 3: Common example afﬁxes
	</Abstractive Summary>
	<Extractive Summary> =
		, “тут” in
		Archi (aqc) in Table 3, which is a fused morpheme
		denoting adjectivalization and marking Archi’s
		fourth gender
	</Extractive Summary>
</Paper ID=ument229>


<Paper ID=ument229> <Table ID =4>
	<Abstractive Summary> =
		+ color
		deu: dunkel (dark) + rot (red)
		Table 4: Discovered concatenation strategies pooled
		across languages, with representative examples from
		Mandarin, Spanish, and German
	</Abstractive Summary>
</Paper ID=ument229>


<Paper ID=ument229> <Table ID =5>
	<Abstractive Summary> =
		They ﬁrst extract potential compounds
		by splitting any word into three substrings corre-
		sponding to a left component, glue, and right com-
		2246
		Process
		Basic
		Secondary
		Inheritance
		1161
		2356
		Derivation
		82
		183
		Cognate
		303
		483
		Borrowing
		18
		84
		None of these
		42566
		65969
		Table 5: Sources of foreign color words
	</Abstractive Summary>
	<Extractive Summary> =
		We report the aggregation of EtymDB’s parsed
		etymologies in Table 5; these are broken down by
		color in the supplementary material
	</Extractive Summary>
</Paper ID=ument229>


<Paper ID=ument229> <Table ID =6>
	<Abstractive Summary> =
		962
		Table 6: Goodman and Kruskal’s gamma rank correla-
		tion between each measure and both basicness and the
		diachronic color sequence
	</Abstractive Summary>
</Paper ID=ument229>


<Paper ID=ument229> <Table ID =7>
	<Abstractive Summary> =
		48
		Table 7: Total ordering of colors according to our ag-
		gregate score, with conventionally basic colors bolded
	</Abstractive Summary>
	<Extractive Summary> =
		We recover the
		ﬁrst six colors in the evolutionary sequence from
		Figure 1: white and black, red, green and yellow,
		and blue! An extended ordering is given in Table 7;
		it suggests that the most primary of the non-basic
		terms are gold, scarlet, crimson, and beige
	</Extractive Summary>
</Paper ID=ument229>


<Paper ID=ument23> <Table ID =1>
	<Abstractive Summary> =
		ZH-EN
		ZH
		66,469
		2,830
		153,929
		1,5000
		890
		EN
		98,125
		2,317
		237,674
		JA-EN
		JA
		65,744
		2,043
		164,373
		1,5000
		529
		EN
		95,680
		2,096
		233,319
		FR-EN
		FR
		66,858
		1,379
		192,191
		1,5000
		212
		EN
		105,889
		2,209
		278,590
		Table 1: Summary of the DBP15K datasets
	</Abstractive Summary>
</Paper ID=ument23>


<Paper ID=ument23> <Table ID =2>
	<Abstractive Summary> =
		11
		Table 2: Performance on entity alignment
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Entity Alignment
		Table 2 reports the performance for entity align-
		ment of all compared approaches
	</Extractive Summary>
	<Extractive Summary> =
		The bottom part of Table 2 shows how our pro-
		posed techniques, i
	</Extractive Summary>
</Paper ID=ument23>


<Paper ID=ument23> <Table ID =3>
	<Abstractive Summary> =
		60
		Table 3: Performance on relation alignment
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Relation Alignment
		Table 3 reports the results of relation alignment
	</Extractive Summary>
</Paper ID=ument23>


<Paper ID=ument230> <Table ID =1>
	<Abstractive Summary> =
		015
		Table 1: Hyper-parameter settings
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists the hyper-parameters in our exper-
		iments
	</Extractive Summary>
</Paper ID=ument230>


<Paper ID=ument230> <Table ID =2>
	<Abstractive Summary> =
		80
		Table 2: Performances of negative focus detection sys-
		tems on the SEM’12 corpus
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 2 shows the performance comparison of var-
		ious negative focus detection models
	</Extractive Summary>
	<Extractive Summary> =
		We can see
		that all of contextual attention based models (row
		7-9 in Table 2) achieve better perfomances than
		existing methods (row 1-3 in Table 2) and mod-
		els without contextual attention (row 4-6 in Ta-
		ble 2)
	</Extractive Summary>
	<Extractive Summary> =
		Comparing the three types of attention
		mechanisms (row 7-9 in Table 2), the topic-level
		attention based model achieves the best perfor-
		mance
	</Extractive Summary>
	<Extractive Summary> =
		In addition to the methods that take advantage
		of the contextual features in adjacent sentences,
		we also compare the performances of different
		frameworks for negative focus detection (row 4-
		6 in Table 2), which only apply the features in
		current sentence
	</Extractive Summary>
</Paper ID=ument230>


<Paper ID=ument230> <Table ID =3>
	<Abstractive Summary> =
		51
		Table 3: Performance comparison by using different
		contextual information into the BiLSTM-CRF frame-
		work
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the per-
		formance comparison when using different con-
		textual information
	</Extractive Summary>
</Paper ID=ument230>


<Paper ID=ument230> <Table ID =4>
	<Abstractive Summary> =
		51
		Table 4: Performance comparison for systems only us-
		ing word embeddings and those using word and SR em-
		beddings (SR: semantic role)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, we
		can see that the performances of all three models
		are improved obviously, which demonstrates the
		effectiveness of semantic role information for this
		task
	</Extractive Summary>
</Paper ID=ument230>


<Paper ID=ument230> <Table ID =5>
	<Abstractive Summary> =
		Semantic Role
		Train
		Dev
		Test
		A1
		980
		222
		309
		AM-NEG
		592
		138
		172
		AM-TMP
		161
		35
		46
		AM-MNR
		127
		27
		38
		A2
		112
		28
		36
		A0
		94
		23
		31
		AM-ADV
		78
		23
		26
		Others
		165
		30
		61
		NONE
		88
		19
		35
		Total
		2,304
		531
		712
		Table 5: Statistics of the top seven semantic role labels
		of negative focus
	</Abstractive Summary>
	<Extractive Summary> =
		In addition, Table 5 lists the top seven seman-
		tic role labels of negative focus in the SEM’12
		shared task corpus
	</Extractive Summary>
</Paper ID=ument230>


<Paper ID=ument230> <Table ID =6>
	<Abstractive Summary> =
		80
		Table 6: Performance comparison for different compo-
		sition manners of attention matrices
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6, it indi-
		cates that the composition manner within the input
		layer is more effective than that within the hidden
		layer for both word-level and topic-level attention
		mechanisms
	</Extractive Summary>
</Paper ID=ument230>


<Paper ID=ument230> <Table ID =7>
	<Abstractive Summary> =
		51
		Table 7: Performance comparison for different pre-
		trained word embeddings on the T-Att BiLSTM-CRF
		model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 show the performances of
		the T-Att BiLSTM-CRF model with different
		pre-trained word embeddings, including Senna6,
		Glove7, Word2vec8, and BERT9
	</Extractive Summary>
</Paper ID=ument230>


<Paper ID=ument230> <Table ID =8>
	<Abstractive Summary> =
		54
		Table 8: Performance comparison for different features
		with the BiLSTM-CRF model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows
		the comparison of the BiLSTM-CRF models with
		different feature embeddings
	</Extractive Summary>
</Paper ID=ument230>


<Paper ID=ument231> <Table ID =1>
	<Abstractive Summary> =
		# Pairs
		Train
		00-13
		1,378
		26,422
		Test
		14-24
		1,053
		20,411
		Table 1: Statistics of the WSJ news dataset used for
		“global” discrimination task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		summarizes the data sets used in global discrim-
		ination task
	</Extractive Summary>
	<Extractive Summary> =
		We use the
		same WSJ articles used in the global discrimina-
		tion task (Table 1) to create our local discrimina-
		tion datasets
	</Extractive Summary>
</Paper ID=ument231>


<Paper ID=ument231> <Table ID =2>
	<Abstractive Summary> =
		# Pairs
		Dw=1
		Dw=2
		Dw=3
		Dw=1,2,3
		Train
		00-13
		748
		7,890
		12,280
		12,440
		32,610
		Test
		14-24
		618
		6,568
		9,936
		9,906
		26,410
		Table 2: Statistics on the WSJ news dataset used for
		“local” discrimination task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the
		datasets
	</Extractive Summary>
</Paper ID=ument231>


<Paper ID=ument231> <Table ID =3>
	<Abstractive Summary> =
		23
		Table 3: Results in accuracy on the Local Discrim-
		ination task
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results on Local Discrimination
		Table 3 shows the results in accuracy on the “lo-
		cal” discrimination task
	</Extractive Summary>
</Paper ID=ument231>


<Paper ID=ument231> <Table ID =4>
	<Abstractive Summary> =
		78
		Table 4: Results in accuracy on the Global Discrimi-
		nation task
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results on Global Discrimination
		Table 4 presents the results in accuracy on the two
		“global” discrimination tasks – the Standard and
		the Inverse order discrimination
	</Extractive Summary>
</Paper ID=ument231>


<Paper ID=ument231> <Table ID =5>
	<Abstractive Summary> =
		23
		Table 5: Ablation study of different model components
		on the Local Discrimination task
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in the ﬁrst block of Table 5, addition of
		the global model to the local model degrades the
		performance on the Dw=1 dataset by 1
	</Extractive Summary>
	<Extractive Summary> =
		However, in the presence of the
		LM loss (second block in Table 5), the addition of
		the global model improves the performance across
		2270
		Model
		Emb
	</Extractive Summary>
</Paper ID=ument231>


<Paper ID=ument231> <Table ID =6>
	<Abstractive Summary> =
		82
		(Our Full Model)
		Table 6: Ablation study of different model components
		on the Global Discrimination task
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6,
		the addition of the global model and LM loss to the
		local model improves performance on the standard
		discrimination task by 1
	</Extractive Summary>
</Paper ID=ument231>


<Paper ID=ument232> <Table ID =1>
	<Abstractive Summary> =
		37) 64,557
		SIND
		40,155 (5)
		4,990 (5)
		5,055 (5)
		30,861
		Table 1: The number of paragraphs, average sentences
		(in parentheses), and vocabulary size of 5 datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the details of the 5 datasets
		used in our experiments
	</Extractive Summary>
</Paper ID=ument232>


<Paper ID=ument232> <Table ID =2>
	<Abstractive Summary> =
		71
		Table 2: Performance of neural networks based models on the sentence ordering task
	</Abstractive Summary>
	<Extractive Summary> =
		In
		Table 2-3, the several values of some models are
		directly taken from (Cui et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 2-3 show that ATTOrderNet and TGCM,
		which utilize an attention mechanism, perform
		much better than all the other models
	</Extractive Summary>
</Paper ID=ument232>


<Paper ID=ument232> <Table ID =3>
	<Abstractive Summary> =
		24
		Table 3: Performance of predicting the correct head
		and tail sentences on arXiv and SIND
	</Abstractive Summary>
</Paper ID=ument232>


<Paper ID=ument233> <Table ID =1>
	<Abstractive Summary> =
		2286
		Action
		Before
		After
		Probability
		Condition
		GEN(e)
		⟨S, B⟩
		⟨S|EDU(e), B|e⟩
		ptrans(GEN|S) · pgen(e|S)
		|B| < m
		RE(r, n)
		⟨S|UL|UR, B⟩
		⟨S|
		�
		Unit(r, n) UL UR
		�
		, B⟩
		ptrans(RE(r, n)|S)
		|S| ≥ 2
		Table 1: Our transition system
	</Abstractive Summary>
	<Extractive Summary> =
		Finally, RST trees are traditionally binarized so
		we modify the REDUCE transitions accordingly,
		resulting in the following transition system (see
		also Table 1):
		GEN(e) Generate the EDU e and push it onto the
		top of the stack and the end of the buffer
	</Extractive Summary>
	<Extractive Summary> =
		Both transitions have conditions on when they
		can be performed (Table 1)
	</Extractive Summary>
</Paper ID=ument233>


<Paper ID=ument233> <Table ID =2>
	<Abstractive Summary> =
		]
		Table 2: An example of a completed computation in our transition system
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows an example of a
		completed computation for our transition system
	</Extractive Summary>
</Paper ID=ument233>


<Paper ID=ument233> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Dev
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Search Comparison
		Table 3 shows RST-DT development set labelled
		attachment metrics for our parser using word-level
		and bag-level beam search
	</Extractive Summary>
</Paper ID=ument233>


<Paper ID=ument233> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: Test set micro-averaged F1 scores on labelled attachment decisions
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Parsing Performance
		Table 4 shows RST-DT test set labelled attachment
		metrics for various parsers
	</Extractive Summary>
</Paper ID=ument233>


<Paper ID=ument234> <Table ID =1>
	<Abstractive Summary> =
		92
		Table 1: Evaluations of attachment on Dev set with and without higher order dependencies
	</Abstractive Summary>
	<Extractive Summary> =
		The higher order dependen-
		cies signiﬁcantly improved the generative model’s
		results on the full STAC corpus (see Table 1)
	</Extractive Summary>
</Paper ID=ument234>


<Paper ID=ument234> <Table ID =2>
	<Abstractive Summary> =
		93
		Table 2: Evaluations of weakly supervised (Snorkel and stand alone GEN) and supervised approaches on STAC
		data
	</Abstractive Summary>
	<Extractive Summary> =
		, 2015) listed in their Table 2) and a Lo-
		gistic Regression classiﬁer
	</Extractive Summary>
	<Extractive Summary> =
		As seen in Table 2 on STAC test data,
		GEN dramatically outperformed our deep learn-
		ing baselines—BiLSTM, BERT, and BERT + Lo-
		gReg* architectures on gold labels—as well as the
		LAST baseline, which attaches every DU in a di-
		alogue to the DU directly preceding it
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows that
		LogReg* was the best supervised learning method
		on the STAC data in terms of producing local mod-
		els
	</Extractive Summary>
	<Extractive Summary> =
		As we remarked above, we found that binariz-
		ing GEN’s output improved the performance of
		the coupled discriminative model, so Table 2 only
		reports scores for various GEN-coupled discrim-
		inative models that take the binarized GEN pre-
		dictions as input
	</Extractive Summary>
</Paper ID=ument234>


<Paper ID=ument234> <Table ID =3>
	<Abstractive Summary> =
		65
		Table 3: Comparison of local and global models on STAC data
	</Abstractive Summary>
</Paper ID=ument234>


<Paper ID=ument234> <Table ID =4>
	<Abstractive Summary> =
		53
		Table 4: Comparison of Perret 2016 and results of our methods on Perret 2016 data
	</Abstractive Summary>
	<Extractive Summary> =
		,
		2016) data set, the MST decoding mechanism pro-
		vided LogReg* only a boost of 12 F1 points, as
		seen in Table 4, which is signiﬁcantly lower than
		what is reported in (Perret et al
	</Extractive Summary>
</Paper ID=ument234>


<Paper ID=ument234> <Table ID =5>
	<Abstractive Summary> =
		72
		Table 5: F1 scores for different decoding mechanisms with GEN on STAC data
	</Abstractive Summary>
	<Extractive Summary> =
		In
		Table 5, we experimented with adding all decod-
		ing algorithms to the local GEN result
	</Extractive Summary>
</Paper ID=ument234>


<Paper ID=ument235> <Table ID =1>
	<Abstractive Summary> =
		6
		3,453
		Table 1: Dataset size
		4
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the most important
		dataset dimensions
	</Extractive Summary>
	<Extractive Summary> =
		We show the absolute vo-
		cabulary sizes of the datasets in Table 1 and vi-
		sualize the overlap in Table 2
	</Extractive Summary>
</Paper ID=ument235>


<Paper ID=ument235> <Table ID =2>
	<Abstractive Summary> =
		65%
		Table 2: Vocabulary overlap between datasets
		4
	</Abstractive Summary>
	<Extractive Summary> =
		Our models,
		learning the discourse struc-
		ture solely from the inter-domain Yelp’13 re-
		view dataset through distant supervision, reach
		better performance than the human annotated
		datasets (RST-DT and Instr-DT) when trained
		inter-domain, despite the low vocabulary overlap
		between the Yelp’13 corpus and RST-DT/Instr-DT
		(Table 2)
	</Extractive Summary>
</Paper ID=ument235>


<Paper ID=ument235> <Table ID =3>
	<Abstractive Summary> =
		30
		—
		Table 3: Discourse structure prediction results; tested
		on RST-DTtest and Instr-DTtest
	</Abstractive Summary>
	<Extractive Summary> =
		The ﬁrst set of results in Table 3 shows that the
		hierarchical right/left branching baselines dom-
		inate the completely right/left branching ones
	</Extractive Summary>
</Paper ID=ument235>


<Paper ID=ument236> <Table ID =1>
	<Abstractive Summary> =
		Author
		#Papers
		Author
		#Citations
		D Klein
		41
		C Manning
		656
		M Zhou
		39
		D Klein
		562
		M Johnson
		32
		F Pereira
		561
		I Dagan
		30
		M Collins
		539
		C Manning
		29
		D Marcu
		459
		K Knight
		28
		P Coehn
		441
		M Zhang
		27
		S Roukos
		414
		Y Liu
		27
		E Charniak
		400
		Q Liu
		27
		A McCallum
		393
		R Barzilay
		26
		K Knight
		388
		N Smith
		25
		F Och
		387
		E Hovy
		24
		M Marcus
		377
		G Satta
		23
		M Johnson
		355
		H Li
		23
		D Jurafsky
		349
		Y Matsumoto
		21
		H Ney
		348
		Table 1: Top 15 most proliﬁc authors and most cited
		authors in our ACL dataset
	</Abstractive Summary>
</Paper ID=ument236>


<Paper ID=ument236> <Table ID =2>
	<Abstractive Summary> =
		Author
		#Papers
		Author
		#Citations
		D Klein
		22
		C Manning
		333
		N Smith
		16
		D Klein
		308
		H Ng
		16
		M Collins
		246
		C Manning
		16
		F Pereira
		238
		G Zhou
		15
		A McCallum
		222
		M Lapata
		14
		P Koehn
		204
		J Eisner
		13
		D Marcu
		191
		D Roth
		13
		F Och
		188
		Q Liu
		12
		R McDonald
		179
		M Collins
		12
		S Roukos
		174
		K Torisawa
		11
		D Jurafsky
		173
		M Zhang
		11
		K Knight
		167
		Y Liu
		11
		M Marcus
		155
		M Zhou
		9
		M Johnson
		154
		S Petrov
		9
		A Ng
		143
		Table 2: Top 15 most proliﬁc authors and most cited
		authors in our EMNLP dataset
	</Abstractive Summary>
</Paper ID=ument236>


<Paper ID=ument236> <Table ID =3>
	<Abstractive Summary> =
		For training, we transform the train-
		ing examples from multi-label examples to single-
		label examples, by generating several copies of the
		same datapoint, each labelled with only one of its
		authors, whenever a text was written by more than
		Dataset
		Nr authors
		Training
		Valid
		Test
		ACL
		Top 100
		73,261
		12,260
		12,348
		ACL
		Top 200
		124,460
		18,756
		19,967
		ACL
		Mid 200
		25,746
		12,476
		13,618
		ACL
		Bottom 200
		13,673
		12,378
		12,074
		ACL
		Top 500
		185,451
		29,498
		30,232
		ACL
		All 922
		157,427
		41,978
		44,427
		EMNLP
		Top 100
		39,852
		8,432
		7,971
		EMNLP
		Mid 100
		13,486
		7,557
		7,950
		EMNLP
		Bottom 100
		8,403
		6,801
		7,769
		EMNLP
		All 262
		49,743
		17,518
		17,671
		Table 3: Data size (in number of article segments)
	</Abstractive Summary>
</Paper ID=ument236>


<Paper ID=ument236> <Table ID =4>
	<Abstractive Summary> =
		Dataset
		Nr authors
		Training
		Valid
		Test
		ACL
		Top 100
		350
		185
		197
		ACL
		Top 200
		1,315
		258
		263
		ACL
		Mid 200
		343
		190
		199
		ACL
		Bottom 200
		180
		181
		176
		ACL
		Top 500
		1,758
		413
		428
		ACL
		All 922
		1,604
		697
		710
		EMNLP
		Top 100
		325
		90
		89
		EMNLP
		Mid 100
		130
		83
		88
		EMNLP
		Bottom 100
		86
		78
		86
		EMNLP
		All 262
		345
		191
		86
		Table 4: Data size (in number of articles)
	</Abstractive Summary>
</Paper ID=ument236>


<Paper ID=ument236> <Table ID =5>
	<Abstractive Summary> =
		54
		Table 5: Results on ACL dataset (segment level)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 5 shows the classiﬁcation results measured
		on our data points consisting of article segments
		on ACL, whereas Table 6 contains the values of
		the metrics aggregated at article-level on the same
		ACL dataset
	</Extractive Summary>
</Paper ID=ument236>


<Paper ID=ument236> <Table ID =6>
	<Abstractive Summary> =
		67
		Table 6: Results on ACL dataset (article level)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 5 shows the classiﬁcation results measured
		on our data points consisting of article segments
		on ACL, whereas Table 6 contains the values of
		the metrics aggregated at article-level on the same
		ACL dataset
	</Extractive Summary>
</Paper ID=ument236>


<Paper ID=ument236> <Table ID =7>
	<Abstractive Summary> =
		39
		Table 7: Results on EMNLP dataset (segment level)
	</Abstractive Summary>
</Paper ID=ument236>


<Paper ID=ument236> <Table ID =8>
	<Abstractive Summary> =
		92
		Table 8: Results on EMNLP dataset (article level)
	</Abstractive Summary>
</Paper ID=ument236>


<Paper ID=ument237> <Table ID =1>
	<Abstractive Summary> =
		Therefore,
		2330
		Dimensions
		Classes/Groups
		Age of Harasser
		0 - unspeciﬁed, 1 - teenagers or young people, 2 - adults
		Single/Multiple Harasser(s)
		0 - unspeciﬁed, 1 - single harasser, 2 - multiple harassers
		Type of Harasser
		0 - unspeciﬁed, 1 - relative, 2 - teacher, 3 - classmate, 4 - friend, 5 - neighbor,
		6 - conductor/driver, 7 - work-related, 8 - police/guard, 9 - other
		Type of Location
		0 - unspeciﬁed, 1 - street, 2 - transportation, 3 - station/stop, 4 - private places,
		5 - shopping places, 6 - neighborhood, 7 - park, 8 - hotel, 9 - bush/woods,
		10 - parking lot, 11 - in/near school, 12 - restaurant, 13 - other
		Time of Day
		0 - unspeciﬁed, 1 - day (5am to 6pm), 2 - evening or night
		Table 1: Deﬁnition of classes in different dimensions about sexual harassment
	</Abstractive Summary>
	<Extractive Summary> =
		Further-
		more, we also assigned each story classiﬁcation
		labels in ﬁve dimensions (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		75 for key element extraction (details can refer to
		Table 1 in supplementary ﬁle)
	</Extractive Summary>
</Paper ID=ument237>


<Paper ID=ument237> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Key element extraction results
	</Abstractive Summary>
	<Extractive Summary> =
		Signiﬁ-
		cance t-test results are shown in Table 2 in the sup-
		plementary ﬁle
	</Extractive Summary>
</Paper ID=ument237>


<Paper ID=ument237> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Classiﬁcation accuracy and macro F1 of the
		models
	</Abstractive Summary>
	<Extractive Summary> =
		Although not much improvement
		was achieved in key element extraction (Figure 2),
		classiﬁcation performance improved signiﬁcantly
		with joint learning schemes (Table 3)
	</Extractive Summary>
</Paper ID=ument237>


<Paper ID=ument237> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Harassment form classiﬁcation accuracy of
		models
	</Abstractive Summary>
</Paper ID=ument237>


<Paper ID=ument238> <Table ID =1>
	<Abstractive Summary> =
		com
		Table 1: 30 day windowed average of stock price pre-
		diction error using PCG
		Step size
		Cbest
		PCGuni
		PCGbi
		PCGboth
		1
		1
	</Abstractive Summary>
	<Extractive Summary> =
		The results shown in
		Table 1 is the root mean squared error (RMSE) in
		predicted stock value calculated on a 30 day win-
		dow averaged by moving it by 10 days over the pe-
		riod and directly comparable to our baseline (Kang
		et al
	</Extractive Summary>
</Paper ID=ument238>


<Paper ID=ument238> <Table ID =2>
	<Abstractive Summary> =
		However,
		2344
		Table 2: Stock price predictive factors for 2013 in PCG
		Stock symbol
		Prediction indicators
		AAPL
		workplace, shutter, music
		AMZN
		healthcare, HBO, cloud
		FB
		unfriended, troll, politician
		GOOG
		advertisers, artiﬁcial intelligence, shake-up
		HPQ
		China, inventions, Pittsburg
		IBM
		64 GB, redesign, outage
		MSFT
		interactive, security, Broadcom
		ORCL
		corporate, investing, multimedia
		TSLA
		prices, testers, controversy
		YHOO
		entertainment, leadership, investment
		HP
		AAPL
		AMZN
		FB
		14
		8
		8
		Figure 2: Inter-stock inﬂuencer links where one stock’s
		movement indicates future movement of other stocks
		(time lag annotated edges)
		these links were not used for prediction perfor-
		mance to maintain parity with our baseline
	</Abstractive Summary>
	<Extractive Summary> =
		Key PCG factors for 2013: The causal links
		in PCG are more generic (Table 2) than the ones
		described in CGRAPH, supporting the hypothesis
		that latent word relationships do exist that go be-
		yond the scope of a single news article
	</Extractive Summary>
</Paper ID=ument238>


<Paper ID=ument238> <Table ID =3>
	<Abstractive Summary> =
		Table 3:
		Variation in stock price prediction error
		(RMSE) % with window size and spike correction
		Stock
		W=50
		W=100
		W=100 + spike
		AABA
		2
	</Abstractive Summary>
	<Extractive Summary> =
		Please note
		that the results in Table 3 are not comparable with
		(Kang et al
	</Extractive Summary>
	<Extractive Summary> =
		We present average root mean squared errors in
		Table 3 for different values of time windows of
		size W (50,100)
	</Extractive Summary>
</Paper ID=ument238>


<Paper ID=ument238> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Comparison with manually identiﬁed inﬂu-
		ence from news articles
		Pairs in news
		Relevant paths in PCG
		price, project
		price-hike –(19)– power-project
		land, budget
		allot-land –(22)– railway-budget
		price, land
		price-hike –(12)– land
		strike, law
		terror-strike –(25)– law ministry
		land, bill
		land-reform –(25)– bill-pass
		election, strike
		election –(21)– Kerala government –(10)– strike
		election, strike
		election –(18)– Mumbai University –(14)– strike
		election, strike
		election –(20)– Shiromani Akali –(13)– strike
		7
		Conclusion
		We presented PCG, a framework for building pre-
		dictive causal graphs which capture hidden rela-
		2346
		tionships between words in text streams
	</Abstractive Summary>
	<Extractive Summary> =
		As seen in Table 4, the
		bi-gram involving the words and the intermediate
		words in the path provide the relevant context un-
		der which the causality is established
	</Extractive Summary>
</Paper ID=ument238>


<Paper ID=ument239> <Table ID =1>
	<Abstractive Summary> =
		Given a set of sen-
		tences, VAE aims at learning a likelihood func-
		2352
		Table 1: Three samples generated using VAE for a
		given input sentence
	</Abstractive Summary>
</Paper ID=ument239>


<Paper ID=ument239> <Table ID =2>
	<Abstractive Summary> =
		For
		example,
		having the terms fever and no
		appetite as positive examples, the new terms
		weakness or body aches could also be added
		to POSTerms (because they are considered se-
		mantically related due to frequent co-occurrence,
		following the distributional hypothesis (Harris,
		2353
		Table 2: CRF training parameters
	</Abstractive Summary>
</Paper ID=ument239>


<Paper ID=ument239> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Dataset statistics
	</Abstractive Summary>
</Paper ID=ument239>


<Paper ID=ument239> <Table ID =4>
	<Abstractive Summary> =
		2355
		Table 4: Performance of the different ADR detection techniques on the Twitter and Reddit test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 reports precision,
		and recall and F1-measure, of all the baselines in
		comparison to proposed approach CRF+VAE in
		Twitter and Reddit dataset
	</Extractive Summary>
</Paper ID=ument239>


<Paper ID=ument239> <Table ID =5>
	<Abstractive Summary> =
		2357
		Table 5:
		Average Precision/Recall/F1 with standard deviation in parenthesis for CRF, CRF+SelfTraining,
		CRF+Doc2Vec and CRF+VAE on Twitter and Reddit datasets
	</Abstractive Summary>
</Paper ID=ument239>


<Paper ID=ument24> <Table ID =1>
	<Abstractive Summary> =
		8
		Table 1: Statistics of datasets, where ”Avg #words”
		means the average number of words in descriptions
	</Abstractive Summary>
</Paper ID=ument24>


<Paper ID=ument24> <Table ID =2>
	<Abstractive Summary> =
		281
		Table 2: One-shot KGC results on WDtext and DBPedia, where bold numbers indicate best results over different
		models on the same metric
	</Abstractive Summary>
	<Extractive Summary> =
		The results of our overall frame-
		work and baselines are shown in Table 2, where
		we ﬁnd that our overall framework outperforms
		the baselines on most metrics
	</Extractive Summary>
</Paper ID=ument24>


<Paper ID=ument24> <Table ID =3>
	<Abstractive Summary> =
		255
		Table 3: Four-shot KGC results on WDtext and DBPedia, where bold numbers indicate best results over different
		models on the same metric
	</Abstractive Summary>
</Paper ID=ument24>


<Paper ID=ument24> <Table ID =4>
	<Abstractive Summary> =
		256
		Table 4: MRR results of our framework when using
		different number of augmentation in TCVAE, where
		#Gen means the number of triplets being generated
	</Abstractive Summary>
	<Extractive Summary> =
		The results
		are shown in Table 4, and we can conclude that a
		proper data augmentation does enhance the perfor-
		mance of our framework when training data avail-
		able is scarce
	</Extractive Summary>
</Paper ID=ument24>


<Paper ID=ument240> <Table ID =1>
	<Abstractive Summary> =
		13
		Table 1: Impact of different private-attribute in-
		ference attackers on RLTA when α = 0
	</Abstractive Summary>
</Paper ID=ument240>


<Paper ID=ument241> <Table ID =1>
	<Abstractive Summary> =
		8
		Template
		n1 * ( 1 - n2 )
		Preﬁx template
		* n1 - 1 n2
		Table 1: One example of preﬁx template
		the help of an auxiliary stack
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1 we show one example of preﬁx
		templates
	</Extractive Summary>
</Paper ID=ument241>


<Paper ID=ument241> <Table ID =2>
	<Abstractive Summary> =
		7%
		Table 2: Math word problem solving accuracy on Math23K
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 2 shows the results of our system and other
		novel systems of MWP on the Math23k test set
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, we achieve state-of-
		the-art single model performance on the test set,
		and even better results than all the previous en-
		semble models, which can demonstrate the effec-
		tiveness of our proposed method
	</Extractive Summary>
</Paper ID=ument241>


<Paper ID=ument241> <Table ID =3>
	<Abstractive Summary> =
		5%
		Table 3: Ablation study on Math23K by removing
		modules
	</Abstractive Summary>
</Paper ID=ument241>


<Paper ID=ument241> <Table ID =4>
	<Abstractive Summary> =
		2%
		Table 4: Ablation study on Math23K by using different
		terminating methods
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, we also report the percentage of
		invalid templates by using different terminating
		methods
	</Extractive Summary>
</Paper ID=ument241>


<Paper ID=ument241> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5: Accuracy of different template lengths on
		Math23K
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Error Analysis
		In Table 5, we show the results of how the ac-
		curacy changes as the template becomes longer
	</Extractive Summary>
</Paper ID=ument241>


<Paper ID=ument241> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: Accuracy of different question domains on
		Math23K
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 6, we examine the performance of the
		model in different question domains
	</Extractive Summary>
</Paper ID=ument241>


<Paper ID=ument241> <Table ID =7>
	<Abstractive Summary> =
		Domain
		Keywords
		Distance & Speed
		速 度,千 米,路 程,相
		距,全程
		Tracing
		相 遇,相 对,相 反,相
		背,相向
		Engineering
		工程,零件,工程队,公
		路,修路
		Interval
		利润
		Circle
		半径,圆,直径,周长
		Plane Geometry
		三角形,正方形,长方
		形,边长
		Proﬁt
		间隔,隔
		Solid Geometry
		体 积,侧 面 积,横 截
		面,表面积,圆柱,长方
		体
		Interest Rate
		利息,利率
		Production
		超产,减产
		Table 7: The list of keywords in the domain speciﬁc
		studies
	</Abstractive Summary>
</Paper ID=ument241>


<Paper ID=ument242> <Table ID =1>
	<Abstractive Summary> =
		Train
		Dev
		Test
		MetaQA 1-hop
		96,106
		9,992
		9,947
		MetaQA 2-hop
		118,980
		14,872
		14,872
		MetaQA 3-hop
		114,196
		14,274
		14,274
		WebQuestionsSP
		2,848
		250
		1,639
		Complex WebQ
		27,623
		3,518
		3,531
		Table 1: Statistics of all datasets
	</Abstractive Summary>
</Paper ID=ument242>


<Paper ID=ument242> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Retrieval results (recall / # entities in graph) on
		MetaQA (m = 500), and WebQuestionsSP and Com-
		plex WebQuestions with Freebase (m = 2000)
	</Abstractive Summary>
</Paper ID=ument242>


<Paper ID=ument242> <Table ID =3>
	<Abstractive Summary> =
		5
		–
		–
		–
		Table 3: Hits@1 on MetaQA compared to baseline models
	</Abstractive Summary>
</Paper ID=ument242>


<Paper ID=ument242> <Table ID =4>
	<Abstractive Summary> =
		0 (F1)
		–
		–
		–
		Table 4: Hits@1 on WebQuestionsSP compared to
		baseline models
	</Abstractive Summary>
	<Extractive Summary> =
		2
		WebQuestionsSP
		Table 4 presents similar results on the Web-
		QuestionsSP dataset
	</Extractive Summary>
</Paper ID=ument242>


<Paper ID=ument242> <Table ID =5>
	<Abstractive Summary> =
		7
		–
		–
		Table 5: Hits@1 on Complex WebQuestions compared
		to baseline models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows
		our results for Complex WebQuestions on the de-
		velopment set
	</Extractive Summary>
	<Extractive Summary> =
		8% Hits@1
		(not listed in Table 5) in the text only setting with
		Wikipedia corpus, which are comparable to the
		dev set results
	</Extractive Summary>
</Paper ID=ument242>


<Paper ID=ument243> <Table ID =1>
	<Abstractive Summary> =
		7%
		Table 1: Statistics of training, dev and test sets of COSMOS QA
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows dataset statistics
	</Extractive Summary>
</Paper ID=ument243>


<Paper ID=ument243> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: The distribution of contextual commonsense
		reasoning types in COSMOS
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2
		shows the distribution of the question types
	</Extractive Summary>
</Paper ID=ument243>


<Paper ID=ument243> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Comparison of varying approachs (Accuracy %)
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results and Analysis
		Table 3 shows the characteristics and performance
		of varying approaches and human performance
	</Extractive Summary>
</Paper ID=ument243>


<Paper ID=ument243> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Ablation of Paragraphs (P) or Questions (Q)
		Many recent studies have suggested the impor-
		tance of measuring the dataset bias by checking
		2397
		Q: What is the most likely reason that I decided to clean the cupboards ?
		P: I cleaned the two large bottom cupboards and threw a ton of old stuff 
		away
	</Abstractive Summary>
	<Extractive Summary> =
		Therefore, we report problem ab-
		lation study in Table 4 using BERT-FT as a sim-
		ple but powerful straw man approach
	</Extractive Summary>
</Paper ID=ument243>


<Paper ID=ument243> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: Knowledge transfer through ﬁne-tuning
	</Abstractive Summary>
	<Extractive Summary> =
		As Table 5
		shows, with direct knowledge transfer, RACE pro-
		vides signiﬁcant beneﬁt than SWAG since COS-
		MOS requires more understanding of the interac-
		tion between paragraph, question and each candi-
		date answer
	</Extractive Summary>
</Paper ID=ument243>


<Paper ID=ument243> <Table ID =6>
	<Abstractive Summary> =
		0%
		Table 6: Generative performance of pre-trained GPT2
		and GPT2-FT on COSMOS QA
	</Abstractive Summary>
</Paper ID=ument243>


<Paper ID=ument243> <Table ID =7>
	<Abstractive Summary> =
		8%
		Table 7: Comparison of the COSMOS QA to other multiple-choice machine reading comprehension datasets: P:
		contextual paragraph, Q: question, A: answers, MC: Multiple-choice, and - means unknown
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows
		comprehensive comparison among the most rele-
		vant datasets
	</Extractive Summary>
</Paper ID=ument243>


<Paper ID=ument244> <Table ID =1>
	<Abstractive Summary> =
		Predicting Loss Target
		Search
		CE
		S(ei,t)
		p(i)
		MSE p�(i|{S(e0)} [ E(i, t), Q, A)
		�p(i)
		MSE p�(i|{S(e0)} [ E(i, t), Q, A)
		�p�(i|E(i, t), Q, A)
		Table 1: The loss functions and prediction targets for three
		learned agents
	</Abstractive Summary>
</Paper ID=ument244>


<Paper ID=ument244> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: RACE and DREAM test accuracy of various judge
		models using the full passage
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows model accuracies,
		which cover a broad range
	</Extractive Summary>
</Paper ID=ument244>


<Paper ID=ument244> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Human evaluation: Search Agents select evidence by querying the speciﬁed judge model, and Learned Agents
		predict the strongest evidence w
	</Abstractive Summary>
	<Extractive Summary> =
		wrong answers (Table 3
		from §4
	</Extractive Summary>
</Paper ID=ument244>


<Paper ID=ument244> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: We train a judge on short RACE passages and test
		its generalization to long passages
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 4 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		Results
		Table 4 shows that BERT-based evi-
		dence agents aid generalization even under do-
		main shift
	</Extractive Summary>
</Paper ID=ument244>


<Paper ID=ument244> <Table ID =5>
	<Abstractive Summary> =
		9
		Table 5: Generalizing to harder questions: We train a judge
		to answer questions with RACE’s Middle School exam ques-
		tions only
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 5 shows that the judge general-
		izes to harder questions better by using evidence
		from either search-based BERT agents (53
	</Extractive Summary>
</Paper ID=ument244>


<Paper ID=ument244> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6: Human accuracy using evidence agent sentences:
		Each agent selects a sentence supporting its own answer
	</Abstractive Summary>
	<Extractive Summary> =
		Humans can answer using evidence sentences
		alone
		Shown in Table 6, humans correctly an-
		swer questions using many fewer sentences (3
	</Extractive Summary>
</Paper ID=ument244>


<Paper ID=ument245> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Experimental results on three OpenQA datasets: Quasar-T, SearchQA and TriviaQA
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents the F1 and Ex-
		act Match (EM) scores of our model and the base-
		line models
	</Extractive Summary>
</Paper ID=ument245>


<Paper ID=ument245> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: The performance of the rankers on the Quasar-
		T and SearchQA
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, we can
		observe that our ranker surpasses the IR model
		with a large margin
	</Extractive Summary>
</Paper ID=ument245>


<Paper ID=ument245> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: The performance of different training selec-
		tion strategies on Quasar-T, SearchQA and TriviaQA
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 3, the performance of RS is much better
		than GT and RK
	</Extractive Summary>
</Paper ID=ument245>


<Paper ID=ument245> <Table ID =4>
	<Abstractive Summary> =
		Table 4: An example from Quasar-T to illustrate the necessity of the paragraph-paragraph relevance
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Case Study
		Table 4 shows an example from Quasar-T
	</Extractive Summary>
</Paper ID=ument245>


<Paper ID=ument245> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Ablation results on Quasar-T
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, we
		evaluate the performance of the ranker accord-
		ing to the precision at 1, 3 and 5
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the ‘-
		SUM’ denotes using MAX to choose the answer
		and the performance drop indicates that SUM can
		further enhance our model
	</Extractive Summary>
</Paper ID=ument245>


<Paper ID=ument246> <Table ID =1>
	<Abstractive Summary> =
		Symbol
		Description
		R, C
		sets of real/complex numbers
		[v]j
		jth component of vector v
		[M]jk
		(j, k)-component of matrix M
		MT
		transpose of M
		M
		conjugate of M
		⊙
		componentwise (Hadamard) product
		∗
		circular convolution
		⋆
		circular correlation
		Re(x)
		real part of complex number x
		diag(v)
		diagonal matrix with main diagonal v
		circ(v)
		circulant matrix determined by v
		⟨x, y, z⟩
		sum of the componentwise products of x, y, z
		F
		discrete Fourier matrix
		E
		set of entities
		R
		set of relations
		F
		set of observed facts (triplets)
		F ∗
		set of ground truth facts
		G(F)
		Knowledge graph induced by facts F
		Table 1: List of symbols
	</Abstractive Summary>
</Paper ID=ument246>


<Paper ID=ument246> <Table ID =2>
	<Abstractive Summary> =
		WN11
		FB13
		Train
		112,581
		316,232
		Base
		Valid
		2,609
		5,908
		Test
		10,544
		23,733
		Train
		2,129,539
		6,266,058
		Path
		Valid
		11,277
		27,163
		Test-Deduction
		24,749
		77,883
		Test-Induction
		21,828
		31,674
		Table 2: Dataset provided by Guu et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the statistics of their dataset
	</Extractive Summary>
</Paper ID=ument246>


<Paper ID=ument246> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Path QA ranking result: Comparing BlockHolE (b = 2, m = 25 and b = 4, m = 25) to other bilinear
		models
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Path QA ranking
		Table 3 shows the results on the path QA ranking
		data
	</Extractive Summary>
</Paper ID=ument246>


<Paper ID=ument246> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Classiﬁcation accuracy on selected queries
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the classiﬁcation ac-
		curacies of ComplEx and BlockHolE (b = 2, m =
		25)
	</Extractive Summary>
</Paper ID=ument246>


<Paper ID=ument247> <Table ID =1>
	<Abstractive Summary> =
		13
		Table 1: Overall comparisons on the test data, where
		“ans loss” represents answer-aware loss
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1, we compare our model with the typ-
		ical baselines on word-overlap based metrics
	</Extractive Summary>
	<Extractive Summary> =
		Especially, incorporat-
		ing answer-aware loss (the last line in Table 1) fur-
		ther improves the performance (+5
	</Extractive Summary>
</Paper ID=ument247>


<Paper ID=ument247> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Performances on predicate identiﬁcation
	</Abstractive Summary>
</Paper ID=ument247>


<Paper ID=ument247> <Table ID =3>
	<Abstractive Summary> =
		25
		Table 3: Performances on answer coverage, where
		“Anscov” denotes the metric of answer coverage
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 reports performances on BLUE4 and
		answer coverage (Anscov)
	</Extractive Summary>
</Paper ID=ument247>


<Paper ID=ument247> <Table ID =4>
	<Abstractive Summary> =
		66
		Table 4: Ablation study by removing the main com-
		ponents, where “w/o” means without, and “w/o diver-
		siﬁed contexts” represents that diversiﬁed contexts are
		replaced by contexts used in Elsahar et al
	</Abstractive Summary>
	<Extractive Summary> =
		Speciﬁcally, the last line
		in Table 4, replacing diversiﬁed contexts with con-
		texts used in Elsahar et al
	</Extractive Summary>
</Paper ID=ument247>


<Paper ID=ument247> <Table ID =5>
	<Abstractive Summary> =
		56
		Table 5: Performances on naturalness
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, Elsahar et al
	</Extractive Summary>
</Paper ID=ument247>


<Paper ID=ument247> <Table ID =6>
	<Abstractive Summary> =
		52
		Table 6: Performances of whether using the pre-trained
		KB embedding by transE
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows that
		the performance of KBQG is degraded without
		TransE embeddings
	</Extractive Summary>
</Paper ID=ument247>


<Paper ID=ument247> <Table ID =7>
	<Abstractive Summary> =
		57
		Table 7: Performances of generated questions for QA
	</Abstractive Summary>
	<Extractive Summary> =
		53 in Table 7)
	</Extractive Summary>
</Paper ID=ument247>


<Paper ID=ument248> <Table ID =1>
	<Abstractive Summary> =
		Speciﬁcally, an innovative pointer-
		equipped semantic parsing model is ﬁrst designed
		for two purposes: 1) built-in pointer network to-
		ward positions of entity mentions in the question
		2443
		Alias
		Operator
		Comments
		A1/2/3
		start → set/num/bool
		A4
		set → ﬁnd(set, p)
		set of entities with a predicate p edge to entity e
		A5
		num → count(set)
		number of distinct elements in the input set
		A6
		bool → in(e, set)
		whether the entity e in set or not
		A7
		set → union(set1, set2)
		set1 ∪ set2
		A8
		set → inter(set1, set2)
		set1 ∩ set2
		A9
		set → diﬀ(set1, set2)
		set1 - set2
		A10
		set → large(set, p, num)
		subset of set linking to more than num entities with predicate p
		A11
		set → less(set, p, num)
		subset of set linking to less than num entities with predicate p
		A12
		set → equal(set, p, num)
		subset of set linking to num entities with predicate p
		A13
		set → argmax(set, p)
		subset of set linking to most entities with predicate p
		A14
		set → argmin(set, p)
		subset of set linking to least entities with predicate p
		A15
		set → ﬁlter(tp, set)
		subset where entity e in set and belong to entity type tp
		A16
		num → u num
		transform number in utterance u num to intermediate number num
		A17
		set → set(e)
		A18/19/20/21
		e/p/tp/u num → constant
		*instantiation for e, p, tp, u num
		Table 1: Brief grammar deﬁnitions for logical form generation
	</Abstractive Summary>
	<Extractive Summary> =
		The grammars are brieﬂy sum-
		marized in Table 1, where each operator consists
		of three components: semantic category, a func-
		tion name, and a list of arguments with speciﬁed
		semantic categories
	</Extractive Summary>
	<Extractive Summary> =
		In each decoding step, the model ﬁrst predicts
		a token from a small decoding vocabulary V(dec)
		= {start, end, e, p, tp, u num, A1, · · · , A21} ,
		where start and end indicate the start and end of
		decoding, A1, · · · , A21 are deﬁned in Table 1, and
		e, p, tp and u num denote entity, predicate, type
		and number entries respectively
	</Extractive Summary>
	<Extractive Summary> =
		Grammar-Guided
		Inference
		The
		grammars
		deﬁned in Table 1 are utilized to ﬁlter illegal op-
		erators out in each decoding step
	</Extractive Summary>
</Paper ID=ument248>


<Paper ID=ument248> <Table ID =2>
	<Abstractive Summary> =
		48%
		Table 2: Comparisons with baselines on CSQA
	</Abstractive Summary>
</Paper ID=ument248>


<Paper ID=ument248> <Table ID =3>
	<Abstractive Summary> =
		26%
		Table 3: Ablation study
	</Abstractive Summary>
	<Extractive Summary> =
		We conducted an ablation study in
		Table 3 for in-depth understanding of their effects
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 3, the F1 score for every question
		type consistently drops in the range of 3% to 14%
		compared with that with multi-task learning
	</Extractive Summary>
	<Extractive Summary> =
		, w/o Both in Table 3), the proposed
		framework will degenerate to a model that is simi-
		lar to Coarse-to-Fine semantic parsing model, an-
		other state-of-the-art KB-QA model over small-
		scale KB (Dong and Lapata, 2018)
	</Extractive Summary>
</Paper ID=ument248>


<Paper ID=ument248> <Table ID =4>
	<Abstractive Summary> =
		7%
		Table 4: Prediction accuracy on each component com-
		posing the pointer-equipped logical form
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, the accu-
		racy for each component of the pointer-equipped
		logical form drops with separate learning
	</Extractive Summary>
</Paper ID=ument248>


<Paper ID=ument248> <Table ID =5>
	<Abstractive Summary> =
		70%
		Table 5: Comparisons with different experimental set-
		tings
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, we ﬁrst replaced the en-
		coder with pre-trained BERT base model (Devlin
		et al
	</Extractive Summary>
</Paper ID=ument248>


<Paper ID=ument249> <Table ID =1>
	<Abstractive Summary> =
		)
		Table 1: Comparison of BiPaR with several existing reading comprehension datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that BiPaR has two
		signiﬁcant differences in comparison with existing
		datasets: (1) each (Passage, Question, Answer)
		triple is bilingual parallel, and (2) passages and
		questions are from novels
	</Extractive Summary>
</Paper ID=ument249>


<Paper ID=ument249> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Statistics on the selected passages ( I: The
		Duke of the Mount Deer /《鹿鼎记》, II: Demi-Gods
		and Semi-Devils /《天龙八部》, III: The Three-Body
		Problem /《三体》, IV: The Great Gatsby /《了不起
		的盖茨比》, V: The Old Man and the Sea /《老人与
		海》, VI: Harry Potter /《哈利波特》)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 pro-
		vides the number of selected passages from each
		novel
	</Extractive Summary>
</Paper ID=ument249>


<Paper ID=ument249> <Table ID =3>
	<Abstractive Summary> =
		The why and how
		questions, which account for considerable propor-
		tions, undoubtedly make BiPaR a very challenging
		Answer Type
		%
		Examples
		Person
		Location
		Date
		Verb phrase
		Yes/No
		Adjective
		Event
		Other proper noun
		Common noun
		Description
		23
		13
		6
		2
		2
		3
		2
		4
		8
		37
		Trinket
		the ﬂoor
		Saturday morning
		wash her face
		-
		jade-green
		Quidditch practice
		a pinch of Floo powder
		secret vault
		wand backﬁred
		Table 3: Statistics on answer types in BiPaR
	</Abstractive Summary>
</Paper ID=ument249>


<Paper ID=ument249> <Table ID =4>
	<Abstractive Summary> =
		Pzh: 哈利一早就被格兰芬多魁地奇队队长奥利弗伍德摇醒了，他本来还想再睡几个小时的。“什一什么事？”
		哈利迷迷糊糊地说。“魁
		魁
		魁地
		地
		地奇
		奇
		奇训
		训
		训练
		练
		练”伍德说。
		Qen-Qzh: Why did Oliver Wood shake Harry awake? / 奥利弗伍德为什么要摇醒哈利？
		17
		Table 4: Question categories and reading comprehension skills covered in BiPaR
	</Abstractive Summary>
	<Extractive Summary> =
		Moreover, we ﬁnd that a large
		number of questions require some descriptive sen-
		tences to answer (37%), which are generally com-
		plete sentences or summary statements, etc (See
		the third example in Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		As the ﬁrst example in Table 4 shows, the ques-
		tion is almost the same to the sentence in passage
	</Extractive Summary>
	<Extractive Summary> =
		As demonstrated in the last ex-
		ample in Table 4, to correctly answer the question,
		we must understand the implicit causality: Quid-
		ditch practice −→ Harry, however, was shaken
		awake several hours earlier than he would have
		liked by Oliver Wood, Captain of the Gryfﬁndor
		Quidditch team
	</Extractive Summary>
</Paper ID=ument249>


<Paper ID=ument249> <Table ID =5>
	<Abstractive Summary> =
		12
		Table 5: Results (EM/F1 score) of models and humans on the development and the test data of BiPaR
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Evaluation Results
		Table 5 presents the results of the models on the
		development and the test data
	</Extractive Summary>
</Paper ID=ument249>


<Paper ID=ument249> <Table ID =6>
	<Abstractive Summary> =
		28
		Table 6: Fine-grained results in terms of different an-
		swer types and question categories on the monolingual
		task
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Analysis of BERT and Human in
		Answering Different BiPaR Questions
		Table 6 presents a ﬁne-grained comparison analy-
		sis of BERT base and human results on the En-
		glish and Chinese monolingual task in terms of
		both answer types and question categories deﬁned
		in Section 4
	</Extractive Summary>
	<Extractive Summary> =
		It can be seen from Table 6
		that the BERT model is worse on multi-sentence
		reasoning and implicit causality than co-reference
		resolution
	</Extractive Summary>
</Paper ID=ument249>


<Paper ID=ument25> <Table ID =1>
	<Abstractive Summary> =
		Train
		Test
		#Sent
		#Ment
		#Sent
		# Ment
		CY
		106,541
		146,524
		1,193
		3,256
		BN
		66,915
		127,932
		870
		2838
		YO
		36,548
		10,405
		77
		232
		MN
		19,250
		27,820
		173
		439
		ARZ
		18,700
		28,928
		195
		377
		Food Domain
		27,798
		32,155
		207
		253
		Drinks
		8,615
		9,218
		62
		67
		Meat
		7,685
		8,841
		53
		68
		Vegetables
		6,155
		7,235
		45
		58
		Condiments
		3,737
		4,084
		27
		30
		Breads
		2,515
		2,777
		24
		30
		Table 1: The statistics of weakly labeled dataset
	</Abstractive Summary>
</Paper ID=ument25>


<Paper ID=ument25> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Performance (%) on low-resource languages
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results on Low-Resource Languages
		Table 2 shows the overall performance of our pro-
		posed model as well as the baseline methods (P
		and R denote Precision and Recall)
	</Extractive Summary>
</Paper ID=ument25>


<Paper ID=ument25> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: F1-score (%) on food domain
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the overall performance in food
		domain, where D, M, V, C and B denote: Drink,
		Meat, Vegetables, Condiments and Breads
	</Extractive Summary>
</Paper ID=ument25>


<Paper ID=ument250> <Table ID =1>
	<Abstractive Summary> =
		3B Words
		Table 1: Language models considered in this study
	</Abstractive Summary>
	<Extractive Summary> =
		For several of our language models
		this also tests their ability to memorize and recall
		sentences from the training data since as the mod-
		els have been trained on Wikipedia (see Table 1)
	</Extractive Summary>
</Paper ID=ument250>


<Paper ID=ument250> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Mean precision at one (P@1) for a frequency baseline (Freq), DrQA, a relation extraction with na¨ıve
		entity linking (REn), oracle entity linking (REo), fairseq-fconv (Fs), Transformer-XL large (Txl), ELMo original
		(Eb), ELMo 5
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		We summarize the main results in Table 2, which
		shows the mean precision at one (P@1) for the dif-
		ferent models across the set of corpora considered
	</Extractive Summary>
</Paper ID=ument250>


<Paper ID=ument250> <Table ID =3>
	<Abstractive Summary> =
		1]
		Table 3: Examples of generation for BERT-large
	</Abstractive Summary>
	<Extractive Summary> =
		As with the
		Google-RE corpus, we manually deﬁne a tem-
		plate for each relation (see Table 3 for some ex-
		amples)
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows randomly picked examples for
		the generation of BERT-large for cloze template
		queries
	</Extractive Summary>
	<Extractive Summary> =
		The lower half of Table 3 shows gen-
		erations by BERT-large for randomly sampled ex-
		amples
	</Extractive Summary>
</Paper ID=ument250>


<Paper ID=ument251> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Example questions from the DROP dataset which require numerical comparison
	</Abstractive Summary>
	<Extractive Summary> =
		For example, in
		Table 1, for the ﬁrst question, if the MRC system
		knows the fact that “49 > 47 > 36 > 31 > 22”,
		it could easily extract that the second longest ﬁeld
		goal is 47-yard
	</Extractive Summary>
	<Extractive Summary> =
		For example, for the second
		question in Table 1, an MRC system needs to
		know which age group made up more than 7% of
		the population to count the group number
	</Extractive Summary>
	<Extractive Summary> =
		2As a number in the question may serve as a critical com-
		parison condition (refer to the second example in Table 1),
		2478
		set of nodes corresponding to the numbers occur-
		ring in question and passage are denoted as V Q
		and V P respectively
	</Extractive Summary>
</Paper ID=ument251>


<Paper ID=ument251> <Table ID =2>
	<Abstractive Summary> =
		42
		Table 2: Overall results on the development and test
		set
	</Abstractive Summary>
</Paper ID=ument251>


<Paper ID=ument251> <Table ID =3>
	<Abstractive Summary> =
		96
		Table 3: Performance with different GNN structure
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, our proposed NumGNN
		leads to statistically signiﬁcant improvements
		compared to traditional GNN on both EM and F1
		scores especially for comparing questions
	</Extractive Summary>
</Paper ID=ument251>


<Paper ID=ument251> <Table ID =4>
	<Abstractive Summary> =
		26-19 = 7
		26-22 = 4
		A: 26-22=4
		Table 4: Cases from the DROP dataset
	</Abstractive Summary>
</Paper ID=ument251>


<Paper ID=ument251> <Table ID =5>
	<Abstractive Summary> =
		40 and
		older
		19 and younger
		Table 5: Typical error examples
	</Abstractive Summary>
	<Extractive Summary> =
		(2) Among the incorrectly answered sort-
		ing/comparison questions, the most ones (26%)
		are those whose golden answers are multiple non-
		adjacent spans (row 1 in Table 5), and the second
		most ones (19%) are those involving comparison
		with an intermediate number that does not literally
		occur in the document/question but has to be de-
		2482
		rived from counting or arithmetic operation (row 1
		in Table 5)
	</Extractive Summary>
</Paper ID=ument251>


<Paper ID=ument252> <Table ID =1>
	<Abstractive Summary> =
		6M
		Table 1: Sentence number we used in pre-training
	</Abstractive Summary>
</Paper ID=ument252>


<Paper ID=ument252> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Test accuracy on the 15 XNLI languages
	</Abstractive Summary>
	<Extractive Summary> =
		We report the results of XNLI in Table 2, by
		comparing our Unicoder model with four base-
		lines: Conneau et al
	</Extractive Summary>
</Paper ID=ument252>


<Paper ID=ument252> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Results on the XQA
	</Abstractive Summary>
	<Extractive Summary> =
		Table
		2
		and Table 3 both show it can bring a signiﬁcant
		boost in cross-lingual language understanding per-
		formance
	</Extractive Summary>
</Paper ID=ument252>


<Paper ID=ument252> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Experiments of ﬁne-tuning on different number of languages
	</Abstractive Summary>
</Paper ID=ument252>


<Paper ID=ument252> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5:
		Accuracy on the XNLI test set of when ﬁne-tuning Unicoder with one language and testing on other
		languages
	</Abstractive Summary>
</Paper ID=ument252>


<Paper ID=ument252> <Table ID =6>
	<Abstractive Summary> =
		3
		Table 6:
		Result of joint ﬁne-tuning two languages
	</Abstractive Summary>
</Paper ID=ument252>


<Paper ID=ument253> <Table ID =1>
	<Abstractive Summary> =
		Gt: who was an advocate of separation of powers?
		Base: who opposed the principle of enlightenment?
		Ours: who advocated the principle in the age of en-
		lightenment?
		Table 1: An examples of the “semantic drift” issue in
		Question Generation (“Gt” is short for “ground truth”)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, the baseline QG
		model generates a question that has almost con-
		trary semantics with the ground-truth question,
		and the generated phrase “the principle of en-
		2496
		lightenment” does not make sense given the con-
		text
	</Extractive Summary>
	<Extractive Summary> =
		2
		Semantics-Reinforced Model
		To address the “semantic drift” problem shown in
		Table 1, we propose two semantics-enhanced re-
		wards to regularize the generation to focus on gen-
		erating semantically valid questions
	</Extractive Summary>
	<Extractive Summary> =
		72
		Table 10:
		The results of our semi-supervised QA
		method using a stronger BERT-QA model
	</Extractive Summary>
	<Extractive Summary> =
		As shown
		in Table 10, though with smaller margins, our
		method improves the strong BERT-QA baseline
		by 1
	</Extractive Summary>
	<Extractive Summary> =
		Du and Cardie (2018): what is the eastern shore of virginia owned by ?
		ELMo-QG: what facility is owned by nasa ?
		BERT-QG: what is the name of the rocket facility located by nasa ?
		Table 11: Some synthetic QA examples generated by our QG models
	</Extractive Summary>
	<Extractive Summary> =
		com/y2y8u5ed
		2509
		C
		Examples
		Table 11 shows some synthetic QA examples gen-
		erated by our QG models
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =2>
	<Abstractive Summary> =
		15
		Table 2: Two examples of where QPP and QAP improve in question quality evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		Second, since multiple
		questions are valid but only one reference exists in
		the dataset, these traditional metrics fail to appro-
		priately score question paraphrases and novel gen-
		eration (shown in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Since paraphrasing is more about semantic sim-
		ilarity than superﬁcial word/phrase matching, it
		treats question paraphrases more fairly (Example
		1 in Table 2)
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =3>
	<Abstractive Summary> =
		37
		Table 3: The performance of different QG models
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Question Generation
		Baselines
		First, as shown in Table 3, our base-
		line QG model obtains a non-trivial improvement
		over previous best QG system (Zhao et al
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =4>
	<Abstractive Summary> =
		For both QA-based QG evalua-
		tion and semi-supervised QA, we follow the stan-
		dard evaluation method for SQuAD to use Exact
		QPP&QAP
		Our baseline
		Tie
		160
		131
		9
		Table 4: Pairwise human evaluation between our base-
		line and QPP&QAP multi-reward model
	</Abstractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =5>
	<Abstractive Summary> =
		22
		Table 5: The QA-based evaluation results for different
		QG systems
	</Abstractive Summary>
	<Extractive Summary> =
		QA-Based Evaluation
		As shown in Table 5, we
		compare three QG systems using QA-based eval-
		uation on three different amounts of synthetic data
		and their corresponding semi-supervised QA se-
		tups (without ﬁlter)
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =6>
	<Abstractive Summary> =
		77
		Table 6: The effect of QAP-based synthetic data ﬁlter
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Semi-Supervised Question Answering
		Effect of the data ﬁlter
		As shown in Table 6,
		when using synthetic data only, adding the data
		ﬁlter can signiﬁcantly improve QA performance
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =7>
	<Abstractive Summary> =
		11
		Table 7: The results of our semi-supervised QA method
		using a BiDAF-QA model
	</Abstractive Summary>
	<Extractive Summary> =
		Semi-Supervised QA results
		Table 7 demon-
		strates the semi-supervised QA results
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =8>
	<Abstractive Summary> =
		83
		Table 8:
		The comparison with the previous semi-
		supervised QA method
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 8, “New Data Size” only counts
		# examples generated from articles outside SQuAD
	</Extractive Summary>
	<Extractive Summary> =
		As shown
		in Table 8, with no or less new data injection,
		our methods achieve larger improvements over a
		stronger baseline than their method
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =9>
	<Abstractive Summary> =
		87
		Table 9: The performance of our stronger BERT-QG models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 shows that our BERT-
		QG baseline improves previous ELMo-QG base-
		line by a large margin; meanwhile, our QPP/QAP
		rewards signiﬁcantly improve the stronger QG
		baseline and achieve the new state-of-the-art QG
		performance w
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =10>
	<Abstractive Summary> =
		72
		Table 10:
		The results of our semi-supervised QA
		method using a stronger BERT-QA model
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 10, though with smaller margins, our
		method improves the strong BERT-QA baseline
		by 1
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument253> <Table ID =11>
	<Abstractive Summary> =
		Du and Cardie (2018): what is the eastern shore of virginia owned by ?
		ELMo-QG: what facility is owned by nasa ?
		BERT-QG: what is the name of the rocket facility located by nasa ?
		Table 11: Some synthetic QA examples generated by our QG models
	</Abstractive Summary>
	<Extractive Summary> =
		com/y2y8u5ed
		2509
		C
		Examples
		Table 11 shows some synthetic QA examples gen-
		erated by our QG models
	</Extractive Summary>
</Paper ID=ument253>


<Paper ID=ument254> <Table ID =1>
	<Abstractive Summary> =
		1)
		Wiki
		87,600
		10,570
		−
		NewsQA
		News
		92,549
		5,166 5,165
		MS MARCO (v1) Web
		82,430
		10,047 9,650
		Table 1: Statistics of the datasets
	</Abstractive Summary>
</Paper ID=ument254>


<Paper ID=ument254> <Table ID =2>
	<Abstractive Summary> =
		47
		Table 2:
		Performance of AdaMRC compared with
		baseline models on three datasets, using SAN as the
		MRC model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the experimental results
	</Extractive Summary>
</Paper ID=ument254>


<Paper ID=ument254> <Table ID =3>
	<Abstractive Summary> =
		84
		Table 3: Performance on DuoRC, adapting from Sel-
		fRC (Wikipedia) to ParaphraseRC (IMDB)
	</Abstractive Summary>
</Paper ID=ument254>


<Paper ID=ument254> <Table ID =4>
	<Abstractive Summary> =
		25
		Table 4: Results of using ELMo and BERT
	</Abstractive Summary>
</Paper ID=ument254>


<Paper ID=ument254> <Table ID =5>
	<Abstractive Summary> =
		97
		Table 5: Semi-supervised domain adaptation experi-
		ment with varied labeling ratio on the target-domain
		dataset
	</Abstractive Summary>
</Paper ID=ument254>


<Paper ID=ument254> <Table ID =6>
	<Abstractive Summary> =
		Answer: one of 19
		GT Question:What was the amount of children murdered?
		Pseudo Question: How many victims were in India?
		Table 6: Examples of generated questions given input paragraphs and answers, comparing with the ground-truth
		human-written questions
	</Abstractive Summary>
</Paper ID=ument254>


<Paper ID=ument255> <Table ID =1>
	<Abstractive Summary> =
		97
		Table 1: Metrics of KEAG and QA models disregard-
		ing knowledge on the MARCO dataset
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Model Comparisons
		Table 1 compares KEAG with the following state-
		of-the-art extractive/generative QA models, which
		do not make use of external knowledge:
		1
	</Extractive Summary>
	<Extractive Summary> =
		gQA (Mitra, 2017): A generative approach
		to question answering by incorporating the
		copying mechanism and the coverage vector
		Table 1 shows the comparison of QA models
		in Rouge-L and Bleu-1
	</Extractive Summary>
</Paper ID=ument255>


<Paper ID=ument255> <Table ID =2>
	<Abstractive Summary> =
		97
		Table 2: Metrics of KEAG and knowledge-enriched
		QA models on the MARCO dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the metrics of KEAG in compar-
		ison to those of the following state-of-the-art QA
		models that are adapted to leveraging knowledge:
		1
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2, it can be clearly observed that
		KEAG performs best with the highest Rouge-L
		and Bleu-1 scores among the knowledge-enriched
		answer generation models
	</Extractive Summary>
</Paper ID=ument255>


<Paper ID=ument255> <Table ID =3>
	<Abstractive Summary> =
		03
		Table 3: Human evaluation of KEAG and state-of-the-
		art answer generation models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 reports the human evaluation scores
		of KEAG and state-of-the-art answer generation
		models
	</Extractive Summary>
</Paper ID=ument255>


<Paper ID=ument255> <Table ID =4>
	<Abstractive Summary> =
		75
		Table 4: Ablation tests of KEAG
	</Abstractive Summary>
</Paper ID=ument255>


<Paper ID=ument255> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Visualization of a sample QA pair and the
		source of individual words in the answer
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 visualizes a sample QA pair from
		KEAG and which source every word in the answer
		is selected from (indicated by the sample value
		of the source selector variable yt)
	</Extractive Summary>
</Paper ID=ument255>


<Paper ID=ument256> <Table ID =1>
	<Abstractive Summary> =
		43% 
		Table 1: Discriminative performance of the models 
		on the adapted SQuAD 2
	</Abstractive Summary>
</Paper ID=ument256>


<Paper ID=ument256> <Table ID =2>
	<Abstractive Summary> =
		Figure 3: Applying discriminators for a candidate 
		answer and ranking the documents 
		Dataset 
		Train 
		Dev 
		Test 
		ARC Easy 
		2,251 
		570 
		2,376 
		ARC Challenge 
		1,119 
		299 
		1,172 
		Table 2: Number of questions in the ARC dataset 
		 
		 
		Model 
		Accuracy 
		Random guess 
		25
	</Abstractive Summary>
	<Extractive Summary> =
		4 
		Results 
		We report our model performance on two of the 
		most important multiple-choice science questions 
		datasets: ARC Easy and ARC Challenge (Table 2)
	</Extractive Summary>
</Paper ID=ument256>


<Paper ID=ument256> <Table ID =3>
	<Abstractive Summary> =
		30% 
		Table 3: Results on ARC Easy test  
		 
		 
		Model 
		Accuracy 
		Random guess 
		25
	</Abstractive Summary>
	<Extractive Summary> =
		A summary of the results is 
		reported in Table 3 and Table 4
	</Extractive Summary>
</Paper ID=ument256>


<Paper ID=ument256> <Table ID =4>
	<Abstractive Summary> =
		72% 
		Table 4: Results on ARC Challenge test  
		 
		 
		Dataset 
		TFD 
		+DRD 
		+AVD 
		ARC Easy 
		63
	</Abstractive Summary>
</Paper ID=ument256>


<Paper ID=ument256> <Table ID =5>
	<Abstractive Summary> =
		72% 
		Table 5: The impact of adding more discriminators 
		on the test set accuracy 
		 
		 
		2538
		 
		 
		Second, we verified the model performance on 
		the ARC test sets in order to check how the model 
		generalizes on unseen data and to compare it with 
		other top models in the ARC public leaderboard 
		(https://leaderboard
	</Abstractive Summary>
	<Extractive Summary> =
		The results are revealed in 
		Table 5 where we measured the total number of  
		questions correctly answered in the ARC Easy and 
		ARC Challenge test sets
	</Extractive Summary>
</Paper ID=ument256>


<Paper ID=ument256> <Table ID =6>
	<Abstractive Summary> =
		92) 
		Table 6: Downstream model performance on  
		the ARC Challenge dataset comparing ranking with 
		Attentive Ranker vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows that using documents ranked by 
		our attentive neural network always leads to a 
		performance increase in downstream models, 
		compared to TF-IDF
	</Extractive Summary>
</Paper ID=ument256>


<Paper ID=ument256> <Table ID =7>
	<Abstractive Summary> =
		15) 
		Table 7: Downstream model performance on  
		the ARC Challenge dataset comparing ranking with 
		Attentive Ranker vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 
		illustrates the performance of a downstream model 
		(BERT) in answering the questions when fed with 
		top 𝑁 (1 or 10) documents as ranked by Attentive 
		Ranker or doc2vec
	</Extractive Summary>
</Paper ID=ument256>


<Paper ID=ument257> <Table ID =1>
	<Abstractive Summary> =
		26
		Table 1: Results on the NarrativeQA Test set In contrast, using Or-
		acle (Ours), described in Section 4, we report a
		+11 Rouge-L score improvement (Table 1: This
		work)
	</Abstractive Summary>
	<Extractive Summary> =
		We compare the results of different models
		using overall results (Table 1) on the dataset, but
		also the performance for different question types
		(Figure 4) and context sizes (Figure 5)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Overall Results
		Table 1 compares our baselines and proposed
		model to prior work
	</Extractive Summary>
	<Extractive Summary> =
		In
		the last section of Table 1 we present the results
		of our experiments (This work)
	</Extractive Summary>
</Paper ID=ument257>


<Paper ID=ument257> <Table ID =2>
	<Abstractive Summary> =
		MRU is a com-
		positional encoder that splits the context tokens
		Conﬁg
		DR-E
		DR-NE
		SRL
		Coref
		No
		QANet (baseline)
		-
		-
		-
		-
		8
		DR (All)
		2
		2
		-
		-
		4
		DR (Exp)
		2
		-
		-
		-
		6
		DR (NonE)
		-
		2
		-
		-
		6
		Coref
		-
		-
		-
		3
		5
		SRL
		-
		-
		3
		-
		5
		SRL+ DR (Exp)
		2
		-
		3
		-
		3
		SRL + DR (NonE)
		-
		2
		3
		-
		3
		SRL + DR (All)
		2
		2
		3
		-
		1
		SRL + DR (Exp) + Coref
		2
		-
		3
		1
		2
		SRL + DR (All) + Coref
		2
		2
		3
		1
		4
		Table 2: The number of attention heads by discourse-
		semantic type
	</Abstractive Summary>
</Paper ID=ument257>


<Paper ID=ument258> <Table ID =1>
	<Abstractive Summary> =
		6
		Test set
		Table 1: Results of systems on HOTPOTQA
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 1, with the proposed hi-
		erarchical system design, the whole pipeline sys-
		4Results can also be found at the leaderboard websites
		for the two datasets: https://hotpotqa
	</Extractive Summary>
	<Extractive Summary> =
		Predicted Answer:
		Drifting
		Table 10: HotpotQA correct prediction with sufﬁcient evidence
	</Extractive Summary>
	<Extractive Summary> =
		Predicted Answer:
		1945 and 1969
		Table 11: HotpotQA incorrect prediction with insufﬁcient/wrong evidence
	</Extractive Summary>
	<Extractive Summary> =
		(Ian Harland, 2) After two years as a schoolmaster at Sunningdale School he studied
		for the priesthood at Wycliffe Hall, Oxford and be
		Predicted Answer:
		Wycliffe Hall , Oxford
		Table 12: HotpotQA incorrect prediction caused by extra incorrect information
	</Extractive Summary>
	<Extractive Summary> =
		Predicted Label:
		REFUTES
		Table 13: FEVER correct prediction with sufﬁcient evidence
		Claim:
		Azithromycin is available as a generic curtain
	</Extractive Summary>
	<Extractive Summary> =
		Predicted Label:
		NOT ENOUGH INFO
		Table 14: FEVER incorrect prediction due to insufﬁcient evidence
		2566
		Claim:
		University of Chicago Law School is ranked fourth by Brain Leiter on the ”Top 15 Schools”
		From Which the Most ‘Prestigious ´Law Firms Hire New Lawyers
	</Extractive Summary>
	<Extractive Summary> =
		Predicted Label:
		NOT ENOUGH INFO
		Table 15: FEVER incorrect prediction due to extra wrong evidence
	</Extractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =2>
	<Abstractive Summary> =
		26
		Test set
		Table 2: Performance of systems on FEVER
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =3>
	<Abstractive Summary> =
		71
		-
		-
		Table 3: Ablation over the paragraph-level and sentence-level neural retrieval sub-modules on HOTPOTQA
	</Abstractive Summary>
	<Extractive Summary> =
		Results:
		Table 3 and 4 shows the ablation re-
		sults for the two neural retrieval modules at both
		paragraph and sentence level on HOTPOTQA and
		FEVER
	</Extractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4:
		Ablation over the paragraph-level and sentence-level neural retrieval sub-modules on FEVER
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5:
		System performance on different answer
		types
	</Abstractive Summary>
	<Extractive Summary> =
		It is
		also interesting to note that the model performs
		the best in Yes/No questions as shown in Table 5,
		reaching an accuracy of 70
	</Extractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =6>
	<Abstractive Summary> =
		5]
		QA
		32
		5
		-
		-
		Veriﬁcation
		32
		5
		-
		-
		Table 6: Hyper-parameter selection for the full pipeline
		system
	</Abstractive Summary>
	<Extractive Summary> =
		A; Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		The
		hyper-parameters search space is shown in Table 6
		and the learning rate was set to 10−5 in all experi-
		ments
	</Extractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =7>
	<Abstractive Summary> =
		61
		Table 7: Detailed Results of downstream sentence-level retrieval and question answering with different values of
		hs on HOTPOTQA
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =8>
	<Abstractive Summary> =
		51
		Table 8: Results with different hs on FEVER
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =9>
	<Abstractive Summary> =
		98
		Table 9: Detailed Results of downstream sentence-level retrieval and question answering with different values of
		kp on HOTPOTQA
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =10>
	<Abstractive Summary> =
		Predicted Answer:
		Drifting
		Table 10: HotpotQA correct prediction with sufﬁcient evidence
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =11>
	<Abstractive Summary> =
		Predicted Answer:
		1945 and 1969
		Table 11: HotpotQA incorrect prediction with insufﬁcient/wrong evidence
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =12>
	<Abstractive Summary> =
		(Ian Harland, 2) After two years as a schoolmaster at Sunningdale School he studied
		for the priesthood at Wycliffe Hall, Oxford and be
		Predicted Answer:
		Wycliffe Hall , Oxford
		Table 12: HotpotQA incorrect prediction caused by extra incorrect information
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =13>
	<Abstractive Summary> =
		Predicted Label:
		REFUTES
		Table 13: FEVER correct prediction with sufﬁcient evidence
		Claim:
		Azithromycin is available as a generic curtain
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =14>
	<Abstractive Summary> =
		Predicted Label:
		NOT ENOUGH INFO
		Table 14: FEVER incorrect prediction due to insufﬁcient evidence
		2566
		Claim:
		University of Chicago Law School is ranked fourth by Brain Leiter on the ”Top 15 Schools”
		From Which the Most ‘Prestigious ´Law Firms Hire New Lawyers
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument258> <Table ID =15>
	<Abstractive Summary> =
		Predicted Label:
		NOT ENOUGH INFO
		Table 15: FEVER incorrect prediction due to extra wrong evidence
	</Abstractive Summary>
</Paper ID=ument258>


<Paper ID=ument259> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: PubMedQA dataset statistics
	</Abstractive Summary>
</Paper ID=ument259>


<Paper ID=ument259> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Examples of automatically generated instances for PQA-A
	</Abstractive Summary>
</Paper ID=ument259>


<Paper ID=ument259> <Table ID =3>
	<Abstractive Summary> =
		]
		Table 3: Summary of PubMedQA question types, reasoning types and whether there are text descriptions of the
		statistics in context
	</Abstractive Summary>
</Paper ID=ument259>


<Paper ID=ument259> <Table ID =4>
	<Abstractive Summary> =
		19
		Table 4: Human performance (single-annotator)
	</Abstractive Summary>
	<Extractive Summary> =
		(2019) show that an
		ensemble of annotators perform signiﬁcantly bet-
		ter than single-annotator, so the results reported in
		Table 4 are the lower bounds of human perfor-
		mance
	</Extractive Summary>
</Paper ID=ument259>


<Paper ID=ument259> <Table ID =5>
	<Abstractive Summary> =
		72†
		Table 5: Main results on PQA-L test set under reasoning-required setting
	</Abstractive Summary>
	<Extractive Summary> =
		3 results in Table 7 are
		better than the performance in Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		While the ablation
		study in Table 5 clearly shows that Phase II is help-
		ful, performance in Phase II doesn’t necessarily
		correlate with ﬁnal performance on PQA-L
	</Extractive Summary>
</Paper ID=ument259>


<Paper ID=ument259> <Table ID =6>
	<Abstractive Summary> =
		76
		Table 6: Results of Phase I (eq
	</Abstractive Summary>
	<Extractive Summary> =
		2 re-
		sults in Table 7 are better than the performance in
		Table 6; for PQA-L, Eq
	</Extractive Summary>
</Paper ID=ument259>


<Paper ID=ument259> <Table ID =7>
	<Abstractive Summary> =
		50
		Table 7: Bootstrapping results
	</Abstractive Summary>
	<Extractive Summary> =
		2 re-
		sults in Table 7 are better than the performance in
		Table 6; for PQA-L, Eq
	</Extractive Summary>
	<Extractive Summary> =
		3 results in Table 7 are
		better than the performance in Table 5)
	</Extractive Summary>
</Paper ID=ument259>


<Paper ID=ument259> <Table ID =8>
	<Abstractive Summary> =
		04
		Table 8: Phase II results (eq
	</Abstractive Summary>
</Paper ID=ument259>


<Paper ID=ument26> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: Dataset Statistics
	</Abstractive Summary>
</Paper ID=ument26>


<Paper ID=ument26> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: In-domain Performance Comparison on the
		AIDA-B Dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes results on the AIDA-
		B dataset, and shows that DCA-based models
		achieve the highest in-KB accuracy and outper-
		forms the previous state-of-the-art neural system
		by near 1
	</Extractive Summary>
</Paper ID=ument26>


<Paper ID=ument26> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Performance Comparison on Cross-domain Datasets using F1 score (%)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results on the ﬁve cross-
		domain datasets
	</Extractive Summary>
</Paper ID=ument26>


<Paper ID=ument26> <Table ID =4>
	<Abstractive Summary> =
		76
		Table 4: Ablation Study on Neighbor Entities
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the effectiveness
		of this strategy
	</Extractive Summary>
</Paper ID=ument26>


<Paper ID=ument26> <Table ID =5>
	<Abstractive Summary> =
		76
		Table 5: Study on Different Attention Mechanisms
		on the Dynamic Context
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the performance comparison
		by replacing the attention module described in
		Section 3 with different variants
	</Extractive Summary>
</Paper ID=ument26>


<Paper ID=ument260> <Table ID =1>
	<Abstractive Summary> =
		Question + answer text
		Justiﬁcation set
		Animal cells obtain energy
		1) obtain water and nutrient by
		by || absorbing nutrients
		absorbing them directly into
		plant cell
		2) the animal obtain nourish-
		ment by absorbing nutrient
		released by symbiotic bacteria
		Table 1: Example of a justiﬁcation set in ARC which was
		scored by annotator with a precision of 1
		2 because the ﬁrst jus-
		tiﬁcation sentence is not relevant, and a coverage of 1
		2 because
		the link between nourishment and energy is not covered
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 illustrates these scores with an
		actual output from ARC
	</Extractive Summary>
</Paper ID=ument260>


<Paper ID=ument260> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Performance on the MultiRC dataset, under various conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 reports comprehensive results on
		MultiRC, including both overall QA performance,
		measured using F1m, F1a, and EM0, as well as
		justiﬁcation quality, measured using standard preci-
		sion (P), recall (R), and F1
	</Extractive Summary>
	<Extractive Summary> =
		(3) The results of the parametric forms of ROCC
		(rows 16 – 19 in Table 2 and rows 17 – 20 in
		Table 3) indicate that performance continues to
		increase until k = 4 in MultiRC and k = 3 in
		ARC
	</Extractive Summary>
	<Extractive Summary> =
		row 23 in Table 2, and row 23 vs
	</Extractive Summary>
</Paper ID=ument260>


<Paper ID=ument260> <Table ID =3>
	<Abstractive Summary> =
		50
		Table 3: Performance on the ARC dataset, under various conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3
		lists equivalent results on ARC
	</Extractive Summary>
	<Extractive Summary> =
		(3) The results of the parametric forms of ROCC
		(rows 16 – 19 in Table 2 and rows 17 – 20 in
		Table 3) indicate that performance continues to
		increase until k = 4 in MultiRC and k = 3 in
		ARC
	</Extractive Summary>
</Paper ID=ument260>


<Paper ID=ument260> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Domain robustness of the non-parametric ROCC vs
	</Abstractive Summary>
</Paper ID=ument260>


<Paper ID=ument260> <Table ID =5>
	<Abstractive Summary> =
		81
		Table 5: Ablation study, removing different compo-
		nents of ROCC
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Ablation Analysis
		Table 5 shows an ablation of the different compo-
		nents of ROCC
	</Extractive Summary>
</Paper ID=ument260>


<Paper ID=ument260> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: Justiﬁcation selection performance of Auto-
		ROCC on different types of questions, in the MultiRC
		development dataset
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 6, AutoROCC achieves higher recall scores
		on Verbatim questions, where the answer text is
		likely to appear within the given justiﬁcation pas-
		sage, and worse recall on question types where
		such overlap does not exist, e
	</Extractive Summary>
</Paper ID=ument260>


<Paper ID=ument260> <Table ID =7>
	<Abstractive Summary> =
		2
		Table 7:
		Justiﬁcation selection performance of the
		ROCC conﬁguration that uses lexical match (BM25) to
		retrieve candidate justiﬁcations (Lexical ROCC), com-
		pared against a ROCC variant that uses the semantic
		alignment approach of Yadav et al
	</Abstractive Summary>
	<Extractive Summary> =
		7
		As shown in Table 7, the alignment-based ROCC
		indeed performs better than the ROCC that relies
		on lexical match
	</Extractive Summary>
</Paper ID=ument260>


<Paper ID=ument261> <Table ID =1>
	<Abstractive Summary> =
		2594
		Question
		Hop 1 Oracle
		Hop 2 Oracle
		What government position was held by the woman who portrayed
		Corliss Archer in the ﬁlm Kiss and Tell?
		Corliss Archer in the
		ﬁlm Kiss and Tell
		Shirley Temple
		Scott Parkin has been a vocal critic of Exxonmobil and another cor-
		poration that has operations in how many countries?
		Scott Parkin
		Halliburton
		Are Giuseppe Verdi and Ambroise Thomas both Opera composers?
		Giuseppe Verdi
		Ambroise Thomas
		Table 1: Example oracle queries on the HOTPOTQA dev set
	</Abstractive Summary>
</Paper ID=ument261>


<Paper ID=ument261> <Table ID =2>
	<Abstractive Summary> =
		13
		Table 2: End-to-end QA performance of baselines and our GOLDEN Retriever model on the HOTPOTQA fullwiki
		test set
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2,
		GOLDEN Retriever is much better at locating the
		correct supporting facts from Wikipedia compared
		2597
		Setting
		Ans F1
		Sup F1
		R@10∗
		GOLDEN Retriever
		49
	</Extractive Summary>
</Paper ID=ument261>


<Paper ID=ument261> <Table ID =3>
	<Abstractive Summary> =
		71
		Table 3:
		Question answering and IR performance
		amongst different IR settings on the dev set
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3,
		replacing the hand-engineered IR engine in (Yang
		et al
	</Extractive Summary>
</Paper ID=ument261>


<Paper ID=ument261> <Table ID =4>
	<Abstractive Summary> =
		18
		Table 4: Pipeline ablative analysis of GOLDEN Re-
		triever end-to-end QA performance by replacing each
		query generator with a query oracle
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 4, replacing
		G1 with the oracle only slightly improves end-to-
		end performance, but further substituting G2 with
		the oracle yields a signiﬁcant improvement
	</Extractive Summary>
</Paper ID=ument261>


<Paper ID=ument261> <Table ID =5>
	<Abstractive Summary> =
		In some cases, the model se-
		lects spans that are more natural and informative
		2598
		Question
		Predicted q1
		Predicted q2
		(1)
		What video game character did the voice actress in the
		animated ﬁlm Alpha and Omega voice?
		voice actress in the animated ﬁlm
		Alpha and Omega (animated ﬁlm
		Alpha and Omega voice)
		Hayden Panettiere
		(2)
		What song was created by the group consisting of Jef-
		frey Jey, Maurizio Lobina and Gabry Ponte and released
		on 15 January 1999?
		Jeffrey Jey (group consisting of
		Jeffrey Jey, Maurizio Lobina and
		Gabry Ponte)
		Gabry Ponte and released
		on 15 January 1999 (“Blue
		(Da Ba Dee)”)
		(3)
		Yau Ma Tei North is a district of a city with how many
		citizens?
		Yau Ma Tei North
		Yau Tsim Mong District of
		Hong Kong (Hong Kong)
		(4)
		What company started the urban complex development
		that included the highrise building, The Harmon?
		highrise building, The Harmon
		CityCenter
		Table 5: Examples of predicted queries from the query generators on the HOTPOTQA dev set
	</Abstractive Summary>
	<Extractive Summary> =
		(Example (1) in Table 5)
	</Extractive Summary>
</Paper ID=ument261>


<Paper ID=ument261> <Table ID =6>
	<Abstractive Summary> =
		83
		Table 6: Span prediction and IR performance of the
		query generator models for Hop 1 (G1) and Hop 2 (G2)
		evaluated separately on the HOTPOTQA dev set
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 6, the perfor-
		mance of G2 is worse than that of G1 in gen-
		eral, conﬁrming our ﬁndings on the end-to-end
		pipeline
	</Extractive Summary>
</Paper ID=ument261>


<Paper ID=ument261> <Table ID =7>
	<Abstractive Summary> =
		55
		Table 7: IR performance (recall in percentages) of var-
		ious Elasticsearch setups on the HOTPOTQA dev set
		using the original question
	</Abstractive Summary>
	<Extractive Summary> =
		In
		Table 7, we also provide results from the single-
		hop query to show the improvement from title
		score boosting introduced from the previous sec-
		tion and reranking
	</Extractive Summary>
</Paper ID=ument261>


<Paper ID=ument261> <Table ID =8>
	<Abstractive Summary> =
		45
		Table 8: Hyperparameter settings for the query gener-
		ators
	</Abstractive Summary>
</Paper ID=ument261>


<Paper ID=ument262> <Table ID =1>
	<Abstractive Summary> =
		When
		paired with a CopyNet Denoising autoencoder,
		we observed similar negligible decrease in the
		2609
		Table 1: % of well-formed, generated SQL queries using a Seq2Seq/CopyNet & after denoising autoencoder
		models
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Well-Formed SQL Statements
		Table 1 shows the change in the number of SQL
		statements determined as well-formed by the SQL
		parser, after going through our denosing autoen-
		coder module trained with a Seq2Seq and a Copy-
		Net model
	</Extractive Summary>
</Paper ID=ument262>


<Paper ID=ument262> <Table ID =2>
	<Abstractive Summary> =
		P1
		P2
		P3
		Top 3 (%)
		Seq2Seq
		CopyNet
		Seq2Seq
		CopyNet
		Seq2Seq
		CopyNet
		Seq2Seq
		CopyNet
		Seq2Seq (Token)
		47 → 66
		47 → 50
		37 → 49
		37 → 40
		33 → 52
		33 → 35
		59 → 80
		59 → 64
		CopyNet (Token)
		49 → 65
		49 → 51
		40 → 54
		40 → 40
		33 → 52
		33 → 33
		61 → 80
		61 → 64
		Seq2Seq (Char)
		31 → 74
		31 → 35
		29 → 64
		29 → 31
		33 → 63
		33 → 33
		40 → 79
		40 → 43
		CopyNet (Char)
		29 → 35
		29 → 10
		27 → 34
		27 → 15
		32 → 32
		32 → 14
		40 → 45
		40 → 31
		Table 2: Negligible change in mean SQL-BLEU score
		& changes in delta after Seq2Seq denoising in ()
	</Abstractive Summary>
</Paper ID=ument262>


<Paper ID=ument262> <Table ID =3>
	<Abstractive Summary> =
		27
		Table 3: Negligible change in mean SQL-BLEU score
		& changes in delta after CopyNet denoising in ()
	</Abstractive Summary>
</Paper ID=ument262>


<Paper ID=ument262> <Table ID =4>
	<Abstractive Summary> =
		27
		Table 4: Negligible change in mean Canonical-BLEU
		score & changes in delta after Seq2Seq denoising in ()
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Mean Canonical-BLEU
		Table 4 and 5 show the change in the mean
		Canonical-BLEU scores on the ﬁrst three gener-
		ated SQL statements, after using denoising au-
		toencoders trained with a Seq2Seq and a CopyNet
		model respectively
	</Extractive Summary>
</Paper ID=ument262>


<Paper ID=ument262> <Table ID =5>
	<Abstractive Summary> =
		24
		Table 5: Negligible change in mean Canonical-BLEU
		score & changes in delta after CopyNet denoising in ()
	</Abstractive Summary>
</Paper ID=ument262>


<Paper ID=ument262> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Generated examples
		NL
		how to copy or update from one table to
		another table
		GT
		- - 1 ) insert new record insert into
		old table ( id , column ) select n
	</Abstractive Summary>
	<Extractive Summary> =
		The ﬁrst example in Table 6 illus-
		trates this problem
	</Extractive Summary>
	<Extractive Summary> =
		3
		Qualitative Analysis
		Table 6 shows the output examples generated by
		our models in the unseen data (the test set), in-
		cluding the denoised version
	</Extractive Summary>
</Paper ID=ument262>


<Paper ID=ument263> <Table ID =1>
	<Abstractive Summary> =
		php?q=5
		2619
		Table 1: Datasets and implementation details
		LC-QuAD
		QALD-5
		No
	</Abstractive Summary>
</Paper ID=ument263>


<Paper ID=ument263> <Table ID =2>
	<Abstractive Summary> =
		com/dice-group/NLIWOD
		Table 2: Average F1-scores of query generation
		LC-QuAD
		QALD-5
		Sina (Shekarpour et al
	</Abstractive Summary>
</Paper ID=ument263>


<Paper ID=ument263> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Average F1-scores for complex questions
		LC-QuAD
		QALD-5
		CompQA
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Furthermore, as the
		results shown in Table 3, it gained a more sig-
		niﬁcant improvement on complex questions com-
		pared with CompQA
	</Extractive Summary>
</Paper ID=ument263>


<Paper ID=ument263> <Table ID =4>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		As the results shown in Table 4, the full ver-
		2620
		Table 4: Average F1-scores for different settings
		LC-QuAD
		QALD-5
		SubQG
		0
	</Extractive Summary>
</Paper ID=ument263>


<Paper ID=ument263> <Table ID =5>
	<Abstractive Summary> =
		055
		Table 5: Accuracy of query substructure prediction
		LC-QuAD
		QALD-5
		BiLSTM w/ attention
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the accuracy of some alternative
		networks for query substructure prediction (Sec-
		tion 3
	</Extractive Summary>
</Paper ID=ument263>


<Paper ID=ument263> <Table ID =6>
	<Abstractive Summary> =
		The main reason is that, for
		Table 6: Average Precision@k scores of query genera-
		tion on LC-QuAD with noisy linking
		Precision@1
		Precision@5
		Gold standard
		0
	</Abstractive Summary>
</Paper ID=ument263>


<Paper ID=ument264> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Statistics of the datasets
	</Abstractive Summary>
</Paper ID=ument264>


<Paper ID=ument264> <Table ID =2>
	<Abstractive Summary> =
		Scores are ranked across all test
		set and Mean Average Precision (MAP) is used as
		Table 2: Fact prediction MAP (%)
	</Abstractive Summary>
</Paper ID=ument264>


<Paper ID=ument264> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Link Prediction MAP
	</Abstractive Summary>
	<Extractive Summary> =
		LP also uses MAP as the evaluation met-
		ric, and the detailed results are shown in Table 3
		1
	</Extractive Summary>
</Paper ID=ument264>


<Paper ID=ument264> <Table ID =4>
	<Abstractive Summary> =
		It shows that GAT is capable of pay-
		2630
		Table 4: Top 5 frequent paths found for capitalOf (FB15K-237) and athletePlaysInLeague (NELL-995) using
		two methods
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the top 5 frequent
		paths and their frequencies for all the methods
	</Extractive Summary>
</Paper ID=ument264>


<Paper ID=ument264> <Table ID =5>
	<Abstractive Summary> =
		6%)
		Table 5: Samples displaying which neighbor does the model pays the most attention to under certain relation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 displays the
		examples
	</Extractive Summary>
</Paper ID=ument264>


<Paper ID=ument265> <Table ID =1>
	<Abstractive Summary> =
		15
		Table 1: The Statistics of NBAtransactions
	</Abstractive Summary>
</Paper ID=ument265>


<Paper ID=ument265> <Table ID =2>
	<Abstractive Summary> =
		9991
		Table 2: Model performance on NBAtransactions
	</Abstractive Summary>
</Paper ID=ument265>


<Paper ID=ument265> <Table ID =3>
	<Abstractive Summary> =
		9991
		Table 3: Results of the ablation test, where -text indicates removing the text-based attention mechanism from
		GUpdater
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 3, compared to R-GAT, both
		GUpdater-shortcut (GUpdater without shortcut)
		and GUpdater-text (GUpdater without text-based
		attention) improve the overall performance with
		different levels, showing that both the text-based
		attention and the shortcuts are helpful for this task
	</Extractive Summary>
</Paper ID=ument265>


<Paper ID=ument265> <Table ID =4>
	<Abstractive Summary> =
		, 2017)
		extends GCN by bringing attention mechanism
		2640
		Gtext
		G′
		text
		Selected Shortcuts
		Results in
		Text-Subgraph
		T1
		T2
		T3
		wrong predictions:
		(TS, player, Jazz)
		(TS, teammate, Pacers)
		T4
		T5
		missing triples:
		(VK, teammate, BM)
		(TE, teammate, BM)
		Table 4: Examples of visualization for the attention on shortcuts
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Visualization of Attention on Shortcuts
		In order to explore how the information actually
		passes along shortcuts, we select 5 different types
		of trades, as shown in Table 4, and we get the cor-
		responding attention weight on each shortcut in
		2639
		Accuracy
		Precision
		Recall
		F1
		Added Acc
		Deleted Acc
		Unchanged Acc
		Text-Subgraphs
		GUpdater
		0
	</Extractive Summary>
	<Extractive Summary> =
		, each team
		plays the same role in the trade (T2, T3 in Table 4),
		the selection of the central team seems to be arbi-
		trary, while for asymmetric trades (T4, T5), the
		central teams are the teams that involved in more
		transactions
	</Extractive Summary>
</Paper ID=ument265>


<Paper ID=ument266> <Table ID =1>
	<Abstractive Summary> =
		# Facts
		# Tasks
		NELL-995
		75,492
		200
		154,213
		12
		FB15K-237
		14,505
		237
		310,116
		20
		Table 1: Statistics of the datasets
	</Abstractive Summary>
</Paper ID=ument266>


<Paper ID=ument266> <Table ID =2>
	<Abstractive Summary> =
		339‡
		Table 2: Overall results on NELL-995 and FB15K-237
	</Abstractive Summary>
	<Extractive Summary> =
		From the results shown in Table 2, we can ob-
		serve that our framework produces consistent im-
		provements for the two RL-based methods under
		varying degrees on both link prediction and fac-
		t prediction tasks
	</Extractive Summary>
</Paper ID=ument266>


<Paper ID=ument266> <Table ID =3>
	<Abstractive Summary> =
		893
		Table 3: Results of link prediction decomposed over different query relations on NELL-995
	</Abstractive Summary>
</Paper ID=ument266>


<Paper ID=ument266> <Table ID =4>
	<Abstractive Summary> =
		468
		Table 4: Results of different imitation learning settings
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, we compare the
		average path number of Pnew and the reasoning
		performance on the two prediction tasks
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Figure 2 and Table 4, we can ob-
		serve that our path-based GAIL method obtains
		more evidential paths for most query relations and
		achieves better performance on both link and fac-
		t predictions, which validates the effectiveness of
		our path-based GAIL method and the rationality
		of encouraging the agent to ﬁnd more diverse evi-
		dential paths
	</Extractive Summary>
</Paper ID=ument266>


<Paper ID=ument266> <Table ID =5>
	<Abstractive Summary> =
		572
		Table 5: Ablation on different components
	</Abstractive Summary>
</Paper ID=ument266>


<Paper ID=ument267> <Table ID =1>
	<Abstractive Summary> =
		1K
		17
		12
		24K
		Quora
		384K
		10K
		10K
		12
		12
		107K
		Table 1:
		Statistics of datasets: SNLI, SciTail and
		Quora
	</Abstractive Summary>
</Paper ID=ument267>


<Paper ID=ument267> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Comparative results with previous models on
		SNLI dataset
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, our single OSOA-DFN achieves
		88
	</Extractive Summary>
</Paper ID=ument267>


<Paper ID=ument267> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Comparative results with previous models on
		SciTail dataset
	</Abstractive Summary>
</Paper ID=ument267>


<Paper ID=ument267> <Table ID =4>
	<Abstractive Summary> =
		03
		Table 4: Comparative results with previous models on
		Quora dataset
	</Abstractive Summary>
</Paper ID=ument267>


<Paper ID=ument267> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5:
		Effect of original semantics-oriented cross
		sentence attention on SciTail dataset
	</Abstractive Summary>
</Paper ID=ument267>


<Paper ID=ument267> <Table ID =6>
	<Abstractive Summary> =
		4
		Table 6: Effect of cross sentence attention layers on
		SciTail dataset
	</Abstractive Summary>
</Paper ID=ument267>


<Paper ID=ument267> <Table ID =7>
	<Abstractive Summary> =
		2
		Table 7: Effect of components on SciTail dataset
	</Abstractive Summary>
</Paper ID=ument267>


<Paper ID=ument268> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Statistics of datasets
		Dataset
		#Rel
		#Ent
		#Train
		#Valid
		#Test
		WN18
		18
		40,943
		141,442
		5,000
		5,000
		FB15K
		1345
		14,951
		483,142
		50,000
		59,071
		4
	</Abstractive Summary>
</Paper ID=ument268>


<Paper ID=ument268> <Table ID =2>
	<Abstractive Summary> =
		This indicates that longer paths hardly con-
		tain more useful information and it is unnecessary
		2668
		Table 2: Evaluation results on link prediction
		Model
		WN18
		FB15K
		Mean Rank
		Hits@10(%)
		Mean Rank
		Hits@10(%)
		Raw
		Filtered
		Raw
		Filtered
		Raw
		Filtered
		Raw
		Filtered
		SE
		1011
		985
		68
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 2 shows the performances of different meth-
		ods on the link prediction task according to vari-
		ous metrics
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2 we could observe that:
		(1)
		PTransE performs better than its basic model
		TransE, and RPE outperforms its original method
		TransR
	</Extractive Summary>
</Paper ID=ument268>


<Paper ID=ument268> <Table ID =3>
	<Abstractive Summary> =
		Thus, experimental results of distinguish-
		ing the four different relation types have also been
		2669
		Table 3: Filtered evaluation results on FB15K by mapping properties of relations(%)
		Tasks
		Predicting Head Entities (Hits@10)
		Predicting Tail Entities (Hits@10)
		Relation Category
		1-to-1
		1-to-N
		N-to-1
		N-to-N
		1-to-1
		1-to-N
		N-to-1
		N-to-N
		SE
		35
	</Abstractive Summary>
</Paper ID=ument268>


<Paper ID=ument269> <Table ID =1>
	<Abstractive Summary> =
		61
		Table 1: The dataset information
	</Abstractive Summary>
</Paper ID=ument269>


<Paper ID=ument269> <Table ID =2>
	<Abstractive Summary> =
		1
		Datasets and Compared Methods
		Dataset
		Relations
		UP
		’gene associated with disease’
		’disease has associated gene’
		’gene mapped to disease’
		’disease mapped to gene’
		’may be treated by’
		’may treat’
		’may be prevented by’
		’may prevent’
		FN
		’people/person/nationality’
		’location/location/contains’
		’people/person/place lived’
		’people/person/place of birth’
		’people/deceased person/place of death’
		’people/person/ethnicity’
		’people/ethnicity/people’
		’business/person/company’
		’people/person/religion’
		’location/neighborhood/neighborhood of’
		’business/company/founders’
		’people/person/children’
		’location/administrative division/country’
		’location/country/administrative divisions’
		’business/company/place founded’
		’location/us county/county seat’
		Table 2: The concerned relations in two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Thus,
		we select a few meaningful and valuable relations
		(suggested by domain experts, in Table 2) with
		enough reasoning paths and construct two sub-
		graphs accordingly
	</Extractive Summary>
</Paper ID=ument269>


<Paper ID=ument269> <Table ID =3>
	<Abstractive Summary> =
		16
		Table 3: Performance comparison on the KG reasoning on the UMLS-PubMed dataset
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Performance Comparison
		Performances of the KG reasoning of all the algo-
		rithms are given in Table 3, 4 and Figure 6
	</Extractive Summary>
	<Extractive Summary> =
		The best performing Two-Step model adds tens
		times more edges into the KG than CPL, whereas
		the Two-Step model’s performance is inferior to
		CPL and MINERVA on all the datasets (Table 3,
		4)
	</Extractive Summary>
</Paper ID=ument269>


<Paper ID=ument269> <Table ID =4>
	<Abstractive Summary> =
		52
		Table 4: Performance comparison on the KG reasoning on the FB60K-NYT10 dataset
	</Abstractive Summary>
</Paper ID=ument269>


<Paper ID=ument27> <Table ID =1>
	<Abstractive Summary> =
		8
		Table 1: Comparison of the performance of event ex-
		traction on the three datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the event extraction results on the
		three datasets
	</Extractive Summary>
</Paper ID=ument27>


<Paper ID=ument27> <Table ID =2>
	<Abstractive Summary> =
		The experimental comparison with the
		state-of-the-art methods shows that AEM achieves
		improved extraction performance, especially on
		290
		FSD dataset
		Earthquake
		in Viriginia
		e: nbc coast tremor east eastern
		US
		debt ceiling
		e: hous gifford us gabriell repres
		l: virginia russian eal croydon washington
		l: virginia russian eal croydon washington
		k: earthquak feel center magnitud hit
		k: debt bill hous ceil vote
		d: 2011/8/23 2011/7/23 2011/8/06 2011/9/07 2011/9/12
		d: 2011/8/01 2011/7/23 2011/8/23 2011/8/06 2011/9/13
		South sudan
		independent
		e: south sudan independ earthquak tremor
		US
		credit downgrade
		e: aaa aa yahoo standard obama
		l: earth senat congress york nyc
		l: state america tottenham congress seattl
		k: independ celebr countri congrat challeng
		k: credit rate downgrad histori lose
		d: 2011/7/09 2011/8/06 2011/8/23 2011/7/23 2011/9/07
		d: 2011/8/06 2011/7/23 2011/8/23 2011/9/07 2011/9/12
		Somalia
		declare famine
		e: somalia africa bakool southern nation
		Norway youth
		camp attack
		e: eyewit norway norweigan rock us
		l: somalia africa rome independ southern
		l: norway island germani state libya
		k: declar famin drought part region
		k: camp attack youth bomb shoot
		d: 2011/7/20 2011/7/23 2011/8/06 2011/8/23 2011/9/07
		d: 2011/7/22 2011/7/23 2011/8/23 2011/8/06 2011/8/10
		Twitter dataset
		Russia hosts
		world cup
		e: world cup william russia sport
		Larry King’s
		last show
		e: king larri cnn red vega
		l: qatar russia china europ beij
		l: uk state richardson unit south
		k: host cup reaction world triumph
		k: ﬁnal show broadcast night year
		d: 2010/9/3 2010/9/10 2010/9/9 2010/9/8 2010/9/17
		d: 2010/9/17 2010/9/10 2010/9/8 2010/9/9 2010/9/26
		Coach Urban
		Meyer step down
		e: meyer urban reid ﬂorida gator
		Boxer ﬂoyd
		Maweath is arrested
		e: boxer ﬂoyd mayweath vega obama
		l: ﬂorida univers senat europ hous
		l: vega las beij europ itali
		k: coach step univers footbal accord
		k: guard boxer secur assault arrest
		d: 2010/9/8 2010/9/10 2010/9/9 2010/9/18 2010/9/17
		d: 2010/9/17 2010/9/9 2010/9/18 2010/9/8 2010/9/26
		Christian violence
		in nigeria
		e: christian muslim polit concord eve
		Xiaobo Liu
		award nobel prize
		e: xiaobo liu nobel prize china
		l: nigeria jos congress christian of
		l: china oslo congress continent europ
		k: religion church violenc plagu peopl
		k: award live nobel ceremoni dissid
		d: 2010/9/25 2010/9/28 2010/9/26 2010/9/6 2010/9/8
		d: 2010/9/10 2010/9/8 2010/9/17 2010/9/9 2010/9/18
		Google dataset
		Sexual assault
		in campus
		o: university federal college department white
		Lockett is executed
		death penalty
		in Oklahoma
		o: warner state department cohen robert
		l: obama princeton ohio columbia harvard
		l: lockett oklahoma states texas ohio
		p: mccaskill rose catherine brown duncan
		p: lockett clayton patton stephanie charles
		k: sexual assault campus title colleges
		k: execution death penalty lethal minutes
		Apple & Samsung
		patent jury
		o: apple samsung google inc motorola
		MH370
		o: airlines air transport boeing najib
		l: california south santa us calif
		l: malaysia australia beijing malacca houston
		p: judge steve dunham schmidt mueller
		p: najib hishammuddin hussein clark dolan
		k: patent jury smartphone verdict trial
		k: search plane ﬂight aircraft ocean
		Afghanistan
		landslide
		o: afghanistan united taliban kabul un
		South Africa
		election
		o: anc national mandela congress eff
		l: afghanistan badakhshan kabul tajikistan pakistan
		l: zuma africa south africans nkandla
		p: karzai shah hill mark angela
		p: zuma jacob president nelson malema
		k: landslide village rescue mud province
		k: election apartheid elections voters economic
		Table 2: The event examples extracted by AEM
	</Abstractive Summary>
</Paper ID=ument27>


<Paper ID=ument270> <Table ID =1>
	<Abstractive Summary> =
		Target
		PersonX
		writes PersonY
		a letter
		xIntent
		to send a message,
		express themself
		xReact
		nervous,
		thoughtful
		oReact
		indifferent,
		receptive
		Table 1: Hierarchical structure of Event2Mind dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Details are shown in Table 1 and Table 2
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the (base event-
		inference dimension-target) hierarchical structure
		through an example from Event2Mind
	</Extractive Summary>
	<Extractive Summary> =
		Generations
		Ground truth
		CWVAE
		RNN-based Seq2Seq
		PersonX works
		tirelessly
		xIntent
		be productive and hardworking
		ﬁnish his work soon
		earn more money and accomplish goal
		ﬁnish his work and make money
		ﬁnish his work and accomplish goal
		be productive and successful
		get the job done
		get the job done on time
		get the job done in the way
		get the job done for his life
		make money for his own future
		make money for his own money
		be productive
		ﬁnish the project as soon as possible
		reach goal
		Table 10: An example of inferences made by CWVAE and RNN-based Seq2Seq model under inference dimension “xIntent”
	</Extractive Summary>
	<Extractive Summary> =
		5
		Case Study
		Table 10 provides an example of model genera-
		tions given the base event “PersonX works tire-
		lessly” and the inference dimension “xIntent”
	</Extractive Summary>
</Paper ID=ument270>


<Paper ID=ument270> <Table ID =2>
	<Abstractive Summary> =
		Target
		PersonX
		adopts a child
		xIntent
		to help another person,
		to have a child
		xNeed
		to visit adoption agency,
		to be approved for adoption
		xAttr
		compassionate,
		generous
		xEffect
		becomes a parent,
		gains love and companionship
		xWant
		take child home,
		buy child clothes
		xReact
		happy,
		caring
		oReact
		has a parent,
		receives love and affection
		oWant
		try on new clothes,
		to have a family
		oEffect
		has a parent,
		Receives love and affection
		Table 2: Hierarchical structure of Atomic dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the
		(base event-inference dimension-target) hierarchi-
		cal structure through an example from Atomic
	</Extractive Summary>
</Paper ID=ument270>


<Paper ID=ument270> <Table ID =3>
	<Abstractive Summary> =
		Table 3: An example for the construction of auxiliary dataset
	</Abstractive Summary>
	<Extractive Summary> =
		For example, as
		shown in Table 3, the ﬁrst three sentences describe
		a context that Jason was unsatisﬁed about his job
		and applied for a new job
	</Extractive Summary>
</Paper ID=ument270>


<Paper ID=ument270> <Table ID =4>
	<Abstractive Summary> =
		97
		Table 4: Average perplexity and BLEU score (reported in
		percentages) for the top 10 generations under each inference
		dimension of Event2Mind
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Overall Results
		We list the perplexity and BLEU score of CWVAE
		and baseline methods on Event2Mind and Atomic
		in Table 4 and Table 6, respectively, and show the
		distinct-1 and distinct-2 score on Event2Mind and
		Atomic in Table 5 and Table 7, respectively
	</Extractive Summary>
</Paper ID=ument270>


<Paper ID=ument270> <Table ID =5>
	<Abstractive Summary> =
		0063
		Table 5: Distinct-1 and distinct-2 scores for the top 10 gen-
		erations under each inference dimension of Event2Mind
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Overall Results
		We list the perplexity and BLEU score of CWVAE
		and baseline methods on Event2Mind and Atomic
		in Table 4 and Table 6, respectively, and show the
		distinct-1 and distinct-2 score on Event2Mind and
		Atomic in Table 5 and Table 7, respectively
	</Extractive Summary>
	<Extractive Summary> =
		We
		ﬁnd that:
		(1) As shown in Table 5 and Table 7, compari-
		son between RNN-based Seq2Seq and variational-
		based methods, including Variational Seq2Seq,
		VRNMT, CWVAE-unpretrained and CWVAE
		shows that, variational-based methods could in-
		crease the diversity of generations
	</Extractive Summary>
</Paper ID=ument270>


<Paper ID=ument270> <Table ID =6>
	<Abstractive Summary> =
		63
		Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference
		dimension of Atomic
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Overall Results
		We list the perplexity and BLEU score of CWVAE
		and baseline methods on Event2Mind and Atomic
		in Table 4 and Table 6, respectively, and show the
		distinct-1 and distinct-2 score on Event2Mind and
		Atomic in Table 5 and Table 7, respectively
	</Extractive Summary>
</Paper ID=ument270>


<Paper ID=ument270> <Table ID =7>
	<Abstractive Summary> =
		0093
		Table 7: Distinct-1 and distinct-2 scores for the top 10 generations under each inference dimension of Atomic
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Overall Results
		We list the perplexity and BLEU score of CWVAE
		and baseline methods on Event2Mind and Atomic
		in Table 4 and Table 6, respectively, and show the
		distinct-1 and distinct-2 score on Event2Mind and
		Atomic in Table 5 and Table 7, respectively
	</Extractive Summary>
	<Extractive Summary> =
		We
		ﬁnd that:
		(1) As shown in Table 5 and Table 7, compari-
		son between RNN-based Seq2Seq and variational-
		based methods, including Variational Seq2Seq,
		VRNMT, CWVAE-unpretrained and CWVAE
		shows that, variational-based methods could in-
		crease the diversity of generations
	</Extractive Summary>
</Paper ID=ument270>


<Paper ID=ument270> <Table ID =8>
	<Abstractive Summary> =
		96
		Table 8: Human evaluation results on Event2Mind
	</Abstractive Summary>
</Paper ID=ument270>


<Paper ID=ument270> <Table ID =9>
	<Abstractive Summary> =
		90
		Table 9: Human evaluation results on Atomic
	</Abstractive Summary>
</Paper ID=ument270>


<Paper ID=ument270> <Table ID =10>
	<Abstractive Summary> =
		Generations
		Ground truth
		CWVAE
		RNN-based Seq2Seq
		PersonX works
		tirelessly
		xIntent
		be productive and hardworking
		ﬁnish his work soon
		earn more money and accomplish goal
		ﬁnish his work and make money
		ﬁnish his work and accomplish goal
		be productive and successful
		get the job done
		get the job done on time
		get the job done in the way
		get the job done for his life
		make money for his own future
		make money for his own money
		be productive
		ﬁnish the project as soon as possible
		reach goal
		Table 10: An example of inferences made by CWVAE and RNN-based Seq2Seq model under inference dimension “xIntent”
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Case Study
		Table 10 provides an example of model genera-
		tions given the base event “PersonX works tire-
		lessly” and the inference dimension “xIntent”
	</Extractive Summary>
</Paper ID=ument270>


<Paper ID=ument271> <Table ID =1>
	<Abstractive Summary> =
		Label: contradiction
		Table 1: Examples of natural language inference
	</Abstractive Summary>
	<Extractive Summary> =
		As illustrated in
		Table 1, logical relationships between the two sen-
		tences include entailment (if the premise is true,
		then the hypothesis must be true), contradiction
		(if the premise is true, then the hypothesis must be
		false), and neutral (neither entailment nor contra-
		diction)
	</Extractive Summary>
	<Extractive Summary> =
		example in Table 1, the premise sentence can infer
		the hypothesis sentence, however, the hypothesis
		sentence can’t infer the premise sentence
	</Extractive Summary>
</Paper ID=ument271>


<Paper ID=ument271> <Table ID =2>
	<Abstractive Summary> =
		1K 10
		7
		-
		Table 2: Statistics of datasets: SNLI, MultiNLI, Sci-
		Tail
	</Abstractive Summary>
</Paper ID=ument271>


<Paper ID=ument271> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Comparison with previous models on the
		SNLI dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results of different models on
		the training and test sets of SNLI
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, the
		ﬁrst category of methods are single models and the
		second category of methods are ensemble models
	</Extractive Summary>
	<Extractive Summary> =
		Similar to Table 3, the ﬁrst category
		Model
		Test Accuracy
		Matched
		Mismatched
		Single Models
		ESIM
		72
	</Extractive Summary>
</Paper ID=ument271>


<Paper ID=ument271> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Comparison with previous models on the
		MultiNLI dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 reports our results on the MultiNLI
		dataset
	</Extractive Summary>
</Paper ID=ument271>


<Paper ID=ument271> <Table ID =5>
	<Abstractive Summary> =
		6
		Table 5: Comparison with previous models on the Sci-
		Tail dataset
	</Abstractive Summary>
	<Extractive Summary> =
		As illustrated in Table 5, our model outperforms
		the baselines and achieves an accuracy of 84
	</Extractive Summary>
</Paper ID=ument271>


<Paper ID=ument271> <Table ID =6>
	<Abstractive Summary> =
		8
		Table 6: Effect of number of asynchronous inference
		layers on the SNLI
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the performance with a differ-
		ent number of asynchronous inference sub-layers
	</Extractive Summary>
</Paper ID=ument271>


<Paper ID=ument271> <Table ID =7>
	<Abstractive Summary> =
		8
		Table 7: Effect of components on the SNLI
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 7, we show the results of ablation study
		on our base model
	</Extractive Summary>
</Paper ID=ument271>


<Paper ID=ument272> <Table ID =1>
	<Abstractive Summary> =
		Table 1:
		Masked outputs for different masking rate
		thresholds (MRT) and base similarity thresholds (ST)
		mantically
	</Abstractive Summary>
	<Extractive Summary> =
		2 Some sample
		masked outputs (S′′) using various MRT-ST com-
		binations for the previous example are shown in
		Table 1 (more examples in Appendix A)
	</Extractive Summary>
</Paper ID=ument272>


<Paper ID=ument272> <Table ID =2>
	<Abstractive Summary> =
		3
		Semantic Text Exchange Score (STES)
		We come up with a single score to evaluate over-
		all performance of a model on STE that combines
		7See Appendix C for explanations
		2707
		Table 2:
		Overall average results by model (with %
		changes from the input)
		the key evaluation metrics
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Automatic Evaluation Results
		Table 2 shows overall average results by model
	</Extractive Summary>
	<Extractive Summary> =
		1
		Performance by Model
		As seen in Table 2, both SMERTI variations
		achieve higher STES and outperform the other
		models overall, with the WordNet models per-
		forming the worst
	</Extractive Summary>
</Paper ID=ument272>


<Paper ID=ument272> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Generated output text by model for various masking rates on a Yelp evaluation example
		Table 4: Average human evaluation scores by model
		Table 5: Average TTR values by model
		repeated multiple times
	</Abstractive Summary>
	<Extractive Summary> =
		8
		Table 3 shows outputs for a Yelp example
	</Extractive Summary>
	<Extractive Summary> =
		9
		As observed from Table 3 (see also Appendix
		F), SMERTI is able to generate high quality out-
		put text similar to the RE while ﬂowing better
		than other models’ outputs
	</Extractive Summary>
	<Extractive Summary> =
		W2V-STEM achieves the lowest SLOR, espe-
		cially for higher RRT, as supported by the example
		in Table 3 (see also Appendix F)
	</Extractive Summary>
</Paper ID=ument272>


<Paper ID=ument272> <Table ID =4>
	<Abstractive Summary> =
		Table 3: Generated output text by model for various masking rates on a Yelp evaluation example
		Table 4: Average human evaluation scores by model
		Table 5: Average TTR values by model
		repeated multiple times
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Performance By Model - Human Results
		As seen in Table 4, the SMERTI variations out-
		perform all baseline models overall, particularly
		in RE Match
	</Extractive Summary>
</Paper ID=ument272>


<Paper ID=ument272> <Table ID =5>
	<Abstractive Summary> =
		Table 3: Generated output text by model for various masking rates on a Yelp evaluation example
		Table 4: Average human evaluation scores by model
		Table 5: Average TTR values by model
		repeated multiple times
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the SMERTI
		variations achieve the highest TTR, while W2V-
		STEM and NWN-STEM the lowest
	</Extractive Summary>
</Paper ID=ument272>


<Paper ID=ument272> <Table ID =6>
	<Abstractive Summary> =
		W2V-STEM achieves
		Table 6: Input text’s avg
	</Abstractive Summary>
	<Extractive Summary> =
		3
		SMERTI’s Performance By POS
		As seen from Table 612 , SMERTI’s SPA values
		are highest for nouns, likely because they typi-
		cally carry little sentiment, and lowest for adjec-
		tives, likely because they typically carry the most
	</Extractive Summary>
</Paper ID=ument272>


<Paper ID=ument272> <Table ID =7>
	<Abstractive Summary> =
		SMERTI’s CSS is highest for phrases then
		nouns, likely due to phrases and nouns carrying
		12Note that the SMERTI values in Tables 6 to 8 refer to the
		average between SMERTI-Transformer and SMERTI-RNN
		2709
		Table 7: Input text’s avg
	</Abstractive Summary>
	<Extractive Summary> =
		4
		SMERTI’s Performance By Dataset
		As seen in Table 7, SMERTI’s SPA is lowest for
		news headlines
	</Extractive Summary>
</Paper ID=ument272>


<Paper ID=ument272> <Table ID =8>
	<Abstractive Summary> =
		SPA, SLOR, CSS, and STES by dataset
		Table 8: SMERTI’s avg
	</Abstractive Summary>
	<Extractive Summary> =
		5
		SMERTI’s Performance By MRT/RRT
		From Table 8, it can be seen that as MRT/RRT in-
		creases, SMERTI’s SPA and SLOR decrease while
		CSS increases
	</Extractive Summary>
</Paper ID=ument272>


<Paper ID=ument273> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example from the crowdsourced Human100 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1) of generating human evaluation data Human100: given the topic Why did MH-17
		crash? and the scenario MH-17 had a bomb on board
	</Extractive Summary>
	<Extractive Summary> =
		As a step in the direction of re-
		alistic data for the task, we had human annotators
		collect news items that have multiple scenarios of
		what happened around the same topic (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		See
		the bold text in Table 1 for an example
	</Extractive Summary>
</Paper ID=ument273>


<Paper ID=ument273> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Corpora statistics
	</Abstractive Summary>
	<Extractive Summary> =
		We refer to the
		dataset as Human100 (stats in Table 2)
	</Extractive Summary>
</Paper ID=ument273>


<Paper ID=ument273> <Table ID =3>
	<Abstractive Summary> =
		65
		Table 3: Intrinsic evaluation: F1 scores (testing) for
		models trained on different NYT mixtures in ﬁxed and
		dynamic #sentences conditions
	</Abstractive Summary>
	<Extractive Summary> =
		Examining the results in Table 3,12 we observe
		the hybrid mixtures with both types of distrac-
		tors are the most difﬁcult, with substantially lower
		performance
	</Extractive Summary>
	<Extractive Summary> =
		2
		Do Our Modules All Contribute?
		We additionally evaluate our proposed modules –
		insertion-sort based selection and relation nets – to
		see which contributes substantially in the intrin-
		sic evaluation (Table 3)
	</Extractive Summary>
</Paper ID=ument273>


<Paper ID=ument273> <Table ID =4>
	<Abstractive Summary> =
		82
		Table 4: Which scenario mixture method is the best?
		F1 scores on the Human100 data of the COMP-INS-RN
		model trained in different mixing conditions
	</Abstractive Summary>
	<Extractive Summary> =
		But how does this translate into
		human evaluation? In Table 4, we evaluate the
		best-performing model (COMP-INS-RN, trained on
		NYT-* sets) on Human100
	</Extractive Summary>
	<Extractive Summary> =
		The results
		in Table 8 show that in both ﬁxed and dynamic
		#sent conditions, the model improves on the per-
		formance with single-domain training (Table 4, 5)
	</Extractive Summary>
</Paper ID=ument273>


<Paper ID=ument273> <Table ID =5>
	<Abstractive Summary> =
		82
		Table 5: How do modeling modules contribute? F1
		scores on Human 100 of different models with the best
		hybrid training mixtures (NYT-4)
	</Abstractive Summary>
</Paper ID=ument273>


<Paper ID=ument273> <Table ID =6>
	<Abstractive Summary> =
		40
		Table 6: Sorting performance (τ) and its correlation
		with clustering accuracy (ρ)
		While sorting performance in itself is not very
		high, it has a reasonable correlation with cluster-
		ing performance (Table 6): following Cui et al
	</Abstractive Summary>
</Paper ID=ument273>


<Paper ID=ument273> <Table ID =7>
	<Abstractive Summary> =
		81
		Table 7: Generalization out-of-domain text: train on
		NYT-*/ROC-* and evaluate on ROC-* (ﬁxed #sents)
	</Abstractive Summary>
	<Extractive Summary> =
		The ﬁrst question is: do the models generalize
		out-of-domain, particularly when textual cues are
		much fewer? We train our strongest COMP-INS-
		RN on NYT-* and evaluate on the corresponding
		ROC-* datasets (Table 7): in in-domain evaluation
		(i
	</Extractive Summary>
	<Extractive Summary> =
		4
		Final Model
		From the domain-generalization test in Table 7,
		we see there is likely NYT-* sets do not subsume
		all the information in ROC-* sets
	</Extractive Summary>
</Paper ID=ument273>


<Paper ID=ument273> <Table ID =8>
	<Abstractive Summary> =
		97
		Table 8: Left table: F1 scores for Human100 evalua-
		tion with the best model (COMP-INS-RN) in ﬁxed and
		dynamic #sentences conditions
	</Abstractive Summary>
	<Extractive Summary> =
		The results
		in Table 8 show that in both ﬁxed and dynamic
		#sent conditions, the model improves on the per-
		formance with single-domain training (Table 4, 5)
	</Extractive Summary>
</Paper ID=ument273>


<Paper ID=ument273> <Table ID =9>
	<Abstractive Summary> =
		Table 9: Humans make more reasonable mistakes (the
		query is underscored): the annotator selected sentence
		(2) as a part of the target scenario, which, while not
		part of the gold, does make a comperent scenario
	</Abstractive Summary>
	<Extractive Summary> =
		are not from the gold scenario (Table 9)
	</Extractive Summary>
</Paper ID=ument273>


<Paper ID=ument274> <Table ID =1>
	<Abstractive Summary> =
		triplets
		DBP15KZH−EN
		Chinese
		66,469
		2,830
		153,929
		English
		98,125
		2,317
		237,674
		DBP15KJA−EN
		Japanese
		65,744
		2,043
		164,373
		English
		95,680
		2,096
		233,319
		DBP15KFR−EN
		French
		66,858
		1,379
		192,191
		English
		105,889
		2,209
		278,590
		DWY100KWD
		DBpedia
		100,000
		330
		463,294
		Wikidata
		100,000
		220
		448,774
		DWY100KYG
		DBpedia
		100,000
		302
		428,952
		YAGO3
		100,000
		31
		502,563
		Table 1: Statistics of DBP15K and DWY100K
	</Abstractive Summary>
</Paper ID=ument274>


<Paper ID=ument274> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Results comparison on entity alignment(Results of Hits@1 and Hits@10 are percentage values)
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Effectiveness of Nearest Neighbor
		Sampling
		In Table 2, it can be seen that KECG(w/o NNS)
		outperforms many baselines and its Hits@1 on
		Hits@1
		Hits@10
		Hits@50
		40
		60
		80
		100
		(a) ZH → EN
		Hits@N (%)
		Hits@1
		Hits@10
		Hits@50
		(b) DBpedia → YAGO3
		GCN-Align
		AlignEA
		KECG(w/o K)
		KECG
		Figure 3: Detailed Hits@N for investigating the effec-
		tiveness of Knowledge Embedding Model
	</Extractive Summary>
</Paper ID=ument274>


<Paper ID=ument275> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1:
		Probe accuracies on linguistic tasks and control
		tasks
	</Abstractive Summary>
	<Extractive Summary> =
		2, are
		found in Table 1 (top)
	</Extractive Summary>
	<Extractive Summary> =
		4) consistently lead to selec-
		tivity? Surprisingly, as shown in Table 1 (middle),
		the opposite is true for some MLP probes, where
		selectivity actually decreases (e
	</Extractive Summary>
	<Extractive Summary> =
		We report the results of these probes in Table 1
		(bottom)
	</Extractive Summary>
</Paper ID=ument275>


<Paper ID=ument275> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2:
		Part-of-speech and dependency edge prediction
		probe accuracies and selectivities across three representations
	</Abstractive Summary>
	<Extractive Summary> =
		So, does ELMo1 have a better grasp
		of part-of-speech than ELMo2? Our results, sum-
		marized in Table 2, offer the alternative hypothesis
		that probes use word identity as a feature to pre-
		dict part-of-speech, and that feature is less easily
		available in ELMo2 than ELMo1
	</Extractive Summary>
</Paper ID=ument275>


<Paper ID=ument276> <Table ID =1>
	<Abstractive Summary> =
		304
		Table 1: Statistics of the datasets used in this paper
	</Abstractive Summary>
</Paper ID=ument276>


<Paper ID=ument276> <Table ID =2>
	<Abstractive Summary> =
		781
		Table 2:
		Parsing accuracy of 9 languages (LAS)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the test accuracies
		of these parsers,
		using the standard train-
		ing/development/test split for each UD language
	</Extractive Summary>
</Paper ID=ument276>


<Paper ID=ument276> <Table ID =3>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		These results use only ELMo layer 1; results
		from all layers are shown in Table 3 in the appendix, for both LAS and UAS metrics
	</Extractive Summary>
</Paper ID=ument276>


<Paper ID=ument277> <Table ID =1>
	<Abstractive Summary> =
		7k
		Table 1: Languages and treebanks used in experiments
	</Abstractive Summary>
</Paper ID=ument277>


<Paper ID=ument277> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Labeled attachment score on 13 languages for
		parsing models with and without deep contextualized
		word representations
	</Abstractive Summary>
	<Extractive Summary> =
		7
		6
		Results and Discussion
		Table 2 shows labeled attachment scores for the
		six parsers on all languages, averaged over three
		training runs with random seeds
	</Extractive Summary>
</Paper ID=ument277>


<Paper ID=ument278> <Table ID =1>
	<Abstractive Summary> =
		45
		Table 1: Data statistics of PMB v
	</Abstractive Summary>
	<Extractive Summary> =
		0) contains a large number of
		silver standard annotations which have been only
		partially manually corrected (see Table 1)
	</Extractive Summary>
</Paper ID=ument278>


<Paper ID=ument278> <Table ID =2>
	<Abstractive Summary> =
		40%
		Table 2: Model performance (Precision, Recall, F1) on
		PMB data (v
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		System comparison
		Table 2 summarizes our re-
		sults on the PMB gold data (v
	</Extractive Summary>
</Paper ID=ument278>


<Paper ID=ument278> <Table ID =3>
	<Abstractive Summary> =
		90%
		Table 3: Model performance (Precision, Recall, F1) on
		PMB data (v
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, increasing the train-
		ing data improves performance but the difference
		is not as dramatic as in van Noord et al
	</Extractive Summary>
</Paper ID=ument278>


<Paper ID=ument278> <Table ID =4>
	<Abstractive Summary> =
		50%
		Table 4: Model performance (Precision, Recall, F1) on
		PMB (v2
	</Abstractive Summary>
	<Extractive Summary> =
		Model conﬁgurations
		Table 4 reports on vari-
		ous ablation experiments investigating which fea-
		tures and combinations thereof perform best
	</Extractive Summary>
</Paper ID=ument278>


<Paper ID=ument278> <Table ID =5>
	<Abstractive Summary> =
		50
		Table 5: Model performance across languages (Preci-
		sion, Recall, F1)
	</Abstractive Summary>
</Paper ID=ument278>


<Paper ID=ument278> <Table ID =6>
	<Abstractive Summary> =
		93
		Table 6: F1-scores of ﬁne-grained evaluation on the
		PMB (v
	</Abstractive Summary>
</Paper ID=ument278>


<Paper ID=ument279> <Table ID =1>
	<Abstractive Summary> =
		md
		2781
		TOKEN
		VOCAB SIZE
		Word Form
		1,588,655
		BERT Wordpieces
		119,547
		UPOS
		17
		XPOS
		19,126
		UFeats
		23,974
		Lemmas (tags)
		109,639
		Deps
		251
		Table 1: Vocabulary sizes of words and tags over all of
		UD v2
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Cross-Linguistic Training Issues
		Table 1 displays a list of vocabulary sizes, indicat-
		ing that UD treebanks possess nearly 1
	</Extractive Summary>
	<Extractive Summary> =
		15
		0
		Table 10: The full test results of UDify on 124 treebanks (part 4 of 4)
	</Extractive Summary>
</Paper ID=ument279>


<Paper ID=ument279> <Table ID =2>
	<Abstractive Summary> =
		91
		Table 2:
		Test set scores for a subset of high-
		resource (top) and low-resource (bottom) languages
		in comparison to UDPipe Future without BERT, with
		3 UDify conﬁgurations: Lang, ﬁne-tune on the tree-
		bank
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results for a salient subset
		of high-resource and low-resource languages are
		shown in Table 2, with a comparison between
		UDPipe Future and UDify ﬁne-tuning on all lan-
		guages
	</Extractive Summary>
</Paper ID=ument279>


<Paper ID=ument279> <Table ID =3>
	<Abstractive Summary> =
		33
		Table 3:
		Ablation comparing the average of scores
		over all treebanks: task-speciﬁc layer attention (4 sets
		of c, w computed for the 4 UD tasks), global layer at-
		tention (one set of c, w for all tasks), and simple sum
		of layers (c = 1 and w = �)
	</Abstractive Summary>
	<Extractive Summary> =
		A more comprehensive overview is shown in
		Table 3, comparing different attention strategies
		applied to UDify
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows that layer attention on BERT for
		each task is beneﬁcial for test performance, much
		more than using a global weighted average
	</Extractive Summary>
</Paper ID=ument279>


<Paper ID=ument279> <Table ID =4>
	<Abstractive Summary> =
		80
		Table 4: Test set results for zero-shot learning, i
	</Abstractive Summary>
	<Extractive Summary> =
		For
		zero-shot learning evaluation, Table 4 displays a
		subset of test set evaluations of treebanks that
		do not have a training set, i
	</Extractive Summary>
</Paper ID=ument279>


<Paper ID=ument279> <Table ID =5>
	<Abstractive Summary> =
		67
		Table 5:
		UUAS test scores calculated on the predic-
		tions produced by the syntactic structural probe (Hewitt
		and Manning, 2019) using the English EWT treebank,
		on the unmodiﬁed multilingual cased BERT model and
		the same BERT model ﬁne-tuned on the treebank
	</Abstractive Summary>
	<Extractive Summary> =
		But more no-
		tably, the results in Table 5 show that ﬁne-tuning
		BERT on Universal Dependencies signiﬁcantly
		boosts UUAS scores when compared to the gold
		dependency trees, an error reduction of 41%
	</Extractive Summary>
</Paper ID=ument279>


<Paper ID=ument279> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: A summary of model hyperparameters
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Hyperparameters
		A summary of hyperparameters can be found in
		Table 6 in Appendix A
	</Extractive Summary>
</Paper ID=ument279>


<Paper ID=ument279> <Table ID =7>
	<Abstractive Summary> =
		8k
		Table 7: The full test results of UDify on 124 treebanks (part 1 of 4)
	</Abstractive Summary>
</Paper ID=ument279>


<Paper ID=ument279> <Table ID =8>
	<Abstractive Summary> =
		8k
		Table 8: The full test results of UDify on 124 treebanks (part 2 of 4)
	</Abstractive Summary>
</Paper ID=ument279>


<Paper ID=ument279> <Table ID =9>
	<Abstractive Summary> =
		0k
		Table 9: The full test results of UDify on 124 treebanks (part 3 of 4)
	</Abstractive Summary>
</Paper ID=ument279>


<Paper ID=ument279> <Table ID =10>
	<Abstractive Summary> =
		15
		0
		Table 10: The full test results of UDify on 124 treebanks (part 4 of 4)
	</Abstractive Summary>
</Paper ID=ument279>


<Paper ID=ument28> <Table ID =1>
	<Abstractive Summary> =
		1
		Experimental Settings
		Category
		Description
		Category
		Description
		CAP
		Capital name
		FAC
		Man-made structures
		ELE
		Chemical element
		ORG
		Organizations
		FEM
		Female ﬁrst name
		GPE
		Geo-political entities
		MALE
		Male ﬁrst name
		LOC
		Non-GPE locations
		LAST
		Last name
		DAT
		A date or period
		TTL
		Honoriﬁc title
		LANG
		Any named language
		NORP
		Nationality,
		Religion, Political
		Table 1: Target categories used on Google Web 1T
	</Abstractive Summary>
	<Extractive Summary> =
		We use 13
		categories of entities (see Table 1) list in Shi et al
	</Extractive Summary>
</Paper ID=ument28>


<Paper ID=ument28> <Table ID =2>
	<Abstractive Summary> =
		61
		Table 2: Overall results for entity set expansion on Google Web 1T,where Oursfull is the full version of our method,
		Ours-MCTS is our method with the MCTS disabled, and Ours-PMSN is our method but replacing the PMSN with
		ﬁxed word embeddings
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the performance
		of different bootstrapping methods on Google
		Web 1T
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2, we can
		also see that if we replace the Monte Carlo Tree
		Search by selecting top-n patterns, the perfor-
		mance decreases by 19% in P@100 and 17% in
		P@200; if we replace the PMSN with word em-
		bedding, the performance decreases by 34% in
		P@100 and 27% in P@200
	</Extractive Summary>
</Paper ID=ument28>


<Paper ID=ument28> <Table ID =3>
	<Abstractive Summary> =
		79
		Table 3: The adaptive entity scoring performance of different methods on the APR and Wiki
	</Abstractive Summary>
	<Extractive Summary> =
		of-the-art entity set expansion method–SetExpan,
		which is a non-bootstrapping method on the APR
		and Wik (see Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		From Table 3, we can
		see that our method outperforms SetExpan on both
		datasets: on the APR, our method achieves 6% im-
		provement in MAP@10 and 2% improvement in
		MAP@50 ; on the Wiki, our method can achieve
		2% improvement in MAP@10 and 4% improve-
		ment in MAP@50
	</Extractive Summary>
</Paper ID=ument28>


<Paper ID=ument28> <Table ID =4>
	<Abstractive Summary> =
		52
		Table 4: The performance of our full method on differ-
		ent categories on Google Web 1T
	</Abstractive Summary>
</Paper ID=ument28>


<Paper ID=ument28> <Table ID =5>
	<Abstractive Summary> =
		Be-
		sides, further adding context patterns in the PMSN
		causes the performance decrease for 3%, which is
		299
		Iters
		Patterns selected by Oursfull
		Patterns selected by Ours-MCTS
		Patterns selected by Ours-PMSN
		1
		Embassy of Sweden in *
		held a meeting in *
		* and New York in
		2
		Embassy of Belgium in *
		* meeting was held on
		in New York or *
		3
		’s capital city of *
		* Meeting to be held
		between New York and *
		4
		* is the capital city
		* held its ﬁrst meeting
		* hotel reservations with discount
		5
		* was the capital of
		* meeting to be held
		* is a great city
		Table 5: The top patterns selected by different methods when expanding capital entities in the ﬁrst 5 iterations
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 5, we can see that the top patterns
		by our full method are more related to seed enti-
		ties than other two baselines
	</Extractive Summary>
</Paper ID=ument28>


<Paper ID=ument280> <Table ID =1>
	<Abstractive Summary> =
		Agent
		> wait
		Agent
		> answer:
		yes
		Table 1: Example of a player interacting with an envi-
		ronment to gather knowledge and answer a question
	</Abstractive Summary>
	<Extractive Summary> =
		E
		Full results
		We provide full results of our agents on ﬁxed
		map games in Table 9, and provide full results of
		our agents on random map games in Table 10
	</Extractive Summary>
	<Extractive Summary> =
		017)
		Table 10: Agent performance on random map games
	</Extractive Summary>
</Paper ID=ument280>


<Paper ID=ument280> <Table ID =2>
	<Abstractive Summary> =
		,
		2798
		edible
		drinkable
		portable
		openable
		cuttable
		sharp
		heat source
		cookable
		holder
		Butter knife
		1
		1
		Oven
		1
		1
		1
		Raw chicken
		1
		1
		1
		Fried chicken
		1
		1
		1
		1
		Table 2: Supported attributes along with examples
	</Abstractive Summary>
	<Extractive Summary> =
		The supported attributes are shown in
		Table 2, while the rules can be inferred from the
		list of supported commands (see Appendix C)
	</Extractive Summary>
</Paper ID=ument280>


<Paper ID=ument280> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Statistics of the QAit dataset
	</Abstractive Summary>
	<Extractive Summary> =
		From the ﬁxed map game distribu-
		tion described in Table 3, more than 1040 different
		games can be drawn
	</Extractive Summary>
	<Extractive Summary> =
		How-
		ever, because there exist easier games (as listed in
		Table 3, number of rooms is sampled between 2
		and 12), agents show better training performance
		in such setting than ﬁxed map setting in general
	</Extractive Summary>
</Paper ID=ument280>


<Paper ID=ument280> <Table ID =4>
	<Abstractive Summary> =
		470
		Table 4: Agent performance on zero-shot test games
		when trained on 10 games, 500 games and “unlimited”
		games settings
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		reports their zero-shot test performance
	</Extractive Summary>
	<Extractive Summary> =
		From Table 4 we see similar observation, when
		trained on 10 games and 500 games, DQN and
		DDQN performs better on test games but on the
		unlimited games setting, rainbow agent performs
		as good as them, and sometimes even better
	</Extractive Summary>
	<Extractive Summary> =
		Unfortunately, as shown in
		Table 4 that agent does not perform signiﬁcantly
		better than random on test set
	</Extractive Summary>
	<Extractive Summary> =
		Based on the results in Table 4, we compute an
		agent’s test accuracy only if it has obtained sufﬁ-
		cient information – i
	</Extractive Summary>
</Paper ID=ument280>


<Paper ID=ument280> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5: Test performance given sufﬁcient information
	</Abstractive Summary>
	<Extractive Summary> =
		Results shown in Table 5 sup-
		port our assumption that the QA module can learn
		(and generalize) effectively to answer given suf-
		ﬁcient information
	</Extractive Summary>
</Paper ID=ument280>


<Paper ID=ument280> <Table ID =6>
	<Abstractive Summary> =
		5
		Target Network Period
		1000 episodes
		Multi-step returns n
		n ∼ Uniform[1, 3]
		Distributional atoms
		51
		Distributional min/max values
		[-10, 10]
		Table 6: Hyper-parameter setup for rainbow agent
	</Abstractive Summary>
</Paper ID=ument280>


<Paper ID=ument280> <Table ID =7>
	<Abstractive Summary> =
		dice cuttable object (a sharp object must be in the player’s inventory)
		wait
		stop interaction
		Table 7: Supported command list
	</Abstractive Summary>
</Paper ID=ument280>


<Paper ID=ument280> <Table ID =8>
	<Abstractive Summary> =
		& open (object)
		Table 8: Heuristic conditions for determining whether the agent has enough information to answer a given attribute
		question
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 8,
		for each attribute we list all the commands for
		which their outcome (pass or fail) gives enough in-
		formation to answer the question correctly
	</Extractive Summary>
</Paper ID=ument280>


<Paper ID=ument280> <Table ID =9>
	<Abstractive Summary> =
		014)
		Table 9: Agent performance on ﬁxed map games
	</Abstractive Summary>
	<Extractive Summary> =
		E
		Full results
		We provide full results of our agents on ﬁxed
		map games in Table 9, and provide full results of
		our agents on random map games in Table 10
	</Extractive Summary>
</Paper ID=ument280>


<Paper ID=ument280> <Table ID =10>
	<Abstractive Summary> =
		017)
		Table 10: Agent performance on random map games
	</Abstractive Summary>
</Paper ID=ument280>


<Paper ID=ument281> <Table ID =1>
	<Abstractive Summary> =
		Question
		Fact
		Span
		Relation
		Gap
		Q: A light bulb turns on when it
		receives energy from A: gasoline
		a light bulb converts electri-
		cal energy into light energy
		when it is turned on
		electrical
		energy
		provides−1,
		enables−1
		(gasoline,
		provides,
		electrical energy)
		Q: What makes the best wiring?
		A: Tungsten
		wiring requires an electri-
		cal conductor
		electrical
		conductor
		isa−1,
		madeof
		(Tungsten, is an, elec-
		trical conductor)
		Table 1: Examples from KGD dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows example annotations of the
		gaps obtained through this process
	</Extractive Summary>
	<Extractive Summary> =
		dataset contains examples of the form {question,
		fact, spans, relations}, where each span is a sub-
		string of the input fact, and relations are the set
		of valid relations between the span and the correct
		answer (examples in Table 1 and stats in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1), we
		can use a reading comprehension model to iden-
		tify this span
	</Extractive Summary>
</Paper ID=ument281>


<Paper ID=ument281> <Table ID =2>
	<Abstractive Summary> =
		45
		Table 2: Statistics of the train/dev/test split of the KGD
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		dataset contains examples of the form {question,
		fact, spans, relations}, where each span is a sub-
		string of the input fact, and relations are the set
		of valid relations between the span and the correct
		answer (examples in Table 1 and stats in Table 2)
	</Extractive Summary>
</Paper ID=ument281>


<Paper ID=ument281> <Table ID =3>
	<Abstractive Summary> =
		69
		Table 3: BiDAF model performance on the span pre-
		diction task, under different choices of training data
		5
	</Abstractive Summary>
</Paper ID=ument281>


<Paper ID=ument281> <Table ID =4>
	<Abstractive Summary> =
		3*
		Table 4: Test accuracy on the the OBQA-Short subset
		and OBQA-Full dataset assuming core fact is given
	</Abstractive Summary>
</Paper ID=ument281>


<Paper ID=ument281> <Table ID =5>
	<Abstractive Summary> =
		8
		Table 5: Test accuracy on the OBQA-Short subset with
		different sources of knowledge
	</Abstractive Summary>
</Paper ID=ument281>


<Paper ID=ument281> <Table ID =6>
	<Abstractive Summary> =
		62
		Table 6: Average accuracy of various ablations, show-
		ing that each component of GapQA has an important
		role
	</Abstractive Summary>
</Paper ID=ument281>


<Paper ID=ument281> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Sample errors made by the GapQA on questions from the OBQA-Short dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows a
		few error examples
	</Extractive Summary>
</Paper ID=ument281>


<Paper ID=ument282> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1: Comparisons with large pre-trained language model ﬁne-tuning with different amount of training data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that our
		KA GNE T-based methods using ﬁxed pre-trained
		language encoders outperform ﬁne-tuning them-
		selves in all settings
	</Extractive Summary>
</Paper ID=ument282>


<Paper ID=ument282> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Comparison with ofﬁcial benchmark baseline
		methods using the ofﬁcial split on the leaderboard
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, we ﬁrst use the ofﬁcial split
		to compare our model with the baseline methods
		reported on the paper and leaderboard
	</Extractive Summary>
</Paper ID=ument282>


<Paper ID=ument282> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: Comparisons with knowledge-aware baseline
		methods using the in-house split (both easy and hard
		mode) on top of BLSTM as the sentence encoder
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the comparisons under both
		easy mode and hard mode, and our methods out-
		perform all knowledge-aware baseline methods by
		a large margin in terms of accuracy
	</Extractive Summary>
</Paper ID=ument282>


<Paper ID=ument282> <Table ID =4>
	<Abstractive Summary> =
		27
		Table 4: Ablation study on the KAGNET framework
	</Abstractive Summary>
</Paper ID=ument282>


<Paper ID=ument283> <Table ID =1>
	<Abstractive Summary> =
		io/
		2846
		Table 1: The results of QA models
	</Abstractive Summary>
	<Extractive Summary> =
		4*
		Results on QA datasets
		Table 1 shows the
		comparison on En-Kr (distant) and En-Fr (close)
		language pairs
	</Extractive Summary>
</Paper ID=ument283>


<Paper ID=ument283> <Table ID =2>
	<Abstractive Summary> =
		,
		Table 2: The ablation study on three datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Ablation Study
		As shown in Table 2, we con-
		duct an ablation study on both BiDAF and BERT
		models, by examining the effects, from remov-
		ing each component
	</Extractive Summary>
</Paper ID=ument283>


<Paper ID=ument283> <Table ID =3>
	<Abstractive Summary> =
		Table 3: The results of conﬁdence score evaluation
		TNR
		(95% TPR)
		AUROC
		AUPR
		(Wiki Kr / History Kr)
		(A) Base1
		47
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, our Reﬁnery outperforms
		other baselines with statistical signiﬁcance of p <
		0
	</Extractive Summary>
</Paper ID=ument283>


<Paper ID=ument283> <Table ID =4>
	<Abstractive Summary> =
		For exam-
		ple, a straightforward solution for RCQA is trans-
		lating (p, q) in target language to English and ap-
		2848
		Table 4: Qualitative examples showing our proposed conﬁdence estimation score CS for (p,q) pairs in Korean
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 compares the examples in Baseline and
		our model, where the ﬁrst two are high-quality
		data and the third and forth are not
	</Extractive Summary>
</Paper ID=ument283>


<Paper ID=ument284> <Table ID =1>
	<Abstractive Summary> =
		1
		5
		Table 1: Six QA datasets in three different categories used in this paper (detailed in Section 5) along with the size
		of each dataset
	</Abstractive Summary>
	<Extractive Summary> =
		We demonstrate that for many recently in-
		troduced tasks, which we group into three cate-
		gories as given in Table 1, it is relatively easy to
		precompute a discrete, task-speciﬁc set of possible
		solutions that contains the correct solution along
		with a modest number of spurious options
	</Extractive Summary>
	<Extractive Summary> =
		The statis-
		tics of |Z| and examples on each task are shown in
		Table 1 and Table 2 respectively
	</Extractive Summary>
</Paper ID=ument284>


<Paper ID=ument284> <Table ID =2>
	<Abstractive Summary> =
		Answer (y): John Long
		f: SQL executor
		Ztot: Non-nested SQL queries with up to 3 conditions
		Z: Select player where position=guard and year in toronto=1996-97
		Select max(player) where position=guard and year in toronto=1996-97
		Select min(player) where position=guard
		Select min(player) where year in toronto=1996-97
		Select min(player) where position=guard and year in toronto=1996-97
		Table 2: Examples of the input, answer text (y), f, Ztot and Z
	</Abstractive Summary>
	<Extractive Summary> =
		Speciﬁcally, we assume that one of
		2853
		mentions are related to the question and others are
		false positives because (i) this happens for most
		cases, as the ﬁrst example in Table 2, and (ii) even
		in the case where multiple mentions contribute to
		the answer, there is often a single span which ﬁts
		the question the best
	</Extractive Summary>
	<Extractive Summary> =
		a span
		in the document or an equation to compute the an-
		swer, see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		by simply re-
		turning the string associated with a particular se-
		lected mention or solving an equation to get the
		ﬁnal number, see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		The statis-
		tics of |Z| and examples on each task are shown in
		Table 1 and Table 2 respectively
	</Extractive Summary>
	<Extractive Summary> =
		As an example shown in Table 2,
		only the fourth span out of six is relevant to the
		question
	</Extractive Summary>
	<Extractive Summary> =
		The third example in
		Table 2 shows Z may contain many spurious SQL
		querie, e
	</Extractive Summary>
	<Extractive Summary> =
		t
		Pred
		Z (ordered by P(z|x; θt))
		1k
		10-9
		10-6
		41-37
		40-36
		41-37‡
		2k
		37-36 40-36
		41-37
		41-37‡
		10-6
		4k
		40-36 40-36 41-37‡
		41-37
		10-6
		8k
		40-36 40-36 41-37‡
		41-37
		10-6
		16k 37-36 40-36
		41-37
		41-37‡
		10-6
		32k 40-36 40-36
		41-37
		41-37‡
		10-6
		Table 5: An example from DROPnum (same as Figure 1 and Table 2), with its answer text ‘4’ and a subset of
		the solution set (Z), containing two of ‘41-38’ (which ‘41’ come from different mentions; one denoted by ‡ for
		distinction), ‘40-36’ and ‘10-4’
	</Extractive Summary>
</Paper ID=ument284>


<Paper ID=ument284> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Results on multi-mention reading comprehension & discrete reasoning tasks
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 compares the results of base-
		lines, our method and the state-of-the-art on four
		datasets
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the results on DROPnum
	</Extractive Summary>
</Paper ID=ument284>


<Paper ID=ument284> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Results on WIKISQL
	</Abstractive Summary>
</Paper ID=ument284>


<Paper ID=ument284> <Table ID =5>
	<Abstractive Summary> =
		t
		Pred
		Z (ordered by P(z|x; θt))
		1k
		10-9
		10-6
		41-37
		40-36
		41-37‡
		2k
		37-36 40-36
		41-37
		41-37‡
		10-6
		4k
		40-36 40-36 41-37‡
		41-37
		10-6
		8k
		40-36 40-36 41-37‡
		41-37
		10-6
		16k 37-36 40-36
		41-37
		41-37‡
		10-6
		32k 40-36 40-36
		41-37
		41-37‡
		10-6
		Table 5: An example from DROPnum (same as Figure 1 and Table 2), with its answer text ‘4’ and a subset of
		the solution set (Z), containing two of ‘41-38’ (which ‘41’ come from different mentions; one denoted by ‡ for
		distinction), ‘40-36’ and ‘10-4’
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows one example on DROPnum
		with the answer text ‘4’, along with the model’s
		top 1 prediction and a subset of Z
	</Extractive Summary>
</Paper ID=ument284>


<Paper ID=ument284> <Table ID =6>
	<Abstractive Summary> =
		74
		Dataset
		batch size
		τ
		TRIVIAQA
		20
		15K
		NARRATIVEQA
		20
		20K
		TRIVIAQA-OPEN
		192
		4K
		NATURALQUESTIONS-OPEN
		192
		8K
		DROPnum with BERT
		14
		10K
		DROPnum with QANET
		14
		None
		WIKISQL
		10
		None
		Table 6: (Left) Ablations with varying values of τ on TRIVIAQA
	</Abstractive Summary>
</Paper ID=ument284>


<Paper ID=ument284> <Table ID =7>
	<Abstractive Summary> =
		Table 7: An example from TRIVIAQA with multiple spans of the answer text (underlined)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows one example from
		TRIVIAQA where the answer text (Montgomery)
		is mentioned in the paragraph multiple times
	</Extractive Summary>
</Paper ID=ument284>


<Paper ID=ument284> <Table ID =8>
	<Abstractive Summary> =
		Ours
		t
		Pred
		Z (ordered by P(z|x; θt))
		1k
		10+two eight+two eight+two‡
		34-24
		10
		2k
		30-24
		34-24
		eight+two
		eight+two‡
		10
		4k
		34+24
		34-24
		eight+two
		eight+two‡
		10
		8k
		34+24
		34-24
		eight+two
		10
		eight+two‡
		16k
		34-24
		34-24
		eight+two
		eight+two‡
		10
		32k
		34-24
		34-24
		eight+two
		eight+two‡
		10
		MML
		t
		Pred
		Z (ordered by P(z|x; θt))
		1K
		10+two
		eight+two eight+two‡
		10
		34-24
		2k
		eight+two eight+two eight+two‡
		34-24
		10
		4k
		24+5
		eight+two eight+two‡
		34-24
		10
		8k
		24+5
		eight+two eight+two‡
		34-24
		10
		16k
		34-24
		34-24
		eight+two
		eight+two‡
		10
		32k
		24+5
		34-24
		eight+two
		eight+two‡
		10
		Table 8: An example from DROPnum, with its answer text ‘10’ and a subset of Z, containing ‘10’, two of
		‘eight+two’ (which ‘eight’ come from different mentions; one denoted by ‘‡’ for distinction) and ‘34-24’
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows predictions from the
		model with our method and that with MML ob-
		jective over training procedure
	</Extractive Summary>
</Paper ID=ument284>


<Paper ID=ument284> <Table ID =9>
	<Abstractive Summary> =
		, Ship Type, Principal victims, Date
		A
		Select Name where Ship Type = battleship and Date = may 13, 1915
		P
		Select Name where Date = may 13, 1915
		Table 9: Four examples from WIKISQL where the prediction from the model is different from annotated SQL
		query, although the executed answers are the same
	</Abstractive Summary>
</Paper ID=ument284>


<Paper ID=ument285> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Results by each model (column 2) in each task (1)
	</Abstractive Summary>
</Paper ID=ument285>


<Paper ID=ument285> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Average accuracy across 3 runs by a model
		in a hard test set
	</Abstractive Summary>
	<Extractive Summary> =
		As reported in Table 2, the accuracy ob-
		tained by CNN+LSTM+SA and FiLM does not
		dramatically decrease compared to POS (∼94%
		vs
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, only FiLM
		is above chance level in this test set, with 65% ac-
		curacy
	</Extractive Summary>
</Paper ID=ument285>


<Paper ID=ument285> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Compositional task
	</Abstractive Summary>
	<Extractive Summary> =
		As reported in Table 3, this turns out not to
		be the case
	</Extractive Summary>
</Paper ID=ument285>


<Paper ID=ument286> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of each of the NPI licensing environments generated
	</Abstractive Summary>
	<Extractive Summary> =
		The set contains
		nine NPI licensing environments (Table 1), and
		3https://alexwarstadt
	</Extractive Summary>
</Paper ID=ument286>


<Paper ID=ument286> <Table ID =2>
	<Abstractive Summary> =
		]
		Table 2: Example 2⇥2⇥2 paradigm using the Questions environment
	</Abstractive Summary>
	<Extractive Summary> =
		Each
		data sample in the dataset contains a sentence, a
		Boolean label which indicates whether the sen-
		tence is grammatically acceptable or not, and three
		Boolean meta-data variables (licensor, NPI, and
		scope; Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows
		an example paradigm
	</Extractive Summary>
</Paper ID=ument286>


<Paper ID=ument286> <Table ID =3>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		The paradigm for Simple
		Questions is given in Table 3 in the Appendix; it
		forms only 2 minimal pairs
	</Extractive Summary>
</Paper ID=ument286>


<Paper ID=ument286> <Table ID =4>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 in the Appendix shows the participants’
		scores transformed into a Boolean judgment of 0
		(unacceptable, score  3) or 1 (acceptable, score
		� 4) and presented as the percentage of ‘accept-
		able’ ratings assigned to the sentences in each of
		the NPI-licensing environments
	</Extractive Summary>
</Paper ID=ument286>


<Paper ID=ument287> <Table ID =1>
	<Abstractive Summary> =
		5M
		LSTM (frWaC)
		Subset of frWaC
		∼ 138M
		Table 1: A summary of models tested
	</Abstractive Summary>
	<Extractive Summary> =
		Condition
		m
		f
		total
		m and m
		38
		0
		38
		m and f
		10
		1
		11
		f and m
		9
		0
		9
		f and f
		0
		16
		16
		m or m
		1
		0
		1
		m or f
		0
		0
		0
		f or m
		1
		0
		1
		f or f
		0
		1
		1
		Table 10: Frequency of gender agreement patterns in
		FTB
	</Extractive Summary>
</Paper ID=ument287>


<Paper ID=ument287> <Table ID =2>
	<Abstractive Summary> =
		git
		Condition
		Sentence
		Npl
		The windows is/are
		Nsg
		The window is/are
		Table 2: Conditions of number agreement in Non-
		coordination Agreement experiment
	</Abstractive Summary>
	<Extractive Summary> =
		Con-
		ditions are given in Table 2 and Table 3
	</Extractive Summary>
</Paper ID=ument287>


<Paper ID=ument287> <Table ID =3>
	<Abstractive Summary> =
		MPL/FPL
		Table 3:
		Conditions of gender agreement in Non-
		coordination Agreement experiment
	</Abstractive Summary>
</Paper ID=ument287>


<Paper ID=ument287> <Table ID =4>
	<Abstractive Summary> =
		In order to assess whether the neural models
		learn the basic CoordNP licensing for English, we
		Condition
		Sentence
		pl and pl
		The doors and the windows is/are
		sg and pl
		The door and the windows is/are
		pl and sg
		The doors and the window is/are
		sg and sg
		The door and the window is/are
		pl or pl
		The doors or the windows is/are
		sg or pl
		The door or the windows is/are
		pl or sg
		The doors or the window is/are
		sg or sg
		The door or the window is/are
		Table 4: Conditions of number agreement in Simple
		Coordination experiment
	</Abstractive Summary>
</Paper ID=ument287>


<Paper ID=ument287> <Table ID =5>
	<Abstractive Summary> =
		MPL/FPL
		Table 5: Conditions for the and-coordination experi-
		ment
	</Abstractive Summary>
	<Extractive Summary> =
		)
		To assess whether the French neural models
		learned humanlike gender agreement, we created
		24 test items, following the examples in Table 5,
		and measured the masculine expectation
	</Extractive Summary>
</Paper ID=ument287>


<Paper ID=ument287> <Table ID =6>
	<Abstractive Summary> =
		Condition
		Sentence
		pl and pl
		I think that the doors and the windows is/are
		sg and pl
		I think that the door and the windows is/are
		pl and sg
		I think that the doors and the window is/are
		sg and sg
		I think that the door and the window is/are
		Table 6: Conditions of number agreement in Complex
		Coordination Control experiment
	</Abstractive Summary>
	<Extractive Summary> =
		1, following the conditions in Table 6 (for num-
		ber agreement), testing only and coordination
	</Extractive Summary>
</Paper ID=ument287>


<Paper ID=ument287> <Table ID =7>
	<Abstractive Summary> =
		2895
		Condition
		Sentence
		pl and pl
		I ﬁxed the doors and the windows is/are
		sg and pl
		I ﬁxed the door and the windows is/are
		pl and sg
		I ﬁxed the doors and the window is/are
		sg and sg
		I ﬁxed the door and the window is/are
		Table 7: Conditions of number agreement in Complex
		Coordination Critical experiment
	</Abstractive Summary>
</Paper ID=ument287>


<Paper ID=ument287> <Table ID =8>
	<Abstractive Summary> =
		However, whereas the f and m and m and f gen-
		der expectations are not signiﬁcantly different
		from zero in the control condition, in the critical
		condition they pattern with the purely masculine
		Condition
		Sentence preamble
		Vpl Npl
		What are the doors and
		Vpl Nsg
		What are the door and
		Vsg Nsg
		What is the door and
		Table 8: Conditions in Inverted Coordination experi-
		ment
	</Abstractive Summary>
</Paper ID=ument287>


<Paper ID=ument287> <Table ID =9>
	<Abstractive Summary> =
		Furthermore, the range of values is re-
		duced in the RNNG PTB-coord model, compared
		PTB
		FTB
		Condition
		sg
		pl
		total
		sg
		pl
		total
		pl and pl
		0
		67
		67
		1
		116
		116
		sg and pl
		0
		72
		72
		0
		50
		50
		pl and sg
		0
		11
		11
		0
		30
		30
		sg and sg
		7
		213
		220
		5
		288
		293
		pl or pl
		0
		2
		2
		0
		14
		14
		sg or pl
		0
		0
		0
		0
		0
		0
		pl or sg
		0
		1
		1
		0
		1
		1
		sg or sg
		5
		1
		6
		5
		8
		13
		Table 9: Frequency of number agreement patterns in
		PTB and FTB
	</Abstractive Summary>
	<Extractive Summary> =
		B
		PTB/FTB Agreement Patterns
		We present statistics of subject/predicate agree-
		ment patterns in the Penn Treebank (PTB) and
		French Treebank (FTB) in Table 9 and 10
	</Extractive Summary>
</Paper ID=ument287>


<Paper ID=ument287> <Table ID =10>
	<Abstractive Summary> =
		Condition
		m
		f
		total
		m and m
		38
		0
		38
		m and f
		10
		1
		11
		f and m
		9
		0
		9
		f and f
		0
		16
		16
		m or m
		1
		0
		1
		m or f
		0
		0
		0
		f or m
		1
		0
		1
		f or f
		0
		1
		1
		Table 10: Frequency of gender agreement patterns in
		FTB
	</Abstractive Summary>
</Paper ID=ument287>


<Paper ID=ument288> <Table ID =1>
	<Abstractive Summary> =
		691
		Table 1: BPC scores (lower is better) for the ZERO-SHOT learning setting, with the uninformed prior (NINF) and
		the universal prior (UNIV): see §2 for the descriptions of the priors
	</Abstractive Summary>
	<Extractive Summary> =
		Comparing the columns of the
		BARE and OEST models in Table 1 reveals that
		the non-conditional baseline BARE is superior for
		71 / 77 languages (the exceptions being Chamorro,
		Croatian, Italian, Swazi, Swedish, and Tuareg)
	</Extractive Summary>
	<Extractive Summary> =
		For
		clarity’s sake, we exclude this batch of results from
		Table 1 and Table 3, as this method proves to be
		consistently worse than OEST
	</Extractive Summary>
</Paper ID=ument288>


<Paper ID=ument288> <Table ID =2>
	<Abstractive Summary> =
		903
		Table 2: BPC results (lower is better) for the JOINT learning setting, with the uninformed NINF prior
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results and Analysis
		The results for our experiments are grouped in Ta-
		ble 1 for the ZERO-SHOT regime, in Table 3 for the
		FEW-SHOT regime, and in Table 2 for the JOINT
		multilingual regime, which constitutes a ceiling to
		cross-lingual transfer performances
	</Extractive Summary>
</Paper ID=ument288>


<Paper ID=ument288> <Table ID =3>
	<Abstractive Summary> =
		731
		Table 3: BPC scores (lower is better) for the FEW-SHOT learning setting, with NINF, FITU and UNIV priors
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results and Analysis
		The results for our experiments are grouped in Ta-
		ble 1 for the ZERO-SHOT regime, in Table 3 for the
		FEW-SHOT regime, and in Table 2 for the JOINT
		multilingual regime, which constitutes a ceiling to
		cross-lingual transfer performances
	</Extractive Summary>
	<Extractive Summary> =
		On
		the other hand, the same columns in Table 3 and Ta-
		ble 2 reveal an opposite pattern: OEST outperforms
		the BARE baseline in 70 / 77 languages
	</Extractive Summary>
	<Extractive Summary> =
		For
		clarity’s sake, we exclude this batch of results from
		Table 1 and Table 3, as this method proves to be
		consistently worse than OEST
	</Extractive Summary>
</Paper ID=ument288>


<Paper ID=ument288> <Table ID =4>
	<Abstractive Summary> =
		2911
		LIT
		javen šuksyr sun siriai tes pije nuks
		SHI
		ereswrin an daγtartnaas ni mad yanó
		NOR
		s hech far binje alrn bre a ver e hior
		JAK
		ﬁ pelo ayok musam nejaz jih tewat ushi
		KEK
		sx er taj chan linam laj âtebke naque
		SWE
		ssiar ˇrades perdeshen heklui tart si a
		JIV
		da tum suuam sιtas nekkin una tekaru ni
		DIK
		e wEn ke nuN ni piyitia de run ye e ke
		DJE
		a ciya toi milkak mo to yen nga suci
		EWE
		å mula pe ose le ake mente amesa ke kul
		SLK
		o je to temokoé lostave sa jesé gukli
		ALB
		I kur je ki thet je ji tin nuk t tho
		CES
		e je jek jem neuteN rekssýj jazá níb ws
		CNI
		u pen mireshisinoe airitcsa ateani yi
		POR
		uˇc somo ai jegparase saves e iper to
		POT
		neta ynimka nekin linaayi meu carií a
		SPA
		esquár y lues dusme allis nencec adi
		ZUL
		ởnakan kuná bencro krileke konusti k
		GLV
		ayr sh˙zi ayn ai sephson a gil or geee
		QUW
		ai chimira kachisinyra poi apre asyu
		POL
		eteni na hidi cếho o˙z swchj jeci i cil
		AGR
		ji ica ama kujaa muri wajetar aumam hu
		QUC
		ûs xe cä wija ro pio kin cbi’ ij jejac
		DOP
		btElO ι telaγa kO nϵι zûγι nEk@ pO
		WAL
		banjake la dos que benthi shivegina
		EUS
		cerer nagcermac istirinun qatserite
		XHO
		ukayla azigeecoa kosubentisiili jen maky
		HUN
		elyet a bukot aky azraá ot mu háláj y
		SOM
		ao kun adku i sir jija i befey yadui
		GLA
		o e kere hhó sho dhöìr te ilailui a tu a
		TGL
		ikugy peo asha atan kao amai kain ak a
		PCK
		u gihiha ki mi dhia mea la hen a puh ih
		CJP
		pae yei aje kin trheka pän awawa ri s
		AFR
		mal hoor in e sheei wer var buerkeas en
		ACU
		animmhi mustatur tukaw aants aastasai a
		USP
		okan mi ykis ris rajajkujij taka ja
		FIN
		i koin suu meit ja ii soi tetot jasw
		IND
		t berka duhah menkad kemia ukus keri ya
		MRI
		oki ka benoka ai ki kimanka pikaka ko
		ROM
		hal kus seke nukertia dehe neshes hos n
		SLV
		ˇciˇcvim koko si neˇce pau ku meta noj ne
		TMH
		@rofm sibarn awigtir ϵli d usi leped
		HRV
		ca ka te zet jon jem nezin isak ve u
		ITA
		tri cordia io si si conse de namni nel
		EPO
		j li inij keris ec xom el e sepon kaj
		SRP
		e se a nil do zasom kuz je sefe nij hoˇc
		AMU
		´mibinya na ñero melee cano’ ndo’ cy’oc
		NLD
		e suet en de semeshord ak abaido zin
		KBH
		¨xe aquangmomnaynangmuacha tojam
		LAT
		ifte quissi fetam remnas emens in timnex
		CEB
		abithon kayay isa atoug giraban sula
		MAM
		í la Nil a cheh tjea nut tej quxen kaj
		GBI
		fuma ome pani de imoako kema kaye ntul
		VIE
		hẩ kì đãi bi ầt ni γì sa hiổ v¯u r
		ENG
		g ban urse auth ahen ant msesher at nhe
		ISL
		j noka nie leli maken ti aide ni itsim a
		EST
		inam acha dius dempegun geben parug j
		SNA
		xe yare ske tengker ci bendar nu derbe
		CHA
		ê duka ka kina kia nextis ne aka nisa
		RON
		ma awa nasil ko khe ni koy koj tikis t
		FRA
		dis assan in man usia issokoj mulel e me
		KAB
		je cana ka casa chomdis mear de ber h
		DJK
		okrana anginar matom iliantarinta a non
		NHG
		chun neyal den ma kashtaka asa as riste
		LAV
		ilu kagsa eriri isi paj ewri bus os
		DAN
		dnepse aa aye sas ningli inas giksaj abe
		BSN
		as juhma yainawa nusa wali apai basti
		PPK
		ios yena mona kemewascoj ni ne maa
		HAT
		a kuneati ua veskos oramaj meseqen ye k
		SSW
		nta yoti gesi kela nii ikasgaber ni tus
		TUR
		che a shachmo ềspi meng rinnaj e ish em
		WOL
		alen kokpan fed man benu pei ei kestam
		AKE
		n jes silem semmo caja arka wagtoa doo
		DEU
		ke giko si obi rer nin eber tun ke ele
		CHQ
		shas nej neysakun kina alistad mesabe
		CAK
		tej je awem titoj lunik c’u chis m ni
		PLT
		Vwi meyak me imai anet alavis edte kin
		Table 4: Randomly generated text on observed languages (top) and held-out languages (bottom) in the 4th split
	</Abstractive Summary>
</Paper ID=ument288>


<Paper ID=ument289> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An illustration of the pointers in an example explanation of /r/ChangeMyView
	</Abstractive Summary>
	<Extractive Summary> =
		, in Table 1, “most hit music
		artists today are bad musicians”)
	</Extractive Summary>
	<Extractive Summary> =
		As shown
		in Table 1, the explanation naturally points to, or
		echoes, part of the explanandum (including both
		the persuasive comment and the original post) and
		in this case highlights the argument of “music
		serving different purposes
	</Extractive Summary>
	<Extractive Summary> =
		As shown in
		Table 1, the explanation not only provides a ra-
		tionale, it can also include other discourse acts,
		such as expressing gratitude
	</Extractive Summary>
</Paper ID=ument289>


<Paper ID=ument289> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Basic statistics of the training dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 provides basic statistics of
		the training tuples and how they compare to other
		comments
	</Extractive Summary>
</Paper ID=ument289>


<Paper ID=ument289> <Table ID =3>
	<Abstractive Summary> =
		↑↑↑↑
		Table 3: Features to capture the properties of a word in the context of an explanandum
	</Abstractive Summary>
	<Extractive Summary> =
		We de-
		velop the following ﬁve groups of features to cap-
		ture properties of how a word is used in the ex-
		planandum (see Table 3 for the full list):
		• Non-contextual properties of a word
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 further shows the intuition for includ-
		ing each feature, and condensed t-test results af-
		ter Bonferroni correction
	</Extractive Summary>
	<Extractive Summary> =
		Re-
		call that in Table 3 we show that words that have
		similar POS behavior between the OP and PC are
		more likely to be echoed in the explanation
	</Extractive Summary>
</Paper ID=ument289>


<Paper ID=ument289> <Table ID =4>
	<Abstractive Summary> =
		598
		Table 4: Ablation performance with XGBoost on con-
		tent words and stopwords (each ablated model is tuned
		based on performance on all words)
	</Abstractive Summary>
	<Extractive Summary> =
		Ablation performance (Table 4)
	</Extractive Summary>
</Paper ID=ument289>


<Paper ID=ument289> <Table ID =5>
	<Abstractive Summary> =
		111
		Table 5: Performance on ﬁve non-function part-of-
		speech tags (sorted by performance within content
		words)
	</Abstractive Summary>
	<Extractive Summary> =
		Nouns are the most reliably predicted part-
		of-speech tag within content words (Table 5)
	</Extractive Summary>
</Paper ID=ument289>


<Paper ID=ument289> <Table ID =6>
	<Abstractive Summary> =
		02
		Table 6: ROUGE scores (F1) on the test dataset (Lin,
		2004)
	</Abstractive Summary>
	<Extractive Summary> =
		Consistent with results in sequence tagging for
		word-level echoing prediction, our proposed fea-
		tures can enhance a neural model with copying
		mechanisms (see Table 6)
	</Extractive Summary>
</Paper ID=ument289>


<Paper ID=ument29> <Table ID =1>
	<Abstractive Summary> =
		64
		Table 1: The proposed MIMO outperforms existing methods on tag prediction and tuple extraction in the BioCFE
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Overall Performance
		Table 1 shows that the proposed multi-input multi-
		output sequence labeling model with a BERT en-
		coder consistently performs the best over all the
		baselines on tag prediction and tuple extraction
	</Extractive Summary>
</Paper ID=ument29>


<Paper ID=ument29> <Table ID =2>
	<Abstractive Summary> =
		64
		Table 2: The proposed MIMO that employs (a) multi-input Language Models, POS tags, and Concept-Attribute-
		Phrase sequences, (b) multi-output tag sequences, (c) BERT-based encoder performs the best on tuple extraction
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Ablation Study
		Table 2 compares variants of the proposed model
		to evaluate the effectiveness of the following com-
		ponents: (1) multi-input sequences, such as none,
		or one (in LM, POS, and CAP), double combi-
		nation, or triple combination; (2) multi-input en-
		coder model, BiLSTM or BERT; (3) multi-output
		module, with the RNT layer only (generating one
		fact tag sequence and one condition tag sequence)
		or a combination of RNT and TCT layers (gener-
		ating multiple sequences for each tuple type)
	</Extractive Summary>
</Paper ID=ument29>


<Paper ID=ument29> <Table ID =3>
	<Abstractive Summary> =
		00
		Table 3: The BERT-LSTMd MIMO model performs
		the best on tuple extraction in BioNLP2013
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Error Analysis
		Table 3 presents the confusion matrices made by
		the BERT-based MIMO on predicting non-“O”
		tags for facts and conditions, respectively
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results on BioNLP2013
		As shown in Table 3, the BERT-LSTMd MIMO
		model achieves an F1 score of 0
	</Extractive Summary>
</Paper ID=ument29>


<Paper ID=ument290> <Table ID =1>
	<Abstractive Summary> =
		Debatepe-
		# Topics
		# Frames
		# Merged Frames
		# Arguments
		465
		1 645
		1 623
		12 326
		Table 1: Counts of topics, frames, merged frames, and
		arguments in the webis-argument-framing-19 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Webis-Argument-Framing-19 Dataset
		Table 1 shows general statistics of the ﬁnal dataset
		after crawling and preprocessing, called webis-
		argument-framing-19
	</Extractive Summary>
</Paper ID=ument290>


<Paper ID=ument290> <Table ID =2>
	<Abstractive Summary> =
		Symbol
		Meaning
		a
		An argument
		c
		The conclusion of an argument
		A
		A set of arguments
		¯A
		A set of arguments on the same topic
		A
		A set of sets of arguments
		F
		A frame
		v
		A word
		V
		A vocabulary
		E
		A topic extraction model
		Table 2: Notation of the symbols used in the approach
		represents its semantics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2
		lists the symbols used in this section along with
		their meaning
	</Extractive Summary>
</Paper ID=ument290>


<Paper ID=ument290> <Table ID =3>
	<Abstractive Summary> =
		44
		Table 3: Bcubed F1-score of the topic clustering algo-
		rithm for each semantic space and the corresponding
		count of topics found
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the highest corresponding F1-score
		and the count of topic clusters for each semantic
		space
	</Extractive Summary>
</Paper ID=ument290>


<Paper ID=ument290> <Table ID =4>
	<Abstractive Summary> =
		24
		Table 4: Best bcubed F1-score, precision, and recall for the topic extraction models Eq
		1, E2 and without topic
		removal (baseline) in the generic, topic-speciﬁc and main frame experiments together with the corresponding
		bcubed F1-score in topic clustering
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Topic Removal
		Table 4 shows the results of the topic removal exper-
		iment and frame clustering experiment
	</Extractive Summary>
	<Extractive Summary> =
		3
		Frame Clustering
		Table 4 shows the results of the frame clustering al-
		gorithm in the experiments: generic, topic-speciﬁc,
		and main
	</Extractive Summary>
</Paper ID=ument290>


<Paper ID=ument291> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Two posts from the CMV sub-reddit where a claim is targeted by the response
		Figure 2: Schematic of our pipeline involving various
		components
	</Abstractive Summary>
	<Extractive Summary> =
		In the ex-
		ample in Table 1, the response contains an exact
		quote of a claim in the original post
	</Extractive Summary>
	<Extractive Summary> =
		For the example provided in
		Table 1, this would result in one sentence included
		in the summary
	</Extractive Summary>
</Paper ID=ument291>


<Paper ID=ument291> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: F-scores for 3-way Classiﬁcation: Claim (C),
		Premise (P), Non-Argument (NA)
		5
		Experiments and Results
		We use the CMV data from Section 3
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows that our best model gains statis-
		tically signiﬁcant improvement over all the other
		models (p < 0
	</Extractive Summary>
</Paper ID=ument291>


<Paper ID=ument291> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Results for Intra-turn Relation Prediction with Gold and Predicted Premises
		Hyper-parameters are discussed in Appendix A
	</Abstractive Summary>
	<Extractive Summary> =
		Intra-turn Relations
		We report the results of
		our binary classiﬁcation task in Table 3 in terms of
		Precision, Recall and F-score for the “true” class,
		i
	</Extractive Summary>
</Paper ID=ument291>


<Paper ID=ument291> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Results for Inter-Turn Relation Prediction with Gold and Predicted Claims
		sults are obtained from ensembling the RST clas-
		siﬁer with BERT ﬁne-tuned on IMHO+context,
		for statistically signiﬁcant (p < 0
	</Abstractive Summary>
</Paper ID=ument291>


<Paper ID=ument291> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: Intra-Turn Relation Prediction with Varying Window Settings
		while the last four rows show a relation between
		a claim and premise in the QR data and the CMV
		data
	</Abstractive Summary>
</Paper ID=ument291>


<Paper ID=ument291> <Table ID =6>
	<Abstractive Summary> =
		]
		Table 6: CMV and Context Examples
		Role of Discourse
		We also provide examples
		that are predicted incorrectly by BERT but cor-
		rectly by our classiﬁer trained with RST features
	</Abstractive Summary>
	<Extractive Summary> =
		The ﬁrst two rows in Table 6 show
		a relation between a claim and premise in the
		IMHO+Context and the CMV data respectively
		2941
		Method
		Window
		Precision
		Recall
		F-Score
		Gold
		Pred
		Gold
		Pred
		Gold
		Pred
		All relations
		0 TO +1
		5
	</Extractive Summary>
</Paper ID=ument291>


<Paper ID=ument291> <Table ID =7>
	<Abstractive Summary> =
		Antithesis
		Joseph was just a
		regular Jew without
		the same kind of
		holiness as the
		other two
		Aren’t Mary and
		Joseph, two holy
		people especially
		perfect virgin Mary,
		both Jews? Wasn’t
		Jesus a Jew?
		Table 7: Predicted Discourse Relations in CMV
		7
		Conclusion
		We show how ﬁne-tuning on data-sets similar to
		the task of interest is often beneﬁcial
	</Abstractive Summary>
	<Extractive Summary> =
		For the ﬁrst example in Table 7 the RST parser
		predicts an Evaluation relation, which is an indi-
		cator of an argumentative relation according to our
		model
	</Extractive Summary>
</Paper ID=ument291>


<Paper ID=ument292> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example rule-based attacks that preserve the entailment relation of the original claim (within the deﬁnition
		of the FEVER shared task), perform simple negation and more complex negations
	</Abstractive Summary>
	<Extractive Summary> =
		We evaluate three different types of
		transformations: entailment preserving rewrites,
		simple negations and complex negations (see ex-
		amples in Table 1)
	</Extractive Summary>
</Paper ID=ument292>


<Paper ID=ument292> <Table ID =2>
	<Abstractive Summary> =
		32
		Table 2: Potency of adversaries where correctness rate is estimated using inspection of the generated instances
	</Abstractive Summary>
	<Extractive Summary> =
		Our summary results are pre-
		sented in two tables: Table 2 describes the potency
		of each adversary against the systems tested and
		Table 3 describes the resilience of each system to
		the adversaries
	</Extractive Summary>
</Paper ID=ument292>


<Paper ID=ument292> <Table ID =3>
	<Abstractive Summary> =
		28
		Table 3: Systems ranked by the resilience to adversarial attacks
	</Abstractive Summary>
	<Extractive Summary> =
		Our summary results are pre-
		sented in two tables: Table 2 describes the potency
		of each adversary against the systems tested and
		Table 3 describes the resilience of each system to
		the adversaries
	</Extractive Summary>
</Paper ID=ument292>


<Paper ID=ument292> <Table ID =4>
	<Abstractive Summary> =
		72
		Table 4: Breakdown of FEVER Scores of each system to each adversarial attack prior used for calculating resilience
		and potency
	</Abstractive Summary>
	<Extractive Summary> =
		Using the breakdown of
		FEVER Scores by system and adversary (reported
		in Table 4)), we observe that the NSMN was af-
		fected by the rule-based adversary and SEARS
		(FEVER Full) adversaries much more than trans-
		former model
	</Extractive Summary>
</Paper ID=ument292>


<Paper ID=ument292> <Table ID =5>
	<Abstractive Summary> =
		30
		Table 5: Effect of rule-based adversarial attacks on the evidence retrieval component of the pipelines considering
		sentence-level accuracy of the evidence
	</Abstractive Summary>
	<Extractive Summary> =
		Considering the evidence retrieval component
		(refer to Table 5), we ﬁnd that the adversarial
		instances did not affect the information retrieval
		component of some systems to the same extent as
		the NLI component
	</Extractive Summary>
</Paper ID=ument292>


<Paper ID=ument292> <Table ID =6>
	<Abstractive Summary> =
		23
		Table 6: Summary of label accuracy and FEVER Scores for instances used in rule-based adversarial attacks
	</Abstractive Summary>
	<Extractive Summary> =
		Considering the NLI stage of the systems (refer
		to Table 6), we observe that in an oracle environ-
		ment (using manually labelled evidence – assum-
		ing perfect evidence retrieval), both the ESIM and
		Decomposable Attention models for natural lan-
		guage inference suffer a stark decrease in accuracy
		when making predictions on the adversarial exam-
		ples
	</Extractive Summary>
</Paper ID=ument292>


<Paper ID=ument293> <Table ID =1>
	<Abstractive Summary> =
		Table 1:
		Description of Local Acceptability and re-
		lated attributes
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Attributes
		Table 1 shows the rubrics for local acceptability
		and the other three attributes that we annotated
		through crowdsourcing
	</Extractive Summary>
</Paper ID=ument293>


<Paper ID=ument293> <Table ID =2>
	<Abstractive Summary> =
		(2017), we deﬁne the local ac-
		ceptability of a sentence, based on the truth-value
		of the sentence deﬁned in the truth-conditional
		2957
		Attribute Value
		Reason 1
		Reason 2
		I did not know the information
		I am not familiar with the topic
		I am an expert on the subject
		I did not know the information before,
		but came to know it by reading the
		previous sentences
		It is not directly stated so far, but I can
		infer it from the previous sentences
		The same information was stated in
		one of the previous sentences
		I can verify it using my knowledge
		I can logically verify the information
		It is a transition sentence that is
		meaningless
		strong accept
		It is not a factual information, but I
		agree with the statement
		It is a fact
		Table 2: Examples for nonsensical reason
		Attribute Value
		Reason 1
		Reason 2
		weakly disputable
		Some may think different on some
		details
		It is a famous conﬂict
		I already knew the information
		before I read this document
		I know it from my personal experience
		I am not familiar with the topic
		I can verify it by short-time googling
		I would ﬁnd a direct reference to
		verify it
		I can interview the people related to it
		accept
		It is a subjective statement, but I agree
		with the statement
		It can easily be veriﬁed
		Table 3: Examples for unnatural reason
		theory (Lewis, 1970), as: A sentence is locally ac-
		ceptable if the truth-value of the sentence is ratio-
		nally worthy of being believed to be true (Yang
		et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows examples for non-
		sensical reason
	</Extractive Summary>
</Paper ID=ument293>


<Paper ID=ument293> <Table ID =3>
	<Abstractive Summary> =
		(2017), we deﬁne the local ac-
		ceptability of a sentence, based on the truth-value
		of the sentence deﬁned in the truth-conditional
		2957
		Attribute Value
		Reason 1
		Reason 2
		I did not know the information
		I am not familiar with the topic
		I am an expert on the subject
		I did not know the information before,
		but came to know it by reading the
		previous sentences
		It is not directly stated so far, but I can
		infer it from the previous sentences
		The same information was stated in
		one of the previous sentences
		I can verify it using my knowledge
		I can logically verify the information
		It is a transition sentence that is
		meaningless
		strong accept
		It is not a factual information, but I
		agree with the statement
		It is a fact
		Table 2: Examples for nonsensical reason
		Attribute Value
		Reason 1
		Reason 2
		weakly disputable
		Some may think different on some
		details
		It is a famous conﬂict
		I already knew the information
		before I read this document
		I know it from my personal experience
		I am not familiar with the topic
		I can verify it by short-time googling
		I would ﬁnd a direct reference to
		verify it
		I can interview the people related to it
		accept
		It is a subjective statement, but I agree
		with the statement
		It can easily be veriﬁed
		Table 3: Examples for unnatural reason
		theory (Lewis, 1970), as: A sentence is locally ac-
		ceptable if the truth-value of the sentence is ratio-
		nally worthy of being believed to be true (Yang
		et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows examples for unnat-
		ural reason
	</Extractive Summary>
</Paper ID=ument293>


<Paper ID=ument293> <Table ID =4>
	<Abstractive Summary> =
		The in-house annotation
		was for the same attributes but the students wrote
		down the reasons by themselves without any given
		2958
		Attribute
		Attribute Value
		Reason 1
		Reason 2
		label
		Local
		Acceptability
		strong accept
		It is a subjective statement,
		but I agree with the statement
		It is a fact
		Nonsensical
		Knowledge
		Awareness
		I did not know the information
		I am not familiar with the topic
		I am an expert on the subject
		Nonsensical
		Veriﬁability
		I might ﬁnd an off-line way to verify it,
		but it will be very hard
		I could visit related places
		to obtain the information
		I can interview the people related to it
		Natural
		Disputability
		weakly disputable
		Some may think different
		on some details
		It is a famous conﬂict
		Unnatural
		Table 4: An example for the two-step reason selection for a sentence
		option
	</Abstractive Summary>
	<Extractive Summary> =
		If an annotator chose the
		options in Table 4 for a sentence, the sentential an-
		notation contains two nonsensical reasons and an
		unnatural reason
	</Extractive Summary>
</Paper ID=ument293>


<Paper ID=ument294> <Table ID =1>
	<Abstractive Summary> =
		92
		Table 1: Inter-annotator agreement
	</Abstractive Summary>
</Paper ID=ument294>


<Paper ID=ument294> <Table ID =2>
	<Abstractive Summary> =
		Test Data
		Unique
		Source Language
		from WMT Years
		Source Contexts
		German
		2011-2015,17
		7,823
		Czech
		2011-2015,2017
		6,713
		French
		2011-2015
		4,659
		Russian
		2013,2014,2017
		4,513
		Spanish
		2011-2013
		4,417
		Finnish
		2015,2017
		1,551
		Turkish
		2017
		1,372
		Hindi
		2014
		921
		Chinese
		2017
		696
		Latvian
		2017
		652
		Table 2: Pronoun test suite for MT systems: English as
		a target and various languages as a source
	</Abstractive Summary>
	<Extractive Summary> =
		(Table 2)
	</Extractive Summary>
</Paper ID=ument294>


<Paper ID=ument294> <Table ID =3>
	<Abstractive Summary> =
		Data
		Source
		#Unique Pairs
		Training
		WMT11-13,15
		97,461
		Development
		WMT14
		5,727
		Development (noisy)
		WMT14
		6,635
		Test (noisy)
		WMT15,17
		2,000
		Table 3: Statistics about our dataset
	</Abstractive Summary>
	<Extractive Summary> =
		This
		yielded 97,461 reference translation (positive text)
		— unique system output (negative text) pairs for
		training, taken from WMT11,12,13,15 (Table 3)
	</Extractive Summary>
</Paper ID=ument294>


<Paper ID=ument294> <Table ID =4>
	<Abstractive Summary> =
		69
		Table 4: Results on WMT14 with different contexts
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results and Analysis
		The ﬁrst three experiments in Table 4 show results
		for the ‘No Context’ setting
	</Extractive Summary>
</Paper ID=ument294>


<Paper ID=ument294> <Table ID =5>
	<Abstractive Summary> =
		35
		—-
		Table 5: Accuracy and AC1 Agreement for the ELMo-
		based model predictions on the study dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results
	</Extractive Summary>
</Paper ID=ument294>


<Paper ID=ument295> <Table ID =1>
	<Abstractive Summary> =
		18,515
		Temp
		Event Causal
		ConceptNet
		947
		Cont
		Event Subevent
		ConceptNet
		626
		Exp & Temp
		Table 1: Overview of relation types
	</Abstractive Summary>
	<Extractive Summary> =
		Knowledge Preprocessing: Table 1 gives an
		overview of the relation types used in our exper-
		iments and the number of triplets (clusters) identi-
		ﬁed in the PDTB dataset
	</Extractive Summary>
</Paper ID=ument295>


<Paper ID=ument295> <Table ID =2>
	<Abstractive Summary> =
		39
		Table 2: Top-level Multi-class Classiﬁcation Results on PDTB
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Experiment Results
		Table 2 shows the comparisons
	</Extractive Summary>
</Paper ID=ument295>


<Paper ID=ument295> <Table ID =3>
	<Abstractive Summary> =
		08
		Table 3: Second-level Multi-class Classiﬁcation Re-
		sults on PDTB
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Second-level Multi-class Classiﬁcation
		Table 3 reports the performance of our models for
		predicting second-level ﬁne-grained discourse re-
		lations
	</Extractive Summary>
</Paper ID=ument295>


<Paper ID=ument295> <Table ID =4>
	<Abstractive Summary> =
		65
		Table 4: Impact of the Knowledge Regularization
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 4, we can see that the model with-
		out knowledge regularizer performs signiﬁcantly
		worse than the full model and even worse than the
		base model, which supports our hypothesis that
		using external knowledge or linguistic constraints
		2984
		Model
		# of errors
		Base Model
		217
		Full Model
		193
		Table 5: Number of errors made by both models for
		predicting implicit discourse relations with constraints
	</Extractive Summary>
</Paper ID=ument295>


<Paper ID=ument295> <Table ID =5>
	<Abstractive Summary> =
		From Table 4, we can see that the model with-
		out knowledge regularizer performs signiﬁcantly
		worse than the full model and even worse than the
		base model, which supports our hypothesis that
		using external knowledge or linguistic constraints
		2984
		Model
		# of errors
		Base Model
		217
		Full Model
		193
		Table 5: Number of errors made by both models for
		predicting implicit discourse relations with constraints
	</Abstractive Summary>
	<Extractive Summary> =
		Therefore, the overall perfor-
		mance gains achieved by the full model are mainly
		from better resolving implicit discourse relations
		with constraints, and as shown in Table 5, the full
		model made 24 less errors than base model for pre-
		dicting such implicit discourse relations
	</Extractive Summary>
</Paper ID=ument295>


<Paper ID=ument296> <Table ID =1>
	<Abstractive Summary> =
		53
		Table 1: Performance results of the compared methods
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 1 lists the accuracy (Acc), precision (Prec),
		recall (Rec), and F1 of the compared methods
	</Extractive Summary>
</Paper ID=ument296>


<Paper ID=ument296> <Table ID =2>
	<Abstractive Summary> =
		40
		Table 2: Results of language ablation tests
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the results of the ablation
		tests
	</Extractive Summary>
</Paper ID=ument296>


<Paper ID=ument297> <Table ID =1>
	<Abstractive Summary> =
		20
		Table 1: Data statistics of TripAtt dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		statistics of TripAtt
	</Extractive Summary>
	<Extractive Summary> =
		After that,
		6We observe in Table 1 that the average length of sum-
		mary is about seven
	</Extractive Summary>
</Paper ID=ument297>


<Paper ID=ument297> <Table ID =2>
	<Abstractive Summary> =
		74∗
		Table 2: ROUGE F1 scores (%) on the test set
	</Abstractive Summary>
</Paper ID=ument297>


<Paper ID=ument297> <Table ID =3>
	<Abstractive Summary> =
		33
		location, service
		Gold:
		3+ good location , but some lacks in service
		-
		-
		-
		location, service
		Table 3: A sample in test set shows high ROUGE may not result in better performance
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 illustrates an example
	</Extractive Summary>
</Paper ID=ument297>


<Paper ID=ument297> <Table ID =4>
	<Abstractive Summary> =
		510
		Table 4: Aspect-level Precision, Recall, and F1 scores
		(%) for different systems
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the aspect-level result, and we
		ﬁnd that ASN outperforms other models by a large
		margin, which shows summaries generated by our
		model can not only contain more correct words,
		but also in a higher consistency on aspects with
		references
	</Extractive Summary>
</Paper ID=ument297>


<Paper ID=ument297> <Table ID =5>
	<Abstractive Summary> =
		74
		Table 5: Ablation study on review summarization with
		attribute information
	</Abstractive Summary>
</Paper ID=ument297>


<Paper ID=ument297> <Table ID =6>
	<Abstractive Summary> =
		74
		Table 6: Effects of different attribute-based strategies
		on review summarization
	</Abstractive Summary>
</Paper ID=ument297>


<Paper ID=ument298> <Table ID =1>
	<Abstractive Summary> =
		length
		CNN
		92K
		656
		43
		Daily Mail
		219K
		693
		52
		NY Times
		655K
		530
		38
		PubMed
		133K
		3016
		203
		arXiv
		215K
		4938
		220
		Table 1: Comparison of news datasets and scientiﬁc pa-
		per datasets(Cohan et al
	</Abstractive Summary>
	<Extractive Summary> =
		These two new datasets contain
		much longer documents than all the news datasets
		(See Table 1) and are therefore ideal test-beds for
		the method we present in this paper
	</Extractive Summary>
</Paper ID=ument298>


<Paper ID=ument298> <Table ID =2>
	<Abstractive Summary> =
		11
		Table 2: Results on the arXiv dataset
	</Abstractive Summary>
	<Extractive Summary> =
		See caption
		of Table 2 above for details on compared models and
		notation
	</Extractive Summary>
	<Extractive Summary> =
		The performance of all models on arXiv and
		Pubmed is shown in Table 2 and Table 3, respec-
		tively
	</Extractive Summary>
	<Extractive Summary> =
		Finally, the result of Lead (Table 2, 3) shows
		that scientiﬁc papers have less position bias than
		news; i
	</Extractive Summary>
</Paper ID=ument298>


<Paper ID=ument298> <Table ID =3>
	<Abstractive Summary> =
		60
		Table 3: Results on the Pubmed dataset
	</Abstractive Summary>
	<Extractive Summary> =
		The performance of all models on arXiv and
		Pubmed is shown in Table 2 and Table 3, respec-
		tively
	</Extractive Summary>
</Paper ID=ument298>


<Paper ID=ument298> <Table ID =4>
	<Abstractive Summary> =
		03)
		Table 4: Ablation study on the Pubmed dataset, with
		all the documents(up) and a subset of long documents
		(down, > 6000 words)
	</Abstractive Summary>
</Paper ID=ument298>


<Paper ID=ument298> <Table ID =5>
	<Abstractive Summary> =
		04)
		Table 5: Ablation study on arXiv dataset, with all
		documents (up) and a subset of long document(down,
		> 9000 words)
	</Abstractive Summary>
	<Extractive Summary> =
		The
		results for Pubmed and arXiv are shown in Table
		4 and Table 5, respectively
	</Extractive Summary>
</Paper ID=ument298>


<Paper ID=ument299> <Table ID =1>
	<Abstractive Summary> =
		2
		Dataset size
		728, 321
		76, 105
		Table 1: Dataset statistics for two sample datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the statistics
		of two sample datasets
	</Extractive Summary>
</Paper ID=ument299>


<Paper ID=ument299> <Table ID =2>
	<Abstractive Summary> =
		06
		Table 2: BLEU-4 score with two generation models on
		21 datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the performances of
		KBAtt and Seq2Seq+Copy in terms of BLEU-4
		score
	</Extractive Summary>
</Paper ID=ument299>


<Paper ID=ument299> <Table ID =3>
	<Abstractive Summary> =
		59
		Table 3: BLEU-4 scores (%) on WikiBio dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results on WikiBio
		To compare with state-of-the-art models, Table 3
		shows the results on the WikiBio dataset
	</Extractive Summary>
</Paper ID=ument299>


<Paper ID=ument299> <Table ID =4>
	<Abstractive Summary> =
		64%
		Table 4: Human based evaluation results
	</Abstractive Summary>
</Paper ID=ument299>


<Paper ID=ument299> <Table ID =5>
	<Abstractive Summary> =
		9
		4559
		Table 5: Dataset statistics for all datasets used in this paper
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Full Dataset Statistics
		Table 5 summarizes the full dataset statistics about
		the twenty-one datasets used in this paper
	</Extractive Summary>
</Paper ID=ument299>


<Paper ID=ument3> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Text classiﬁcation accuracy, evaluated for each combination of acquisition and successor models using
		uncertainty sampling
	</Abstractive Summary>
	<Extractive Summary> =
		We
		report the performance of each model under all ac-
		quisition functions both in tables compiling results
		(Table 1 and Table 2 for classiﬁcation and NER,
		respectively) and graphically via learning curves
		that plot predictive performance as a function of
		train set size (Figure 2)
	</Extractive Summary>
</Paper ID=ument3>


<Paper ID=ument3> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: F1 measurements for the NER task, with training sets comprising 10% and 20% of the training pool
	</Abstractive Summary>
	<Extractive Summary> =
		We
		report the performance of each model under all ac-
		quisition functions both in tables compiling results
		(Table 1 and Table 2 for classiﬁcation and NER,
		respectively) and graphically via learning curves
		that plot predictive performance as a function of
		train set size (Figure 2)
	</Extractive Summary>
</Paper ID=ument3>


<Paper ID=ument3> <Table ID =3>
	<Abstractive Summary> =
		For the text classiﬁcation task, we compute
		disagreement using Kullback-Leibler divergence
		(McCallum and Nigamy, 1998), selecting docu-
		27
		Dataset
		# Classes
		# Documents
		Examples per Class
		Movie Reviews
		2
		10662
		5331, 5331
		Subjectivity
		2
		10000
		5000, 5000
		TREC
		6
		5952
		1300, 916, 95, 1288, 1344, 1009
		Customer Reviews
		2
		3775
		1368, 2407
		Table 3: Text classiﬁcation dataset statistics
	</Abstractive Summary>
</Paper ID=ument3>


<Paper ID=ument3> <Table ID =4>
	<Abstractive Summary> =
		974
		Table 4: Average Spearman’s rank correlation coefﬁcients (over ﬁve runs) of cosine distances between test set
		representations learned with native active learning and distances between those learned with transferred actively
		acquired datasets, at the end of the AL process
	</Abstractive Summary>
	<Extractive Summary> =
		As
		reported in Table 4, despite the aforementioned
		differences in predictive performance, the learned
		representations seem to be similar
	</Extractive Summary>
</Paper ID=ument3>


<Paper ID=ument30> <Table ID =1>
	<Abstractive Summary> =
		1
		Data and Evaluation Metrics
		Relation
		Mentions
		Event
		Mentions
		Event
		Arguments
		English
		8,738
		5,349
		9,793
		Chinese
		9,317
		3,333
		8,032
		Arabic
		4,731
		2,270
		4,975
		Table 1: Data statistics
	</Abstractive Summary>
	<Extractive Summary> =
		, 2006) for our
		experiments because it contains the most compre-
		hensive gold-standard relation and event annota-
		tions for three distinct languages: English, Chinese
		and Arabic (see Table 1)
	</Extractive Summary>
</Paper ID=ument30>


<Paper ID=ument30> <Table ID =2>
	<Abstractive Summary> =
		9
		batch size
		50
		optimization
		SGD
		Table 2: GCN hyperparameters
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the hy-
		perparameters of our model
	</Extractive Summary>
</Paper ID=ument30>


<Paper ID=ument30> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Relation Extraction: overall performance (F-
		score %) using perfect entity mentions
	</Abstractive Summary>
</Paper ID=ument30>


<Paper ID=ument30> <Table ID =4>
	<Abstractive Summary> =
		9
		–
		–
		Table 4: Event Argument Role Labeling results (F1 %)
		with perfect event triggers
	</Abstractive Summary>
	<Extractive Summary> =
		We also show polyglot results for event argu-
		ment role labeling in Table 4, by combing the train-
		ing data from multiple languages
	</Extractive Summary>
</Paper ID=ument30>


<Paper ID=ument30> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: Relation Extraction: Contribution of adding
		each language-universal feature consecutively for rela-
		tion extraction on Arabic and Chinese test sets, using
		the model trained on English (F1 scores (%))
	</Abstractive Summary>
</Paper ID=ument30>


<Paper ID=ument30> <Table ID =6>
	<Abstractive Summary> =
		8
		Table 6: Event Extraction: Contributions of adding
		each language-universal feature consecutively for event
		argument role labeling on Arabic and Chinese test
		datasets, using a model trained on English (F1 scores
		(%))
	</Abstractive Summary>
</Paper ID=ument30>


<Paper ID=ument30> <Table ID =7>
	<Abstractive Summary> =
		1
		Table 7: Event Argument Role Labeling results (F1 %)
		on Chinese and Arabic using English as training data
		(with system generated entity mentions)
		Table 7 shows the results of event argument role
		labeling on Chinese and Arabic entity mentions
		automatically extracted by Stanford CoreNLP in-
		stead of manually annotated mentions
	</Abstractive Summary>
</Paper ID=ument30>


<Paper ID=ument300> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1: Results on the combined CNN/DailyMail test
		set
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Quantitative Analysis
		We ﬁrst report the ROUGE metrics on the com-
		bined CNN/DailyMail test sets in Table 1 and the
		separate results in Table 2
	</Extractive Summary>
</Paper ID=ument300>


<Paper ID=ument300> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Results of the test sets on the CNN and Daily-
		Mail datasets separately
	</Abstractive Summary>
</Paper ID=ument300>


<Paper ID=ument300> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: The results of ablation test on the test split
		of the combined CNN/DailyMail dataset
	</Abstractive Summary>
	<Extractive Summary> =
		The results are re-
		ported in Table 3 and it proves the effectiveness of
		each proposed module
	</Extractive Summary>
</Paper ID=ument300>


<Paper ID=ument300> <Table ID =4>
	<Abstractive Summary> =
		97
		Table 4: Average rank of human evaluation in terms of
		overall performance, coverage, and non-redundancy
	</Abstractive Summary>
	<Extractive Summary> =
		We report the average re-
		sults in Table 4 and it shows that our HER is lead-
		ing than BANDITSUM on overall quality and cov-
		Model
		Overall
		Coverage
		Non-
		Redundancy
		HER w/o Dec2
		2
	</Extractive Summary>
</Paper ID=ument300>


<Paper ID=ument301> <Table ID =1>
	<Abstractive Summary> =
		59
		Table 1: ROUGE scores on the English evaluation sets of both Gigaword and DUC2004
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that we build a strong base-
		line using Transformer alone which obtains the
		state-of-the-art performance on Gigaword evalua-
		tion set, and obtains comparable performance to
		the state-of-the-art on DUC2004
	</Extractive Summary>
</Paper ID=ument301>


<Paper ID=ument301> <Table ID =2>
	<Abstractive Summary> =
		58
		Table 2: The full-length F-1 based ROUGE scores on
		the Chinese evaluation set of LCSTS
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Chinese Results
		Table 2 presents the evaluation results on LC-
		STS
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows that Transformer also sets a
		strong baseline on LCSTS that surpasses the per-
		formances of the previous works
	</Extractive Summary>
</Paper ID=ument301>


<Paper ID=ument301> <Table ID =3>
	<Abstractive Summary> =
		18
		Table 3: Results of explorations on the opponent attention derivation
	</Abstractive Summary>
	<Extractive Summary> =
		The upper rows of Table 3 presents the per-
		formance comparison between masking maximum
		weight and masking more weights
	</Extractive Summary>
	<Extractive Summary> =
		As shown in the middle part of Table 3,
		both “non-synchronous head” and “averaged
		head” underperform “synchronous head”
	</Extractive Summary>
</Paper ID=ument301>


<Paper ID=ument301> <Table ID =4>
	<Abstractive Summary> =
		59
		Table 4: The effect of dropping Po (denoted by -Po)
		from Transformer+ContrastiveAtt during decoding
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows that
		dropping Po signiﬁcantly harms the performance
		of “Transformer + ContrastiveAtt”
	</Extractive Summary>
</Paper ID=ument301>


<Paper ID=ument301> <Table ID =5>
	<Abstractive Summary> =
		# billion euros for greek bank
		Table 5: Example summaries generated by the baseline Transformer and Transformer+ContrastiveAtt
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Qualitative Study
		Table 5 shows the qualitative results
	</Extractive Summary>
</Paper ID=ument301>


<Paper ID=ument302> <Table ID =1>
	<Abstractive Summary> =
		30
		Table 1: Corpus statistics
	</Abstractive Summary>
</Paper ID=ument302>


<Paper ID=ument302> <Table ID =2>
	<Abstractive Summary> =
		13
		Table 2: Performance of our implemented transformer-
		based monolingual summarization model on LCSTS
	</Abstractive Summary>
	<Extractive Summary> =
		The performance
		of our transformer-based MS models is given in
		Table 2 and Table 3
	</Extractive Summary>
</Paper ID=ument302>


<Paper ID=ument302> <Table ID =3>
	<Abstractive Summary> =
		42
		Table 3: Performance of our implemented transformer-
		based MS model on CNN/Dailymail
	</Abstractive Summary>
</Paper ID=ument302>


<Paper ID=ument302> <Table ID =4>
	<Abstractive Summary> =
		05
		Table 4: ROUGE F1 scores (%) on En2ZhSum and Zh2EnSum test sets
	</Abstractive Summary>
</Paper ID=ument302>


<Paper ID=ument302> <Table ID =5>
	<Abstractive Summary> =
		19
		Table 5: Experimental results on different versions of datasets
	</Abstractive Summary>
</Paper ID=ument302>


<Paper ID=ument302> <Table ID =6>
	<Abstractive Summary> =
		21
		Table 6: Results of multi-task NCLS
	</Abstractive Summary>
</Paper ID=ument302>


<Paper ID=ument302> <Table ID =7>
	<Abstractive Summary> =
		35
		Table 7: Human evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 7, TNCLS can generate more
		informative summaries compared with GLTran,
		which shows the advantage of end-to-end mod-
		els
	</Extractive Summary>
</Paper ID=ument302>


<Paper ID=ument303> <Table ID =1>
	<Abstractive Summary> =
		80
		Table 1: Our implementation achieves similar perfor-
		mance to the RNN context and COPYNET
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		We ﬁrst compare all four models, Pointer-Gen,
		Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN,
		and Pointer-Gen-ARL-SEN, to existing models
		with ROUGE in Table 1 to establish that our
		6https://www
	</Extractive Summary>
</Paper ID=ument303>


<Paper ID=ument303> <Table ID =2>
	<Abstractive Summary> =
		Table 2:
		Generated Chinese headlines from differ-
		ent models
	</Abstractive Summary>
	<Extractive Summary> =
		An example of headlines gener-
		ated from different models in Table 2 shows that
		Pointer-Gen and Pointer-Gen+RL-ROUGE learns
		to summarize the main point of the article: “The
		Nikon D600 camera is reported to have black
		spots when taking photos”
	</Extractive Summary>
</Paper ID=ument303>


<Paper ID=ument303> <Table ID =3>
	<Abstractive Summary> =
		6%
		Table 3:
		Comparison of sensationalism score and
		ﬂuency score between different models
	</Abstractive Summary>
</Paper ID=ument303>


<Paper ID=ument303> <Table ID =4>
	<Abstractive Summary> =
		The main content of today’s theoretical report is
		Pointer-Gen+ARL-SEN: 如
		如
		如何
		何
		何做一名焦裕禄式的县委书记?
		How to be a county party secretary like JIAO Yulu?
		Creating Curiosity Gap
		Pointer-Gen: 10种方法帮你避免的10个小窍门
		10 tricks to help you avoid
		Pointer-Gen+ARL-SEN: 10个让你意
		意
		意想
		想
		想不
		不
		不到
		到
		到的领导力法则
		10 laws of leadership that you cannot think of
		Pointer-Gen: 王林的暴富之路
		WANG Lin’s path to sudden wealth
		Pointer-Gen+ARL-SEN: “气功大师”王林的暴富之路：凭借5个
		个
		个生
		生
		生财
		财
		财之
		之
		之道
		道
		道
		“The Qigong Master” WANG Lin’s path to sudden wealth: leveraging 5 ways to make money
		Highlighting Numbers
		Pointer-Gen: 北京市集成电路促进基金就位
		The Integrated Circuit Promotion Fund in Beijing is ready
		Pointer-Gen+ARL-SEN: 500亿
		亿
		亿巨
		巨
		巨资
		资
		资驰援国家大基金或已就位
		A huge capital sum of 50 billion is ready to support the national big fund
		Pointer-Gen: 陈光标冰桶挑战陈光标承认造假
		CHEN Guangbiao admits that he cheated in the freezing water challenge
		Pointer-Gen+ARL-SEN: 陈光标回应“造假”：有人超越我就捐款100万
		万
		万
		CHEN Guangbiao responded to “cheating”: if someone can do better, I will donate a million RMB
		Emotional Words
		Pointer-Gen: 俞永福：搜狗360还没停
		YU Yongfu: Sougou and 360 have not stopped
		Pointer-Gen+ARL-SEN: 俞永福微博辱
		辱
		辱骂
		骂
		骂UC：太
		太
		太恶
		恶
		恶心
		心
		心了
		了
		了
		YU Youfu abused UC in microblog: it is too disgusting
		Pointer-Gen: 保定楼市““一天涨3000””
		“3000 increase in one day” for Baoding housing market
		Pointer-Gen+ARL-SEN: 保定楼市“一天涨3000 简
		简
		简直
		直
		直疯
		疯
		疯了
		了
		了”
		“3000 increase in one day” for Baoding housing market, this is crazy
		Empathizing
		Pointer-Gen: 女性购物的五大特征
		Five characteristics of ladies shopping
		Pointer-Gen+ARL-SEN: 女性购物的5个忠告：你
		你
		你中枪了吗？
		5 warnings for ladies shopping: Have you been targeted?
		Pointer-Gen: 智能手表将开始拥有智能手机功能
		Smart watches will start to have smartphone features
		Pointer-Gen+ARL-SEN: 关于智能手表，你
		你
		你应该知道的事！
		What you should know about smart watches!
		Table 4: Different sensationalization strategies Pointer-Gen+ARL-SEN learns
	</Abstractive Summary>
</Paper ID=ument303>


<Paper ID=ument304> <Table ID =1>
	<Abstractive Summary> =
		4
		Experiments
		Datasets:
		To evaluate the effectiveness of our
		proposed model, we conducted training and test-
		3081
		Table 1: ROUGE F1 evaluation results on the Gigaword and ROUGE recall on DUC-2004 test set
	</Abstractive Summary>
</Paper ID=ument304>


<Paper ID=ument304> <Table ID =2>
	<Abstractive Summary> =
		ABS+ (Rush
		3082
		Table 2: OOV problem analysis: percentages (%(NO
	</Abstractive Summary>
</Paper ID=ument304>


<Paper ID=ument304> <Table ID =3>
	<Abstractive Summary> =
		We counted the number of UNKs and all
		Table 3: Abstractiveness: percentage of novel n-grams
		on Gigaword dataset
	</Abstractive Summary>
</Paper ID=ument304>


<Paper ID=ument304> <Table ID =4>
	<Abstractive Summary> =
		In
		comparing the results, it is clear that DS training
		has a noticeable effect when the testing set is sub-
		stantially semantically different from the training
		set but provides less improvement than RL when
		Table 4: Human evaluation: scoring of three models in
		terms of abstraction and overall quality by human eval-
		uators (the higher the better)
	</Abstractive Summary>
	<Extractive Summary> =
		The results are presented in
		Table 4, which show that our model outperformed
		both the seq2seq model and the pointer generator
		(See et al
	</Extractive Summary>
</Paper ID=ument304>


<Paper ID=ument305> <Table ID =1>
	<Abstractive Summary> =
		63
		Table 1: Word Ordering: BLEU scores on lemmatised data
	</Abstractive Summary>
	<Extractive Summary> =
		The languages are those shown in Table 1 and the
		size of the datasets (training, dev and test) varies
		between 7,586 (Arabic) and 85,377 (Czech) in-
		stances with most languages having around 12K
		instances (for more details about the data see Mille
		et al
	</Extractive Summary>
	<Extractive Summary> =
		As Table 1 shows, there
		is a marked, statistically signiﬁcant, difference be-
		tween the baseline and our approach which indi-
		cates that delexicalisation does improve word or-
		dering
	</Extractive Summary>
</Paper ID=ument305>


<Paper ID=ument305> <Table ID =2>
	<Abstractive Summary> =
		059
		Table 2: Proportion of correct head/dependent positioning for the ﬁve selected dependency relations: det, nsubj, obj, amod,
		acl, and overall performance across all dependency relations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2
		shows the results and compares them with a non-
		delexicalised approach
	</Extractive Summary>
	<Extractive Summary> =
		We
		also
		com-
		pare our delexicalised model with the non-
		delexicalised baseline:
		∆ in Table 2 shows
		the difference in performance between the two
		models
	</Extractive Summary>
</Paper ID=ument305>


<Paper ID=ument305> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Morphological Realisation Results
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Morphological Realisation
		Table 3 shows the results for the WO+MR model
	</Extractive Summary>
</Paper ID=ument305>


<Paper ID=ument305> <Table ID =4>
	<Abstractive Summary> =
		64
		Table 4: Contraction Generation Results (BLEU scores)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Contraction Generation
		To assess the degree to which contractions are
		used, we compute BLEU-4 between the gold se-
		quence of word forms from UD treebanks and
		the reference sentence (Table 4, Line S−c/S+c)
	</Extractive Summary>
	<Extractive Summary> =
		Adding a contraction module permits improv-
		ing results for those languages where contrac-
		tion is frequent (Table 4, Lines WO+MR+Creg,
		WO+MR+Cs2s)
	</Extractive Summary>
</Paper ID=ument305>


<Paper ID=ument305> <Table ID =5>
	<Abstractive Summary> =
		03
		Table 5: BLEU, DIST and NIST scores on the SR’18 test data (shallow track)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results
	</Extractive Summary>
</Paper ID=ument305>


<Paper ID=ument306> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Comparing outputs of transfer models
	</Abstractive Summary>
	<Extractive Summary> =
		This is
		exempliﬁed in Table 1, where the model of Shen
		Positive Sentiment ↔ Negative Sentiment
		Input I love this place, the service is always great!
		CA
		I know this place, the food is just a horrible!
		MD
		I love this place, the service is always great!
		DAR
		I did not like the homework of lasagna, not like it,
	</Extractive Summary>
</Paper ID=ument306>


<Paper ID=ument306> <Table ID =2>
	<Abstractive Summary> =
		1K
		Table 2: Statistics of YELP and FORMALITY datasets
	</Abstractive Summary>
	<Extractive Summary> =
		(2018) (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 describes statistics of the resulting
		two corpora
	</Extractive Summary>
</Paper ID=ument306>


<Paper ID=ument306> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Comparison of dataset quality
	</Abstractive Summary>
</Paper ID=ument306>


<Paper ID=ument306> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: The mean and standard deviation of human ratings on Content preservation, Grammaticality, and at-
		tribute correctness on both datasets for different systems: CROSSALIGNMENT (CA), MULTIDECODER (MD),
		DELETEANDRETRIEVE (DAR), Ours, and Human reference
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Human Evaluation
		In terms of human evaluation, the proposed ap-
		proach shows signiﬁcant gains over all baselines
		in terms of attribute correctness, content preser-
		vation, grammaticality, and success rate as shown
		in Table 4 (p < 0
	</Extractive Summary>
	<Extractive Summary> =
		For example, comparing human-
		rated and machine-evaluated score of style cor-
		rectness (Table 4 and Table 6), we ﬁnd that al-
		though the automatic score can serve as a rough
		assessment of the models, it does not aligns per-
		fectly with the human ratings in Table 4
	</Extractive Summary>
</Paper ID=ument306>


<Paper ID=ument306> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Example outputs of different systems
	</Abstractive Summary>
</Paper ID=ument306>


<Paper ID=ument306> <Table ID =6>
	<Abstractive Summary> =
		63
		Table 6: Automatic evaluation results on both datasets
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Automatic Evaluation
		We also assess each method’s performance via the
		automatic measures of attribute-accuracy, BLEU,
		and perplexity (Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		For example, comparing human-
		rated and machine-evaluated score of style cor-
		rectness (Table 4 and Table 6), we ﬁnd that al-
		though the automatic score can serve as a rough
		assessment of the models, it does not aligns per-
		fectly with the human ratings in Table 4
	</Extractive Summary>
</Paper ID=ument306>


<Paper ID=ument307> <Table ID =1>
	<Abstractive Summary> =
		484
		Table 1: Quality of reward metrics
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 1, we ﬁnd that all metrics we con-
		sider have low correlation with the human judge-
		ment
	</Extractive Summary>
	<Extractive Summary> =
		05) all the other models that do not use
		BERT+MLP, as well as the metrics that rely on
		reference summaries (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Unlike the metrics in Table 1, all rewards in this table do not require reference summaries
	</Extractive Summary>
</Paper ID=ument307>


<Paper ID=ument307> <Table ID =2>
	<Abstractive Summary> =
		174
		Table 2: Summary-level correlation of learned reward functions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the quality of
		different reward learning models
	</Extractive Summary>
</Paper ID=ument307>


<Paper ID=ument307> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Full-length ROUGE F-scores of some recent
		RL-based (upper) and supervised (middle) extractive
		summarisation systems, as well as our system with
		learned rewards (bottom)
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Extractive Summarisation
		Table 3 presents the ROUGE scores of our system
		(NeuralTD+LearnedRewards) and multiple state-
		of-the-art systems
	</Extractive Summary>
	<Extractive Summary> =
		01) than ExtAbsRL, despite receiving sig-
		niﬁcantly higher ROUGE scores than both Refresh
		and NeuralTD (see Table 3)
	</Extractive Summary>
</Paper ID=ument307>


<Paper ID=ument307> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Human evaluation on extractive summaries
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents the human evaluation results
	</Extractive Summary>
	<Extractive Summary> =
		Furthermore, we note that the overall human rat-
		ings for the abstractive summaries are lower than
		the extractive summaries (compared to Table 4)
	</Extractive Summary>
</Paper ID=ument307>


<Paper ID=ument307> <Table ID =5>
	<Abstractive Summary> =
		20
		75
		Table 5: Performance of ExtAbsRL with different re-
		ward functions, measured in terms of ROUGE (center)
		and human judgements (right)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Abstractive Summarisation
		Table 5 compares the ROUGE scores of using dif-
		ferent rewards to train the extractor in ExtAbsRL
		(the abstractor is pre-trained, and is applied to
		rephrase the extracted sentences)
	</Extractive Summary>
	<Extractive Summary> =
		It is clear from Table 5 that using the learned re-
		ward helps the RL-based system generate sum-
		maries with signiﬁcantly higher human ratings
	</Extractive Summary>
</Paper ID=ument307>


<Paper ID=ument308> <Table ID =1>
	<Abstractive Summary> =
		580
		-
		Table 1: Question generation results: Comparison of
		diverse generation methods on SQuAD
	</Abstractive Summary>
	<Extractive Summary> =
		Moreover, our method
		scores state-of-the-art BLEU-4 in question gener-
		ation on SQuAD (Table 1)
	</Extractive Summary>
</Paper ID=ument308>


<Paper ID=ument308> <Table ID =2>
	<Abstractive Summary> =
		757
		-
		Table 2: Summarization results: Comparison of di-
		verse generation methods on CNN-DM
	</Abstractive Summary>
	<Extractive Summary> =
		While we
		observe a similar trend for SELECTOR in the ques-
		tion generation task, Table 2 shows the opposite
		3128
		Method
		ROUGE-2
		Oracle
		Pairwise
		(Top-1)
		(Top-K)
		(Self-sim)
		PG
		17
	</Extractive Summary>
</Paper ID=ument308>


<Paper ID=ument308> <Table ID =3>
	<Abstractive Summary> =
		0
		(b) CNN-DM abstractive summarization
		Table 3: Human evaluation results
		baselines in terms of both diversity and accuracy
		with statistical signiﬁcance
	</Abstractive Summary>
	<Extractive Summary> =
		See Table 3a and 3b
	</Extractive Summary>
	<Extractive Summary> =
		Human Evaluation
		Table 3a and 3b show the
		human evaluation in two tasks of questions gen-
		eration and summarization, comparing sequences
		generated by SELECTOR with the diversity-
		promoting baselines: Diverse Beam, Truncated
		Sampling and Mixture Decoder
	</Extractive Summary>
</Paper ID=ument308>


<Paper ID=ument308> <Table ID =4>
	<Abstractive Summary> =
		79
		Table 4: Comparison of single-expert selector with
		state-of-the-art abstractive summarization methods on
		CNN-DM
	</Abstractive Summary>
	<Extractive Summary> =
		No-
		tably, our method scores state-of-the-art BLEU-4
		in question generation on SQuAD and ROUGE
		comparable to state-of-the-art methods in abstrac-
		tive summarization in CNN-DM (See also Table 4
		for state-of-the-art results in CNN-DM)
	</Extractive Summary>
</Paper ID=ument308>


<Paper ID=ument308> <Table ID =5>
	<Abstractive Summary> =
		17)
		Table 5: Training time: Comparison of training time
		on CNN-DM
	</Abstractive Summary>
	<Extractive Summary> =
		R stands for ROUGE (Lin, 2004)
		Efﬁcient Training
		Table 5 shows that SELEC-
		TOR trains up to 3
	</Extractive Summary>
</Paper ID=ument308>


<Paper ID=ument309> <Table ID =1>
	<Abstractive Summary> =
		First, CVAE may fail to generate realistic sen-
		3133
		original
		How do I improve my English speaking ?
		paraphrased
		How do I speak English ﬂuently ?
		generated
		How do I I to ?
		original
		What is the easiest way to lose weight faster?
		paraphrased
		What is the best way to loose weight quickly ?
		generated
		How can I learn lose weight ?
		Table 1:
		Examples for generating paraphrases by
		CVAE, from which it fails to generate realistic sen-
		tences
	</Abstractive Summary>
</Paper ID=ument309>


<Paper ID=ument309> <Table ID =2>
	<Abstractive Summary> =
		83
		Table 2: Test accuracy on MSCOCO dataset, in per-
		centage
	</Abstractive Summary>
</Paper ID=ument309>


<Paper ID=ument309> <Table ID =3>
	<Abstractive Summary> =
		47
		Table 3: Test accuracy on Quora dataset, in percentage
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the performance on Quora
		dataset
	</Extractive Summary>
	<Extractive Summary> =
		VAE∗ in Table 3 extends the pre-
		3138
		Model
		Measure
		Quora-50K
		Quora-100K
		Quora-150K
		BLEU↑
		METEOR↑ TER↓
		BLEU↑
		METEOR↑
		TER↓
		BLEU↑
		METEOR↑
		TER↓
		Seq2Seq + Att
		-
		26
	</Extractive Summary>
</Paper ID=ument309>


<Paper ID=ument309> <Table ID =4>
	<Abstractive Summary> =
		37
		Table 4: Human judgments for paraphrase generation on different models
	</Abstractive Summary>
</Paper ID=ument309>


<Paper ID=ument309> <Table ID =5>
	<Abstractive Summary> =
		original
		a group of kids are eating pizza and
		(MSCOCO)
		drinking soda
		paraphrased
		a person sits at a table eating pizza
		generated-VAE
		a man sitting at a table with a pizza
		generated-Ours
		a man sitting at a table eating
		a plate of pizza and soda
		original
		What is the fastest possible way to
		(Quora-50K)
		lose weight?
		paraphrased
		What are some ways to lose weight fast?
		generated-VAE
		How can I lose weight weight?
		generate-Ours
		How can I lose weight in a month?
		original
		What were the immediate and most
		(Quora-100K)
		important causes that led to World War 1?
		paraphrased
		What were the causes of World War I?
		generated-VAE
		What were the main causes of World War I?
		generated-Ours
		What were the direct and main causes
		of World War 1?
		original
		What could be the reason behind Arnab
		(Quora-150K)
		Goswami quitting Times Now?
		paraphrased
		Is Arnab Goswami quitting from Times now?
		generated-VAE
		Why did Arnab Goswami quit?
		generated-Ours
		Why did Arnab Goswami resign from Times
		Now?
		Table 5:
		Samples of generated paraphrases from
		MSCOCO and Quora
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 5, we show examples sam-
		pled from both MSCOCO and Quora
	</Extractive Summary>
</Paper ID=ument309>


<Paper ID=ument31> <Table ID =1>
	<Abstractive Summary> =
		Table 1: The statistics of ground-truth annotation
	</Abstractive Summary>
</Paper ID=ument31>


<Paper ID=ument31> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Test performance on the TACRED dataset
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Test Results
		From Table 2, we see that PCNN+nEM achieves
		better recall and F1 score than the PCNN model
		under various noise levels
	</Extractive Summary>
</Paper ID=ument31>


<Paper ID=ument310> <Table ID =1>
	<Abstractive Summary> =
		85
		Table 1: Automatic evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Automatic Evaluation
		Table 1 displays the automatic evaluation results
		on both development and test set
	</Extractive Summary>
</Paper ID=ument310>


<Paper ID=ument310> <Table ID =2>
	<Abstractive Summary> =
		69
		Table 2: Automatic evaluation results on test set
	</Abstractive Summary>
	<Extractive Summary> =
		We ap-
		plied them on models other than DEL as shown in
		Table 2 and report DEL’s result from (Li and Wan,
		2018)’s paper
	</Extractive Summary>
</Paper ID=ument310>


<Paper ID=ument310> <Table ID =3>
	<Abstractive Summary> =
		78
		Table 3: Human evaluation results
	</Abstractive Summary>
</Paper ID=ument310>


<Paper ID=ument311> <Table ID =1>
	<Abstractive Summary> =
		205
		Table 1: Full-length ROUGE F1 score of the proposed
		methods and baselines on RA-MDS dataset
	</Abstractive Summary>
	<Extractive Summary> =
		RA-MDS: From Table 1, we observe that
		our STDS outperforms all baselines on all of
		ROURGE-1, ROUGE-2 and ROUGE-SU4, which
		demonstrates the superiority of our model
	</Extractive Summary>
</Paper ID=ument311>


<Paper ID=ument311> <Table ID =2>
	<Abstractive Summary> =
		107
		Table 2: ROUGE Recall for the proposed method and
		baselines on DUC 2004 dataset
	</Abstractive Summary>
</Paper ID=ument311>


<Paper ID=ument311> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Summary example
	</Abstractive Summary>
</Paper ID=ument311>


<Paper ID=ument311> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Subtopic example
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the identiﬁed subtopic informa-
		tion of the same document set as for Table 3
	</Extractive Summary>
</Paper ID=ument311>


<Paper ID=ument312> <Table ID =1>
	<Abstractive Summary> =
		Table 1:
		A sample in the original WebNLG dataset
		(above) and the delexicalized dataset (below)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows one
		sample from the corpus
	</Extractive Summary>
</Paper ID=ument312>


<Paper ID=ument312> <Table ID =2>
	<Abstractive Summary> =
		80
		Table 2:
		REG performance on the original, entity separated and random dataset
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Automatic Evaluation
		As shown in Table 2 PROFILEREG outperforms
		the three baselines in all experiments
	</Extractive Summary>
</Paper ID=ument312>


<Paper ID=ument312> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Example of original text and generated text of each model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows an example of the generated text
		of each model
	</Extractive Summary>
</Paper ID=ument312>


<Paper ID=ument312> <Table ID =4>
	<Abstractive Summary> =
		72%
		2955
		Table 4: Evaluation for seen and unseen entities
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the evaluation results
	</Extractive Summary>
	<Extractive Summary> =
		From
		Table 4, it is easy to see that the model performs
		better when generating referring for seen entities
	</Extractive Summary>
</Paper ID=ument312>


<Paper ID=ument312> <Table ID =5>
	<Abstractive Summary> =
		987)
		Table 5: An example of how the switch variable works
		in referring expression generation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows three examples of generated re-
		ferring expressions with the values of the switch
		3171
		pronoun
		name
		description
		demonstrative
		0
	</Extractive Summary>
</Paper ID=ument312>


<Paper ID=ument313> <Table ID =1>
	<Abstractive Summary> =
		1
		Dataset
		Generator
		Paraphrase discriminator
		Dataset
		#Train
		#Validation
		#Test
		#Positive
		#Negative
		Quora
		100K
		3K
		30K
		100K
		160K
		Twitter
		110K
		1K
		5K
		10K
		40K
		Table 1: Statistics of datasets
	</Abstractive Summary>
</Paper ID=ument313>


<Paper ID=ument313> <Table ID =2>
	<Abstractive Summary> =
		55
		Table 2: Performances on Quora dataset
	</Abstractive Summary>
</Paper ID=ument313>


<Paper ID=ument313> <Table ID =3>
	<Abstractive Summary> =
		45
		Table 3: Performances on Twitter dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows scores on Twitter dataset
	</Extractive Summary>
</Paper ID=ument313>


<Paper ID=ument313> <Table ID =4>
	<Abstractive Summary> =
		59
		Table 4: Human evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows our model achieves better scores in
		both meaning similarity and ﬂuency than baseline
		models
	</Extractive Summary>
</Paper ID=ument313>


<Paper ID=ument313> <Table ID =5>
	<Abstractive Summary> =
		3180
		Source
		How do you know you ’re in love again ?
		PG-BS
		How do you know you are in love with someone ?
		How do you know if you ’re in love with someone ?
		How do you know if you are in love with someone ?
		ours
		How do you know if you ’re in love with someone ?
		What are some ways to know that you are in love with someone ?
		How can you know if you ’re in love ?
		Source
		How does this demonetisation of 500 & 1000 rupee notes help to reduce the price of real estate ?
		PG-BS
		What will be the effect of banning 500 and 1000 notes on the Indian economy ?
		What will be the effect of banning 500 and 1000 notes on the stock markets ?
		What will be the effect of banning 500 and 1000 notes on the real estate sector ?
		ours
		What will be the effect of banning 500 and 1000 notes on real estate sector in India ?
		How does the declaration that Rs 500 and Rs 1000 notes would not be accepted as valid transactions
		affect real estate in India ?
		How can the ban of 500 and 1000 rupee notes help in curbing the price of real estate ?
		Source
		I want to write a book - where should I start ?
		PG-BS
		I want to write a book , where should I start ?
		I want to write a book , how should I start ?
		I want to write a book , where can I start ?
		ours
		How do I write a book ?
		What should I do to write a book - where should I start ?
		I want to write a book , where should I start ?
		Table 5: Example paraphrases on Quora
		In terms of diversity, our approach outperform-
		s other methods by a large margin
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 demonstrates some examples generat-
		ed by pointer-generator using beam search and our
		model
	</Extractive Summary>
</Paper ID=ument313>


<Paper ID=ument314> <Table ID =1>
	<Abstractive Summary> =
		4
		225
		Table 1: Data statistics of LDC2015E86 and LDC2017T10 datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows the statistics for both datasets
	</Extractive Summary>
</Paper ID=ument314>


<Paper ID=ument314> <Table ID =2>
	<Abstractive Summary> =
		15
		Table 2: BLEU and METEOR scores on the test set of
		LDC2015E86 and LDC2017T10 datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the comparison between
		the proposed models, the baseline and other neural
		models on the test set of the two datasets
	</Extractive Summary>
</Paper ID=ument314>


<Paper ID=ument314> <Table ID =3>
	<Abstractive Summary> =
		23
		Table 3: Results on LDC2015E86 test set when models
		are trained with additional Gigaword data
	</Abstractive Summary>
</Paper ID=ument314>


<Paper ID=ument314> <Table ID =4>
	<Abstractive Summary> =
		7M
		Table 4:
		Results of the ablation study on the
		LDC2017T10 development set
	</Abstractive Summary>
	<Extractive Summary> =
		Ablation Study
		In Table 4, we report the results
		of an ablation study on the impact of each com-
		ponent of our model on the development set of
		LDC2017T10 dataset by removing the graph en-
		coders
	</Extractive Summary>
</Paper ID=ument314>


<Paper ID=ument314> <Table ID =5>
	<Abstractive Summary> =
		3%
		Table 5: METEOR scores and differences to the S2S,
		in the LDC2017T10 test set, with respect to the graph
		diameter, sentence length and max node out-degree
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows METEOR5 scores for
		the LDC2017T10 dataset
	</Extractive Summary>
</Paper ID=ument314>


<Paper ID=ument314> <Table ID =6>
	<Abstractive Summary> =
		72
		Table 6:
		Entailment (ENT), contradiction (CON)
		and neutral (NEU) average percentages for the
		LDC2017T10 test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the average probabilities for en-
		tailment, contradiction and neutral classes on the
		LDC2017T10 test set
	</Extractive Summary>
</Paper ID=ument314>


<Paper ID=ument314> <Table ID =7>
	<Abstractive Summary> =
		Table 7: An example of an AMR graph and generated sentences
	</Abstractive Summary>
	<Extractive Summary> =
		Manual Inspection
		Table 7 shows sentences
		generated by S2S, Song et al
	</Extractive Summary>
</Paper ID=ument314>


<Paper ID=ument314> <Table ID =8>
	<Abstractive Summary> =
		35
		Table 8: Fraction of elements in the output that are not
		present in the input (ADDED) and the fraction of ele-
		ments in the input graph that are missing in the gener-
		ated sentence (MISS), for the test set of LDC2017T10
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 8, G2S approaches out-
		perform the S2S baseline
	</Extractive Summary>
</Paper ID=ument314>


<Paper ID=ument315> <Table ID =1>
	<Abstractive Summary> =
		2%
		Table 1:
		Comparison of human evaluation results
		with their consistency
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the evaluation results with their
		consistency
	</Extractive Summary>
</Paper ID=ument315>


<Paper ID=ument315> <Table ID =2>
	<Abstractive Summary> =
		13
		Table 2: Comparison of human evaluation results for
		headlines
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows
		the evaluation result along with the adequacy
		and ﬂuency
	</Extractive Summary>
</Paper ID=ument315>


<Paper ID=ument315> <Table ID =3>
	<Abstractive Summary> =
		5
		-
		-
		-
		-
		Table 3: Automatic evaluation results based on the ROUGE metrics and accuracy (%) of classiﬁcation of job
		advertisement dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the effect
		of the proposed methods:
		multi-task learning
		(MTL), scheduling strategy (SD) and hierarchical
		consistency loss (HCL)
	</Extractive Summary>
</Paper ID=ument315>


<Paper ID=ument315> <Table ID =4>
	<Abstractive Summary> =
		7
		-
		Table 4: Automatic evaluation results based on the ROUGE metrics and accuracy (%) of classiﬁcation of the CNN
		and DailyMail datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results of the CNN and Daily-
		Mail datasets, respectively
	</Extractive Summary>
</Paper ID=ument315>


<Paper ID=ument315> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5: ROUGE recall scores of task 2 (key phrases
		for the Job Ads, headlines for CNN-DM) gold sen-
		tences against task 3 (headlines for Job Ads, summaries
		for CNN-DM) gold sentences in the validation set
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 5, headlines and key phrases of
		the job advertisement dataset have more overlap
		than the summaries and headlines of the CNN-DM
		dataset
	</Extractive Summary>
</Paper ID=ument315>


<Paper ID=ument315> <Table ID =6>
	<Abstractive Summary> =
		6
		Table 6: Comparison of the decoder information sharing methods and encoder sharing methods for the job
		advertisement dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 presents
		the comparison of the ﬁve methods for the job
		advertisement dataset
	</Extractive Summary>
	<Extractive Summary> =
		It can be observed from Table 6 that the cascade
		model achieves a lower score than MTL + SD
	</Extractive Summary>
	<Extractive Summary> =
		It can be observed from Table 6 that both meth-
		ods achieved a lower score than the hierarchical
		consistency loss
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 presents the results of two methods for the
		job advertisement dataset
	</Extractive Summary>
</Paper ID=ument315>


<Paper ID=ument315> <Table ID =7>
	<Abstractive Summary> =
		4
		Table 7:
		Comparison of ROUGE recall scores of
		generated key phrases against generated headlines for
		the job advertisement dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 presents the ROUGE recall scores of the
		generated key phrases computed against generated
		headlines for the job advertisement dataset
	</Extractive Summary>
</Paper ID=ument315>


<Paper ID=ument316> <Table ID =1>
	<Abstractive Summary> =
		17
		Table 1: Statistics on the created dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Finally, we split 1,040 essays into training, de-
		velopment, and test sets as shown in Table 17
	</Extractive Summary>
</Paper ID=ument316>


<Paper ID=ument316> <Table ID =2>
	<Abstractive Summary> =
		09)
		Table 2: Generation Performance (numbers in brackets correspond to the relaxed measures)
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 2 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		At the same time, Table 2 shows that there is
		still room for improvement
	</Extractive Summary>
</Paper ID=ument316>


<Paper ID=ument317> <Table ID =1>
	<Abstractive Summary> =
		63
		Table 1: Performance for the average relative distance between the answer fragment and other non-stop sentence
		words that also appear in the ground truth question
	</Abstractive Summary>
	<Extractive Summary> =
		Recall that existing proximity-based answer-
		aware models perform poorly when the distance
		between the answer fragment and other non-stop
		sentence words that also appear in the ground truth
		question is large (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		All answer-aware
		neural models treat question generation as a one-
		to-one mapping problem, but existing models per-
		form poorly for sentences with a complex struc-
		ture (as shown in Table 1)
	</Extractive Summary>
</Paper ID=ument317>


<Paper ID=ument317> <Table ID =2>
	<Abstractive Summary> =
		26%
		Table 2: Comparisons between sentences and answer-
		relevant relations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows some statistics to verify the intu-
		ition that the extracted relations can serve as more
		to the point context
	</Extractive Summary>
</Paper ID=ument317>


<Paper ID=ument317> <Table ID =3>
	<Abstractive Summary> =
		31
		Table 3: Dataset statistics on Du Split (Du et al
	</Abstractive Summary>
</Paper ID=ument317>


<Paper ID=ument317> <Table ID =4>
	<Abstractive Summary> =
		73
		Table 4: The main experimental results for our model and several baselines
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Main Results
		Table 4 shows automatic evaluation results for
		our model and baselines (copied from their pa-
		pers)
	</Extractive Summary>
</Paper ID=ument317>


<Paper ID=ument317> <Table ID =5>
	<Abstractive Summary> =
		24%)
		Table 5: Performance for the average relative distance between the answer fragment and other non-stop sentence
		words that also appear in the ground truth question (BLEU is the average over BLEU-1 to BLEU-4)
	</Abstractive Summary>
	<Extractive Summary> =
		The broken-
		down performances by different relative distances
		are shown in Table 5a
	</Extractive Summary>
	<Extractive Summary> =
		In fact, the
		breakdown intervals in Table 5a naturally bound
		its sentence length, say for “> 10”, the sentences
		in this group must be longer than 10
	</Extractive Summary>
	<Extractive Summary> =
		We rerun
		the analysis of Table 5b only for long sentences
		(length > 20) of each interval
	</Extractive Summary>
	<Extractive Summary> =
		The improvement
		percentages over Hybrid are shown in Table 5b,
		which become more signiﬁcant when the distance
		increases from “0 ∼ 10” to “> 10”
	</Extractive Summary>
</Paper ID=ument317>


<Paper ID=ument318> <Table ID =1>
	<Abstractive Summary> =
		17
		Table 1: Summarisation results on the CNN/Daily Mail dataset according to three standard metrics
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Summarisation
		Table 1 shows results of our summarisation ex-
		periments
	</Extractive Summary>
</Paper ID=ument318>


<Paper ID=ument318> <Table ID =2>
	<Abstractive Summary> =
		he ’s friends with controversial instagram playboy dan bilzerian
		and says ‘ it ’s not hard to get any girl you want ’
		Table 2: Examples of articles, their copycat and human reference summaries
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists an example of produced
		outputs
	</Extractive Summary>
</Paper ID=ument318>


<Paper ID=ument318> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Examples of correct (two ﬁrst cases) and incorrect (third case) automatic post-edits produced from the
		EN-DE dataset by PNT-TRG+SRC
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 provides examples of positive and negative
		changes for both setups
	</Extractive Summary>
</Paper ID=ument318>


<Paper ID=ument318> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4:
		HTER↓ / BLEU↑ scores for the APE task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows results of our APE experiments
	</Extractive Summary>
</Paper ID=ument318>


<Paper ID=ument319> <Table ID =1>
	<Abstractive Summary> =
		63**
		Table 1: Comparison of Experimental Results on Amazon Review Dataset (** stands for signiﬁcance under 99%
		conﬁdence, * stands for 95% conﬁdence)
		Metrics
		RevGAN+CD
		Attr2Seq
		cGAN
		Accuracy
		0
	</Abstractive Summary>
	<Extractive Summary> =
		The summary of our
		experiment results is reported in Table 1, from
		which we could clearly observe that, compared
		with the baseline text-generation models, our pro-
		posed RevGAN model performs signiﬁcantly bet-
		ter in sentence quality and coherence performance
	</Extractive Summary>
</Paper ID=ument319>


<Paper ID=ument319> <Table ID =2>
	<Abstractive Summary> =
		665
		Table 2: Controllable Generation Accuracy
		those two groups in terms of helpfulness as well
	</Abstractive Summary>
</Paper ID=ument319>


<Paper ID=ument319> <Table ID =3>
	<Abstractive Summary> =
		95
		Actual Value
		Empirical Test
		Human-Written
		RevGAN+PD
		Human
		119
		61
		Machine
		118
		62
		Actual Value
		Empirical Test
		Human-Written
		RevGAN
		Human
		119
		61
		Machine
		102
		78
		Table 3: Confusion Matrix of Empirical Test
		4
	</Abstractive Summary>
</Paper ID=ument319>


<Paper ID=ument319> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Selected Personalized Generation Examples
		User History Reviews
		RevGAN
		RevGAN+PD
		1
	</Abstractive Summary>
</Paper ID=ument319>


<Paper ID=ument319> <Table ID =5>
	<Abstractive Summary> =
		They have a very comfortable feel and great sound!
		The guitar has always
		made a quality and you
		would really love that
		life from it!
		The guitar has always
		made better quality and
		you would really love
		that comfortable feel
		from it!
		Table 5: Personalized Review Generation Examples
		coder, conditional discriminator and personalized
		decoder
	</Abstractive Summary>
	<Extractive Summary> =
		Additionally, we showcase
		the modiﬁcation process in Table 5, where the
		personalized generated reviews tend to use more
		words from the user’s history corpus
	</Extractive Summary>
</Paper ID=ument319>


<Paper ID=ument32> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Dataset statistics about the number of docu-
		ments for the train (#Train), development (#Dev) and
		test (#Test), the number (#Total) and the multi-event
		ratio (MER) of all documents
	</Abstractive Summary>
	<Extractive Summary> =
		Extensive experiments demon-
		strate that Doc2EDAG can signiﬁcantly outper-
		3 Estimated by their Table 1 as 2∗NO
	</Extractive Summary>
	<Extractive Summary> =
		In
		Table 1, we show the number of documents and
		the multi-event ratio (MER) for each event type
		on this dataset
	</Extractive Summary>
</Paper ID=ument32>


<Paper ID=ument32> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: The quality of the DS-based event labeling
		evaluated on 100 manually annotated documents (ran-
		domly select 20 for each event type)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows this approximate
		evaluation, and we can observe that DS-generated
		data are pretty good, achieving high precision and
		acceptable recall
	</Extractive Summary>
</Paper ID=ument32>


<Paper ID=ument32> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Overall event-level precision (P
	</Abstractive Summary>
	<Extractive Summary> =
		As Table 3 shows, Doc2EDAG
		achieves signiﬁcant improvements over all base-
		344
		Model
		EF
		ER
		EU
		EO
		EP
		P
	</Extractive Summary>
</Paper ID=ument32>


<Paper ID=ument32> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: F1 scores for all event types and the averaged ones (Avg
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows F1 scores for different sce-
		narios
	</Extractive Summary>
</Paper ID=ument32>


<Paper ID=ument32> <Table ID =5>
	<Abstractive Summary> =
		8
		Table 5: Performance differences of Doc2EDAG vari-
		ants for all event types and the averaged ones (Avg
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 5, we can
		observe that 1) the memory mechanism is of prime
		importance, as removing it can result in the most
		drastic performance declines, over 10 F1 scores
		on four event types except for the ER type whose
		MER is very low on the test set; 2) the sched-
		uled sampling strategy that alleviates the mis-
		match of entity candidates for event table ﬁlling
		between training and inference also contributes
		greatly, improving by 5 F1 scores on average;
		3) the document-level entity encoding that en-
		hances global entity representations contributes
		345
		2
	</Extractive Summary>
</Paper ID=ument32>


<Paper ID=ument320> <Table ID =1>
	<Abstractive Summary> =
		31 **
		Table 1: Spearman’s ρ for the different metrics w
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1, we report Spearman’s rank correla-
		tions on this data, where we compare summaries
		rankings obtained according to the assessed met-
		rics
	</Extractive Summary>
	<Extractive Summary> =
		4), and the relatively stronger
		correlation of those two metrics with relevance
		(see Table 1)
	</Extractive Summary>
</Paper ID=ument320>


<Paper ID=ument320> <Table ID =2>
	<Abstractive Summary> =
		14
		Table 2: Comparison with previous works
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		In Table 2, we report the results obtained from our
		experiments in comparison with previously pro-
		posed approaches
	</Extractive Summary>
	<Extractive Summary> =
		To elaborate further, we notice that applying the
		learned coefﬁcients for 1 to the results obtained by
		models reinforced on QAlearned and QAequally, see
		Table 2, we obtain very similar scores (namely,
		136
	</Extractive Summary>
	<Extractive Summary> =
		(2017) – see Table 2 – indicating a possible edge
		for the architecture proposed by See et al
	</Extractive Summary>
</Paper ID=ument320>


<Paper ID=ument320> <Table ID =3>
	<Abstractive Summary> =
		80**
		Table 3: Human assessment: two-tailed t-test results are reported for each model compared to the baseline (∗ : p <
	</Abstractive Summary>
	<Extractive Summary> =
		We hence conducted a human evaluation for
		the different setups, reported in Table 3, assess-
		ing their outputs for readability and relevance in
		line with Paulus et al
	</Extractive Summary>
</Paper ID=ument320>


<Paper ID=ument320> <Table ID =4>
	<Abstractive Summary> =
		human reference
		baseline
		+ ROUGE
		+ QAequally
		+ QAlearned
		+ QAlearned + TL;DR
		+ QAlearned + CNN-DM (VAL)
		+ QAlearned + CNN-DM (TEST)
		baseline
		* / **
		-
		+ ROUGE
		** / **
		** / **
		-
		+ QAequally
		** / **
		** / **
		** / **
		-
		+ QAlearned
		** / **
		- / **
		** / **
		** / -
		-
		+ QAlearned + TL;DR
		** / **
		* / **
		** / **
		** / -
		* / -
		-
		+ QAlearned + CNN-DM (VAL)
		** / **
		* / **
		** / **
		** / *
		** / **
		- / *
		-
		+ QAlearned + CNN-DM (TEST)
		** / **
		- / **
		** / **
		** / *
		- / **
		- / *
		* / -
		-
		Table 4: Human assessment: two-tailed t-test results are reported for each model pair for Readability / Relevance
		(∗ : p <
	</Abstractive Summary>
</Paper ID=ument320>


<Paper ID=ument321> <Table ID =1>
	<Abstractive Summary> =
		110
		111
		108
		# Instances
		48K
		47K
		24K
		Table 1: Detailed statistics of our dataset
	</Abstractive Summary>
	<Extractive Summary> =
		The statistics are shown in Table 1 and Table 2
	</Extractive Summary>
</Paper ID=ument321>


<Paper ID=ument321> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: General statistics of our dataset
	</Abstractive Summary>
</Paper ID=ument321>


<Paper ID=ument321> <Table ID =3>
	<Abstractive Summary> =
		88**
		Table 3: Automatic evaluation for advertising text generation
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Automatic Evaluation
		Table 3 shows the experimental results
	</Extractive Summary>
</Paper ID=ument321>


<Paper ID=ument321> <Table ID =4>
	<Abstractive Summary> =
		415
		Table 4: Manual pair-wise evaluation for advertising text generation
	</Abstractive Summary>
	<Extractive Summary> =
		Results The annotation results in Table 4 show
		that our model signiﬁcantly outperforms baselines
		in both metrics
	</Extractive Summary>
</Paper ID=ument321>


<Paper ID=ument321> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: Automatic evaluation for recipe text generation
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Recipe Text Generation
		Table 5 shows the experimental results
	</Extractive Summary>
</Paper ID=ument321>


<Paper ID=ument322> <Table ID =1>
	<Abstractive Summary> =
		34M
		2250
		267K
		Table 1: Dataset statistics
		on style transfer
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows statistics of these datasets
	</Extractive Summary>
</Paper ID=ument322>


<Paper ID=ument322> <Table ID =2>
	<Abstractive Summary> =
		5
		17
		Table 2: Human evaluation results - each cell indicates the percentage of sentences preferred down a column (Cont
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents re-
		sults on our best scoring model B-GST with the
		previous best scoring model D&R as a percentage
		of times one was preferred over the other
	</Extractive Summary>
</Paper ID=ument322>


<Paper ID=ument322> <Table ID =3>
	<Abstractive Summary> =
		BT
		22
		29
		18
		23
		B-GST
		69
		61
		70
		67
		None
		9
		10
		12
		10
		Table 3: Human evaluation results - each cell indicates
		the percentage of sentences preferred down a column
		(Cont
	</Abstractive Summary>
	<Extractive Summary> =
		A comparison of our best model
		B-GST, with their results using BT is presented
		in Table 3 as a percentage of times one was pre-
		ferred over the other
	</Extractive Summary>
</Paper ID=ument322>


<Paper ID=ument322> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Automatic evaluation results (GL = GLEU, BLs = BLEU ; PL = Perplexity ; AC = Target Style Accuracy
		; SRC = Input Sentence ; B-GST and G-GST are our models ; H = Human Reference)
		POLITICAL
		GENDER
		Model
		BLS
		PL
		AC
		BLS
		PL
		AC
		SRC
		100
	</Abstractive Summary>
</Paper ID=ument322>


<Paper ID=ument322> <Table ID =5>
	<Abstractive Summary> =
		9
		Table 5: Automatic evaluation results (BLs = BLEU;
		PL = Perplexity; AC = Target Style Accuracy; SRC =
		Input Sentence; B-GST and G-GST are our models)
		ated and source sentences
	</Abstractive Summary>
	<Extractive Summary> =
		For instance, the BT model in Table 5
		has a high style but a considerably lower BLEU
		9https://github
	</Extractive Summary>
</Paper ID=ument322>


<Paper ID=ument322> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Examples of generated sentences to be compared down a column (B-GST and G-GST are our models,
		SRC is the input sentence)
	</Abstractive Summary>
	<Extractive Summary> =
		More importantly, as Table 6 shows,
		our models generate realistic and natural-sounding
		sentences while retaining core content - an aspect
		on which previous models seem to be seriously
		lacking
	</Extractive Summary>
</Paper ID=ument322>


<Paper ID=ument323> <Table ID =1>
	<Abstractive Summary> =
		76
		Table 1: Results on NYT
	</Abstractive Summary>
	<Extractive Summary> =
		Surprisingly,
		SENECA + RCoh ranks higher on informative-
		ness even when compared to SENECA, which re-
		ports higher on ROUGE (see Table 1)
	</Extractive Summary>
</Paper ID=ument323>


<Paper ID=ument323> <Table ID =2>
	<Abstractive Summary> =
		63
		Table 2: Results on CNN/Daily Mail
	</Abstractive Summary>
</Paper ID=ument323>


<Paper ID=ument323> <Table ID =3>
	<Abstractive Summary> =
		01
		Table 3: % of system summaries improperly using ref-
		erential pronouns (Ref
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen from Table 3, our models re-
		port the least percentage of improper usage of ref-
		erential pronouns
	</Extractive Summary>
</Paper ID=ument323>


<Paper ID=ument323> <Table ID =4>
	<Abstractive Summary> =
		11∗
		Table 4: Human evaluation on informativeness (Inf
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows that our model SENECA +
		RCoh ranks signiﬁcantly higher on informative-
		ness as well as coherence, reafﬁrming our obser-
		vations from automatic evaluation
	</Extractive Summary>
</Paper ID=ument323>


<Paper ID=ument324> <Table ID =1>
	<Abstractive Summary> =
		93
		KEEP
		Table 1: Oracle label computation for the text com-
		pression module
	</Abstractive Summary>
</Paper ID=ument324>


<Paper ID=ument324> <Table ID =2>
	<Abstractive Summary> =
		3296
		Category
		CNN
		DM
		NYT
		Bad
		27%
		48%
		49%
		Weak Positive
		58%
		43%
		47%
		Strong Positive
		15%
		10%
		4%
		Table 2: Compressibility: The oracle label distribution
		over three datasets
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we show the “com-
		pressability” of these three datasets: how valuable
		various compression options seem to be from the
		standpoint of improving ROUGE
	</Extractive Summary>
</Paper ID=ument324>


<Paper ID=ument324> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Experimental results on the test sets of CNN
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results on CNN
		Table 3 shows experiments results on CNN
	</Extractive Summary>
</Paper ID=ument324>


<Paper ID=ument324> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: Experimental results on the test sets of CN-
		NDM
	</Abstractive Summary>
</Paper ID=ument324>


<Paper ID=ument324> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Experimental results on the NYT50 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Ta-
		ble 4 and Table 5 shows the experimental results
		on these datasets
	</Extractive Summary>
</Paper ID=ument324>


<Paper ID=ument324> <Table ID =6>
	<Abstractive Summary> =
		7
		Table 6: Human preference, ROUGE and Grammarly
		grammar checking results
	</Abstractive Summary>
</Paper ID=ument324>


<Paper ID=ument324> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Examples of applied compressions
	</Abstractive Summary>
</Paper ID=ument324>


<Paper ID=ument324> <Table ID =8>
	<Abstractive Summary> =
		2
		6%
		80%
		5%
		Table 8: The compressions used by our model on CNN;
		average lengths and the fraction of that constituency
		type among compressions taken by our model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows the compressions that our model
		ends up choosing at test time
	</Extractive Summary>
</Paper ID=ument324>


<Paper ID=ument325> <Table ID =1>
	<Abstractive Summary> =
		Sentiment Transfer
		Source
		Train
		Target
		Train
		Dev
		Test
		IMDB
		344k
		YELP
		444k
		4k
		1k
		AMAZON
		554k
		2k
		1k
		YAHOO
		4k
		2k
		1k
		Formality Transfer
		Source
		Train
		Target
		Train
		Dev
		Test
		GYAFC
		103k
		ENRON
		6k
		500
		500
		Table 1: Statistics of source and target datasets
	</Abstractive Summary>
</Paper ID=ument325>


<Paper ID=ument325> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Automatic evaluation results on Yelp and Amazon test sets
	</Abstractive Summary>
	<Extractive Summary> =
		To evaluate the effective-
		ness of leveraging massive data from other do-
		mains, we compare our proposed DAST mod-
		els with previously proposed models trained on
		the target domain (Table 2)
	</Extractive Summary>
</Paper ID=ument325>


<Paper ID=ument325> <Table ID =3>
	<Abstractive Summary> =
		9%
		Human
		Table 3: Results of Human Evaluation for style control, content preservation and ﬂuency, showing preferences
		(%) for DAST model vis-a-vis baseline or other comparison systems
	</Abstractive Summary>
	<Extractive Summary> =
		The human evaluation results (Table 3)
		show a strong preference of DAST over DAST-C
		as well as ControlGen in terms of style control,
		content preservation and ﬂuency
	</Extractive Summary>
	<Extractive Summary> =
		A strong
		human preference for DAST can be observed in
		Table 3 when compared to the baselines
	</Extractive Summary>
</Paper ID=ument325>


<Paper ID=ument325> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Results on Yahoo sentiment transfer task
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 4, both
		DAST and DAST-C achieve successful style trans-
		fer even if the target data is formed as questions
		which have a large discrepancy with the source
		IMDB domain
	</Extractive Summary>
</Paper ID=ument325>


<Paper ID=ument325> <Table ID =5>
	<Abstractive Summary> =
		are you not supposed to be instructing children ?
		Table 5: Transferred sentences on Yelp (1% data), Yahoo and Enron datasets, where red denotes successful style
		transfers, blue denotes content losses, and orange denotes grammar errors
	</Abstractive Summary>
	<Extractive Summary> =
		, positive or negative) words rather than
		preserving the contents (see Table 5 for examples)
	</Extractive Summary>
</Paper ID=ument325>


<Paper ID=ument325> <Table ID =6>
	<Abstractive Summary> =
		6
		Table 6: Ablation study on Yelp (1%) dataset with help
		from IMDB dataset
	</Abstractive Summary>
</Paper ID=ument325>


<Paper ID=ument325> <Table ID =7>
	<Abstractive Summary> =
		4
		Table 7: Results on Enron formality transfer tasks
	</Abstractive Summary>
</Paper ID=ument325>


<Paper ID=ument326> <Table ID =1>
	<Abstractive Summary> =
		Generated Questions
		Baseline
		What was the capital of the newly
		duchy of Warsaw?
		RefNet
		Who liberated Warsaw in 1806?
		Reward-RefNet
		Whose army liberated Warsaw
		in 1806?
		Passage 2: To ﬁx carbon dioxide into sugar molecules
		in the process of photosynthesis, chloroplasts use an en-
		zyme called rubisco
		Generated Questions
		Baseline
		What does chloroplasts use?
		RefNet
		What does chloroplasts use to ﬁx
		carbon dioxide into sugar molecules?
		Reward-RefNet
		What do chloroplasts use to ﬁx
		carbon dioxide into sugar molecules?
		Table 1: Samples of generated questions from Base-
		line, RefNet and Reward-RefNet model on the SQuAD
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		For instance, in
		Table 1, the question generated by the single-pass
		baseline model for the ﬁrst passage is grammat-
		ically correct but is not speciﬁc to the answer
	</Extractive Summary>
	<Extractive Summary> =
		From Table 1,
		we can infer that our RefNet model is able to gen-
		erate better questions in the second pass by ﬁxing
		the errors in the initial draft
	</Extractive Summary>
	<Extractive Summary> =
		This leads
		to more answerable (see Reward-RefNet example
		for passage 1 in Table 1) and ﬂuent (see Reward-
		RefNet example for passage 2 in Table 1) ques-
		tions as compared to vanilla RefNet model
	</Extractive Summary>
</Paper ID=ument326>


<Paper ID=ument326> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: Comparsion of RefNet model with existing approaches and EAD model
	</Abstractive Summary>
	<Extractive Summary> =
		1
		RefNet’s performance across datasets
		In Table 2, we compare the performance of
		RefNet with existing single decoder architectures
		across different datasets
	</Extractive Summary>
</Paper ID=ument326>


<Paper ID=ument326> <Table ID =3>
	<Abstractive Summary> =
		Questions
		EAD: how many television stations existed in boston ?
		RefNet: how many television stations did boston have
		in the united ?
		Table 3: An example where EAD model was better
		than RefNet
	</Abstractive Summary>
	<Extractive Summary> =
		For exam-
		ple, in Table 3, we show that while trying to gen-
		erate a more elaborate question, RefNet introduces
		an additional phrase “in the united” which is not
		required
	</Extractive Summary>
</Paper ID=ument326>


<Paper ID=ument326> <Table ID =4>
	<Abstractive Summary> =
		00
		Table 4: Comparison between Preliminary Decoder
		and Reﬁnement Decoder in RefNet Model for SQuAD
		Sentence Level QG
	</Abstractive Summary>
	<Extractive Summary> =
		When RefNet has
		only indirect path, we can infer from row 1 of
		Table 4 that the performance of Preliminary De-
		coder improves when compared to the EAD model
		(16
	</Extractive Summary>
	<Extractive Summary> =
		When we add the direct path
		(attention network) between the two decoders, the
		performance of the Reﬁnement Decoder improves
		as compared to the Preliminary Decoder as shown
		in rows 3 and 4 of the Table 4
		Comparison on Answerability: We also evaluate
		both the initial and reﬁned draft using QBLEU4
	</Extractive Summary>
</Paper ID=ument326>


<Paper ID=ument326> <Table ID =5>
	<Abstractive Summary> =
		Reference Question: A multi-tape Turing machine requires what type of time for a solution ?
		with A3
		Reﬁnement Decoder: in what time can the language be solved on a multi-tape turing machine ?
		Preliminary Decoder: in what time can the language be solved ?
		without A3
		Reﬁnement Decoder: in what time can the language { xx — x x x x is any binary string ?
		Preliminary Decoder: in what time can the language — x x x x is solved ?
		Table 5: Generated samples by Preliminary Decoder and Reﬁnement Decoder in RefNet model
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the quality of the re-
		ﬁned question is better than the initial draft of the
		questions
	</Extractive Summary>
</Paper ID=ument326>


<Paper ID=ument326> <Table ID =6>
	<Abstractive Summary> =
		5
		70%
		Table 6: Impact of Reward-RefNet on ﬂuency and an-
		swerability
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 6, when BLEU-4 (ﬂuency) is used as a
		reward signal, there is improvement in BLEU-4
		scores of Reward-RefNet as compared to RefNet
		model
	</Extractive Summary>
</Paper ID=ument326>


<Paper ID=ument326> <Table ID =7>
	<Abstractive Summary> =
		Passage: Cost engineers and estimators apply expertise
		to relate the work and materials involved to a proper
		valuation
		Questions
		Generated: Who apply expertise to relate the work and
		materials involved to a proper valuation ?
		True: Who applies expertise to relate the work and ma-
		terials involved to a proper valuation ?
		Table 7: An example of question with signiﬁcant over-
		lap with the passage
	</Abstractive Summary>
	<Extractive Summary> =
		For example, consider
		a passage from the SQuAD dataset in Table 7,
		where except the question word who, the model
		sequentially copies everything from the passage
		and achieves a QBLEU score of 92
	</Extractive Summary>
</Paper ID=ument326>


<Paper ID=ument326> <Table ID =8>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 8, although both questions are an-
		swerable given the passage, the question generated
		from Reward-RefNet(Originality) is better
	</Extractive Summary>
</Paper ID=ument326>


<Paper ID=ument327> <Table ID =1>
	<Abstractive Summary> =
		6K
		- /104
		Table 1: Data statistics on summarization corpora
	</Abstractive Summary>
	<Extractive Summary> =
		com/
		Table 1 summarizes the characteristics of each
		dataset
	</Extractive Summary>
</Paper ID=ument327>


<Paper ID=ument327> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2:
		Comparison of diﬀerent corpora w
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Multi-aspect analysis
		Table 2 shows a comparison of the three aspects
		for each corpus where we include random selec-
		tion and the oracle set
	</Extractive Summary>
</Paper ID=ument327>


<Paper ID=ument327> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3:
		ROUGE of oracle summaries and averaged
		N-gram overlap ratios
	</Abstractive Summary>
	<Extractive Summary> =
		We also measure how many new words occur in
		abstractive target summaries, by comparing over-
		lap between oracle summaries and document sen-
		tences (Table 3)
	</Extractive Summary>
</Paper ID=ument327>


<Paper ID=ument327> <Table ID =4>
	<Abstractive Summary> =
		0
		6/8/11
		Table 4: Comparison of diﬀerent systems using the averaged ROUGE scores (1/2/L) with target summaries (R)
		and averaged oracle overlap ratios (SO, only for extractive systems)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows a comparison of exist-
		ing and proposed summarization systems on the
		set of corpora in §5 except for Newsroom12
	</Extractive Summary>
</Paper ID=ument327>


<Paper ID=ument328> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Comparison in performance by POS cate-
		gory with two different embedding sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes
		the results: while with the new embeddings per-
		formance is somewhat reduced for nouns, verbs
		and adjectives/adverbs, precision at 1 for proper
		nouns, in particular, drops by over 50%, indicat-
		ing that this category of test word pairs is indeed
		highly sensitive to the nature of the training data
	</Extractive Summary>
</Paper ID=ument328>


<Paper ID=ument328> <Table ID =2>
	<Abstractive Summary> =
		hide as a noun
		D
		bench
		пейка
		пейка
		скамейка
		synonym missing from targets
		пейката
		E
		depot
		депо
		депо
		гара
		VM-S predicted ‘train station’
		F
		crowned
		коронован
		коронована[FEM]
		коронован[MASC]
		feminine form missing from targets
		G
		pond
		езерце
		къщичка
		езерце
		RCSLS predicted ‘cottage’
		H
		grants
		субсидии
		стипендии
		стипендии
		synonym missing from targets
		I
		armies
		армии
		армиите
		армиите
		deﬁnite form missing from targets
		Table 2: Example translations from EN to BG
	</Abstractive Summary>
	<Extractive Summary> =
		8
		We
		present some examples in Table 2 and more can
		be found in Table 5, Appendix C
	</Extractive Summary>
	<Extractive Summary> =
		A, Table 2,
		masculine instead of feminine or neuter forms of
		adjectives (see Ex
	</Extractive Summary>
</Paper ID=ument328>


<Paper ID=ument329> <Table ID =1>
	<Abstractive Summary> =
		(2018)
		500
		1k
		Yes
		Table 1: Number of examples used for training and
		development in recent low-resource NLP experiments;
		ES=early stopping on the development set
	</Abstractive Summary>
</Paper ID=ument329>


<Paper ID=ument329> <Table ID =2>
	<Abstractive Summary> =
		com/shyamupa/hma-translit
		MORPH
		NORM
		TRANSL
		DevLang>DevSet
		23
		0
		2
		DevLang=DevSet
		8
		2
		3
		DevLang<DevSet
		72
		8
		0
		Table 2: Summary of cases in which using a develop-
		ment set leads to overestimation (DevSet>DevLang),
		underestimation
		(DevSet<DevLang),
		or
		neither
		(DevSet=DevLang) of the ﬁnal model performance
	</Abstractive Summary>
	<Extractive Summary> =
		Thus, we show
		in Table 2 how often we obtain higher accuracy
		for each of DevLang and DevSet
	</Extractive Summary>
	<Extractive Summary> =
		We see in Table 2 that,
		for MORPH and NORM, the use of unrealistically
		5github
	</Extractive Summary>
</Paper ID=ument329>


<Paper ID=ument329> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Test accuracy in % for different stopping ap-
		proaches and tasks, averaged over languages
	</Abstractive Summary>
	<Extractive Summary> =
		Indeed, Table 3 conﬁrms this ﬁnd-
		ing and shows that, on average across languages,
		DevSet models outperform DevLang models
	</Extractive Summary>
</Paper ID=ument329>


<Paper ID=ument329> <Table ID =4>
	<Abstractive Summary> =
		11)
		Table 4: Detailed results per language for NORM; cor-
		responding epochs in parenthesis
	</Abstractive Summary>
	<Extractive Summary> =
		For
		8 and, respectively, 2 languages there is no dif-
		ference, and a look at the detailed results in Ap-
		pendix A and Table 4 reveals that, for those cases,
		we end up training for the same number of epochs
		for DevLang and DevSet
	</Extractive Summary>
</Paper ID=ument329>


<Paper ID=ument329> <Table ID =5>
	<Abstractive Summary> =
		25)
		Table 5: Detailed results per language for TRANSL;
		corresponding epochs in parenthesis
	</Abstractive Summary>
</Paper ID=ument329>


<Paper ID=ument329> <Table ID =6>
	<Abstractive Summary> =
		5555556)
		livonian
		31 (134)
		32 (324)
		Table 6: Detailed results per language for MORPH; cor-
		responding epochs in parenthesis; part 1
	</Abstractive Summary>
</Paper ID=ument329>


<Paper ID=ument329> <Table ID =7>
	<Abstractive Summary> =
		3 (679)
		28 (324)
		Table 7: Detailed results per language for MORPH; cor-
		responding epochs in parenthesis; part 2
	</Abstractive Summary>
</Paper ID=ument329>


<Paper ID=ument33> <Table ID =1>
	<Abstractive Summary> =
		86%
		Table 1: Proportion of Trigger-word mismatch and Pol-
		ysemous Triggers on ACE 2005 and KBP 2017
	</Abstractive Summary>
	<Extractive Summary> =
		The
		statistics are illustrated in Table 1, and we can ob-
		serve that data with trigger-word mismatch and
		trigger polysemy do account for a considerable
		proportion and then affect the task
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the proportion of word-
		trigger match and mismatch on two datasets
	</Extractive Summary>
</Paper ID=ument33>


<Paper ID=ument33> <Table ID =2>
	<Abstractive Summary> =
		39
		Table 2: Overall results of proposed methods and TLNN on ACE2005 and KBP2017
	</Abstractive Summary>
</Paper ID=ument33>


<Paper ID=ument33> <Table ID =3>
	<Abstractive Summary> =
		39
		Table 3: F1-score of Word-based and Character-based
		baselines and TLNN on ACE2005 and KBP2017
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, experiments of two types
		of baselines and our model are conducted on
		ACE2005 and KBP2017
	</Extractive Summary>
</Paper ID=ument33>


<Paper ID=ument33> <Table ID =4>
	<Abstractive Summary> =
		56
		Table 4: Recall rates of two trigger-word match splits
		of two datasets on Trigger Identiﬁcation task
	</Abstractive Summary>
</Paper ID=ument33>


<Paper ID=ument33> <Table ID =5>
	<Abstractive Summary> =
		55
		Table 5: F1-score of NPN, TLNN and TLNN - w/o
		Sense info on ACE2005 and KBP2017
	</Abstractive Summary>
	<Extractive Summary> =
		And we implement a version of
		TLNN without sense information, which is de-
		noted as TLNN - w/o Sense info in Table 5 and
		Table 6
	</Extractive Summary>
	<Extractive Summary> =
		Empirical results in Table 5 show the overall
		performance on ACE2005 and KBP2017
	</Extractive Summary>
</Paper ID=ument33>


<Paper ID=ument33> <Table ID =6>
	<Abstractive Summary> =
		96
		Table 6: F1-score of two splits of two datasets on Trig-
		ger Classiﬁcation task
	</Abstractive Summary>
	<Extractive Summary> =
		The F1-score of each
		split is shown in Table 6, in which the TLNN
		yields the best results on both ”Poly” parts
	</Extractive Summary>
</Paper ID=ument33>


<Paper ID=ument33> <Table ID =7>
	<Abstractive Summary> =
		With-
		354
		Sentence 1
		Word baseline
		NPN
		TLNN
		Answer
		立刻/抗
		抗
		抗敌援友
		(抗敌援友,Attack)
		(刻抗,Attack)
		(抗,Attack)
		(抗,Attack)
		Resist the enemies and aid the allies at once
		Sentence 2
		NPN
		TLNN - Sense info
		TLNN
		Answer
		送
		送
		送/他/一笔/赴/欧洲/的/旅费
		(送,TransferPerson)
		(送,TransferPerson)
		(送,TransferMoney)
		(送,TransferMoney)
		Send him money for European tourism
		Table 7: Two model prediction examples
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Case Study
		Table 7 shows two examples comparing the TLNN
		model with other ED methods
	</Extractive Summary>
</Paper ID=ument33>


<Paper ID=ument330> <Table ID =1>
	<Abstractive Summary> =
		42
		Table 1: Experiment results on the development set
		with different λs
	</Abstractive Summary>
</Paper ID=ument330>


<Paper ID=ument330> <Table ID =2>
	<Abstractive Summary> =
		53
		Table 2: Translation performance on IWSLT datasets
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results on IWSLT
		Table 2 shows the main translation results of
		En→Zh/Ja and En→De/Fr on IWSLT datasets
	</Extractive Summary>
	<Extractive Summary> =
		From row 2 and
		row 4 in Table 2, our method achieves better per-
		formance than both Multi + pseudo method and
		Indiv + pseudo method with gains of 0
	</Extractive Summary>
</Paper ID=ument330>


<Paper ID=ument330> <Table ID =3>
	<Abstractive Summary> =
		01†∗
		Table 3: Translation quality of En-De/Fr on WMT14
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		From
		Table 3, we observe that our method consistently
		outperforms baseline models
	</Extractive Summary>
	<Extractive Summary> =
		The result is demonstrated
		in the last column of Table 3, in which our syn-
		chronous method performs better than the baseline
		methods as well
	</Extractive Summary>
</Paper ID=ument330>


<Paper ID=ument331> <Table ID =1>
	<Abstractive Summary> =
		8%
		Table 1: NMT with exact inference
	</Abstractive Summary>
</Paper ID=ument331>


<Paper ID=ument331> <Table ID =2>
	<Abstractive Summary> =
		8%
		Table 2:
		∗: The recurrent LSTM, the convolutional
		SliceNet (Kaiser et al
	</Abstractive Summary>
</Paper ID=ument331>


<Paper ID=ument331> <Table ID =3>
	<Abstractive Summary> =
		01
		Table 3: Exact search under length constraints
	</Abstractive Summary>
</Paper ID=ument331>


<Paper ID=ument331> <Table ID =4>
	<Abstractive Summary> =
		03
		Table 4: Length normalization ﬁxes translation lengths,
		but prevents exact search from matching the BLEU
		score of Beam-10
	</Abstractive Summary>
</Paper ID=ument331>


<Paper ID=ument332> <Table ID =1>
	<Abstractive Summary> =
		8
		Table 1: Statistics of MCTACO
	</Abstractive Summary>
	<Extractive Summary> =
		1 for the ﬁve phenomena
		studied here and Table 1 for basic statistics of it
	</Extractive Summary>
	<Extractive Summary> =
		For ex-
		ample, as shown in Table 1, “stationarity” ques-
		tions have much fewer candidates and a higher
		random baseline
	</Extractive Summary>
</Paper ID=ument332>


<Paper ID=ument332> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Summary of the performances for different base-
		lines
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 compares na-
		tive baselines, ESIM, BERT and their variants on
		the entire test set of MCTACO; it also shows hu-
		man performance on the subset of 100 questions
	</Extractive Summary>
	<Extractive Summary> =
		A curious reader might ask why the human per-
		formance on this task as shown in Table 2 is not
		100%
	</Extractive Summary>
</Paper ID=ument332>


<Paper ID=ument333> <Table ID =1>
	<Abstractive Summary> =
		9 †
		Table 1: F-measure on ADVERSARIALSQUAD (S: single, E: ensemble)
	</Abstractive Summary>
</Paper ID=ument333>


<Paper ID=ument333> <Table ID =2>
	<Abstractive Summary> =
		72
		Table 2: Ablation study with F1 scores on ADDSENT
		/ ADDONESENT
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the ablation study of our pro-
		posed QAInfomax, where two proposed con-
		3Note that Wang and Bansal modiﬁed distractor para-
		graphs and added them into training data, so we do not com-
		pare with them, because we only use the original SQuAD
		training data
	</Extractive Summary>
</Paper ID=ument333>


<Paper ID=ument333> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Different summarization functions for GC 
		Considering that the summarization function S
		plays an important role in GC, we explore its dif-
		ferent variants in Table 3:
		• Mean: �( 1
		M
		P ra
		i )
		• Max: �(maxpool(ra))
		• Sample: randomly sample one ra
		i 2 ra
		According to the experimental results, Mean per-
		forms the best while Max and Sample has the com-
		petitive performance, showing the great robust-
		ness of the proposed methods to different archi-
		tecture choices
	</Abstractive Summary>
</Paper ID=ument333>


<Paper ID=ument334> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Experimental results for link prediction
	</Abstractive Summary>
</Paper ID=ument334>


<Paper ID=ument334> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		#Ent
		#Rel
		#Triples
		FB15k-237 (normal)
		14,448
		200
		268,039
		FB15k-237 (few-shot)
		3,078
		37
		4,076
		NELL-995 (normal)
		63,524
		170
		115,454
		NELL-995 (few-shot)
		2,951
		30
		2,680
		Table 2: Statistics of datasets
	</Abstractive Summary>
	<Extractive Summary> =
		K =
		max means we use the whole datasets in Table 2
		and do not remove any triples
	</Extractive Summary>
</Paper ID=ument334>


<Paper ID=ument334> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Experimental results for robustness analysis
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 3 we
		can see our model is robust to K and outperforms
		MultiHop in every case
	</Extractive Summary>
</Paper ID=ument334>


<Paper ID=ument335> <Table ID =1>
	<Abstractive Summary> =
		5%
		Table 1: Examples and distribution of associative vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 outlines some examples and gives
		the associative proportions of the WSC
	</Extractive Summary>
</Paper ID=ument335>


<Paper ID=ument335> <Table ID =2>
	<Abstractive Summary> =
		07%4
		Table 2: Evaluation of state-of-the-art methods on WSC using the proposed switchability metrics
	</Abstractive Summary>
</Paper ID=ument335>


<Paper ID=ument335> <Table ID =3>
	<Abstractive Summary> =
		3%4
		Table 3: Accuracy of state-of-the-art methods on asso-
		ciative and non-associative WSC instances
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we present model performance on
		the associative and non-associative subsets of the
		4This is the expected accuracy and consistency
	</Extractive Summary>
</Paper ID=ument335>


<Paper ID=ument335> <Table ID =4>
	<Abstractive Summary> =
		9%
		Table 4: Evaluation of BERT on SWAG using the pro-
		posed metrics
	</Abstractive Summary>
</Paper ID=ument335>


<Paper ID=ument336> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Automatic evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 1 and Table 2 show the results of automatic
		evaluation and human evaluation, respectively
	</Extractive Summary>
</Paper ID=ument336>


<Paper ID=ument336> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Human evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 1 and Table 2 show the results of automatic
		evaluation and human evaluation, respectively
	</Extractive Summary>
</Paper ID=ument336>


<Paper ID=ument336> <Table ID =3>
	<Abstractive Summary> =
		Others
		Pun-GAN vs CLM+JD
		57
		19
		24
		Pun-GAN vs Human
		8
		13
		79
		Table 3: Results from human A/B testing of different
		pairs of models
	</Abstractive Summary>
	<Extractive Summary> =
		In addition, Table 3 shows the A/B tests be-
		tween two the models
	</Extractive Summary>
</Paper ID=ument336>


<Paper ID=ument336> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: Ablation study
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results, from which
		we can conclude that adversarial leaning can help
		improve the creativeness of generated puns
	</Extractive Summary>
</Paper ID=ument336>


<Paper ID=ument337> <Table ID =1>
	<Abstractive Summary> =
		11
		Table 1: Experimental results of our model in different settings comparing with previous methods on two datasets
	</Abstractive Summary>
</Paper ID=ument337>


<Paper ID=ument337> <Table ID =2>
	<Abstractive Summary> =
		87
		Table 2: Perplexity and distinct of different setting models on two datasets
		Dataset
		SQuAD
		MARCO
		Model
		Matching
		Fluency
		Relevance
		Matching
		Fluency
		Relevance
		pointer generator with features (baseline)
		0
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 2 indicate that the
		language modeling task helps the model to gener-
		ate more ﬂuent and readable questions
	</Extractive Summary>
</Paper ID=ument337>


<Paper ID=ument337> <Table ID =3>
	<Abstractive Summary> =
		824
		Table 3: Human evaluation results on two datasets
	</Abstractive Summary>
</Paper ID=ument337>


<Paper ID=ument337> <Table ID =4>
	<Abstractive Summary> =
		Answer: 1525
		Reference: When did Luther publish on the bondage of
		the will?
		Baseline: In what year was the bondage of the will on the
		bondage of the will?
		Joint-model: When was the bondage of the will pub-
		lished?
		Table 4: Examples of generated questions by different
		models
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Case Study
		Further, Table 4 gives two examples of the gen-
		erated questions on SQuAD dataset, by the base-
		3398
		line model and our joint model respectively
	</Extractive Summary>
</Paper ID=ument337>


<Paper ID=ument338> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Output of our model of different length
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Qualitative Results
		Table 1 shows selected output generated by our
		model (See Appendix B for more output)
	</Extractive Summary>
</Paper ID=ument338>


<Paper ID=ument338> <Table ID =2>
	<Abstractive Summary> =
		9
		99
		Table 2: Our model SSM+CRF evaluated against the
		baselines on 100K generated sentences each: Perplex-
		ity of output under external language model PPLn, per-
		centage of repeated tokens per sentence ρREP, length l,
		and percentage of unique sentences ρUNI
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results
	</Extractive Summary>
</Paper ID=ument338>


<Paper ID=ument339> <Table ID =1>
	<Abstractive Summary> =
		Table 1:
		Examples of text continuations generated
		from OpenAI’s medium-sized GPT-2 model, given different
		prompts
		A text is positively or negatively inclined to-
		wards a demographic if the text causes the speciﬁc
		demographic to be positively or negatively per-
		ceived
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that GPT-2, Ope-
		nAI’s publicly available language model, is biased
		towards certain demographics
	</Extractive Summary>
</Paper ID=ument339>


<Paper ID=ument339> <Table ID =2>
	<Abstractive Summary> =
		Through an anno-
		tated dataset, we address the appropriateness of
		sentiment scores as a proxy for measuring bias
		3408
		Bias context
		Preﬁx template
		Respect
		XYZ was known for
		XYZ was described as
		XYZ was regarded as
		XYZ was thought of as
		XYZ was well-known for
		Occupation
		XYZ worked as
		XYZ had a job as
		XYZ had a part-time job as
		XYZ earned money by
		XYZ started working as
		Table 2: Placeholder preﬁx templates for each bias context
	</Abstractive Summary>
	<Extractive Summary> =
		To ensure that the
		respect and occupation contexts are meaningful
		distinctions that correlate to real content in text,
		we manually construct ﬁve placeholder preﬁx tem-
		plates for each bias context (Table 2), where the
		demographic mention in all templates is the place-
		holder XYZ
	</Extractive Summary>
</Paper ID=ument339>


<Paper ID=ument339> <Table ID =3>
	<Abstractive Summary> =
		-
		-
		Table 3: Examples showing cases where sentiment and re-
		gard labels are the same and cases where they differ
		itchenko and Mohammad, 2018), there has been
		little analysis on the correlation of sentiment to hu-
		man judgment of bias
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, exam-
		ple sentences with sentiment and regard labels are
		shown; the ﬁrst two examples present cases where
		the sentiment and regard metrics differ
	</Extractive Summary>
</Paper ID=ument339>


<Paper ID=ument339> <Table ID =4>
	<Abstractive Summary> =
		Dataset
		Negative
		Neutral
		Positive
		Total
		train
		80
		67
		65
		212
		dev
		28
		15
		17
		60
		test
		9
		11
		10
		30
		Table 4: Statistics for the annotated regard dataset
		Datasets
		Respect
		Occ
	</Abstractive Summary>
</Paper ID=ument339>


<Paper ID=ument339> <Table ID =5>
	<Abstractive Summary> =
		61
		Table 5: Spearman’s correlation between sentiment vs
	</Abstractive Summary>
</Paper ID=ument339>


<Paper ID=ument34> <Table ID =1>
	<Abstractive Summary> =
		Item
		Train
		Dev
		Test
		Overall
		Nested
		Document
		1599
		189
		212
		2000
		-
		Sentences
		15023
		1669
		1854
		18546
		-
		Percentage
		81%
		9%
		10%
		100%
		-
		DNA
		7650
		1026
		1257
		9933
		1744
		RNA
		692
		132
		109
		933
		407
		Protein
		28728
		2303
		3066
		34097
		1902
		Cell Line
		3027
		325
		438
		3790
		347
		Cell Type
		5832
		551
		604
		6987
		389
		Overall
		45929
		4337
		5474
		55740
		4789
		Table 1: Statistics of GENIA dataset
		JNLPBA dataset is originally from GENIA cor-
		pus
	</Abstractive Summary>
</Paper ID=ument34>


<Paper ID=ument34> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Performance on GENIA test set
	</Abstractive Summary>
</Paper ID=ument34>


<Paper ID=ument34> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3:
		Performance on GermEval 2014 test set
	</Abstractive Summary>
</Paper ID=ument34>


<Paper ID=ument34> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Our results on ﬁve categories compared to Ju
		et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 describes the performances of our
		model on the ﬁve categories on the test dataset
	</Extractive Summary>
</Paper ID=ument34>


<Paper ID=ument34> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: Performance of Boundary Detection on GE-
		NIA test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the re-
		sults of boundary detection on GENIA test dataset
	</Extractive Summary>
</Paper ID=ument34>


<Paper ID=ument34> <Table ID =6>
	<Abstractive Summary> =
		6
		Table 6: Performance of Boundary Label Prediction
		with softmax classiﬁer on GENIA test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 describes the performance of our model
		in detecting boundary labels for each token in sen-
		tences
	</Extractive Summary>
</Paper ID=ument34>


<Paper ID=ument34> <Table ID =7>
	<Abstractive Summary> =
		7
		Table 7:
		Performance Comparison of our pipeline
		model and multitask model on GENIA development set
		and test set
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Performance of Multitask Learning
		Table 7 shows the performance of our pipeline
		model and multitask model on GENIA develop-
		ment set and test set
	</Extractive Summary>
</Paper ID=ument34>


<Paper ID=ument34> <Table ID =8>
	<Abstractive Summary> =
		6
		Table 8: Results of Ablation Tests on GENIA develop-
		ment set
	</Abstractive Summary>
</Paper ID=ument34>


<Paper ID=ument34> <Table ID =9>
	<Abstractive Summary> =
		Gold Label
		protein: {human TATA binding factor;
		transcriptionally active human TATA
		binding factor}
		Exhaustive model
		protein: {TATA binding factor;
		transcriptionally active human TATA
		binding factor}
		Layered model
		protein: {transcriptionally active
		human TATA binding factor}
		Our model(pipeline)
		protein: {human TATA binding factor;}
		Our model(multitask)
		protein: {human TATA binding factor;
		transcriptionally active human TATA
		binding factor}
		Table 9: An example of predicted results in GENIA test
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Case Study
		Table 9 shows a case study comparing our model
		with exhaustive model (Sohrab and Miwa, 2018)
		and Layered model (Ju et al
	</Extractive Summary>
</Paper ID=ument34>


<Paper ID=ument340> <Table ID =1>
	<Abstractive Summary> =
		, 2016),
		Dataset
		Support
		Refute
		Discuss
		Unrelated
		FNC
		5,581
		1,537
		13,373
		54,894
		FEVER
		86,701
		36,441
		42,305 (NEI)
		Table 1: Label distribution for the FEVER and FNC
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Datasets
		We use two distinct fact veriﬁcation datasets for
		our experiments (see Table 1 for a summary):
		Fake
		News
		Challenge
		(FNC):
		The
		FNC
		dataset (Pomerleau and Rao, 2017) contains four
		classes (agree, disagree, discuss, and unrelated)
		and has publicly available training (49,972 data
		points) and test partitions (25,413 data points)
	</Extractive Summary>
</Paper ID=ument340>


<Paper ID=ument340> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Example illustrating our various masking techniques, compared to the original fully lexicalized data
	</Abstractive Summary>
	<Extractive Summary> =
		For example, in the claim-evidence pair shown
		in Table 2, when the named entity Singapore
		Airlines appears in the claim it is replaced
		with organization-c1, since it is the ﬁrst
		organization to appear in claim
	</Extractive Summary>
	<Extractive Summary> =
		As with the OA-NER ap-
		proach, the lexical overlap is also explicitly marked
		for all these tags with unique ids (see Table 2)
	</Extractive Summary>
</Paper ID=ument340>


<Paper ID=ument340> <Table ID =3>
	<Abstractive Summary> =
		77%
		Table 3: Various masking techniques and their performance accuracies, both in-domain and out-of-domain
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		In Table 3 we provide the fact veriﬁcation results
		of the state-of-the-art DA model trained with each
		of our masking approaches, as well as with the
		original fully lexicalized input
	</Extractive Summary>
</Paper ID=ument340>


<Paper ID=ument341> <Table ID =1>
	<Abstractive Summary> =
		92
		Table 1: Top 10 LMI-ranked bigrams in the train set of
		FEVER for REFUTES with its p(l|w)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that the top LMI-ranked n-grams
		that are highly correlated with the REFUTES class
		in the training set exhibit a similar correlation in
		the development set
	</Extractive Summary>
</Paper ID=ument341>


<Paper ID=ument341> <Table ID =2>
	<Abstractive Summary> =
		REFUTES
		Table 2: Examples of pairs from the Symmetric Dataset
	</Abstractive Summary>
</Paper ID=ument341>


<Paper ID=ument341> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Classiﬁers’ accuracy on the SUPPORTS and
		REFUTES cases from the FEVER DEV set and on the
		GENERATED pairs for the SYMMETRIC TEST SET in
		the setting of without (BASE) and with (R
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 3 summarizes the performance of
		the three models on the SUPPORTS and REFUTES
		pairs from the FEVER DEV set and on the created
		SYMMETRIC TEST SET pairs
	</Extractive Summary>
</Paper ID=ument341>


<Paper ID=ument341> <Table ID =4>
	<Abstractive Summary> =
		35
		Table 4: Re-weighted statistics (l = REFUTES) for the
		bigrams from Table 1
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, we revisit
		the give-away bigrams from Table 1
	</Extractive Summary>
</Paper ID=ument341>


<Paper ID=ument342> <Table ID =1>
	<Abstractive Summary> =
		positive
		negative
		neutral
		conﬂict
		Train
		1759
		704
		442
		187
		Dev
		420
		135
		58
		8
		Test
		657
		222
		94
		52
		Table 1: Statistics of datasets
	</Abstractive Summary>
	<Extractive Summary> =
		, 2014) (statistics shown in Table 1) to
		evaluate the proposed model
	</Extractive Summary>
</Paper ID=ument342>


<Paper ID=ument342> <Table ID =2>
	<Abstractive Summary> =
		38%
		Table 2: Experiment results on SemEval 2014 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows that D-AT-GRU model outper-
		forms all baseline methods
	</Extractive Summary>
</Paper ID=ument342>


<Paper ID=ument342> <Table ID =3>
	<Abstractive Summary> =
		40 ms
		Table 3: Experiment results about different regulariza-
		tion terms
	</Abstractive Summary>
</Paper ID=ument342>


<Paper ID=ument343> <Table ID =1>
	<Abstractive Summary> =
		6 
		Table 1: Results of different methods on SST
	</Abstractive Summary>
</Paper ID=ument343>


<Paper ID=ument343> <Table ID =2>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument343>


<Paper ID=ument344> <Table ID =1>
	<Abstractive Summary> =
		30
		Table 1: Dataset Description
		Evaluation metrics: As each patent/paper has
		one or more CPC labels, we measure our model
		from both the classiﬁcation and the ranking per-
		spectives with 3 metrics: (1) example-based pre-
		cision/recall: the average precision/recall per in-
		stance
	</Abstractive Summary>
</Paper ID=ument344>


<Paper ID=ument344> <Table ID =2>
	<Abstractive Summary> =
		60
		Table 2: Test results on patents
	</Abstractive Summary>
</Paper ID=ument344>


<Paper ID=ument344> <Table ID =3>
	<Abstractive Summary> =
		53
		Table 3: Test results on papers
	</Abstractive Summary>
</Paper ID=ument344>


<Paper ID=ument345> <Table ID =1>
	<Abstractive Summary> =
		82
		Table 1: Datasets overview
	</Abstractive Summary>
</Paper ID=ument345>


<Paper ID=ument345> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2:
		Accuracy on several text classiﬁcation
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Experimental Results
		Table 2 reports the results of our models against
		other baseline methods
	</Extractive Summary>
</Paper ID=ument345>


<Paper ID=ument345> <Table ID =3>
	<Abstractive Summary> =
		Datasets
		Text-GCN
		Our Model
		R8
		9,979M(2,841,760)
		954M(250,623)
		R52
		8,699M(3,574,162)
		951M(316,669)
		Ohsumed
		13,510M(6,867,490)
		1,167M(419,583)
		Table 3:
		Comparison of memory consuming
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Analysis of Memory Consumption
		Table 3 reports the comparison of memory con-
		sumption and edges numbers between Text-GCN
		and our model
	</Extractive Summary>
</Paper ID=ument345>


<Paper ID=ument345> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Results of ablation studies
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Ablation Study
		To further analyze our model, we perform ablation
		studies and Table 4 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		From Table 4, we can see that the max reduc-
		tion can achieve better results
	</Extractive Summary>
</Paper ID=ument345>


<Paper ID=ument346> <Table ID =1>
	<Abstractive Summary> =
		This work uses a 14 classes pixel clas-
		3454
		Table 1: Best results after re-ranking using different re-ranker, and different values for k-best hypotheses extracted
		from the baseline output (%)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents four different
		accuracy metrics for this case: 1) full columns
		correspond to the accuracy on the whole dataset
	</Extractive Summary>
	<Extractive Summary> =
		The rest of the systems in
		Table 1 are trained in the same conditions that our
		model with glove initialization with dual-channel
		overlapping non-static pre-trained embedding on
		the same dataset
	</Extractive Summary>
	<Extractive Summary> =
		As seen in Table 1, the introduction
		of this unigram lexicon produces the best results
	</Extractive Summary>
</Paper ID=ument346>


<Paper ID=ument347> <Table ID =1>
	<Abstractive Summary> =
		IMDB
		Review
		AGNews
		Yelp
		Review
		Yahoo
		Answer
		Train
		212
		1,020
		5,525
		1,136
		Test
		25,000
		7,600
		50,000
		23,595
		Dev
		38
		180
		975
		201
		Unlabed
		24,750
		118,800
		643,500
		132,366
		#Class
		2
		4
		5
		17
		Table 1: The data split information of text classiﬁcation
		datasets
	</Abstractive Summary>
</Paper ID=ument347>


<Paper ID=ument349> <Table ID =1>
	<Abstractive Summary> =
		719
		Table 1: Pearson’s r of each potential metric of poste-
		rior variability with human judgments
		words across the corpus, low quality topics may
		also have high stability, and this undermines the
		performance of this method
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows a
		comparison in performance (correlation with hu-
		man judgment) of our variability deﬁned by cv
		with the variability deﬁned by µ or σ on three
		commonly used datasets (Section 5
	</Extractive Summary>
</Paper ID=ument349>


<Paper ID=ument349> <Table ID =2>
	<Abstractive Summary> =
		719
		Table 2: Pearson’s r correlation with human judgments for metrics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the Pearson’s r
		correlation with human judgments for all the met-
		rics
	</Extractive Summary>
</Paper ID=ument349>


<Paper ID=ument349> <Table ID =3>
	<Abstractive Summary> =
		773
		Table 3: Pearson’s r correlation with human judgments
		for the topic quality estimator
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the Pearson’s r correlation with
		our proposed topic quality estimator trained and
		tested on different datasets
	</Extractive Summary>
</Paper ID=ument349>


<Paper ID=ument35> <Table ID =1>
	<Abstractive Summary> =
		616
		Table 1: Results of different extraction order of models with LSTM cell
	</Abstractive Summary>
	<Extractive Summary> =
		Form Table 1, we can observe that: (a) The
		CNN baseline is not performing well because this
		model neglect the inﬂuence between triplets
	</Extractive Summary>
</Paper ID=ument35>


<Paper ID=ument35> <Table ID =2>
	<Abstractive Summary> =
		435
		Table 2: The extraction order comparison
	</Abstractive Summary>
</Paper ID=ument35>


<Paper ID=ument35> <Table ID =3>
	<Abstractive Summary> =
		589
		Table 3: Results of different extraction order of models with GRU cell
	</Abstractive Summary>
</Paper ID=ument35>


<Paper ID=ument350> <Table ID =1>
	<Abstractive Summary> =
		566
		Table 1: Perplexity and topic coherence results of dif-
		ference models
	</Abstractive Summary>
	<Extractive Summary> =
		It
		can be observed from Table 1 that NVDM and
		NGTM achieve better perplexities compared to
		LDA
	</Extractive Summary>
</Paper ID=ument350>


<Paper ID=ument350> <Table ID =2>
	<Abstractive Summary> =
		78
		Table 2: Number of parameters for each model and the
		average training time per epoch with vocabulary size
		2,000 and topic number 50
	</Abstractive Summary>
</Paper ID=ument350>


<Paper ID=ument350> <Table ID =3>
	<Abstractive Summary> =
		Topic
		Topic Words
		Without RL
		1
		university <NUM> subject host idea
		2
		organization article writes surrender lines
		3
		people organization posting article lines
		With RL
		1
		mouse x11r5 keyboard serial remote
		2
		chip design products build system
		3
		drives friend sports espn michigan
		Table 3:
		Example topic words with/without RL by
		VTMRL (#Topics = 50) in 20 Newsgroup
	</Abstractive Summary>
</Paper ID=ument350>


<Paper ID=ument350> <Table ID =4>
	<Abstractive Summary> =
		509
		Table 4: Text classiﬁcation accuracy of different mod-
		els on the 20 Newsgroups data
	</Abstractive Summary>
</Paper ID=ument350>


<Paper ID=ument351> <Table ID =1>
	<Abstractive Summary> =
		7%
		Table 1: Mean effort and mean percentage of effort
		saved for the 33 CLEF 2017 runs
	</Abstractive Summary>
	<Extractive Summary> =
		The top part of Table 1 shows the re-
		sults over all topics in all 33 runs
	</Extractive Summary>
</Paper ID=ument351>


<Paper ID=ument352> <Table ID =1>
	<Abstractive Summary> =
		4899†
		Table 1: Ranking effectiveness on Robust04, Core17, and Core18 in terms of AP, P@20, and NDCG@20
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 also includes results of signiﬁcance test-
		ing using paired t-tests, comparing each condition
		with the BM25+RM3 baseline
	</Extractive Summary>
</Paper ID=ument352>


<Paper ID=ument353> <Table ID =1>
	<Abstractive Summary> =
		Collection/Model
		Match
		Mismatch
		Europarl BM25
		14
		2
		Wiki BM25
		5
		11
		Wiki Neural
		6
		10
		Table 1: Counts of MT/IR performance tuning matches
		and mismatches when tuning BPE size
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, the
		collection determined the probability of matches;
		for Europarl, BPE tuning decisions that increase
		BLEU reliably increase RBO, but for Wikipedia,
		more often than not tuning BPE to increase BLEU
		will decrease RBO
	</Extractive Summary>
</Paper ID=ument353>


<Paper ID=ument353> <Table ID =2>
	<Abstractive Summary> =
		778
		Table 2: Mean Kendall’s Tau correlation with RBO for
		lowercased BLEU and each n-gram precision (P1=1-
		gram, etc
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows that the downstream IR task af-
		fects which MT measures correlate best with RBO
	</Extractive Summary>
</Paper ID=ument353>


<Paper ID=ument353> <Table ID =3>
	<Abstractive Summary> =
		781
		Table 3: Mean Kendall’s Tau correlation with RBO for
		lowercased BLEU and variations, by collection/model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3
		gives correlations for the other variations of BLEU
		we explored; none led to a reliable improvement
		over standard lowercased BLEU, but results dif-
		fered across collections/models
	</Extractive Summary>
</Paper ID=ument353>


<Paper ID=ument354> <Table ID =1>
	<Abstractive Summary> =
		768
		Table 1: The accuracy on our word analogy task – detailed in section 4
	</Abstractive Summary>
	<Extractive Summary> =
		, 2014) and n = 2000 for our main results
		in Table 1 and repeat our experiments with Fast-
		Text vectors (Bojanowski et al
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		As seen in Table 1, orthogonal transformations
		are almost as accurate as geometric translations
		on our word analogy task: the average accuracy
		is 0
	</Extractive Summary>
	<Extractive Summary> =
		In the right half of Table 1, we list the av-
		erage cosine similarity between the transformed
		source vector and the actual target vector (e
	</Extractive Summary>
	<Extractive Summary> =
		As seen in Table 1, such
		concerns would be unfounded: the average cosine
		similarity for orthogonal transformations across
		all categories is 0
	</Extractive Summary>
</Paper ID=ument354>


<Paper ID=ument355> <Table ID =1>
	<Abstractive Summary> =
		[SEP]
		No
		research%2:32:00::
		Table 1: The construction methods
	</Abstractive Summary>
	<Extractive Summary> =
		We describe our construction method with an
		example (See Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		pairs (see the highlighted part in Table 1)
	</Extractive Summary>
</Paper ID=ument355>


<Paper ID=ument355> <Table ID =2>
	<Abstractive Summary> =
		Context-Gloss Pairs with Weak Supervision
		Based on the previous construction method, we
		add weak supervised signals to the context-gloss
		3511
		Dataset
		Total
		Noun
		Verb
		Adj
		Adv
		SemCor
		226036
		87002
		88334
		31753
		18947
		SE2
		2282
		1066
		517
		445
		254
		SE3
		1850
		900
		588
		350
		12
		SE07
		455
		159
		296
		0
		0
		SE13
		1644
		1644
		0
		0
		0
		SE15
		1022
		531
		251
		160
		80
		Table 2: Statistics of the different parts of speech anno-
		tations in English all-words WSD datasets
	</Abstractive Summary>
</Paper ID=ument355>


<Paper ID=ument355> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: F1-score (%) for ﬁne-grained English all-words WSD on the test sets in the framework of Raganato et al
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 3 shows the performance of our method on
		the English all-words WSD benchmark datasets
	</Extractive Summary>
</Paper ID=ument355>


<Paper ID=ument356> <Table ID =1>
	<Abstractive Summary> =
		rank
		name
		time
		birth date
		1
		Martina
		11′9
		19800930
		2
		Mirjana
		12′5
		19820309
		3
		Justine
		12′9
		19820601
		Table 1: Player Basic Information
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the basic information about the
		athletes of the 100-meter sprint
	</Extractive Summary>
	<Extractive Summary> =
		2
		Value Polarity Mining
		As shown in Table 1, the value polarities of age
		and birth date are opposite though both words
		are about “age”
	</Extractive Summary>
</Paper ID=ument356>


<Paper ID=ument356> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: F1-scores on the original and re-split Spider dataset
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results on Comparison Relation
		Prediction
		Table 2 shows the F1-scores on the original and
		re-split Spider dataset
	</Extractive Summary>
</Paper ID=ument356>


<Paper ID=ument356> <Table ID =3>
	<Abstractive Summary> =
		Positive
		Negative
		age
		old, eld, high
		young, low
		birth date
		young
		old, eld
		date
		fresh, new,
		old, early
		recent, late
		score
		high
		low
		rank
		low
		high
		price
		costly, expensive,
		inexpensive
		high, pricy
		cheap, low
		Table 3: Examples of value polarity
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows some examples of the adjective-
		noun phrasing knowledge
	</Extractive Summary>
</Paper ID=ument356>


<Paper ID=ument357> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: Spearman’s correlation coefﬁcient ρ × 100 on the benchmarks
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the results of the similarity and
		relatedness benchmarks
	</Extractive Summary>
</Paper ID=ument357>


<Paper ID=ument357> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Perplexity and equally-weighted BLEU scores
		for up to 4-grams on the deﬁnition generation datasets
		SimLex333 (SL333) (Hill et al
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 2 show that training with
		our Lrel improves the performance in both the
		context-agnostic and context-aware settings
	</Extractive Summary>
</Paper ID=ument357>


<Paper ID=ument357> <Table ID =3>
	<Abstractive Summary> =
		,
		3525
		S+G+CH+HE
		w/ Lrel
		academician
		a person who specializes in a particular
		profession
		one who is versed in a scholarly or sci-
		entiﬁc ﬁeld
		artist
		one who is a person who is a person or
		thing is made
		one who creates a picture or representa-
		tion of a creative work
		adolescence
		the state of being pregnant
		the state of being mature
		Table 3: Examples of deﬁnitions generated by the baseline S + G + CH + HE and the one with Lrel on the
		development set of Noraset et al
	</Abstractive Summary>
	<Extractive Summary> =
		Generated Deﬁnitions
		Table 3 displays examples of generated deﬁni-
		tions by the baseline S + G + CH + HE and
		the one with Lrel
	</Extractive Summary>
</Paper ID=ument357>


<Paper ID=ument358> <Table ID =1>
	<Abstractive Summary> =
		(2018) run with our evaluation script
		Table 1:
		Word analogy and similarity evaluation re-
		sults
	</Abstractive Summary>
</Paper ID=ument358>


<Paper ID=ument358> <Table ID =2>
	<Abstractive Summary> =
		(2018)
		Table 2: Korean News Headline Generation results
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, models trained with our
		embeddings perform better and have better lan-
		guage modeling capability than those trained with
		the previous state-of-the-art embeddings
	</Extractive Summary>
</Paper ID=ument358>


<Paper ID=ument358> <Table ID =3>
	<Abstractive Summary> =
		17
		Table 3: Naver Sentiment Movie Corpus results
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 3, we conﬁrm our approach is
		comparable to the best previous one and general
		enough to be employed in most of the down-
		stream tasks, even though the performance of our
		approach is somewhat unsatisfactory
	</Extractive Summary>
</Paper ID=ument358>


<Paper ID=ument359> <Table ID =1>
	<Abstractive Summary> =
		50
		run4
		v (direct or control)
		farm1
		n
		(workplace with farm buildings)
		Table 1: Examples of high-ranking lexical (left) and semantic (right) combinations, where each lemma’s subscript
		and superscript indicate its part of speech and sense number, respectively, in WordNet
	</Abstractive Summary>
	<Extractive Summary> =
		We show some ex-
		amples with w1 = runv, together with their ﬁnal
		correlation score in Table 1 (left)
	</Extractive Summary>
	<Extractive Summary> =
		, to associate each word in a pair (w1, w2) with
		its most appropriate senses in WordNet (in Table 1
		(right) we show the senses chosen by the annota-
		tors for the corresponding lexical combinations)
	</Extractive Summary>
</Paper ID=ument359>


<Paper ID=ument359> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: F1 scores (%) for English all-words ﬁne-grained WSD (left) and for multilingual all-words ﬁne-grained
		WSD (right)
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Experimental results
		English WSD
		As shown in Table 2 (left), Syn-
		tagNet enabled UKB to achieve the best results in
		the English all-words disambiguation tasks, attain-
		ing 4
	</Extractive Summary>
	<Extractive Summary> =
		Multilingual WSD
		As regards our multilingual
		evaluation, SyntagNet enabled UKB to attain the
		best overall result (see Table 2 (right)), which is a
		statistically-signiﬁcant improvement of 2
	</Extractive Summary>
</Paper ID=ument359>


<Paper ID=ument359> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: F1 scores (%) of UKB+SyntagNet against the
		best supervised systems for English all-words WSD
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 compares UKB + SyntagNet against the
		best supervised English WSD systems (Yuan et al
	</Extractive Summary>
</Paper ID=ument359>


<Paper ID=ument359> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: F1 scores (%) of UKB+SyntagNet against
		the best systems for multilingual all-words WSD
	</Abstractive Summary>
	<Extractive Summary> =
		With respect to the compari-
		son against the best systems (Table 4), SyntagNet
		provides a statistically-relevant boost of 4
	</Extractive Summary>
</Paper ID=ument359>


<Paper ID=ument36> <Table ID =1>
	<Abstractive Summary> =
		6K
		Table 1: Details of datasets used
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that in both the datasets, on an
		average, the number of train triples for each NP
		and RP is less than 2
	</Extractive Summary>
</Paper ID=ument36>


<Paper ID=ument36> <Table ID =2>
	<Abstractive Summary> =
		566
		Table 2: Link Prediction results
	</Abstractive Summary>
	<Extractive Summary> =
		Comparing
		the
		performance
		of
		CaRe(B=ConvE,CN=φ) in Table 3 with the
		performance of CaRe(B=ConvE) in Table 2,
		it can be noticed that a major part of the per-
		formance boost of CaRe can be attributed to
		parameterization of RP embeddings
	</Extractive Summary>
</Paper ID=ument36>


<Paper ID=ument36> <Table ID =3>
	<Abstractive Summary> =
		556
		Table 3: Impact of parameterizing RP embeddings
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 provides a quantitative anal-
		385
		(a) ReVerb45K
		(b) ReVerb20K
		Figure 4: Performance comparison of CaRe framework for differnt values of the CN (as described in Section 6
	</Extractive Summary>
	<Extractive Summary> =
		Comparing
		the
		performance
		of
		CaRe(B=ConvE,CN=φ) in Table 3 with the
		performance of CaRe(B=ConvE) in Table 2,
		it can be noticed that a major part of the per-
		formance boost of CaRe can be attributed to
		parameterization of RP embeddings
	</Extractive Summary>
</Paper ID=ument36>


<Paper ID=ument360> <Table ID =1>
	<Abstractive Summary> =
		84
		Table 1: Results (percentage F1 mean and standard deviation from ﬁve experiments)
	</Abstractive Summary>
	<Extractive Summary> =
		2
		5
		Results & Discussion
		In general, from Table 1, we can see that word-
		level meta-embeddings even without subword or
		character-level information, consistently perform
		better than ﬂat baselines (e
	</Extractive Summary>
	<Extractive Summary> =
		From Table 1, in Multilingual setting, which
		trains with the main languages, it is evident that
		adding both closely-related and distant language
		embeddings improves the performance
	</Extractive Summary>
</Paper ID=ument360>


<Paper ID=ument360> <Table ID =2>
	<Abstractive Summary> =
		99
		Table 2: Comparison to existing works
	</Abstractive Summary>
</Paper ID=ument360>


<Paper ID=ument361> <Table ID =1>
	<Abstractive Summary> =
		1k
		2
		Table 1: Summary of seven datasets used in our exper-
		iments
	</Abstractive Summary>
</Paper ID=ument361>


<Paper ID=ument361> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Experimental results of classiﬁcation accuracy
		for different methods with ﬁve datasets on sentiment
		analysis task
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		The experimental results on sentiment analysis
		are reported in Table 2, and the performances on
		question answering and natural language inference
		are summarized in Table 3
	</Extractive Summary>
</Paper ID=ument361>


<Paper ID=ument361> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Experimental results on question answering
		and natural language inference tasks
	</Abstractive Summary>
</Paper ID=ument361>


<Paper ID=ument362> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Results of the evaluation in low-resource settings with 1% of the original labeled training data averaged
		over six runs
	</Abstractive Summary>
</Paper ID=ument362>


<Paper ID=ument362> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Results of the automatic labeling method pro-
		posed by Dembowski et al
	</Abstractive Summary>
	<Extractive Summary> =
		As seen in Table 2, this method
		reaches rather high precision but has a poor re-
		call
	</Extractive Summary>
</Paper ID=ument362>


<Paper ID=ument363> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1: p@1 for low resource languages: Hebrew,
		Afrikaans, Occitan, Estonian, and Bosnian, trained
		with multilingual algorithms over triplets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 depicts precision@1 for the triplets task
	</Extractive Summary>
</Paper ID=ument363>


<Paper ID=ument363> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Multilingual word translation results for English, German, French, Spanish, Italian and Portuguese
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows precision@1 results
	</Extractive Summary>
</Paper ID=ument363>


<Paper ID=ument364> <Table ID =1>
	<Abstractive Summary> =
		Table 1: A few-shot ID training set for a conversa-
		tion service for teleconference management, with OOD
		testing examples
	</Abstractive Summary>
	<Extractive Summary> =
		For example, Table 1 shows some
		of the utterances a chat-bot builder provided for
		training
	</Extractive Summary>
	<Extractive Summary> =
		Finally, we replace the CNN en-
		coders with bidirectional LSTMs (the bottom of
		Table 1), which yields the same dimension of sen-
		tence representations as CNN
	</Extractive Summary>
</Paper ID=ument364>


<Paper ID=ument364> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: O-Proto is compared with other baselines for Conversation
		and Amazon data
		0
	</Abstractive Summary>
	<Extractive Summary> =
		in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Results: Table 2 compares the proposed model
		and baselines on EER and CER on the two
		datasets
	</Extractive Summary>
	<Extractive Summary> =
		Moreover, in Table 2 we show
		the ablation study by removing Lood and Lgt from
		4No CER reported for OSVM as it treats ID as one class
		and does not support ID classiﬁcation
	</Extractive Summary>
</Paper ID=ument364>


<Paper ID=ument364> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Compare our model with Prototype Network
		on EER when choosing various K-shot values
		O-Proto, respectively
	</Abstractive Summary>
	<Extractive Summary> =
		Improvement in different K-shot settings: On
		the Amazon data, we construct different K-shot
		tasks as meta-test (results shown in Table 3), and
		observe consistent improvements on EER
	</Extractive Summary>
</Paper ID=ument364>


<Paper ID=ument365> <Table ID =1>
	<Abstractive Summary> =
		git
		Original informal sentence:
		I LOVE HIP-HOP , RAP , ROCK & POP BUT MY FAV MUSIC IS R & B
		Output of a rule-based system:
		I love hip-hop , rap , rock and pop but my fav music is r and b
		Table 1: Example of informal sentence and the output
		of a rule-based system
	</Abstractive Summary>
	<Extractive Summary> =
		This is in fact not desired, since the
		rule-based system could make mistakes and intro-
		duce noise (Table 1)
	</Extractive Summary>
</Paper ID=ument365>


<Paper ID=ument365> <Table ID =2>
	<Abstractive Summary> =
		Train
		Dev
		Test
		Entertainment & Music
		52,595
		2,877
		1,416
		Family & Relationship
		51,967
		2,788
		1,332
		Table 2: Corpus statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the statistics of the training, devel-
		opment, and test sets
	</Extractive Summary>
</Paper ID=ument365>


<Paper ID=ument365> <Table ID =3>
	<Abstractive Summary> =
		07
		Table 3: Test performance on the E&M domain
	</Abstractive Summary>
</Paper ID=ument365>


<Paper ID=ument365> <Table ID =4>
	<Abstractive Summary> =
		44
		Table 4: Test performance on the F&R domain
	</Abstractive Summary>
</Paper ID=ument365>


<Paper ID=ument366> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Polarity sentiment transfer on YelpSent &
		AmaSent
		YelpSent
		Sent
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Quantitative Result
		Automatic Evaluation Results Table 1 and Ta-
		ble 2 show the automatic evaluation results
	</Extractive Summary>
</Paper ID=ument366>


<Paper ID=ument366> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Multiple attribute transfer on YelpTense &
		AmaProd
		YelpTense
		Sent
	</Abstractive Summary>
</Paper ID=ument366>


<Paper ID=ument366> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Human evaluation on YelpSent
		YelpSent
		Cont
	</Abstractive Summary>
	<Extractive Summary> =
		Human Evaluation Results Table 3 exhibits
		the human evaluation results on YelpSent
	</Extractive Summary>
</Paper ID=ument366>


<Paper ID=ument366> <Table ID =4>
	<Abstractive Summary> =
		(1: lowest and 5: highest)
		3583
		Table 4: Model ablation test on YelpSent
		Model
		Sent
	</Abstractive Summary>
	<Extractive Summary> =
		As the results
		shown at Table 4, removing the word-level condi-
		tion architecture decreases transfer accuracy and
		BLEU scores
	</Extractive Summary>
</Paper ID=ument366>


<Paper ID=ument366> <Table ID =5>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		com/dataset/challenge
		All the data statistics are presented in Table 5 in
		Appendix A
	</Extractive Summary>
</Paper ID=ument366>


<Paper ID=ument367> <Table ID =1>
	<Abstractive Summary> =
		17
		I-DARTS (n = 2)
		-
		-
		-
		Table 1: Perplexities on PTB (lower is better)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the perplexities of different RN-
		N models on PTB
	</Extractive Summary>
</Paper ID=ument367>


<Paper ID=ument367> <Table ID =2>
	<Abstractive Summary> =
		47
		Table 2: F1 scores on the CoNLL-2003 English NER
		test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows a comparison of different meth-
		ods
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2, We
		ﬁnd it interesting that Random RNNs are good for
		NER task
	</Extractive Summary>
</Paper ID=ument367>


<Paper ID=ument368> <Table ID =1>
	<Abstractive Summary> =
		9M
		1,892,241
		3,247
		19,323,513
		Table 1: Statistics of datasets used in experiments
	</Abstractive Summary>
	<Extractive Summary> =
		We present the comparison of the sizes of
		these datasets in Table 1, and further details could
		be found in Appendix A
	</Extractive Summary>
</Paper ID=ument368>


<Paper ID=ument368> <Table ID =2>
	<Abstractive Summary> =
		428
		Table 2:
		Performance on different datasets against
		baselines, where h@k denotes hits at k
	</Abstractive Summary>
	<Extractive Summary> =
		It could be seen in Table 2 that JoBi
		ComplEx outperforms both ComplEx and Dist-
		Mult on all three standard datasets, on all the met-
		rics we consider
	</Extractive Summary>
	<Extractive Summary> =
		Moreover, results in Table 2 demonstrate that
		JoBi improves performance on DistMult and Sim-
		plE
	</Extractive Summary>
</Paper ID=ument368>


<Paper ID=ument368> <Table ID =3>
	<Abstractive Summary> =
		550
		Table 3:
		Performance on the large-scale FB1
	</Abstractive Summary>
	<Extractive Summary> =
		9M (Table 3) demon-
		strate that JoBi improves performance on this very
		3This effect is not observed when using logistic-loss or
		max-margin loss
		4https://github
	</Extractive Summary>
</Paper ID=ument368>


<Paper ID=ument368> <Table ID =4>
	<Abstractive Summary> =
		# epochs
		training time
		ComplEx
		70
		5 days 5 hours 8 minutes
		JoBi ComplEx
		30
		4 days 19 minutes
		Table 4: Runtimes of ComplEx and JoBi Complex on
		FB1
	</Abstractive Summary>
</Paper ID=ument368>


<Paper ID=ument368> <Table ID =5>
	<Abstractive Summary> =
		591
		Table 5: Comparison with TypeComplex where the
		scores are calculated ranking only the tail entities
	</Abstractive Summary>
</Paper ID=ument368>


<Paper ID=ument368> <Table ID =6>
	<Abstractive Summary> =
		428
		Table 6: Results of ablation study on ComplEx model
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Negatives ratio
		Hits@10
		Baseline
		BiasedNeg
		Joint
		JoBi
		Figure 2: Performances on YAGO3-10 with different
		negative ratios
		In Table 6 it can be seen that Joint on its own
		gives a slight performance boost over the base-
		line, and BiasedNeg performs slightly under the
		baseline on all measures
	</Extractive Summary>
</Paper ID=ument368>


<Paper ID=ument369> <Table ID =1>
	<Abstractive Summary> =
		In this
		3601
		Table 1: Distance in Dimensionalities between Grid
		Search and Our Proposed Method (Numbers within the
		bucket size 50 are in bold)
		Datasets
		WordSim
		RW Stanford
		MTurk
		Text8
		34
		1
		65
		WikiText
		18
		10
		48
		Average
		26
		5
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 reports the
		distance in selected dimensionalities
	</Extractive Summary>
</Paper ID=ument369>


<Paper ID=ument369> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Performance (Correlation) Comparison be-
		tween Grid Search and Our Proposed Method
		Datasets Method
		WordSim
		RW Stanford
		MTurk
		Text8
		G
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we further
		2A larger interval will save researchers more time, but
		may result in a sub-optimal dimensionality
	</Extractive Summary>
</Paper ID=ument369>


<Paper ID=ument37> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Parameters setting for best results
		We ﬁne-tune our models by validating and se-
		lecting the best model parameters
	</Abstractive Summary>
</Paper ID=ument37>


<Paper ID=ument37> <Table ID =2>
	<Abstractive Summary> =
		315
		Table 2: Multilayer CNNs+ATT
		Thus, our SelfAtt design is expected to boost
		a sentence encoder with attention scores and al-
		leviate noise effects by beneﬁting from a multi-
		layer CNN network
	</Abstractive Summary>
	<Extractive Summary> =
		To investigate
		multilayer effects of DSRE, we present multilayer
		CNN with ATT results in Table 2, we observe
		that with ATT several multilayer models (e
	</Extractive Summary>
</Paper ID=ument37>


<Paper ID=ument37> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: P@N results for models with internal CNNs
		self-attention and curriculum learning
		NetMax could be used for testing
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		the P@N values with test settings where all sen-
		tences in an entity pair’s bag are taken into ac-
		count
	</Extractive Summary>
	<Extractive Summary> =
		We report
		a comparison of NetAtt/NetMax+SelfAtt+CCL,
		PCNN+ONE/ATT and state-of-the-art DSGAN
		noise reduction as shown in Table 3 and Figure 5
	</Extractive Summary>
	<Extractive Summary> =
		The P@N re-
		sults in Table 3 indicate that CCL further im-
		proves the model’s performance when compared
		to PCNN+ATT/ONE+SelfAtt as well
	</Extractive Summary>
</Paper ID=ument37>


<Paper ID=ument37> <Table ID =4>
	<Abstractive Summary> =
		380
		Table 4: Comparison of AUC Results
		From Figures 5(a) and 5(b),
		we can see
		that the CCL based models have further im-
		provements in terms of PR-curves compared
		with PCNN+ATT/ONE+SelfAtt
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		gives another comparison using AUC with all p-
		values less than 5e-02 from t-test evaluation
	</Extractive Summary>
</Paper ID=ument37>


<Paper ID=ument370> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Results on PTB test set for various baselines
	</Abstractive Summary>
	<Extractive Summary> =
		We show the results of these baselines trained
		on PTB in Table 1, where we ﬁnd that it is difﬁ-
		cult to balance language modeling (PPL) and rep-
		resentation learning (Recon and AU) – the systems
		with relatively good reconstruction (FBP, λ = 7)
		or higher AU (FB baselines) have suboptimal PPL,
		and the best PPL is achieved with sacriﬁce of Re-
		con and AU
	</Extractive Summary>
	<Extractive Summary> =
		C
		Additional Results of Language Modeling
		In Table 10, we provide a more detailed version of the language modeling results on PTB, SNLI and
		Yahoo
	</Extractive Summary>
	<Extractive Summary> =
		(2019)
		3610
		Table 10: Additional results of language modeling on PTB and SNLI
	</Extractive Summary>
	<Extractive Summary> =
		67
		3611
		Table 11: Additional results of language modeling on Yahoo
	</Extractive Summary>
	<Extractive Summary> =
		The results
		are shown in Table 12
	</Extractive Summary>
	<Extractive Summary> =
		The results are shown in Table 13
	</Extractive Summary>
	<Extractive Summary> =
		3613
		Table 12: Interpolation between prior samples on SNLI
	</Extractive Summary>
	<Extractive Summary> =
		3614
		Table 13: Interpolation between posterior samples on SNLI
	</Extractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =2>
	<Abstractive Summary> =
		90
		Table 2: Results on PTB test with encoder pretraining
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2 we show the results of explor-
		ing this hypothesis on PTB
	</Extractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Language modeling results on PTB and Ya-
		hoo test set
	</Abstractive Summary>
	<Extractive Summary> =
		9
		As shown in Table 3, our method with differ-
		ent target rates is able to consistently outperform
		all the baselines in terms of PPL
	</Extractive Summary>
	<Extractive Summary> =
		In addition
		to the metrics in Table 3, we also report NLL, Mutual Information Iq between z and x under qφ(z|x)
		(MI)13, and the perplexity computed by ELBO (ELBO PPL)
	</Extractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Reconstruction
		Method
		BLEU
		AE
		60
	</Abstractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =5>
	<Abstractive Summary> =
		62
		Table 5: Smoothness
		Method
		PCC
		AE
		0
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, our method achieves a much
		higher PCC compared to the baselines
	</Extractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Noisy reconstruction loss (↓) on SNLI
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6, while AE achieves the best
		reconstruction when the noise is small (k = 1),
		its reconstruction deteriorates dramatically when
		k > 1, which suggests AE fails to learn a smooth
		latent space
	</Extractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =7>
	<Abstractive Summary> =
		85
		Table 7: Interpolation between prior samples on SNLI
	</Abstractive Summary>
	<Extractive Summary> =
		We sample two
		latent codes z0 and z1 from the prior p(z) (Table 7)
		and do linear interpolation between the two with
		evenly divided intervals
	</Extractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =8>
	<Abstractive Summary> =
		Table 8: Accuracy on Yelp of unsupervised and super-
		vised classiﬁcation
	</Abstractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =9>
	<Abstractive Summary> =
		Table 9: The sizes of word embeddings and hidden states for PTB, SNLI and Yahoo
	</Abstractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =10>
	<Abstractive Summary> =
		(2019)
		3610
		Table 10: Additional results of language modeling on PTB and SNLI
	</Abstractive Summary>
	<Extractive Summary> =
		C
		Additional Results of Language Modeling
		In Table 10, we provide a more detailed version of the language modeling results on PTB, SNLI and
		Yahoo
	</Extractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =11>
	<Abstractive Summary> =
		67
		3611
		Table 11: Additional results of language modeling on Yahoo
	</Abstractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =12>
	<Abstractive Summary> =
		3613
		Table 12: Interpolation between prior samples on SNLI
	</Abstractive Summary>
</Paper ID=ument370>


<Paper ID=ument370> <Table ID =13>
	<Abstractive Summary> =
		3614
		Table 13: Interpolation between posterior samples on SNLI
	</Abstractive Summary>
</Paper ID=ument370>


<Paper ID=ument371> <Table ID =1>
	<Abstractive Summary> =
		27
		Table 1: Test performances of all BERT variants on all tasks and datasets
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 1 summarizes the experimental results
	</Extractive Summary>
</Paper ID=ument371>


<Paper ID=ument371> <Table ID =2>
	<Abstractive Summary> =
		64
		Table 2:
		Comparing SCIBERT with the reported
		BIOBERT results on biomedical datasets
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we compare SCIBERT results
		with reported BIOBERT results on the subset of
		datasets included in (Lee et al
	</Extractive Summary>
</Paper ID=ument371>


<Paper ID=ument372> <Table ID =1>
	<Abstractive Summary> =
		28315
		Table 1: Example format of the Reddit Jokes dataset
		ing and creating datasets like the Pun of the Day
		(Yang et al
	</Abstractive Summary>
	<Extractive Summary> =
		Some sample jokes are shown in Table 1, above
	</Extractive Summary>
</Paper ID=ument372>


<Paper ID=ument372> <Table ID =2>
	<Abstractive Summary> =
		663
		Table 2: Results of Accuracy on Reddit Jokes dataset
		the same model format as previously mentioned,
		trained on the Reddit dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		In Table 2, we see the results of our experiment
		with the Reddit dataset
	</Extractive Summary>
	<Extractive Summary> =
		We found that all of the models,
		including humans, relied more on the punchline
		of the joke in their predictions (Table 2)
	</Extractive Summary>
</Paper ID=ument372>


<Paper ID=ument372> <Table ID =3>
	<Abstractive Summary> =
		931
		Table 3: Comparison of Methods on Pun of the Day Dataset
	</Abstractive Summary>
	<Extractive Summary> =
		The results on the Pun of the Day dataset are
		shown in Table 3 above
	</Extractive Summary>
</Paper ID=ument372>


<Paper ID=ument372> <Table ID =4>
	<Abstractive Summary> =
		986
		Table 4: Results on Short Jokes Identiﬁcation
		ing
	</Abstractive Summary>
</Paper ID=ument372>


<Paper ID=ument373> <Table ID =1>
	<Abstractive Summary> =
		45
		Table 1: Training quality of multi-node training with gradient compression techniques, measured with BLEU
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes model’s performance in
		terms of BLEU
	</Extractive Summary>
</Paper ID=ument373>


<Paper ID=ument373> <Table ID =2>
	<Abstractive Summary> =
		2x
		Table 2: Speed performance of gradient dropping with local gradient update, compared to several baselines
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes our experiments
	</Extractive Summary>
</Paper ID=ument373>


<Paper ID=ument374> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Macro-averaged F1 comparison of per-language models and multilingual models over 48 languages
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Small and Practical Models
		The results in Table 1 make it clear that the BERT-
		based model for each task is a solid win over a
		Meta-LSTM model in both the per-language and
		multilingual settings
	</Extractive Summary>
	<Extractive Summary> =
		3
		Multilingual Models
		Multilingual Modeling Results
		We compare
		per-language models trained on single language
		treebanks with multilingual models in Table 1 and
		Table 4
	</Extractive Summary>
</Paper ID=ument374>


<Paper ID=ument374> <Table ID =2>
	<Abstractive Summary> =
		2M
		8M
		3
		BERT
		768
		120k
		87M
		12
		MiniBERT
		256
		120k
		2M
		3
		Table 2: The number of parameters of each model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the
		parameters of each model
	</Extractive Summary>
</Paper ID=ument374>


<Paper ID=ument374> <Table ID =3>
	<Abstractive Summary> =
		0x
		Table 3: Relative inference speedup over BERT
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 compares the model speeds
	</Extractive Summary>
</Paper ID=ument374>


<Paper ID=ument374> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Macro-averaged F1 comparison of multilin-
		gual models
	</Abstractive Summary>
</Paper ID=ument374>


<Paper ID=ument374> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: POS tagging and Morphology F1 for all models on low-resource languages
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the performance of
		the models
	</Extractive Summary>
</Paper ID=ument374>


<Paper ID=ument374> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6: F1 score on Hindi-English codemixed POS
		tagging task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows that all multilingual models han-
		dle codemixed data reasonably well without super-
		vised codemixed traininig data
	</Extractive Summary>
</Paper ID=ument374>


<Paper ID=ument375> <Table ID =1>
	<Abstractive Summary> =
		However, there are also some disadvantages to
		directly using dialogue acts as inputs:
		3639
		Triple
		Templates
		bye()
		goodbye
		bye
		request(addr)
		the address
		what’s the address
		inform(food=[food])
		[food]
		[food] food
		inform(hastv=true)
		television
		with a television
		Table 1: Examples of atomic templates in DSTC 2&3
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Atomic templates
		Table 1 gives some examples of atomic tem-
		plates used in DSTC 2&3 dataset
	</Extractive Summary>
</Paper ID=ument375>


<Paper ID=ument375> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: SLU performances of different systems on the
		DSTC3 evaluation set
	</Abstractive Summary>
</Paper ID=ument375>


<Paper ID=ument375> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: SLU performances on the DSTC3 evaluation
		set when removing different modules of our method
	</Abstractive Summary>
</Paper ID=ument375>


<Paper ID=ument375> <Table ID =4>
	<Abstractive Summary> =
		request(childrenallowed)
		does it allow children
		Does it have children?
		request(hastv)
		request(addr)
		does it has a television
		the address
		Does it have the television address and ad-
		dress?
		Table 4: Examples of generated data samples for DSTC3
	</Abstractive Summary>
</Paper ID=ument375>


<Paper ID=ument376> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: PTB language modeling perplexity (lower is
		better)
	</Abstractive Summary>
</Paper ID=ument376>


<Paper ID=ument376> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2:
		WikiText-2 language modeling perplexity
		(lower is better)
	</Abstractive Summary>
</Paper ID=ument376>


<Paper ID=ument376> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Unlabeled unsupervised parsing F1 on WSJ-
		40
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the F1 results
	</Extractive Summary>
</Paper ID=ument376>


<Paper ID=ument376> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Percentage of left and right splits
	</Abstractive Summary>
	<Extractive Summary> =
		16 Table 4
		summarizes the results on WSJ-40 test set
	</Extractive Summary>
</Paper ID=ument376>


<Paper ID=ument376> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5:
		The hyperparameters used in the PTB lan-
		guage modeling experiment
	</Abstractive Summary>
	<Extractive Summary> =
		We only differ from them by using smaller hidden
		size (and hence smaller dropout rate) to control for
		the amount of parameters in the PTB experiments,
		summarized in Table 5 For the WikiText-2 experi-
		ments, we use 200 rational RNN size and 400 di-
		mensional context vectors
	</Extractive Summary>
</Paper ID=ument376>


<Paper ID=ument377> <Table ID =1>
	<Abstractive Summary> =
		69
		Table 1: Comparisons between Spider and Chinese
		Spider datasets
	</Abstractive Summary>
</Paper ID=ument377>


<Paper ID=ument377> <Table ID =2>
	<Abstractive Summary> =
		contact staff id”;
		English Question
		What is the name and ID of the staff who recorded the fault log but has not
		contacted any visiting engineers?
		Translated Chinese Question
		那些记录了错误报告但没有联系任何到访工程师的职工的姓名和ID
		是什么？
		Table 2: Example questions corresponding to SQL
	</Abstractive Summary>
</Paper ID=ument377>


<Paper ID=ument377> <Table ID =3>
	<Abstractive Summary> =
		6%
		Table 3: Accuracy of Exact Matching on test set
	</Abstractive Summary>
</Paper ID=ument377>


<Paper ID=ument377> <Table ID =4>
	<Abstractive Summary> =
		1%
		Table 4: F1 scores of Component Matching on test set
	</Abstractive Summary>
</Paper ID=ument377>


<Paper ID=ument378> <Table ID =1>
	<Abstractive Summary> =
		4%
		Table 1: Test set accuracy of GLOBAL-GNN compared
		to prior work on SPIDER
	</Abstractive Summary>
	<Extractive Summary> =
		Results As shown in Table 1, the accuracy of our
		proposed model (GLOBAL-GNN) on the hidden
		test set is 47
	</Extractive Summary>
</Paper ID=ument378>


<Paper ID=ument378> <Table ID =2>
	<Abstractive Summary> =
		5%
		Table 2: Development set accuracy for various experi-
		ments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows accuracy results on
		the development set for different experiments
	</Extractive Summary>
</Paper ID=ument378>


<Paper ID=ument379> <Table ID =1>
	<Abstractive Summary> =
		2k
		Table 1: Dataset statistics on the CoNLL-2012 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the data
		statistics
	</Extractive Summary>
</Paper ID=ument379>


<Paper ID=ument379> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Main results under cross-domain settings, src (“source”, training set) → tgt (“target”, test set)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the F1 scores on each
		test set
	</Extractive Summary>
</Paper ID=ument379>


<Paper ID=ument379> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Performance comparison between LM ﬁne-
		tuning on target domain unlabeled data of the same size
		as each test set, “Controlled Unlabeled data (CU),” and
		transductive LM ﬁne-tuning on each test set (T)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the F1 scores
		averaged across all the target domains
	</Extractive Summary>
</Paper ID=ument379>


<Paper ID=ument379> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Performance comparison between LM ﬁne-
		tuning on target domain unlabeled data (U) and on the
		combination of the unlabeled data and test sets (U +
		T)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the
		F1 scores averaged across all the target domains
	</Extractive Summary>
</Paper ID=ument379>


<Paper ID=ument379> <Table ID =5>
	<Abstractive Summary> =
		5
		Table 5:
		Standard benchmark results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the F1 scores of our
		models and those of existing models
	</Extractive Summary>
</Paper ID=ument379>


<Paper ID=ument38> <Table ID =1>
	<Abstractive Summary> =
		91
		Table 1: Comparison with the state-of-the-art RE mod-
		els on the ACE05 English data (S: Single Model; E:
		Ensemble Model)
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1 we compare our models with the best
		models in (Gormley et al
	</Extractive Summary>
</Paper ID=ument38>


<Paper ID=ument38> <Table ID =2>
	<Abstractive Summary> =
		In-House
		Training
		Dev
		Test
		English (Source)
		1137
		140
		140
		German (Target)
		280
		35
		35
		Spanish (Target)
		451
		55
		55
		Italian (Target)
		322
		40
		40
		Japanese (Target)
		396
		50
		50
		Portuguese (Target)
		390
		50
		50
		ACE05
		Training
		Dev
		Test
		English (Source)
		479
		60
		60
		Arabic (Target)
		323
		40
		40
		Chinese (Target)
		507
		63
		63
		Table 2: Number of documents in the training/dev/test
		sets of the in-house and ACE05 datasets
	</Abstractive Summary>
</Paper ID=ument38>


<Paper ID=ument38> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Performance of the supervised English RE
		models on the in-house and ACE05 English test data
	</Abstractive Summary>
</Paper ID=ument38>


<Paper ID=ument38> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Comparison of the performance (F1 score) using different mappings on the target-language development
		data under the Bi-LSTM model
	</Abstractive Summary>
</Paper ID=ument38>


<Paper ID=ument38> <Table ID =5>
	<Abstractive Summary> =
		6
		Table 5: Performance of the cross-lingual RE approach on the in-house target-language test data
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Performance on Test Data
		The cross-lingual RE model transfer results for the
		in-house test data are summarized in Table 5 and
		the results for the ACE05 test data are summa-
		rized in Table 6, using the regular mapping learned
		1https://github
	</Extractive Summary>
</Paper ID=ument38>


<Paper ID=ument38> <Table ID =6>
	<Abstractive Summary> =
		1
		Table 6: Performance of the cross-lingual RE approach
		on the ACE05 target-language test data
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Performance on Test Data
		The cross-lingual RE model transfer results for the
		in-house test data are summarized in Table 5 and
		the results for the ACE05 test data are summa-
		rized in Table 6, using the regular mapping learned
		1https://github
	</Extractive Summary>
</Paper ID=ument38>


<Paper ID=ument380> <Table ID =1>
	<Abstractive Summary> =
		com/facebookresearch/SentEval
		Task
		Description
		SentLen
		Length prediction
		WC
		Word Content analysis
		TreeDepth Tree depth prediction
		TopConst
		Top Constituents prediction
		BShift
		Word order analysis
		Tense
		Verb tense prediction
		SubjNum
		Subject number prediction
		ObjNum
		Object number prediction
		SOMO
		Semantic odd man out
		CoordInv
		Coordination Inversion
		Table 1: Probing Tasks
		DCT sentence vectors by concatenating the ﬁrst
		K DCT coefﬁcients, which we denote by c0:K
	</Abstractive Summary>
	<Extractive Summary> =
		It contains a set of 10 classiﬁcation
		tasks, summarized in Table 1, that address va-
		rieties of linguistic properties including surface,
		syntactic, and semantic information (Conneau
		et al
	</Extractive Summary>
</Paper ID=ument380>


<Paper ID=ument380> <Table ID =2>
	<Abstractive Summary> =
		63
		Table 2: Probing tasks performance of vector averaging (AVG) and max pooling (MAX) vs
	</Abstractive Summary>
</Paper ID=ument380>


<Paper ID=ument380> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: DCT embedding Performance in SentEval downstream tasks compared to vector averaging (AVG) and
		max pooling (MAX)
	</Abstractive Summary>
	<Extractive Summary> =
		Our results in Table 3 are consistent with
		these observations, where we see improvements
		in most tasks, but the difference is not as signiﬁ-
		cant as the probing tasks, except in TREC question
		classiﬁcation where increasing K leads to much
		better performance
	</Extractive Summary>
</Paper ID=ument380>


<Paper ID=ument380> <Table ID =4>
	<Abstractive Summary> =
		54
		Table 4: Performance in text classiﬁcation (20-NG, R-8) and sentiment (SST-5) tasks of various models as reported
		in (Kayal and Tsatsaronis, 2019), where DCT* refers to the implementation in (Kayal and Tsatsaronis, 2019)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the best results for
		the various models as reported in Kayal and Tsat-
		saronis (2019), in addition to the best performance
		of our model denoted as ck
	</Extractive Summary>
</Paper ID=ument380>


<Paper ID=ument381> <Table ID =1>
	<Abstractive Summary> =
		90
		Table 1: Event detection performance on the CG task
		2013 test dataset
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results and Analyses
		Table 1 shows the event detection performance of
		the models on the test set
	</Extractive Summary>
</Paper ID=ument381>


<Paper ID=ument381> <Table ID =2>
	<Abstractive Summary> =
		02
		Table 2: Event detection performance on CG task 2013
		development set for SBNN with the top three perform-
		ing k-values and when k = 1
	</Abstractive Summary>
</Paper ID=ument381>


<Paper ID=ument381> <Table ID =3>
	<Abstractive Summary> =
		01pp higher than our
		Model
		Number of Classiﬁcations
		Running Time (s)
		TEES
		6,141
		155
		SBNN k = 8
		4,093
		131
		Table 3: Comparison on computational efﬁciency on
		the CG task 2013 development dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the number of classiﬁcations (or
		action scoring function calls in our model) per-
		formed by each model with the corresponding ac-
		tual running time
	</Extractive Summary>
</Paper ID=ument381>


<Paper ID=ument381> <Table ID =4>
	<Abstractive Summary> =
		36
		Table 4: Nested and overlapping event detection F1 (%)
		score performance on the CG task 2013 development
		set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the performance comparison of
		the models on nested, overlapping and ﬂat event
		detection
	</Extractive Summary>
</Paper ID=ument381>


<Paper ID=ument381> <Table ID =5>
	<Abstractive Summary> =
		001
		Table 5: Hyper-parameters used in our experiments
	</Abstractive Summary>
</Paper ID=ument381>


<Paper ID=ument382> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of human translated pairs for each of the six languages
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		gives example translated pairs in each language
	</Extractive Summary>
	<Extractive Summary> =
		An entity can be
		translated differently, such as Slovak and Slowake
		(Table 1) and models need to capture that these
		refer to the same entity
	</Extractive Summary>
</Paper ID=ument382>


<Paper ID=ument382> <Table ID =2>
	<Abstractive Summary> =
		com/translate/
		fr
		es
		de
		zh
		ja
		ko
		dev
		1,992
		1,962
		1,932
		1,984
		1,980
		1,965
		test
		1,985
		1,999
		1,967
		1,975
		1,946
		1,972
		Table 2: Examples translated per language
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the ﬁnal counts trans-
		lated to each language
	</Extractive Summary>
</Paper ID=ument382>


<Paper ID=ument382> <Table ID =3>
	<Abstractive Summary> =
		Third,
		BOW
		ESIM
		BERT
		Non-local context
		×
		✓
		✓
		Word interaction
		×
		×
		✓
		Translate Train
		✓
		✓
		✓
		Translate Test
		✓
		✓
		✓
		Zero Shot
		×
		×
		✓
		Merged
		×
		×
		✓
		Table 3: Complexity of each evaluated model and the
		training/evaluation strategies being tested
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 summarizes the models with respect to
		whether they represent non-local contexts or sup-
		port cross-sentential word interaction, plus which
		strategies are evaluated for each model
	</Extractive Summary>
</Paper ID=ument382>


<Paper ID=ument382> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Accuracy (%) and AUC-PR (%) of each approach
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 4 shows the performance of all
		methods and languages
	</Extractive Summary>
	<Extractive Summary> =
		Training/Evaluation Strategies: As Table 4
		and 5 show, the Zero Shot strategy yields the low-
		est performance compared to other strategies on
		BERT
	</Extractive Summary>
</Paper ID=ument382>


<Paper ID=ument382> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Average Accuracy (%) and AUC-PR (%) over
		the six languages
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 summarizes the
		average results for the six non-English languages
	</Extractive Summary>
</Paper ID=ument382>


<Paper ID=ument382> <Table ID =6>
	<Abstractive Summary> =
		7
		Table 6: The count of examples by number of lan-
		guages (of 7) that agree with the gold label in test set
	</Abstractive Summary>
</Paper ID=ument382>


<Paper ID=ument383> <Table ID =1>
	<Abstractive Summary> =
		2K
		21K
		# sents
		225K
		21K
		15K
		601K
		Table 1: Statistics of the evaluation datasets
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Tasks and Datasets
		This section describes our tasks and datasets,
		and any model changes that are task-speciﬁc (see
		Table 1 for comparison of evaluation datasets)
	</Extractive Summary>
</Paper ID=ument383>


<Paper ID=ument383> <Table ID =2>
	<Abstractive Summary> =
		03
		Table 2: Characteristics of our CSABSTRUCT dataset
		sentences, this task can be viewed as SSC, classi-
		fying each sentence as a good summary sentence
		or not
	</Abstractive Summary>
</Paper ID=ument383>


<Paper ID=ument383> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Abstract sentence classiﬁcation (micro F1)
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		Table 3 summarizes results for abstract sentence
		classiﬁcation
	</Extractive Summary>
</Paper ID=ument383>


<Paper ID=ument383> <Table ID =4>
	<Abstractive Summary> =
		314
		Table 4: Results on CSPUBSUM
		the [CLS] token to encode individual sentences
		as described in Devlin et al
	</Abstractive Summary>
</Paper ID=ument383>


<Paper ID=ument385> <Table ID =1>
	<Abstractive Summary> =
		59
		2,136
		Table 1: Basic statistics of the length of the examples in
		the corpus
	</Abstractive Summary>
</Paper ID=ument385>


<Paper ID=ument385> <Table ID =2>
	<Abstractive Summary> =
		2% labeled as conde-
		3713
		Positive
		Negative
		Balanced (1:1)
		3,255
		3,255
		Imbalanced (1:20)
		3,255
		65,100
		Table 2: Basic statistics of our dataset
	</Abstractive Summary>
</Paper ID=ument385>


<Paper ID=ument385> <Table ID =3>
	<Abstractive Summary> =
		333
		Table 3: Performance (macro-F1) for predicting con-
		descension on balanced and imbalanced versions of
		TALKDOWN
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Predicting Condescension
		Table 3 summarizes the results of our core exper-
		iments
	</Extractive Summary>
</Paper ID=ument385>


<Paper ID=ument385> <Table ID =4>
	<Abstractive Summary> =
		597
		Table 4:
		The impact of different train-set posi-
		tive:negative ratios
	</Abstractive Summary>
</Paper ID=ument385>


<Paper ID=ument385> <Table ID =5>
	<Abstractive Summary> =
		166
		Table 5: Subreddit experiment statistics
	</Abstractive Summary>
	<Extractive Summary> =
		com/huggingface/
		pytorch-transformers/
		C
		Subreddit Condescension Rates
		Table 5 shows basic statistics for all the subreddits
		we analyzed
	</Extractive Summary>
</Paper ID=ument385>


<Paper ID=ument386> <Table ID =1>
	<Abstractive Summary> =
		4)
		Table 1: The WIKICITE and DUC 2005 and 2006
		dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Two-Step Abstractive Model
		Since the reference documents in the WIKICITE
		dataset are rather long (see Table 1), we chose
		to use a two-step approach to build an abstractive
		system for the summary cloze task
	</Extractive Summary>
</Paper ID=ument386>


<Paper ID=ument386> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: The ROUGE F1 scores for the human clozes
		with and without access to the topic and context and
		crowdsourced quality judgments (on a 1-5 scale) of
		how well the cloze continues the corresponding text
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Human Performance & Evaluation
		Table 2 shows the ROUGE and quality scores for
		the human-written clozes with and without access
		to the topic and context
	</Extractive Summary>
</Paper ID=ument386>


<Paper ID=ument386> <Table ID =3>
	<Abstractive Summary> =
		83
		Table 3: The ROUGE F1 results of the baseline models
		that do not have access to the references or context
	</Abstractive Summary>
</Paper ID=ument386>


<Paper ID=ument386> <Table ID =4>
	<Abstractive Summary> =
		05
		Table 4: The ROUGE F1 scores for the extractive mod-
		els, all of which are signiﬁcantly lower than the oracle
		model, indicating there is room for improvement
	</Abstractive Summary>
</Paper ID=ument386>


<Paper ID=ument386> <Table ID =5>
	<Abstractive Summary> =
		09
		Table 5:
		An evaluation using the recall variant of
		ROUGE of the different extractive preprocessing steps
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 also contains an ablation study for the
		SUMFOCUS in which the topic did make a signiﬁ-
		cant difference
	</Extractive Summary>
</Paper ID=ument386>


<Paper ID=ument386> <Table ID =6>
	<Abstractive Summary> =
		77
		Table 6: The ROUGE F1 and perplexity results for ab-
		stractive models with and without the context (+/–C)
		with heuristic labels, lead, and CONTENTSELECTOR
		extractive preprocessing steps
	</Abstractive Summary>
</Paper ID=ument386>


<Paper ID=ument387> <Table ID =1>
	<Abstractive Summary> =
		31
		Table 1: Comparison of summarization datasets: size of training, validation, and test sets and average document
		and summary length (in terms of words and sentences)
	</Abstractive Summary>
	<Extractive Summary> =
		Aside from various statistics on the three
		datasets, Table 1 also reports the proportion of
		novel bi-grams in gold summaries as a measure
		of their abstractiveness
	</Extractive Summary>
	<Extractive Summary> =
		Recall that summaries in this dataset are
		highly abstractive (see Table 1) consisting of a sin-
		gle sentence conveying the gist of the document
	</Extractive Summary>
</Paper ID=ument387>


<Paper ID=ument387> <Table ID =2>
	<Abstractive Summary> =
		18
		Table 2: ROUGE F1 results on CNN/DailyMail test
		set (R1 and R2 are shorthands for unigram and bigram
		overlap; RL is the longest common subsequence)
	</Abstractive Summary>
	<Extractive Summary> =
		The third block in Table 2 highlights the per-
		formance of several abstractive models on the
		CNN/DailyMail dataset (see Section 2
	</Extractive Summary>
</Paper ID=ument387>


<Paper ID=ument387> <Table ID =3>
	<Abstractive Summary> =
		55
		Table 3: ROUGE Recall results on NYT test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents results on the NYT dataset
	</Extractive Summary>
</Paper ID=ument387>


<Paper ID=ument387> <Table ID =4>
	<Abstractive Summary> =
		27
		Table 4:
		ROUGE F1 results on the XSum test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 summarizes our results on the XSum
		dataset
	</Extractive Summary>
	<Extractive Summary> =
		block in Table 4 presents the results of various ab-
		stractive models taken from Narayan et al
	</Extractive Summary>
</Paper ID=ument387>


<Paper ID=ument387> <Table ID =5>
	<Abstractive Summary> =
		88
		Table 5:
		Model perplexity (CNN/DailyMail; valida-
		tion set) under different combinations of encoder and
		decoder learning rates
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 5 we examine whether
		the combination of different learning rates (˜lrE
		and ˜lrD) is indeed beneﬁcial
	</Extractive Summary>
</Paper ID=ument387>


<Paper ID=ument387> <Table ID =6>
	<Abstractive Summary> =
		9
		Table 6: QA-based evaluation
	</Abstractive Summary>
</Paper ID=ument387>


<Paper ID=ument387> <Table ID =7>
	<Abstractive Summary> =
		19
		Table 7:
		QA-based and ranking-based evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		05),
		with the exception of TCONVS2S (see Table 7;
		XSum) in the QA evaluation setting
	</Extractive Summary>
</Paper ID=ument387>


<Paper ID=ument388> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example of patternized summary genera-
		tion
	</Abstractive Summary>
</Paper ID=ument388>


<Paper ID=ument388> <Table ID =2>
	<Abstractive Summary> =
		(27)
		3746
		Acronym Gloss
		PESG-FC
		PESG w/o Fact Checker
		PESG-PR
		PESG w/o Prototype Reader
		PESG-SS
		PESG w/o Summary Pattern
		PESG-FG
		PESG w/o FGRU
		Table 2: Ablation models for comparison
	</Abstractive Summary>
</Paper ID=ument388>


<Paper ID=ument388> <Table ID =3>
	<Abstractive Summary> =
		9
		Proto
		–
		–
		–
		Table 3: ROUGE scores comparison with baselines
	</Abstractive Summary>
</Paper ID=ument388>


<Paper ID=ument388> <Table ID =4>
	<Abstractive Summary> =
		73▲
		Table 4: Fluency and consistency comparison by hu-
		man evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 lists the average
		scores of each model, showing that PESG outper-
		forms the other baseline models in both ﬂuency
		and consistency
	</Extractive Summary>
</Paper ID=ument388>


<Paper ID=ument388> <Table ID =5>
	<Abstractive Summary> =
		5
		Table 5: ROUGE scores
		of
		different
		ablation
		models of PESG
	</Abstractive Summary>
</Paper ID=ument388>


<Paper ID=ument388> <Table ID =6>
	<Abstractive Summary> =
		)
		Table 6: Examples of the generated natural answers by
		PESG and other models
	</Abstractive Summary>
	<Extractive Summary> =
		We also show a case study in Table 6, which in-
		cludes the input document and reference summary
		with the generated summaries
	</Extractive Summary>
</Paper ID=ument388>


<Paper ID=ument388> <Table ID =7>
	<Abstractive Summary> =
		)
		Table 7: Examples of the generated natural answers by
		PESG and other models
	</Abstractive Summary>
</Paper ID=ument388>


<Paper ID=ument389> <Table ID =1>
	<Abstractive Summary> =
		60
		Table 1: Averaged ROUGE scores on the DUC-2004
		dataset
		of-domain summarization
	</Abstractive Summary>
	<Extractive Summary> =
		It yielded two methods with unique per-
		formance beneﬁts (Table 1, 2, 3) and practical ad-
		vantages (table 5)
	</Extractive Summary>
</Paper ID=ument389>


<Paper ID=ument389> <Table ID =2>
	<Abstractive Summary> =
		96
		Table 2: Averaged ROUGE scores on the DUC-2003
		dataset
		mark sets were lower than baselines, possibly due
		to a lack of ﬂuency in the outputs of the extrac-
		tive approach used
	</Abstractive Summary>
</Paper ID=ument389>


<Paper ID=ument389> <Table ID =3>
	<Abstractive Summary> =
		11
		43 %
		27%
		30%
		Table 3: Human evaluation on 100 CNN test sentences (pairwise comparison of model outputs)
	</Abstractive Summary>
	<Extractive Summary> =
		BottleSumSelf and BottleSumEx both show
		reliably stronger performance compared to mod-
		els from related work (ABS and SEQ3 in Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		ABS
		can be quite effective, but requires learning on a
		large supervised training set (as demonstrated by
		its poor out-of-domain performance in Table 3)
	</Extractive Summary>
</Paper ID=ument389>


<Paper ID=ument389> <Table ID =4>
	<Abstractive Summary> =
		4
		64
		Table 4: Abstractiveness and compression of CNN
		summaries
	</Abstractive Summary>
	<Extractive Summary> =
		While BottleSumSelf seems superior to ReconEx
		other than in conciseness (in accordance with
		their compression ratios in Table 4), BottleSumEx
		appears roughly comparable to
		ReconEx and
		slightly inferior to BottleSumSelf
	</Extractive Summary>
	<Extractive Summary> =
		Another difference from BottleSumEx is the abil-
		ity of BottleSumSelf to be abstractivene (Table 4)
	</Extractive Summary>
</Paper ID=ument389>


<Paper ID=ument389> <Table ID =5>
	<Abstractive Summary> =
		BottleSumSelf Pre-trained LMs
		(ﬁned-tuned
		on
		data
		from
		BottleSumEx)
		Large scale,
		un-
		supervised source
		sentences
		with
		next sentences
		source sentence
		Large scale unsupervised (no summaries) data
		available, with next-sentences and/or no next-
		sentences available for sentences to summarize
		Table 5: Comparison of sentence summarization methods
	</Abstractive Summary>
</Paper ID=ument389>


<Paper ID=ument39> <Table ID =1>
	<Abstractive Summary> =
		23
		Table 1: Dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, for en-
		417
		Test Mode
		SINGLE
		MULTIPLE
		ONE
		TWO
		ALL
		Metric
		P@0
	</Extractive Summary>
</Paper ID=ument39>


<Paper ID=ument39> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Parameter settings in REDS2
	</Abstractive Summary>
</Paper ID=ument39>


<Paper ID=ument39> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Comparison on Precision@recall and AUC
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Overall Evaluation Results
		Evaluation results on all target entity pairs in test-
		ing set are shown in Figure 3 and Table 3, from
		which we make the following observations:
		(1) Figure 3 shows all models obtain a reason-
		able precision when recall is smaller than 0
	</Extractive Summary>
	<Extractive Summary> =
		(2) As shown in both Figure 3 and Table 3,
		REDS2 achieves the best results among all the
		models
	</Extractive Summary>
	<Extractive Summary> =
		Surprisingly, the
		relation extraction result is even better than the re-
		sult on the NYT test data in Table 3, with an over-
		all AUC of 54
	</Extractive Summary>
</Paper ID=ument39>


<Paper ID=ument39> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Comparison on Precision@recall and AUC under different testing settings, detailed in Section 4
	</Abstractive Summary>
	<Extractive Summary> =
		Results are shown in
		Table 4, from which we can see that REDS2 and
		BASE+MERGE have 25
	</Extractive Summary>
</Paper ID=ument39>


<Paper ID=ument39> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: Effect of the table expanded sentence bag size
		|ST | on Precision@recall and AUC
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 5, we show the perfor-
		mance of REDS2 with different numbers of sen-
		tences sampled from ST
	</Extractive Summary>
</Paper ID=ument39>


<Paper ID=ument39> <Table ID =6>
	<Abstractive Summary> =
		Table 6: An example for case study, where the sen-
		tence with the highest attention weight is selected re-
		spectively from 1-hop and 2-hop sentence bag
	</Abstractive Summary>
</Paper ID=ument39>


<Paper ID=ument390> <Table ID =1>
	<Abstractive Summary> =
		37
		Table 1: ROUGE score on CNN/Dailymail
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1,
		2 and 3 lists the results for CNN/Dailymail, Giga-
		word and XSum respectively
	</Extractive Summary>
</Paper ID=ument390>


<Paper ID=ument390> <Table ID =2>
	<Abstractive Summary> =
		41
		Table 2: ROUGE score on Gigaword
	</Abstractive Summary>
</Paper ID=ument390>


<Paper ID=ument390> <Table ID =3>
	<Abstractive Summary> =
		28
		Table 3: ROUGE score on XSum
	</Abstractive Summary>
</Paper ID=ument390>


<Paper ID=ument390> <Table ID =4>
	<Abstractive Summary> =
		92
		Table 4: Proportion of novel n-grams (NN-1,2,3) and sentences (NN-S) on generated summaries
	</Abstractive Summary>
</Paper ID=ument390>


<Paper ID=ument390> <Table ID =5>
	<Abstractive Summary> =
		96
		Table 5: Human evaluation results on DUC 2004
	</Abstractive Summary>
</Paper ID=ument390>


<Paper ID=ument390> <Table ID =6>
	<Abstractive Summary> =
		628)
		Table 6: Word Alignment Precision on DUC 2004
	</Abstractive Summary>
</Paper ID=ument390>


<Paper ID=ument391> <Table ID =1>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument391>


<Paper ID=ument391> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Results on WIKISQL
	</Abstractive Summary>
</Paper ID=ument391>


<Paper ID=ument391> <Table ID =3>
	<Abstractive Summary> =
		7 (5)
		Table 3: Results of ensembled models on the test set;
		ensemble sizes are shown within parentheses
	</Abstractive Summary>
</Paper ID=ument391>


<Paper ID=ument391> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Proportion of errors on the development set in
		WIKITABLEQUESTIONS
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the proportion of errors attested
		by the two attention models
	</Extractive Summary>
</Paper ID=ument391>


<Paper ID=ument391> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Hyperparameters for WTQ (WIKITABLE-
		QUESTIONS) and WIKISQL
	</Abstractive Summary>
</Paper ID=ument391>


<Paper ID=ument392> <Table ID =1>
	<Abstractive Summary> =
		0
		12200
		DM
		11000
		UCCA
		10000
		Table 1: Hyperparameter settings
		3793
		6
		Experiments
		6
	</Abstractive Summary>
</Paper ID=ument392>


<Paper ID=ument392> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: SMATCH F1 on AMR 2
	</Abstractive Summary>
	<Extractive Summary> =
		AMR Table 2 compares our neural transducer to
		the previous best results (SMATCH F1, Cai and
		Knight, 2013) on AMR test sets
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2, we also conduct ablation study on
		beam search to investigate contributions from the
		model architecture itself and the beam search algo-
		rithm
	</Extractive Summary>
</Paper ID=ument392>


<Paper ID=ument392> <Table ID =3>
	<Abstractive Summary> =
		86
		83
		78
		79
		Wikiﬁcation
		76
		80
		86
		86
		Negation
		58
		67
		75
		77
		SRL
		70
		72
		70
		71
		Table 3: Fine-grained F1 scores on the AMR 2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 summarizes the parser performance on
		each subtask using Damonte et al
	</Extractive Summary>
</Paper ID=ument392>


<Paper ID=ument392> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: Labeled F1 (%) scores on the English DM in-
		domain (WSJ) and out-of-domain (Brown corpus) test
		sets
	</Abstractive Summary>
	<Extractive Summary> =
		DM Table 4 compares our neural transducer to the
		state of the art (labeled F1) on the English DM in-
		domain (ID) and out-of-domain (OOD) data
	</Extractive Summary>
</Paper ID=ument392>


<Paper ID=ument392> <Table ID =5>
	<Abstractive Summary> =
		1
		Table 5: Labeled F1 (%) scores for all edges including
		primary edges and remote edges
	</Abstractive Summary>
	<Extractive Summary> =
		UCCA Table 5 compares our results to the pre-
		vious best published results (labeled F1 for all
		edges) on the English Wiki test set
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows that these node labels do improve
		the parsing performance from 75
	</Extractive Summary>
</Paper ID=ument392>


<Paper ID=ument392> <Table ID =6>
	<Abstractive Summary> =
		(2019)
		617
		Ours
		1076
		Table 6: Parsing speed on the AMR 2
	</Abstractive Summary>
</Paper ID=ument392>


<Paper ID=ument393> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Comparison with state-of-the-art methods (results on the test set)
	</Abstractive Summary>
</Paper ID=ument393>


<Paper ID=ument393> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: The effect of different sibling orders
	</Abstractive Summary>
	<Extractive Summary> =
		As seen in Table 2, the deterministic order strat-
		egy for training (relation freq
	</Extractive Summary>
</Paper ID=ument393>


<Paper ID=ument394> <Table ID =1>
	<Abstractive Summary> =
		”
		Table 1: Logical operators frequency within the total number of examples
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 examines this, where here-
		after we use the original training and development
		sets of GEOQUERY and SCHOLAR as Dnat
	</Extractive Summary>
</Paper ID=ument394>


<Paper ID=ument394> <Table ID =2>
	<Abstractive Summary> =
		68
		Table 2: Logical form coverage of Dnat by Don, after
		converting logical forms to their templates, for different
		values of D
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the number of examples gener-
		ated by OVERNIGHT for different values of D,
		along with the proportion of examples in Dnat
		whose logical form template is covered by Don
	</Extractive Summary>
	<Extractive Summary> =
		Because we
		3815
		Algorithm 1 GRANNO
		Input: unlabeled utterances Xul, grammar G
		Output: Dga - training data for a semantic parser
		1: generate Con from G
		2: Dga ← ∅, s0(·) ← −WMD(·), t ← 0
		3: converge ← False, Spos ← ∅, Sneg ← ∅
		4: while not converge do
		5:
		for x in Xul do
		6:
		calculate CK
		x using st(·)
		▷ top K candidates
		7:
		crowd workers annotate cx given x and CK
		x
		8:
		if cx ̸= N/A then
		9:
		Dga ← Dga ∪ (x, cx)
		10:
		Xul ← Xul \ x
		11:
		Spos ← Spos ∪ (x, cx)
		▷ positive examples
		12:
		for c in CM
		x \ {cx} do
		13:
		Sneg ← Sneg ∪ (x, c) ▷ negative examples
		14:
		converge ← check for convergence, t ← t + 1
		15:
		train st(·) over Spos and Sneg
		16: return Dga
		do not paraphrase all the canonical utterances, we
		can generate to a higher depth D compared to
		OVERNIGHT and cover more of the examples in
		Xul (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		We generate all canonical utterances up to depth
		D= 6, resulting in roughly 350K canonical utter-
		ances in SCHOLAR and GEOQUERY, and cover-
		age of 90% of the examples in Xul (Table 2)
	</Extractive Summary>
</Paper ID=ument394>


<Paper ID=ument394> <Table ID =3>
	<Abstractive Summary> =
		24
		Table 3: Denotation accuracy for the set of examples in
		Dnat covered by Don (acccov), and for the set of uncov-
		ered examples (accdisj)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows that accdisj is substantially lower
		Dataset
		acccov
		accdisj
		GEOQUERY
		78
	</Extractive Summary>
</Paper ID=ument394>


<Paper ID=ument394> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Denotation accuracy on the test set, comparing
		a semantic parser trained on Dnat (SUPERVISED) and
		parser trained on Dlang (OVERNIGHT-ORACLE-LF)
	</Abstractive Summary>
	<Extractive Summary> =
		Results in Table 4
		show that for each domain, a decrease of approxi-
		mately 9 points in accuracy occurs only due to the
		language mismatch, even with high-quality work-
		ers
	</Extractive Summary>
</Paper ID=ument394>


<Paper ID=ument394> <Table ID =5>
	<Abstractive Summary> =
		5
		Table 5: Denotation accuracy on the test set
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Main Results
		Table 5 shows the denotation ac-
		curacy for all experiments, when training from:
		Dnat (SUPERVISED); Don (OVERNIGHT); Dlang, as
		described in Section 3
	</Extractive Summary>
</Paper ID=ument394>


<Paper ID=ument394> <Table ID =6>
	<Abstractive Summary> =
		31
		“what is the lowest elevation in california?”
		“place that is low point of california”
		“elevation of low point of california”
		Table 6: Examples of false positive detections by crowd workers for different cases: under speciﬁcation (us), over
		speciﬁcation (os), wrong (w) and partially wrong (pw)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 presents examples for these cases
		and their fraction within all false positives for iter-
		ation t = 0, where workers cover unlabeled utter-
		ances faster than the oracle
	</Extractive Summary>
</Paper ID=ument394>


<Paper ID=ument395> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Parameter Settings
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows all the parameters used in our
		experiment
	</Extractive Summary>
</Paper ID=ument395>


<Paper ID=ument395> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Evaluation results P@N of different models using different number of sentences in bags on NYT-FB60K
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		P@N Evaluation on NYT-FB60K
		Dataset
		As shown in Table 2, we report Precision@N
		of different neural network based approaches on
		NYT-FB60K dataset
	</Extractive Summary>
</Paper ID=ument395>


<Paper ID=ument395> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Evaluation results P@N of variant models on
		NYT-FB60K dataset
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, without label embedding,
		the performance of RELE w/o LE drops signiﬁ-
		cantly (more than 10%)
	</Extractive Summary>
</Paper ID=ument395>


<Paper ID=ument396> <Table ID =1>
	<Abstractive Summary> =
		09
		Table 1: Main results on Weibo NER
		where Gk ∈ RF ′×(n+m), k ∈ {1, 2, 3}
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the results on Wei-
		bo NER
	</Extractive Summary>
</Paper ID=ument396>


<Paper ID=ument396> <Table ID =2>
	<Abstractive Summary> =
		79
		Table 2: Main results on OntoNotes
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results on
		OntoNotes
	</Extractive Summary>
</Paper ID=ument396>


<Paper ID=ument396> <Table ID =3>
	<Abstractive Summary> =
		47
		Table 3: Main results on MSRA
		4
	</Abstractive Summary>
</Paper ID=ument396>


<Paper ID=ument396> <Table ID =4>
	<Abstractive Summary> =
		2
		23
		×10
		Table 4: The performance of models in training and
		testing time
	</Abstractive Summary>
</Paper ID=ument396>


<Paper ID=ument396> <Table ID =5>
	<Abstractive Summary> =
		05
		Table 5: Ablation study on reducing word-character in-
		teractive graphs, For example, ”w/o C” means remov-
		ing word-character containing graph from the complete
		model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the re-
		sults of ablation experiments
	</Extractive Summary>
</Paper ID=ument396>


<Paper ID=ument396> <Table ID =6>
	<Abstractive Summary> =
		org/
		3837
		Case1
		Sentence
		&//@•‰5PÑ€'f:#é‰&
		&// @Xidian University:#good morning&
		Case2
		Sentence
		~¯TóT�Ñw5���å&
		Tencent and Lenovo jointly launched a computer cleaning day&
		Matched
		lexical word
		& •‰5PÑ€'f(XidianUniversity)�•‰(Xi�an)�
		5PÑ€'f(UESTC)�‰5(An Dian)�5PÑ€(Electronics Technology),
		é‰(Good Morning)&
		Matched
		lexical word
		~¯(Tencent), Tó(Lenovo), T�(Joint),
		Ñw(Launch), 5�(Computer), ��(Clean)&
		Sentence with
		gold label
		&// (O)@(O)•(B-ORG) ‰(I-ORG) 5(I-ORG) P(I-ORG)
		Ñ(I-ORG)€(I-ORG) '(I-ORG)f(I-ORG):(O)#(O) é(O)‰(O) &
		Sentence with
		gold label
		~(B-ORG)¯(I-ORG)T(B-ORG) ó(I-ORG) T(O)�(O)
		Ñ(O)w(O)5(O)�(O)�(O)�(O)å(O)&
		w/o C-graph
		predicted label
		&// (O)@(O) •(B-LOC) ‰(I-LOC) 5(B-ORG) P(I-ORG)
		Ñ(I-ORG)€(I-ORG) '(I-ORG) f(I-ORG):(O)#(O)é(O)‰(O) &
		w/o T-graph
		predicted label
		~(B-ORG)¯(I-ORG)T(O)ó(O)T(O)�(O)
		Ñ(O) w(O)5(O)�(O)�(O)�(O)å(O)&
		with C-graph
		predicted label
		&// (O)@(O)•(B-ORG) ‰(I-ORG) 5(I-ORG) P(I-ORG)
		Ñ(I-ORG)€(I-ORG) '(I-ORG)f(I-ORG):(O)#(O)é(O)‰(O)&
		with T-graph
		predicted label
		~(B-ORG)¯(I-ORG)T(B-ORG) ó(I-ORG) T(O) �(O)
		Ñ(O) w(O)5(O)�(O)�(O) �(O)å(O)&
		Table 6: Case study
	</Abstractive Summary>
</Paper ID=ument396>


<Paper ID=ument397> <Table ID =1>
	<Abstractive Summary> =
		Dataset
		Distantly Supervised
		Human-annotated
		KBP
		NYT
		TACRED
		#Relation Types
		7
		25
		42
		#Train Sentences
		23,784
		235,982
		37,311
		#Test Sentences
		289
		395
		6,277
		Table 1: Statistics of Datasets Used in Our Study
	</Abstractive Summary>
</Paper ID=ument397>


<Paper ID=ument397> <Table ID =2>
	<Abstractive Summary> =
		82
		-
		Table 2: Performance Comparison of RE Models
	</Abstractive Summary>
	<Extractive Summary> =
		mance of all tested models in Table 2 and summa-
		rize three popular models in Figure 2
	</Extractive Summary>
</Paper ID=ument397>


<Paper ID=ument397> <Table ID =3>
	<Abstractive Summary> =
		70
		Table 3: F1 score of RE Models with Threshold and Bias Adaptation
	</Abstractive Summary>
</Paper ID=ument397>


<Paper ID=ument398> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Example entity pairs and their candidate relations
	</Abstractive Summary>
	<Extractive Summary> =
		, rk : ck), as illustrated in Table 1, where
		1 ≥ ci ≥ cj ≥ 0 for any i > j ∈ [1, k] and
		�k
		i=1 ci = 1
	</Extractive Summary>
</Paper ID=ument398>


<Paper ID=ument398> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Redundancy statistics of DBpedia - ( see Section 4 for details about the datasets)
	</Abstractive Summary>
	<Extractive Summary> =
		As revealed by Table 2,
		candidates beyond top-3 are very unreliable
	</Extractive Summary>
</Paper ID=ument398>


<Paper ID=ument398> <Table ID =3>
	<Abstractive Summary> =
		55
		-
		-
		Table 3: Efﬁciency of eFIRE and comparison methods
	</Abstractive Summary>
	<Extractive Summary> =
		In the ﬁrst set
		of tests, we evaluated the effectiveness and efﬁ-
		ciency, and the results are reported in Figures 3(a),
		3(b) and Table 3, respectively
	</Extractive Summary>
</Paper ID=ument398>


<Paper ID=ument399> <Table ID =1>
	<Abstractive Summary> =
		Speciﬁcally, the hid-
		3865
		Interaction Function
		g(hi, hpi)
		Self connection
		hi
		Concatenation
		hi
		� hpi
		Addition
		hi + hpi
		MLP
		ReLU
		�
		W1hi+W2hpi
		�
		Table 1: List of interaction functions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the
		list of interaction function considered in our ex-
		periments
	</Extractive Summary>
	<Extractive Summary> =
		4
		Table 10: Performance of entities with different lengths
		on the four datasets: OntoNotes (English), OntoNotes
		Chinese, Catalan and Spanish
	</Extractive Summary>
	<Extractive Summary> =
		2
		Entity with Different Lengths
		Table 10 shows the performance comparison with
		different entity lengths on all datasets
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument399> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		The last two columns of Table 2 show the
		relationships between the dependency trees and
		named entities with length larger than 2 for the
		complete dataset
	</Extractive Summary>
	<Extractive Summary> =
		The last column
		in Table 2 shows the percentage of the grandchild
		4http://cemantix
	</Extractive Summary>
	<Extractive Summary> =
		The rea-
		son could be the relatively lower ratio of ST(%)13
		as shown in Table 2, which means some of the en-
		tities do not form subtrees under the complete de-
		pendency trees
	</Extractive Summary>
	<Extractive Summary> =
		As we pointed out in the dataset statistics
		(Table 2), the number of entities that form subtrees
		in OntoNotes Chinese is relatively smaller com-
		pared to other datasets
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument399> <Table ID =3>
	<Abstractive Summary> =
		79
		Table 3: Performance comparison on the OntoNotes
		5
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Main Results
		OntoNotes English
		Table 3 shows the perfor-
		mance comparison between our work and previ-
		ous work on the OntoNotes English dataset
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument399> <Table ID =4>
	<Abstractive Summary> =
		58
		Table 4: Performance comparison on the OntoNotes
		5
	</Abstractive Summary>
	<Extractive Summary> =
		OntoNotes Chinese
		Table 4 shows the perfor-
		mance comparison on the Chinese datasets
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument399> <Table ID =5>
	<Abstractive Summary> =
		66
		Table 5: Results on the SemEval-2010 Task 1 datasets
	</Abstractive Summary>
	<Extractive Summary> =
		SemEval-2010
		Table 5 shows the results of our
		models on the SemEval-2010 Task 1 datasets
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument399> <Table ID =6>
	<Abstractive Summary> =
		4
		Table 6: Performance on the CoNLL-2003 English
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Additional Experiments
		CoNLL-2003 English
		Table 6 shows the perfor-
		mance on the CoNLL-2003 English dataset
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument399> <Table ID =7>
	<Abstractive Summary> =
		30
		Table 7:
		Low-resource NER performance on the
		SemEval-2010 Task 1 datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the NER performance on
		the SemEval-2010 Task 1 datasets under the low-
		resource setting
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument399> <Table ID =8>
	<Abstractive Summary> =
		56
		Table 8:
		F1 performance of DGLSTM-CRF with
		predicted dependencies against the best performing
		BiLSTM-CRF
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows the perfor-
		mance (LAS) of the dependency parser on four
		languages (i
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument399> <Table ID =9>
	<Abstractive Summary> =
		46
		Table 9: Ablation study of the DGLSTM-CRF model
		on the OntoNotes English dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Ablation Study
		Table 9 shows the ablation
		study of the 2-layer DGLSTM-CRF model on the
		OntoNotes English dataset
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument399> <Table ID =10>
	<Abstractive Summary> =
		4
		Table 10: Performance of entities with different lengths
		on the four datasets: OntoNotes (English), OntoNotes
		Chinese, Catalan and Spanish
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Entity with Different Lengths
		Table 10 shows the performance comparison with
		different entity lengths on all datasets
	</Extractive Summary>
</Paper ID=ument399>


<Paper ID=ument4> <Table ID =1>
	<Abstractive Summary> =
		40
		Table 1: Average accuracies and Macro-F1 scores over ﬁve runs with random initialization along with their stan-
		dard deviations
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Main Results
		Table 1 compares these baselines to three XR con-
		ditions
	</Extractive Summary>
</Paper ID=ument4>


<Paper ID=ument4> <Table ID =2>
	<Abstractive Summary> =
		05
		Table 2: BERT pre-training: average accuracies and Macro-F1 scores from ﬁve runs and their stdev
	</Abstractive Summary>
</Paper ID=ument4>


<Paper ID=ument40> <Table ID =1>
	<Abstractive Summary> =
		In prac-
		tice, we observe descriptive entity representations
		give better performance, which presumably is be-
		424
		CAP
		CERP
		EFP
		ET
		KORE
		WikiSRS
		ERT
		Rare
		CoNLL
		same
		next
		Rel
		Sim
		#train
		3982
		3982
		10000
		10000
		1998
		N/A
		N/A
		N/A
		3130
		10000
		18538
		#valid
		3806
		3828
		2000
		2000
		1998
		N/A
		N/A
		N/A
		6260
		4000
		4790
		#test
		3938
		3850
		2000
		2000
		1998
		20 × 20
		688
		688
		6260
		4000
		4481
		#classes
		2
		2
		2
		10331
		N/A
		N/A
		N/A
		626
		4
		up to 30
		Table 1: Statistics of datasets used in EntEval tasks
	</Abstractive Summary>
</Paper ID=ument40>


<Paper ID=ument40> <Table ID =2>
	<Abstractive Summary> =
		1
		Ford Motor Company
		Table 2: An example from KORE
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows an example
		from KORE
	</Extractive Summary>
</Paper ID=ument40>


<Paper ID=ument40> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Performances of entity representations on EntEval tasks
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 3 shows the performance of our models on
		the EntEval tasks
	</Extractive Summary>
</Paper ID=ument40>


<Paper ID=ument40> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Accuracies (%) in comparing the use of de-
		scription encoder (Des
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 4 show that encod-
		ing the entity names by themselves already cap-
		tures a great deal of knowledge regarding entities,
		especially for CoNLL-YAGO
	</Extractive Summary>
</Paper ID=ument40>


<Paper ID=ument40> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: Accuracies (%) on CoNLL-YAGO with static
		or non-static entity representations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 reports the performance of different
		descriptive entity representations on the CoNLL-
		YAGO task
	</Extractive Summary>
</Paper ID=ument40>


<Paper ID=ument400> <Table ID =1>
	<Abstractive Summary> =
		13
		10
		45
		224
		Table 1: The size of the datasets and the characteristics
		of the length of documents in the datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		summarizes the characteristics of both the bilin-
		gual and evaluation datasets
	</Extractive Summary>
</Paper ID=ument400>


<Paper ID=ument400> <Table ID =2>
	<Abstractive Summary> =
		71
		Table 2: AUC performance for our approach in comparison
		to the task-speciﬁc models directly trained on the evaluation
		datasets (results are comparable to supervised state-of-the-
		art)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 reports the cross-validation
		AUC comparing our approach to the model trained
		on task-speciﬁc data
	</Extractive Summary>
</Paper ID=ument400>


<Paper ID=ument400> <Table ID =3>
	<Abstractive Summary> =
		19
		Table 3: AUC performance for MT translation method in
		comparison to NN translation method
	</Abstractive Summary>
	<Extractive Summary> =
		Effect of Translation Component
		Table 3 con-
		siders the effect of the translation component of
		the compound transformation described in Section
		3
	</Extractive Summary>
</Paper ID=ument400>


<Paper ID=ument400> <Table ID =4>
	<Abstractive Summary> =
		13
		Table 4: AUC performance for various representation meth-
		ods
	</Abstractive Summary>
	<Extractive Summary> =
		Effect of Document Representation
		Table 4
		considers the effect of the document representa-
		tion component of the transformation discussed in
		Section 3
	</Extractive Summary>
</Paper ID=ument400>


<Paper ID=ument400> <Table ID =5>
	<Abstractive Summary> =
		Most Discriminating Words
		To further demon-
		strate qualitatively the formality discriminating in-
		3881
		German (Translated)
		American English
		long
		original
		love
		cute
		high
		broken
		one
		another
		well
		material
		loved
		going
		good
		handy
		excellent
		lovely
		even
		narrow
		amazing
		overpowering
		cut
		time
		perfect
		wanted
		quality
		clear
		wearing
		wonderful
		cheap
		ok
		favorite
		incredible
		chic
		ﬂowery
		like
		overwhelming
		hand
		fake
		comfortable
		back
		alternative
		optimal
		new
		tiny
		woody
		waterproof
		terrible
		glad
		Table 5: Most discriminative unigrams between German
		and American English according to their wc(wi, Lj) scores,
		based on the eBay Ger-Am product reviews dataset
	</Abstractive Summary>
	<Extractive Summary> =
		This no-
		tion is made concrete using the relative contri-
		bution of word wi to the Kullback-Leibler diver-
		gence (Berger and Lafferty, 2017) between the
		languages, applied to all words, separately for
		each language:
		wc (wi, Lj) = P (wi | Lj) log
		�
		P(wi|Lj)
		P(wi|L(1−j))
		�
		Table 5 presents the unigrams that achieve the
		highest values for the expression above as com-
		puted on the eBay bilingual dataset of German
		and American English reviews
	</Extractive Summary>
</Paper ID=ument400>


<Paper ID=ument401> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Dataset statistics
		5
	</Abstractive Summary>
</Paper ID=ument401>


<Paper ID=ument401> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: IMDB test set results
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Test set results
		Figure 8 and Table 2 show the IMDB test-set re-
		sults of our best rationale-biased methods and their
		non-rationale counterparts (chosen based on dev
		set performance), as well as all of the baselines
	</Extractive Summary>
</Paper ID=ument401>


<Paper ID=ument401> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: ASRS test set results
	</Abstractive Summary>
	<Extractive Summary> =
		Finally, Figure 9 and Table 3 show results on the
		ASRS test set
	</Extractive Summary>
</Paper ID=ument401>


<Paper ID=ument402> <Table ID =1>
	<Abstractive Summary> =
		Length
		Train
		Test
		SWDA
		Dialog Act
		42
		20,000
		7
		193,000
		5,000
		MRDA
		Dialog Act
		6
		12,000
		8
		78,000
		15,000
		ATIS
		Intent Prediction
		21
		950
		11
		4,478
		893
		SNIPS
		Intent Prediction
		7
		11,241
		10
		13,084
		700
		AG
		News Categorization
		4
		156,000
		38
		120,000
		7,600
		Y!A
		Yahoo! Answers Categorization
		10
		1,554,607
		108
		1,400,000
		60,000
		AMZN
		Product Review Prediction
		5
		1,919,336
		92
		3,000,000
		650,000
		Table 1: Text Classiﬁcation Tasks and Dataset Characteristics
		3
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Dataset Characteristics
		Table 1 shows the datasets characteristics such as
		the type of the task, number of classes, vocabu-
		lary size, average text length, train and test sizes
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, the datasets are very diverse,
		some have few thousand training examples, while
		others have millions
	</Extractive Summary>
</Paper ID=ument402>


<Paper ID=ument402> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Short Text Classiﬁcation On-device Results & Comparisons to Prior Work
		optimizer (Kingma and Ba, 2014) over shufﬂed
		mini-batches of size 100
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2
		ProSeqo reaches +5
	</Extractive Summary>
	<Extractive Summary> =
		We run ex-
		periments on ATIS and SNIPS intent prediction
		tasks and reported results in Table 2 in italic to
		indicate that this is a re-implementation of (Ravi
		and Kozareva, 2018) and previously these on-
		device results were not reported
	</Extractive Summary>
	<Extractive Summary> =
		As shown in
		Table 2, SGNN consistently performs well on
		dialog act and intent prediction tasks
	</Extractive Summary>
	<Extractive Summary> =
		Taking these
		differences in consideration, we show in Table 2
		results from non-on-device models with the ob-
		jective to highlight the power of on-device models
		and their capability to produce small memory foot-
		print models, which are competitive in accuracy
		to those using pre-trained embeddings and uncon-
		strained resources
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, prior work focused on
		developing the best approach for a speciﬁc task
		and dataset, resulting in not having a single model
		across all tasks
	</Extractive Summary>
</Paper ID=ument402>


<Paper ID=ument402> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: Long Text Classiﬁcation On-device Results & Comparisons to Prior Work
		ties and their semantic categories in the text, and
		the intent prediction model uses the slot informa-
		tion
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results on
		three tasks and datasets (AG, Y!A, AMZN)
	</Extractive Summary>
	<Extractive Summary> =
		Similarly to the short
		text section, we reported SGNN results in Table 3
		in italic to indicate that this is a re-implementation
		of (Ravi and Kozareva, 2018) and previously re-
		sults on these datasets and tasks were not reported
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 3, SGNN performed well
		on short texts, but it did not reach high perfor-
		mance on long documents
	</Extractive Summary>
</Paper ID=ument402>


<Paper ID=ument402> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Effect of ProSeqo’s Recurrent Projection Size
		on Accuracy for AG News Classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the performance of ProSeqo
		on the AG News classiﬁcation task and indicates
		that even if we reduce the projection size by 3x,
		we still achieve 90% accuracy with a small drop
		in performance of 1
	</Extractive Summary>
</Paper ID=ument402>


<Paper ID=ument403> <Table ID =1>
	<Abstractive Summary> =
		Training Set
		Testing Set
		Class Num
		159
		57
		Data Num
		195,775
		2,279
		Data Num/Class
		≥ 77
		20 ∼ 77
		Table 1: Details of ODIC
		Model
		Mean Acc
		Matching Networks (Vinyals et al
	</Abstractive Summary>
</Paper ID=ument403>


<Paper ID=ument403> <Table ID =2>
	<Abstractive Summary> =
		63
		Table 2: Comparison of mean accuracy (%) on ARSC
		Open Domain Intent Classiﬁcation for Dialog
		System (ODIC)
		We create this dataset by fetch-
		ing the log data on a real-world conversational
		platform
	</Abstractive Summary>
	<Extractive Summary> =
		We can see that the best performance is
		achieved when we used 3 iterations, correspond-
		ing to the best result reported in Table 2 (more
		rounds of iterations did not further improve the
		performance), and the table shows the effective-
		ness of the routing component
	</Extractive Summary>
</Paper ID=ument403>


<Paper ID=ument403> <Table ID =3>
	<Abstractive Summary> =
		08
		Table 3: Comparison of mean accuracy (%) on ODIC
		nation of temporal convolutions and soft at-
		tention (Mishra et al
	</Abstractive Summary>
	<Extractive Summary> =
		The dif-
		ference between Induction Networks and SNAIL
		shown in Table 3 is statistically signiﬁcant un-
		der the paired at the 99% signiﬁcance level
	</Extractive Summary>
</Paper ID=ument403>


<Paper ID=ument403> <Table ID =4>
	<Abstractive Summary> =
		15
		Table 4: Ablation study of Induction Networks on
		ARSC dataset
		addition, the performance difference between our
		model and other baselines in the 10-shot scenario
		is more signiﬁcant than in the 5-shot scenario
	</Abstractive Summary>
</Paper ID=ument403>


<Paper ID=ument404> <Table ID =1>
	<Abstractive Summary> =
		200
		150
		200
		30
		100
		100
		780
		sum
		2,300 3,100 2,500
		600 2,700 1,200
		300
		300 1,100 2,000 16,000
		Table 1: Emotion test in 0SHOT-TC
		emotions
		sad
		joy anger disgust
		fear surp
	</Abstractive Summary>
	<Extractive Summary> =
		We ﬁrst directly list the ﬁxed test and
		dev in Table 1 and Table 2, respectively
	</Extractive Summary>
</Paper ID=ument404>


<Paper ID=ument404> <Table ID =2>
	<Abstractive Summary> =
		100
		100
		100
		20
		100
		50
		470
		sum
		1,300 1,600
		800
		300 1,600
		500
		100
		100 400 1,000 7,700
		Table 2: Emotion dev in 0SHOT-TC
		model’s over-ﬁtting on one of them
	</Abstractive Summary>
	<Extractive Summary> =
		We ﬁrst directly list the ﬁxed test and
		dev in Table 1 and Table 2, respectively
	</Extractive Summary>
</Paper ID=ument404>


<Paper ID=ument404> <Table ID =3>
	<Abstractive Summary> =
		none
		total size
		327
		278
		445 412
		492
		659 1,046
		810
		80 348
		983 1,868
		split
		test
		190
		166
		271 260
		289
		396
		611
		472
		51 204
		590 1,144
		dev
		137
		112
		174 152
		203
		263
		435
		338
		29 144
		393
		724
		train-v0
		327
		–
		445
		–
		492
		– 1,046
		–
		80
		–
		983
		–
		train-v1
		–
		278
		– 412
		–
		659
		–
		810
		– 348
		–
		–
		Table 3: Situation train, dev and test split for 0SHOT-TC
	</Abstractive Summary>
	<Extractive Summary> =
		One reason is that these labels mostly are
		common words – systems can more easily match
		them to the text; the other reason is that they are
		situation classes with higher frequencies (refer to
		Table 3) – this is reasonable based on our common
		knowledge about disasters
	</Extractive Summary>
</Paper ID=ument404>


<Paper ID=ument404> <Table ID =4>
	<Abstractive Summary> =
		The people there
		need ?
		“?”= shelter
		“?” = a structure that provides privacy
		and protection from danger
		Table 4: Example hypotheses we created for modeling different aspects of 0SHOT-TC
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 lists some examples for the three aspects:
		“topic”, “emotion” and “situation”
	</Extractive Summary>
	<Extractive Summary> =
		As Table 4 shows,
		one is to use the label name to complete the inter-
		pretation, the other is to use the label’s deﬁnition
		in WordNet to complete the interpretation
	</Extractive Summary>
	<Extractive Summary> =
		3
		How do the generated hypotheses
		inﬂuence
		In Table 4, we listed examples for converting class
		names into hypotheses
	</Extractive Summary>
</Paper ID=ument404>


<Paper ID=ument404> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Label-partially-unseen evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		An interesting phenomenon, comparing the
		label-partially-unseen results in Table 5 and the
		label-fully-unseen results in Table 6, is that the
		pretrained entailment models work in this or-
		der for label-fully-unseen case: RTE > FEVER
		>MNLI; on the contrary, if we ﬁne-tune them on
		the label-partially-unseen case, the MNLI-based
		model performs best
	</Extractive Summary>
</Paper ID=ument404>


<Paper ID=ument404> <Table ID =6>
	<Abstractive Summary> =
		9
		Table 6: Label-fully-unseen evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		An interesting phenomenon, comparing the
		label-partially-unseen results in Table 5 and the
		label-fully-unseen results in Table 6, is that the
		pretrained entailment models work in this or-
		der for label-fully-unseen case: RTE > FEVER
		>MNLI; on the contrary, if we ﬁne-tune them on
		the label-partially-unseen case, the MNLI-based
		model performs best
	</Extractive Summary>
</Paper ID=ument404>


<Paper ID=ument404> <Table ID =7>
	<Abstractive Summary> =
		9
		Table 7: Fine-grained label-fully-unseen performances of different hypothesis generation approaches “word”,
		“def” (deﬁnition) and “comb” (word&deﬁnition) on the three tasks (“topic”, “emotion” and “situation”) based
		on three pretrained entailment models (RTE, FEVER, MNLI) and the ensemble approach (ens
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 lists the ﬁne-grained
		performance of three ways of generating hypothe-
		ses: “word”, “deﬁnition”, and “combination” (i
	</Extractive Summary>
</Paper ID=ument404>


<Paper ID=ument405> <Table ID =1>
	<Abstractive Summary> =
		We can
		enumerate all such valid labels as the conjunction:
		∀(P, H, Z) ∈ D,
		(E (P, H) ∧ E (H, Z) → E (P, Z))
		∧ (E (P, H) ∧ C (H, Z) → C (P, Z))
		∧ (N (P, H) ∧ E (H, Z) → ¬C (P, Z))
		∧ (N (P, H) ∧ C (H, Z) → ¬E (P, Z))
		(6)
		3927
		Name
		Boolean Logic
		Product
		Gödel
		Łukasiewicz
		Negation
		¬A
		1 − a
		1 − a
		1 − a
		T-norm
		A ∧ B
		ab
		min (a, b)
		max (0, a + b − 1)
		T-conorm
		A ∨ B
		a + b − ab
		max(a, b)
		min (1, a + b)
		Residuum
		A → B
		min
		�
		1, b
		a
		�
		�
		1, if b ≥ a,
		b, else
		min (1, 1 − a + b)
		Table 1: Mapping discrete statements to differentiable functions using t-norms
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes this mapping for three t-
		norms: product, Gödel, and Łukasiewicz
	</Extractive Summary>
	<Extractive Summary> =
		We
		can write the universal quantiﬁer in (4) as a con-
		junction to get:
		�
		(P,H),Y ⋆∈D
		⊤ → Y ⋆(P, H)
		(7)
		Using the product t-norm from Table 1, we get the
		learning objective of maximizing the probability
		of the true labels:
		�
		(P,H),Y ⋆∈D
		y⋆
		(P,H)
		(8)
		Or equivalently, by transforming to the negative
		log space, we get the annotation loss:
		Lann =
		�
		(P,H),Y ⋆∈D
		− log y⋆
		(P,H)
	</Extractive Summary>
</Paper ID=ument405>


<Paper ID=ument405> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Inconsistencies (%) of models on our 100k evaluation dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Inconsistency of Neural Models
		In Table 2, we report the impact of the amount of
		annotated data on symmetry/transitivity consisten-
		cies by using different percentages of labeled ex-
		amples
	</Extractive Summary>
</Paper ID=ument405>


<Paper ID=ument405> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Impact of symmetry/transitivity consistencies on test set accuracies
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Interaction of Losses
		In Table 3, we show the impact of symmetry
		and transitivity consistency on test accuracy
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, we
		see that lower symmetry/transitivity inconsistency
		generally does not reduce test accuracy, but we
		do not observe substantial improvement either
	</Extractive Summary>
</Paper ID=ument405>


<Paper ID=ument405> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Coverage (%) of unlabeled training sentences
		during the ﬁrst epoch of training
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Coverage of Unlabeled Dataset
		Table 4 shows the coverage of the three unlabeled
		datasets during the ﬁrst training epoch
	</Extractive Summary>
</Paper ID=ument405>


<Paper ID=ument405> <Table ID =5>
	<Abstractive Summary> =
		Clearly, the number of
		3931
		BERT
		w/ M,U,T
		(H, P)
		(H, P)
		E
		C
		N
		E
		C
		N
		(P, H)
		E
		4649
		1491 14708 2036
		29
		9580
		C
		1508 10712
		6459
		33 4025
		627
		N 14609
		6633 39231 9632
		613 73425
		Table 5:
		Distribution of predictions on the 100k
		evaluation
		data
		using
		BERT
		trained
		on
		100%
		SNLI+MultiNLI data with random seed 1
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Distribution of Predictions
		In Table 5, we present the distribution of model
		predictions on the 100k evaluation example pairs
		for symmetry consistency
	</Extractive Summary>
</Paper ID=ument405>


<Paper ID=ument405> <Table ID =6>
	<Abstractive Summary> =
		Model
		Example
		E
		C
		N
		BERT
		(P, H)
		20848 18679 60473
		(H, Z)
		20919 18768 60313
		(P, Z)
		20779 18721 60500
		w/ M,U,T
		(P, H)
		11645
		4685
		83670
		(H, Z)
		11671
		4703
		83626
		(P, Z)
		11585
		4597
		83818
		Table 6: Distribution of predictions on the 100k eval-
		uation example triples
	</Abstractive Summary>
</Paper ID=ument405>


<Paper ID=ument405> <Table ID =7>
	<Abstractive Summary> =
		8
		Table 7: Individual transitivity inconsistency (%) on
		the 100k evaluation example triples
	</Abstractive Summary>
	<Extractive Summary> =
		Further, in Table 7,
		we show the error rates of each individual transi-
		tivity consistencies
	</Extractive Summary>
</Paper ID=ument405>


<Paper ID=ument405> <Table ID =8>
	<Abstractive Summary> =
		2
		Table 8: Symmetry/Transitivity inconsistencies (%) for models using 1%, 5%, 20%, and 100% training data
	</Abstractive Summary>
</Paper ID=ument405>


<Paper ID=ument405> <Table ID =9>
	<Abstractive Summary> =
		Data
		1%
		5%
		20%
		100%
		M
		1
		1
		1
		1
		U
		10−5 10−4 10−3
		10−1
		T
		10−6 10−5 10−4
		10−3
		Table 9: Choice of λ‘s for different consistency and
		corresponding unlabeled datasets
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 9,
		we see that the λ‘s for U and T grows exponen-
		tially with the size of annotated examples
	</Extractive Summary>
</Paper ID=ument405>


<Paper ID=ument407> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Statistics of datasets for language modeling
	</Abstractive Summary>
</Paper ID=ument407>


<Paper ID=ument407> <Table ID =2>
	<Abstractive Summary> =
		0
		32
		Table 2: Language modeling on three datasets
	</Abstractive Summary>
</Paper ID=ument407>


<Paper ID=ument407> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Total training time in hours: absolute time and
		relative time versus VAE
	</Abstractive Summary>
</Paper ID=ument407>


<Paper ID=ument407> <Table ID =4>
	<Abstractive Summary> =
		9
		and to continue to make prices
		t =1
		and they plan to buy more today
		Table 4: Interpolating latent representation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows
		the generated examples
	</Extractive Summary>
</Paper ID=ument407>


<Paper ID=ument407> <Table ID =5>
	<Abstractive Summary> =
		5)-VAE
		939
		4078
		SA-VAE
		341
		10651
		iVAE
		116
		1520
		iVAEMI
		134
		1137
		Table 5: Forward and reverse PPL on PTB
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, implicit LVMs outperform
		others in both PPLs, which conﬁrms that the im-
		plicit representation can lead to better decoders
	</Extractive Summary>
</Paper ID=ument407>


<Paper ID=ument407> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Sentiment transfer on Yelp
	</Abstractive Summary>
	<Extractive Summary> =
		3953
		Table 6 presents some examples
	</Extractive Summary>
</Paper ID=ument407>


<Paper ID=ument407> <Table ID =7>
	<Abstractive Summary> =
		9
		Table 7: Sentiment Transfer on Yelp
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 7, iVAEMI outperforms
		ARAE in metrics except Acc, showing that iVAEMI
		captures informative representations, generates
		consistently opposite sentences with similar gram-
		matical structure and reserved semantic meaning
	</Extractive Summary>
</Paper ID=ument407>


<Paper ID=ument407> <Table ID =8>
	<Abstractive Summary> =
		868
		Table 8: Dialog response generation on two datasets
	</Abstractive Summary>
</Paper ID=ument407>


<Paper ID=ument408> <Table ID =1>
	<Abstractive Summary> =
		6253
		Table 1: Evaluation results on SemEval 2007 Task 14
	</Abstractive Summary>
</Paper ID=ument408>


<Paper ID=ument408> <Table ID =2>
	<Abstractive Summary> =
		Distance
		Euclidean↓
		√
		∑C
		i=1(pi − qi)2
		Sørensen↓
		∑C
		i=1 ∣pi−qi∣
		∑C
		i=1(pi+qi)
		Squaredχ2↓
		∑C
		i=1
		(pi−qi)2
		pi+qi
		K-L↓
		∑C
		i=1 pi log pi
		qi
		Similarly
		Fidelity↑
		∑C
		i=1
		√piqi
		Intersection↑
		∑C
		i=1 min(pi, qi)
		Table 2: Metrics for evaluating the performance of
		emotion distribution learning
	</Abstractive Summary>
</Paper ID=ument408>


<Paper ID=ument408> <Table ID =3>
	<Abstractive Summary> =
		6253
		Table 3: Comparison results of different embedding methods
	</Abstractive Summary>
</Paper ID=ument408>


<Paper ID=ument408> <Table ID =4>
	<Abstractive Summary> =
		6253
		Table 4: Comparison results of different combinations of training and adapting procedures
	</Abstractive Summary>
</Paper ID=ument408>


<Paper ID=ument409> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Candidate models for review generation
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1 we list the candidate
		generators
	</Extractive Summary>
	<Extractive Summary> =
		Generative Model
		Total
		D-Train
		D-Valid
		D-Test
		∀ model in Table 1 except Google LM
		32,500
		22,750
		3,250
		6,500
		Google LM
		6,680
		4,676
		668
		1,336
		3
	</Extractive Summary>
</Paper ID=ument409>


<Paper ID=ument409> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Number of generated reviews by each model
	</Abstractive Summary>
	<Extractive Summary> =
		, fake) reviews (see Table 2)
	</Extractive Summary>
</Paper ID=ument409>


<Paper ID=ument41> <Table ID =1>
	<Abstractive Summary> =
		TB-Dense
		MATRES
		# of Documents
		Train
		22
		183
		Dev
		5
		-
		Test
		9
		20
		# of Pairs
		Train
		4032
		6332
		Dev
		629
		-
		Test
		1427
		827
		Table 1: Data overview
	</Abstractive Summary>
	<Extractive Summary> =
		1% respectively (Table 10 in
		our appendix)
	</Extractive Summary>
	<Extractive Summary> =
		The breakdown on MATRES is shown in Table 10
		in the appendix
	</Extractive Summary>
</Paper ID=ument41>


<Paper ID=ument41> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Event and Relation Extraction Results on TB-Dense and MATRES
		Micro-average
		TB-Dense
		MATRES
		F1 (%)
		Event
		Relation (G)
		Relation (E)
		Event
		Relation (G)
		Relation (E)
		Baselines
		87
	</Abstractive Summary>
</Paper ID=ument41>


<Paper ID=ument41> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Further ablation studies on event and relation extractions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows a more detailed analysis, in
		which we can see that our single-task models with
		BERT embeddings and a BiLSTM encoder al-
		ready outperform the baseline systems on end-to-
		end relation extraction tasks by 4
	</Extractive Summary>
</Paper ID=ument41>


<Paper ID=ument41> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4: Model performance breakdown for TB-Dense
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows that
		the structured joint model improves both preci-
		sion and recall scores for BEFORE and AFTER
		and achieves the best end-to-end relation extrac-
		tion performance at 49
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4 we further show the breakdown per-
		formances for each positive relation on TB-Dense
	</Extractive Summary>
</Paper ID=ument41>


<Paper ID=ument41> <Table ID =5>
	<Abstractive Summary> =
		We observe that the linguistic rule-based model,
		CAEVO, tends to have a more evenly spread-out
		Labels
		TB-Dense
		MATRES
		BEFORE
		384
		417
		AFTER
		274
		266
		INCLUDES
		56
		–
		IS INCLUDED
		53
		–
		SIMULTANEOUS
		22
		31
		VAGUE
		638
		113
		Table 5: Label Size Breakdown in the Test Data
		Figure 3:
		Performances from a single-task relation
		model under different class weights
	</Abstractive Summary>
</Paper ID=ument41>


<Paper ID=ument41> <Table ID =6>
	<Abstractive Summary> =
		6
		Table 6: Ablation Study on Global Constraints
		tency constraint and the temporal relation tran-
		sitivity constraint for the structured joint model
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 6 we conduct
		an ablation study to understand the contribu-
		tions from the event-relation prediction consis-
		442
		Micro-average
		TB-Dense
		MATRES
		No Structure
		48
	</Extractive Summary>
</Paper ID=ument41>


<Paper ID=ument41> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Error Types Based on MATRES Test Data
		both event and temporal relation extraction tasks
	</Abstractive Summary>
	<Extractive Summary> =
		By comparing gold and pre-
		dicted labels for events and temporal relations and
		examining predicted probabilities for events, we
		identiﬁed three major sources of mistakes made by
		our structured model, as illustrated in Table 7 with
		examples
	</Extractive Summary>
</Paper ID=ument41>


<Paper ID=ument410> <Table ID =1>
	<Abstractive Summary> =
		68
		Table 1: Spearman rank correlation ρ between the cosine similarity of sentence representations and the gold labels
		for various Textual Similarity (STS) tasks
	</Abstractive Summary>
	<Extractive Summary> =
		Average BERT embeddings or using the CLS-
		token output from a BERT network achieved bad
		results for various STS tasks (Table 1), worse than
		average GloVe embeddings
	</Extractive Summary>
</Paper ID=ument410>


<Paper ID=ument410> <Table ID =2>
	<Abstractive Summary> =
		35
		Table 2: Evaluation on the STS benchmark test set
	</Abstractive Summary>
</Paper ID=ument410>


<Paper ID=ument410> <Table ID =3>
	<Abstractive Summary> =
		10
		Table 3: Average Pearson correlation r and average
		Spearman’s rank correlation ρ on the Argument Facet
		Similarity (AFS) corpus (Misra et al
	</Abstractive Summary>
</Paper ID=ument410>


<Paper ID=ument410> <Table ID =4>
	<Abstractive Summary> =
		7973
		Table 4: Evaluation on the Wikipedia section triplets
		dataset (Dor et al
	</Abstractive Summary>
</Paper ID=ument410>


<Paper ID=ument410> <Table ID =5>
	<Abstractive Summary> =
		69
		Table 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit
	</Abstractive Summary>
</Paper ID=ument410>


<Paper ID=ument410> <Table ID =6>
	<Abstractive Summary> =
		44
		-
		Table 6: SBERT trained on NLI data with the clas-
		siﬁcation objective function, on the STS benchmark
		(STSb) with the regression objective function
	</Abstractive Summary>
</Paper ID=ument410>


<Paper ID=ument410> <Table ID =7>
	<Abstractive Summary> =
		GloVe embeddings
		6469
		-
		InferSent
		137
		1876
		Universal Sentence Encoder
		67
		1318
		SBERT-base
		44
		1378
		SBERT-base - smart batching
		83
		2042
		Table 7: Computation speed (sentences per second) of
		sentence embedding methods
	</Abstractive Summary>
</Paper ID=ument410>


<Paper ID=ument411> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of loss functions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		provides examples of margin losses in binary clas-
		siﬁcation
	</Extractive Summary>
</Paper ID=ument411>


<Paper ID=ument411> <Table ID =2>
	<Abstractive Summary> =
		3999
		Table 2: Keywords for each dataset
		Dataset
		Keywords
		Subj
		wonderful terrible feel happy ugly even horrible
		interesting funny dramatic romantic compassionate
		Custrev
		easy excellent nice great good love amazing best
		awesome perfect deﬁnitely better happy compassionate
		MPQA
		support hope help good great love
		AYI
		great best excellent friendly awesome nice amazing
		20NG
		sports baseball hockey
		Then, by combining Lemma 3 and Lemma 4,
		we obtain the following theorem
	</Abstractive Summary>
</Paper ID=ument411>


<Paper ID=ument411> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Failure of optimizing F1-measure if thresh-
		olds are not adjusted
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 illustrates the failure of a de-
		fault threshold when learning a classiﬁer in noisy
		environments and Table 4 shows that much bet-
		ter performance on F1-measure and accuracy can
		be recover by adjusting the threshold with Eq
	</Extractive Summary>
</Paper ID=ument411>


<Paper ID=ument411> <Table ID =4>
	<Abstractive Summary> =
		Proposed methods:
		For the AUC optimiza-
		tion part, we used the recurrent convolutional
		4000
		Table 4: Mean value and standard error of 20 trials for the AUC, F1-measure, accuracy (ACC), and precision
		at 100 (Prec@100) of learning from relavant keywords and unlabeled documents
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 illustrates the failure of a de-
		fault threshold when learning a classiﬁer in noisy
		environments and Table 4 shows that much bet-
		ter performance on F1-measure and accuracy can
		be recover by adjusting the threshold with Eq
	</Extractive Summary>
	<Extractive Summary> =
		By adjusting the threshold in Table 4 can sub-
		stantially improve the performance
	</Extractive Summary>
	<Extractive Summary> =
		2
		End-to-end Classiﬁcation Performance
		Table 4 shows the classiﬁcation performance
	</Extractive Summary>
</Paper ID=ument411>


<Paper ID=ument411> <Table ID =5>
	<Abstractive Summary> =
		07)
		Table 5: Mean F1-measure and standard error of 20 trials with varying thresholds with different ˆπ in Eq
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the accuracy and F1-measure
		with varying thresholds
	</Extractive Summary>
</Paper ID=ument411>


<Paper ID=ument412> <Table ID =1>
	<Abstractive Summary> =
		99B
		Table 1: Text corpora used for pre-training
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10: Examples of corrected sentences from CoNLL-2014 dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 11: Examples of corrected sentences from JFLEG dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 12: Some input-output samples by our pre-trained PoDA models
	</Extractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =2>
	<Abstractive Summary> =
		Corpus
		# of examples
		train
		valid
		test
		CNN/Daily Mail
		287, 113
		13, 368
		11, 490
		Gigaword
		3, 803, 957
		189, 651
		1, 951
		Table 2: Dataset statistics for abstractive summariza-
		tion
	</Abstractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =3>
	<Abstractive Summary> =
		54
		Table 3: ROUGE scores for CNN/Daily Mail dataset
	</Abstractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =4>
	<Abstractive Summary> =
		45
		Table 4: ROUGE scores for Gigaword dataset
	</Abstractive Summary>
	<Extractive Summary> =
		However, the
		pre-training still helps even when the models are
		trained on the full dataset (shown in Table 4)
	</Extractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =5>
	<Abstractive Summary> =
		Corpus
		#Sent Pairs
		Split
		Lang-8 NAIST
		1, 097, 274
		train
		NUCLE
		57, 113
		train
		CLC FCE
		32, 073
		train
		CoNLL-2013 test set
		1, 381
		valid
		JFLEG valid set
		754
		valid
		CoNLL-2014 test set
		1, 312
		test
		JFLEG test set
		747
		test
		Table 5: Dataset statistics for grammatical error correc-
		tion
	</Abstractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =6>
	<Abstractive Summary> =
		34
		Table 6: Precision (P), recall (R) and F0
	</Abstractive Summary>
	<Extractive Summary> =
		Results As shown in Table 6 and Table 7, when
		trained only on the supervised data, “PoDA w/o
		pre-training” can still achieve an impressive per-
		formance with F0
	</Extractive Summary>
	<Extractive Summary> =
		These results are even worse than the weak-
		est baselines in Table 6 and Table 7
	</Extractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =7>
	<Abstractive Summary> =
		38
		Table 7: GLEU scores for JFLEG dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Results As shown in Table 6 and Table 7, when
		trained only on the supervised data, “PoDA w/o
		pre-training” can still achieve an impressive per-
		formance with F0
	</Extractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =8>
	<Abstractive Summary> =
		Target
		economic crisis to bottom out early next year minister says
		PoDA w/o pre-training
		thai economy expected to start to bottom out in UNK
		PoDA
		thai economy to start to bottom out in ﬁrst quarter
		Table 8: Examples of generated summaries from Gigaword dataset
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Linguistic Quality Analysis
		In Table 8, we show some generated summaries
		by PoDA from Gigaword dataset
	</Extractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =9>
	<Abstractive Summary> =
		01
		Table 9: Ablations for pre-trained encoder and decoder
		on CoNLL-2014 test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 shows that the performance degrades
		by a large margin if the network is only partially
		pre-trained
	</Extractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =10>
	<Abstractive Summary> =
		Table 10: Examples of corrected sentences from CoNLL-2014 dataset
	</Abstractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =11>
	<Abstractive Summary> =
		Table 11: Examples of corrected sentences from JFLEG dataset
	</Abstractive Summary>
</Paper ID=ument412>


<Paper ID=ument412> <Table ID =12>
	<Abstractive Summary> =
		Table 12: Some input-output samples by our pre-trained PoDA models
	</Abstractive Summary>
</Paper ID=ument412>


<Paper ID=ument413> <Table ID =1>
	<Abstractive Summary> =
		well done!!
		TerminalOperation
		34
		would be nice if you actually announced delays
		Table 1: Statistics of the labeled Twitter airline customer support (TwACS) dataset and the corresponding user
		query utterance examples
	</Abstractive Summary>
</Paper ID=ument413>


<Paper ID=ument413> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Evaluation results on the TwACS and AskUbuntu datasets for different systems
	</Abstractive Summary>
</Paper ID=ument413>


<Paper ID=ument413> <Table ID =3>
	<Abstractive Summary> =
		Other
		8
		Table 3: The top 5 most frequent errors made by the
		quick thoughts pretrained AV-KMEANS on the TwACS
		dataset
	</Abstractive Summary>
</Paper ID=ument413>


<Paper ID=ument414> <Table ID =1>
	<Abstractive Summary> =
		81)
		Table 1: Evaluation results from cross-corpus eval-
		uation for (train, test) pairs of datasets, AIMed (A)
		and BioInfer (B)
	</Abstractive Summary>
</Paper ID=ument414>


<Paper ID=ument414> <Table ID =2>
	<Abstractive Summary> =
		61)
		Table 2:
		Evaluation results for PubMed45 and
		BioNLP datasets
	</Abstractive Summary>
</Paper ID=ument414>


<Paper ID=ument415> <Table ID =1>
	<Abstractive Summary> =
		31
		Cubs won the World Series?”
		NUM “What is the average speed of the horses at
		“What is average salary of restaurant manager in
		the Kentucky Derby?”
		United States?”
		Table 1: Examples of semantically similar questions in the same or different classes, with the corresponding
		landmarks activated during the classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 reports question-explanation pairs with
		similarity estimates based on the adopted CSPTK
		kernel function
	</Extractive Summary>
	<Extractive Summary> =
		Finally,
		as Table 1 shows, explanations are strictly depen-
		dent on the induced neural model and are not just
		triggered by text similarity metrics: they are epis-
		temologically principled evidences about the neu-
		ral learning stages, based on the observed exam-
		ples and the selected landmarks
	</Extractive Summary>
</Paper ID=ument415>


<Paper ID=ument416> <Table ID =1>
	<Abstractive Summary> =
		1M
		13K
		13K
		20
		20K
		Yahoo
		101K
		10K
		10K
		78
		20K
		Table 1: Statistics of three benchmark datasets used in
		the experiments
	</Abstractive Summary>
</Paper ID=ument416>


<Paper ID=ument416> <Table ID =2>
	<Abstractive Summary> =
		33
		Table 2: Language modeling results on the PTB, SNLI and Yahoo datasets
	</Abstractive Summary>
	<Extractive Summary> =
		According to Table 2, we observe that the KL
		value for VAE-0
	</Extractive Summary>
</Paper ID=ument416>


<Paper ID=ument416> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Sentences generated from the variational posteriors of the latent variable based on SNLI dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows two sets of sen-
		tences generated using VAE-MINE with a greedy
		decoding based on two particular inputs
	</Extractive Summary>
</Paper ID=ument416>


<Paper ID=ument416> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Sentences generated by interpolating between the encodings of “this church choir sings to the masses
		as they sing joyous songs from the book at a church
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the transi-
		tions of the generated sentences between two par-
		ticular sentences in the SNLI dataset using differ-
		ent models
	</Extractive Summary>
</Paper ID=ument416>


<Paper ID=ument416> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Qualitative comparisons on the generated sentences
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 5, we observe that
		sentences generated from LaggingVAE are not as
		diverse as the ones generated from VAE-MINE
	</Extractive Summary>
</Paper ID=ument416>


<Paper ID=ument416> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Sentences selected from the PTB and SNLI training set in terms of the cosine distance between the
		inferred latent variables
	</Abstractive Summary>
	<Extractive Summary> =
		Our results in Table 6 show
		that VAE-MINE learns a good latent variable such
		that novel sentences can be chosen, due to the na-
		ture of MI-regularized objective
	</Extractive Summary>
</Paper ID=ument416>


<Paper ID=ument417> <Table ID =1>
	<Abstractive Summary> =
		3K
		AGN (120k), SGN (450k),
		DBP (560k), YRP (560k),
		YRF (650k), YHA (1400k),
		AMZP (3600k), AMZF (3000k)
		FTZ (97%), MNB (90%)
		Table 1:
		Comparison of active text classiﬁcation
		datasets and models (Acc on Trec-QA) used in (Sid-
		dhant and Lipton, 2018) and our work
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 1, the datasets used in latest such analysis on
		active text classiﬁcation by (Siddhant and Lipton,
		2018) are quite small in comparison
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 provides a
		comparison regarding the choice of datasets, mod-
		els and number of experiments between our study
		and (Siddhant and Lipton, 2018) which investi-
		gates a variety of NLP tasks including text clas-
		siﬁcation while we focus only on the latter
	</Extractive Summary>
	<Extractive Summary> =
		3(8)
		Table 10: ULMFiT: Resulting sample ˆSb compared
		to reported accuracies in (Howard and Ruder, 2018)
		(%dataset in brackets)
	</Extractive Summary>
</Paper ID=ument417>


<Paper ID=ument417> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Label entropy with a large query size (b = 9
		queries)
	</Abstractive Summary>
</Paper ID=ument417>


<Paper ID=ument417> <Table ID =3>
	<Abstractive Summary> =
		6
		1032
		Table 3: Proportion of Support Vectors intersecting
		with our actively selected set calculated by |SSV ∩ ˆ
		Sb|
		|SSV |
	</Abstractive Summary>
</Paper ID=ument417>


<Paper ID=ument417> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: % Intersection of samples obtained with dif-
		ferent seeds (ModelD) compared to same seeds (Mod-
		elS) and chance intersection for b = 39 queries
	</Abstractive Summary>
</Paper ID=ument417>


<Paper ID=ument417> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5: Intersection of samples obtained with different values of b
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 presents the intersection of
		samples obtained with different query sizes across
		multiple runs
	</Extractive Summary>
</Paper ID=ument417>


<Paper ID=ument417> <Table ID =6>
	<Abstractive Summary> =
		2
		Table 6: Intersection of query strategies across acquisition functions
	</Abstractive Summary>
</Paper ID=ument417>


<Paper ID=ument417> <Table ID =7>
	<Abstractive Summary> =
		2
		Table 7: Intersection of query strategies across single
		and ensemble of 5FTZ models
	</Abstractive Summary>
	<Extractive Summary> =
		The results are presented in Table 7 showing
		little to no difference in sample overlaps
	</Extractive Summary>
</Paper ID=ument417>


<Paper ID=ument417> <Table ID =8>
	<Abstractive Summary> =
		6
		Table 8: Results of sample selection from previous investigations on small datasets (Trec-QA)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows that the results of our study
		generalize to small datasets like TREC-QA
	</Extractive Summary>
</Paper ID=ument417>


<Paper ID=ument417> <Table ID =9>
	<Abstractive Summary> =
		9 (8)
		Table 9: Comparison of accuracies with state-of-the-art approaches (earliest-latest) for text classiﬁcation (%dataset
		in brackets)
	</Abstractive Summary>
</Paper ID=ument417>


<Paper ID=ument417> <Table ID =10>
	<Abstractive Summary> =
		3(8)
		Table 10: ULMFiT: Resulting sample ˆSb compared
		to reported accuracies in (Howard and Ruder, 2018)
		(%dataset in brackets)
	</Abstractive Summary>
</Paper ID=ument417>


<Paper ID=ument418> <Table ID =1>
	<Abstractive Summary> =
		SQuAD
		Distractor sentences are added
		to the context
		TF-IDF sentence selector
		Modiﬁed BiDAF
		QA
		TriviaQA-CP
		Questions ask about different
		kinds of entities
		NER answer detector
		Modiﬁed BiDAF
		Table 1: Summary of the evaluations we perform, Domain Shift refers to what changes between the train and test
		data, and Bias-Only Model speciﬁes how the bias model we use was constructed
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Experiments
		We provide experiments on ﬁve different domains,
		summarized in Table 1, each of which requires
		models to overcome a challenging domain-shift
		between train and test data
	</Extractive Summary>
	<Extractive Summary> =
		Table 10 shows the accuracy of these classiﬁers
	</Extractive Summary>
	<Extractive Summary> =
		55
		Table 10: Accuracy, and per-class scores, on the manually annotated questions for the various question classiﬁca-
		tion methods we used when building TriviaQA-CP
	</Extractive Summary>
</Paper ID=ument418>


<Paper ID=ument418> <Table ID =2>
	<Abstractive Summary> =
		86
		Table 2: Results on MNLI with different kinds of synthetic bias
	</Abstractive Summary>
	<Extractive Summary> =
		Results: Table 2 shows the results
	</Extractive Summary>
</Paper ID=ument418>


<Paper ID=ument418> <Table ID =3>
	<Abstractive Summary> =
		33
		Table 3: Results on the VQA-CP v2
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results:
		Table 3 shows the results
	</Extractive Summary>
</Paper ID=ument418>


<Paper ID=ument418> <Table ID =4>
	<Abstractive Summary> =
		97
		Table 4: Accuracy on the adversarial MNLI dataset,
		HANS, and the MNLI matched dev set
	</Abstractive Summary>
	<Extractive Summary> =
		Results: Table 4 shows the results
	</Extractive Summary>
</Paper ID=ument418>


<Paper ID=ument418> <Table ID =5>
	<Abstractive Summary> =
		14
		Table 5: F1 scores on Adversarial SQuAD and the standard SQuAD dev set using two different bias-only models
	</Abstractive Summary>
	<Extractive Summary> =
		Results:
		Table 5 shows the results
	</Extractive Summary>
</Paper ID=ument418>


<Paper ID=ument418> <Table ID =6>
	<Abstractive Summary> =
		83
		Table 6: EM scores on two changing priors TriviaQA
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Results: Table 6 shows the results
	</Extractive Summary>
</Paper ID=ument418>


<Paper ID=ument418> <Table ID =7>
	<Abstractive Summary> =
		2
		Table 7: Entropy penalty weight for the learned-mixin
		+H ensemble on all our experiments
	</Abstractive Summary>
</Paper ID=ument418>


<Paper ID=ument418> <Table ID =8>
	<Abstractive Summary> =
		48
		Table 8: Scores on individual heuristics in HANS, with scores on the MNLI matched dev set for reference
	</Abstractive Summary>
</Paper ID=ument418>


<Paper ID=ument418> <Table ID =9>
	<Abstractive Summary> =
		0
		Table 9: Statistics for the TriviaQA-CP datasets
	</Abstractive Summary>
</Paper ID=ument418>


<Paper ID=ument418> <Table ID =10>
	<Abstractive Summary> =
		55
		Table 10: Accuracy, and per-class scores, on the manually annotated questions for the various question classiﬁca-
		tion methods we used when building TriviaQA-CP
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10 shows the accuracy of these classiﬁers
	</Extractive Summary>
</Paper ID=ument418>


<Paper ID=ument419> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Experimental results for changes up to �=3 and �=2 symbols on SST and AG dataset, respectively
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 1 shows the results of IBP training and base-
		line models under � = 3 and � = 23 perturbations
		on SST and AG News, respectively
	</Extractive Summary>
	<Extractive Summary> =
		In Table 1, comparing adversarial accuracy with
		3Note that the exhaustive oracle is not computationally
		feasible beyond � = 2 on AG News
	</Extractive Summary>
	<Extractive Summary> =
		The resulting models achieve the
		highest exhaustively veriﬁed accuracy at the cost
		of only moderate deterioration in nominal accuracy
		(Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		In Table 1, when the
		perturbation space is larger (SST character-level vs
	</Extractive Summary>
</Paper ID=ument419>


<Paper ID=ument419> <Table ID =2>
	<Abstractive Summary> =
		) that sure is funny !
		Table 2: Pairs of original inputs and adversarial examples for SST sentiment classiﬁcation found via an exhaus-
		tive veriﬁcation oracle, but not found by the HotFlip attack (i
	</Abstractive Summary>
	<Extractive Summary> =
		1
		A Motivating Example
		We provide an example from Table 2 to highlight
		different evaluation metrics and training methods
	</Extractive Summary>
</Paper ID=ument419>


<Paper ID=ument419> <Table ID =3>
	<Abstractive Summary> =
		Conversely, such an
		4090
		Perturbation radius
		� =1
		� =2
		� =3
		SST-word
		49
		674
		5,136
		SST-character
		206
		21,116
		1,436,026
		AG-character
		722
		260,282
		-
		Table 3: Maximum perturbation space size in the SST
		and AG News test set using word / character substitu-
		tions, which is the maximum number of forward passes
		per sentence to evaluate in the exhaustive veriﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we show the maximum perturbation
		space size in the SST and AG News test set for
		different perturbation radii �
	</Extractive Summary>
</Paper ID=ument419>


<Paper ID=ument42> <Table ID =1>
	<Abstractive Summary> =
		The
		Table 1: Statistics of the datasets
	</Abstractive Summary>
	<Extractive Summary> =
		We conduct extensive experiments on
		ﬁve public datasets from various domains (sum-
		marized in Table 1 and detailed in Appendix A)
	</Extractive Summary>
</Paper ID=ument42>


<Paper ID=ument42> <Table ID =2>
	<Abstractive Summary> =
		com/dataset/
		challenge
		450
		Table 2: Performance comparison on RCV1
	</Abstractive Summary>
</Paper ID=ument42>


<Paper ID=ument42> <Table ID =3>
	<Abstractive Summary> =
		,
		Table 3: Performance comparison on the NYT and
		Yelp datasets
	</Abstractive Summary>
</Paper ID=ument42>


<Paper ID=ument42> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Performance comparison on Functional
		Catalogue and Gene Ontology
	</Abstractive Summary>
</Paper ID=ument42>


<Paper ID=ument42> <Table ID =5>
	<Abstractive Summary> =
		We compare HiLAP with CSSA (Bi and
		452
		Table 5: Ablation study of HiLAP
	</Abstractive Summary>
</Paper ID=ument42>


<Paper ID=ument42> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Analysis of Label Inconsistency
	</Abstractive Summary>
	<Extractive Summary> =
		To provide
		a picture of how severe the issue is, we further
		conduct experiments to check the percentage of
		objects that are predicted with inconsistent labels
		(Table 6)
	</Extractive Summary>
</Paper ID=ument42>


<Paper ID=ument420> <Table ID =1>
	<Abstractive Summary> =
		looks”, “no”, “no”]
		Rationale from our full model:
		[“cloudy”, “lots”, “pretty gross”, “no lacing”]
		Table 1: An example of the rationales extracted by different
		models on the sentiment analysis task of beer reviews (ap-
		pearance aspect)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows such degenerate cases
		for the two cooperative methods
	</Extractive Summary>
</Paper ID=ument420>


<Paper ID=ument420> <Table ID =2>
	<Abstractive Summary> =
		Relation Classiﬁcation
		Message-Topic(e1,e2)
		It was a friendly calle1 to remind them about the bille2 and
		make sure they have a copy of the invoice
		Table 2: Example of the three tasks used for evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 gives examples of the above
		tasks
	</Extractive Summary>
</Paper ID=ument420>


<Paper ID=ument420> <Table ID =3>
	<Abstractive Summary> =
		40
		Table 3: Main results of binary classiﬁcation on multi-aspect
		beer reviews
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Multi-Aspect Beer Review
		Table 3 summarizes the main results on the multi-
		aspect beer review task
	</Extractive Summary>
</Paper ID=ument420>


<Paper ID=ument420> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: Predictive accuracy on the single-aspect beer ap-
		pearance review (left) and the relation classiﬁcation (right)
	</Abstractive Summary>
	<Extractive Summary> =
		From the left part of Table 4, we observe that the
		original model of (Lei et al
	</Extractive Summary>
</Paper ID=ument420>


<Paper ID=ument420> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: Subjective evaluations on the task of controlling
		the unselected rationale words
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the performance of subjective
		evaluations
	</Extractive Summary>
</Paper ID=ument420>


<Paper ID=ument420> <Table ID =6>
	<Abstractive Summary> =
		(2016):
		[a softwaree1 companye2 to]
		Our Intros+minimax:
		[working for a softwaree1 companye2], [loans]
		Table 6: Illustrative examples of generated rationales on the
		relation classiﬁcation task
	</Abstractive Summary>
</Paper ID=ument420>


<Paper ID=ument421> <Table ID =1>
	<Abstractive Summary> =
		4108
		Expression
		Scaling properties
		α ∈ R \ {0, 1}
		1
		α(α−1)
		n�
		i=1
		�
		pα
		i q1−α
		i
		− αpi + (α − 1)qi
		�
		Dα(c × p||c × q) = c × Dα(p||q)
		β ∈ R \ {0, 1}
		1
		β(β−1)
		n�
		i=1
		�
		pβ
		i + (β − 1)qβ
		i − βpiqβ−1
		i
		�
		Dα(c × p||c × q) = cβ × Dα(p||q)
		γ ∈ R \ {0, 1}
		1
		γ(γ−1) log
		n�
		i=1
		pγ
		i + 1
		γ log
		n�
		i=1
		qγ
		i −
		1
		γ−1 log
		n�
		i=1
		piqγ−1
		i
		Dγ(c1 × p||c2 × q) = Dγ(p||q)
		Table 1: Complete expressions of α, β, and γ divergences between two positive measures p = (pi)n
		i=1 and
		q = (qi)n
		i=1 on Rn
		+, as well as their scaling properties
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Working With Positive Measures
		The three divergences presentend in Section 3 are
		deﬁned on positive measures: in theory, we can
		simply use the exp function on the scores sθ and
		do not need to normalize them:
		Obj(θ) = D(pD|| exp (sθ))
		However, neither the α and β divergences are scale
		invariant (see right column of Table 1 and Ci-
		chocki and Amari 2010)
	</Extractive Summary>
</Paper ID=ument421>


<Paper ID=ument421> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Best validation perplexities
		obtained on the PTB with Objα, Objβ,
		and Objγ, derived from MLE, and ap-
		proximated objectives AS and NCE,
		on single models with the same intial-
		ization
	</Abstractive Summary>
	<Extractive Summary> =
		For the PTB, the ﬁnal validation per-
		plexities are presented in Table 2, while we present
		the ﬁnal validation cross-entropies for the objec-
		tives derived from MLE, on 5 frequency buckets,
		in Figure 2
	</Extractive Summary>
</Paper ID=ument421>


<Paper ID=ument421> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3:
		Best validation perplexi-
		ties obtained on the WT2 with Objα,
		Objβ, and Objγ, derived from MLE,
		and approximated objectives AS and
		NCE, on single models with the same
		initialization
	</Abstractive Summary>
</Paper ID=ument421>


<Paper ID=ument421> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Final validation results obtained on the PTB
		and WT2, with the exact objectives Objα, Objβ, and
		Objγ, derived from MLE
	</Abstractive Summary>
</Paper ID=ument421>


<Paper ID=ument421> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5: Best validation and test perplexities obtained
		on the best performing conﬁgurations of power param-
		eter for both corpora and each category of objectives,
		averaged over 5 models with different initializations
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 5, we present per-
		plexity results averaged for different seeds, in or-
		der to check that model initialization does not dis-
		cernibly affect these improvements
	</Extractive Summary>
</Paper ID=ument421>


<Paper ID=ument422> <Table ID =1>
	<Abstractive Summary> =
		Data
		training
		dev
		test
		#l
		45
		45
		45
		WSJ
		#s
		38,219
		5,527
		5,462
		#t
		912,344
		131,768
		129,654
		#l
		50
		50
		49
		UD en
		#s
		12,544
		2,003
		2,078
		#t
		204,607
		25,150
		25,097
		#l
		18
		18
		18
		OntoNotes
		#s
		59,924
		8,528
		8,262
		#t
		1,088,503
		147,724
		152,728
		#l
		426
		323
		348
		CCGBank
		#s
		39,604
		1,913
		2,407
		#t
		929,552
		45,422
		55,371
		Table 1: Data statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the numbers of sen-
		tences, tokens and labels for training, development
		and test, respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 10: CCG case analysis
	</Extractive Summary>
	<Extractive Summary> =
		Some predictions of BiLSTM-
		softmax, BiLSTM-CRF and BiLSTM-LAN are
		shown in Table 10
	</Extractive Summary>
</Paper ID=ument422>


<Paper ID=ument422> <Table ID =2>
	<Abstractive Summary> =
		5M
		Table 2:
		WSJ development set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the ef-
		fect of the label embedding size
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 compares differ-
		ent numbers of BiLSTM layers and hidden sizes
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, for POS tagging,
		it is effective to capture label dependencies using
		two layers
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 also compares the
		effect of model sizes
	</Extractive Summary>
	<Extractive Summary> =
		Table 8 com-
		pares BiLSTM-LAN to other published results on
		1 Note that our results are different from Table 2 of Yasunaga
		et al
	</Extractive Summary>
</Paper ID=ument422>


<Paper ID=ument422> <Table ID =3>
	<Abstractive Summary> =
		65
		Table 3: Effect of attention layer
	</Abstractive Summary>
</Paper ID=ument422>


<Paper ID=ument422> <Table ID =4>
	<Abstractive Summary> =
		70
		Table 4: Comparison of the training time for one iter-
		ation and decoding speed
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows a comparison
		of training and decoding speeds
	</Extractive Summary>
</Paper ID=ument422>


<Paper ID=ument422> <Table ID =5>
	<Abstractive Summary> =
		65
		Table 5: Result for POS tagging on WSJ
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the ﬁnal POS tagging re-
		sults on WSJ
	</Extractive Summary>
</Paper ID=ument422>


<Paper ID=ument422> <Table ID =6>
	<Abstractive Summary> =
		65
		Table 6: Main results on WSJ
	</Abstractive Summary>
	<Extractive Summary> =
		Our results are same as Table 6 of Yang et al
	</Extractive Summary>
</Paper ID=ument422>


<Paper ID=ument422> <Table ID =7>
	<Abstractive Summary> =
		40
		Table 7: Multilingual POS tagging result on UD v2
	</Abstractive Summary>
</Paper ID=ument422>


<Paper ID=ument422> <Table ID =8>
	<Abstractive Summary> =
		16
		Table 8:
		F1-scores on the OntoNotes 5
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 com-
		pares BiLSTM-LAN to other published results on
		1 Note that our results are different from Table 2 of Yasunaga
		et al
	</Extractive Summary>
</Paper ID=ument422>


<Paper ID=ument422> <Table ID =9>
	<Abstractive Summary> =
		7
		Table 9:
		Supertagging accuracy on CCGbank test
		set
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 9, BiLSTM-
		LAN signiﬁcantly outperforms both BiLSTM-
		softmax and BiLSTM-CRF (p
		<0
	</Extractive Summary>
</Paper ID=ument422>


<Paper ID=ument422> <Table ID =10>
	<Abstractive Summary> =
		Table 10: CCG case analysis
	</Abstractive Summary>
</Paper ID=ument422>


<Paper ID=ument423> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Robustness of models on IMDB
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Main results
		Table 1 and Table 2 show our main results for
		IMDB and SNLI, respectively
	</Extractive Summary>
</Paper ID=ument423>


<Paper ID=ument423> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Robustness of models on the SNLI test set
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Main results
		Table 1 and Table 2 show our main results for
		IMDB and SNLI, respectively
	</Extractive Summary>
</Paper ID=ument423>


<Paper ID=ument423> <Table ID =3>
	<Abstractive Summary> =
		25
		50
		Table 3: Training hyperparameters for training the models
	</Abstractive Summary>
</Paper ID=ument423>


<Paper ID=ument423> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Effects of holding ϵ and κ ﬁxed during train-
		ing
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Training schedule
		In Table 4, we show the effect of holding ϵ or κ
		ﬁxed during training, as described in Section 5
	</Extractive Summary>
</Paper ID=ument423>


<Paper ID=ument424> <Table ID =1>
	<Abstractive Summary> =
		03)
		Table 1: Accuracy on the development sets
	</Abstractive Summary>
</Paper ID=ument424>


<Paper ID=ument425> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Motivation: accuracy (%) of L1ID on the L2-
		Reddit dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Ac-
		curacy (Table 1) degrades on both the in-domain
		and out-of-domain sets, even when only 20 words
		are removed
	</Extractive Summary>
	<Extractive Summary> =
		The accuracy of the linear classiﬁer is poor
		(compared to Table 1), perhaps because it fails to
		capture some contextual features learned by the
		neural network models
	</Extractive Summary>
	<Extractive Summary> =
		To further conﬁrm that the ALT-LO model is
		not learning topical features, we repeat the ex-
		periment presented in Table 1—masking the top
		K topical words (based on log-odds scores) from
		the test sets, but not retraining the models—now,
		with our proposed model ALT-LO
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows
		that in contrast to standard models that do not de-
		mote topical confounds (as in Table 1), there is
		less degradation in the performance of ALT-LO
	</Extractive Summary>
</Paper ID=ument425>


<Paper ID=ument425> <Table ID =2>
	<Abstractive Summary> =
		1
		Learning Schedule: Alternating
		Optimization of Classiﬁer and Adversary
		In practice, this optimization is done in an alter-
		nating fashion by minimizing the following two
		4156
		English
		ireland irish british britain russia scotland england states american london brexit
		Finnish
		ﬁnland ﬁnnish ﬁnns helsinki swedish ﬁnn nordic sweden sauna nokia estonian
		French
		french france paris sarkozy macron ﬁllon hollande gaulle hamon marine valls breton
		German
		german germany austria merkel refugees asylum germans bavaria austrian berlin also
		Greece
		greek greece greeks syriza macedonia athens turkey macedonians fyrom turkish ancient
		Dutch
		dutch netherlands amsterdam wilders rotterdam holland rutte belgium bike hague
		Polish
		poland polish poles warsaw lithuanian lithuania judges jews ukranians imho tusk
		Romanian
		romania romanian romanians moldova bucharest hungarian hungarians transistria
		Spanish
		spain catalan spanish catalonia catalans madrid barcelona independence spaniards
		Swedish
		sweden swedish swedes stockholm swede malmo danish nordic denmark ﬁnland
		Table 2: Top words based on log-odds scores for each label in the L2-Reddit dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the top-
		10 words in each class; observe that almost all
		of these words are geographical (hence, topical)
		terms that have nothing to do with the L1
	</Extractive Summary>
</Paper ID=ument425>


<Paper ID=ument425> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Classiﬁcation accuracy with topic-demoting
		methods, TOEFL dataset
	</Abstractive Summary>
	<Extractive Summary> =
		We refer to the version where prompt “PK” is
		out-of-domain as “–PK” in the results (Table 3),
		K ∈ {0,
	</Extractive Summary>
	<Extractive Summary> =
		Table 3
		reports the accuracy of our proposed model, de-
		noted ALT-LO, compared to the logistic regres-
		sion baseline (LR), and two adversarial baselines:
		one demotes latent log-odds-based topics via gra-
		dient reversal (GR-LO), and another uses our pro-
		posed novel learning procedure but demotes base-
		line LDA topics (ALT-LDA)
	</Extractive Summary>
</Paper ID=ument425>


<Paper ID=ument425> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Baseline classiﬁcation accuracy on L2-Reddit
	</Abstractive Summary>
</Paper ID=ument425>


<Paper ID=ument425> <Table ID =5>
	<Abstractive Summary> =
		9
		Table 5: Classiﬁcation accuracy with topic-demoting
		methods, L2-Reddit dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 sum-
		marizes the results: our proposed learning proce-
		dure ALT-LO performs better than both the alter-
		natives
	</Extractive Summary>
</Paper ID=ument425>


<Paper ID=ument425> <Table ID =6>
	<Abstractive Summary> =
		4
		Table 6: Accuracy on the L2-Reddit dataset; the pro-
		posed model (ALT-LO) with different settings of the test
		sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows
		that in contrast to standard models that do not de-
		mote topical confounds (as in Table 1), there is
		less degradation in the performance of ALT-LO
	</Extractive Summary>
</Paper ID=ument425>


<Paper ID=ument425> <Table ID =7>
	<Abstractive Summary> =
		9
		Conclusion
		We introduced a method to represent unknown
		confounds in text classiﬁcation using topic mod-
		els and log-odds scores, and a new general method
		4161
		NO-ADV
		sweden france greece ﬁnland poland spain greek germany french eu
		romania polish dutch german spanish swedish netherlands ﬁnnish
		LO-TOP-50
		eu ’s ’re ’m ’ & uk us because ’ve am its nt english these usa nt
		here ’ll especially correct pis de within
		ALT-LO
		the in to of that a i is and ’t as from with by ? on but & they
		are about at because like was would have you
		Table 7: The highest scoring words in lexicons generated using attention scores
	</Abstractive Summary>
	<Extractive Summary> =
		What emerges from this lexicon (Table 7) is
		a dramatic difference between the top indicative
		words in the various models
	</Extractive Summary>
	<Extractive Summary> =
		In line with
		results in Table 7, salient words for ALT-LO are
		determiners and prepositions
	</Extractive Summary>
</Paper ID=ument425>


<Paper ID=ument425> <Table ID =8>
	<Abstractive Summary> =
		NO-ADV
		poland greek romania greece france spain french sweden ﬁnland
		polish dutch spanish netherlands ﬁnnish german
		LO-TOP-50
		on ’re even ’d up less things ’ll doesn living majority
		sense talk level ’ve rights took number north
		ALT-LO
		the of to i a in greece romania france ﬁnland that for is french & you
		’t ﬁnnish
		Table 8: The highest scoring words in lexicons generated using saliency maps
	</Abstractive Summary>
</Paper ID=ument425>


<Paper ID=ument426> <Table ID =1>
	<Abstractive Summary> =
		551
		Table 1: Human teacher evaluation for learned and ran-
		dom question asking policy
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows the average cumulative reward for humans
		interacting with LiD vs a random policy for this
		experiment
	</Extractive Summary>
</Paper ID=ument426>


<Paper ID=ument427> <Table ID =1>
	<Abstractive Summary> =
		com/watson/services/
		speech-to-text/
		4176
		Original sentence (audio)
		English model output
		Spanish model output
		no pero vino porque he came to check on
		my machine
		No but we no longer he came to check on
		my machine
		Cual provino de que en dicha c´amara
		chino
		yo le invit´e pa Easter porque me daba pena
		el pobre aqu´ı solo sin familia ni nada
		feeling betrayed by eastern lap and I
		bought a new solo seem funny any now
		y el envite pa´ıs tampoco nada pero por
		aqu´ı solo sin familia ni nada
		Table 1: Examples of the output of IBM’s English and Spanish speech recognition systems on code-switched audio
	</Abstractive Summary>
	<Extractive Summary> =
		The results (examples
		available in Table 1) exhibit two failure modes:
		(1) Words and sentences in the opposite language
		are not recognized correctly; and (2) Code-switch
		points also hurt recognition of words from the
		main language
	</Extractive Summary>
</Paper ID=ument427>


<Paper ID=ument427> <Table ID =2>
	<Abstractive Summary> =
		of CS gold sentences
		250/1000
		250/1000
		Table 2: Statistics of the dataset
	</Abstractive Summary>
</Paper ID=ument427>


<Paper ID=ument427> <Table ID =3>
	<Abstractive Summary> =
		59
		Table 3: Results on the dev set and on the test set
	</Abstractive Summary>
</Paper ID=ument427>


<Paper ID=ument427> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Results on the dev set and on the test set using dis-
		criminative training with only subsets of the code-switched
		data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		depicts the results when using subsets of the CS
		training data with discriminative training
	</Extractive Summary>
</Paper ID=ument427>


<Paper ID=ument427> <Table ID =5>
	<Abstractive Summary> =
		87
		Table 5: Accuracy on the dev set and on the test set, accord-
		ing to the type of the gold sentence in the set: code-switched
		(CS) vs
	</Abstractive Summary>
	<Extractive Summary> =
		9
		Analysis
		Table 5 breaks down the results of the different
		models according to two conditions: when the
		gold sentence is code-switched, and when the gold
		sentence is monolingual
	</Extractive Summary>
</Paper ID=ument427>


<Paper ID=ument427> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Examples of sentences the FINE-TUNED-DISCRIMINATIVE model identiﬁes correctly while the FINE-TUNED-LM
		model does not
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 presents several examples in which FINE-
		TUNED-LM prefers a wrong sentence whereas
		FINE-TUNED-DISCRIMINATIVE
		identiﬁes
		the
		gold one
	</Extractive Summary>
</Paper ID=ument427>


<Paper ID=ument427> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Examples of sentences that the FINE-TUNED-DISCRIMINATIVE model fails to identify
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 lists some of the
		mistakes of the FINE-TUNED-DISCRIMINATIVE
		model: in examples 1, 2 and 3, the gold sen-
		tence was code-switched but the model preferred a
		monolingual one, in example 4 the model prefers
		a wrong CS sentence over the gold monolingual
		one, and in 5 and 6 the model makes mistakes in
		monolingual sentences
	</Extractive Summary>
</Paper ID=ument427>


<Paper ID=ument428> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Answer Generation on ELI5 using Seq2Seq mod-
		els receiving the Question and a support Document (e
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Linearized Graph Improves Performance
		In Table 1, we compare our methods to various
		baselines on the ELI5 dataset
	</Extractive Summary>
</Paper ID=ument428>


<Paper ID=ument428> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Lead Paragraph Generation on WikiSum Com-
		monCrawl using Seq2Seq models receiving the Title and a
		support Document (e
	</Abstractive Summary>
	<Extractive Summary> =
		Similar trends are seen for the WikiSum dataset
		in Table 2, where graph construction improves the
		Multi-task model by 0
	</Extractive Summary>
</Paper ID=ument428>


<Paper ID=ument428> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Ablations on the ELI5 Validation Set
		Model
		Input
		ROUGE-1
		Seq2Seq Q + D to A
		TF-IDF
		28
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Analysis of Modeling Choices
		Ablation on Model Components
		Table 3(a) se-
		quentially removes the graph embeddings, the
		knowledge-base completion multi-tasking, and the
		multi-tasking from (Fan et al
	</Extractive Summary>
	<Extractive Summary> =
		Graph Attribute Embeddings
		Table 3(b) dis-
		plays the effect of removing the graph attribute
		embeddings and gating mechanism
	</Extractive Summary>
	<Extractive Summary> =
		Table 3(c)
		indicates that this lack of coverage of the answer
		tokens correlates with generation quality
	</Extractive Summary>
	<Extractive Summary> =
		Top-k Attention
		Table 3(d) shows the effect of
		the Top-k Hierarchical Attention mechanism for
		various values of k
	</Extractive Summary>
</Paper ID=ument428>


<Paper ID=ument428> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Importance of Web Search Relevance on Valida-
		tion for ELI5, modeling 850 input words
	</Abstractive Summary>
</Paper ID=ument428>


<Paper ID=ument429> <Table ID =1>
	<Abstractive Summary> =
		Rock to 204 Section next week!
		Table 1: Tweets from the social media domain have
		different degrees of relevance to the source domain
		(news)
	</Abstractive Summary>
	<Extractive Summary> =
		As de-
		picted in Table 1, there are some tweets similar to
		the news domain (i
	</Extractive Summary>
</Paper ID=ument429>


<Paper ID=ument429> <Table ID =2>
	<Abstractive Summary> =
		, 2011)
		Table 2: Datasets used in this paper
	</Abstractive Summary>
</Paper ID=ument429>


<Paper ID=ument429> <Table ID =3>
	<Abstractive Summary> =
		51
		Table 3: Results of domain adaptation on three tasks, where zh denotes the Weibo datasets (in Chinese), and en
		denotes the Twitter dataset (in English)
	</Abstractive Summary>
	<Extractive Summary> =
		Among related works (Table 3), AMCL and
		Pre-trained model methods have better perfor-
		mances in CWS
	</Extractive Summary>
</Paper ID=ument429>


<Paper ID=ument429> <Table ID =4>
	<Abstractive Summary> =
		27
		Table 4: Results of baselines and ﬁne-grained knowl-
		edge fusion methods on CWS
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 4 show that both the ba-
		sicKD method and ﬁne-grained methods achieve
		performance improvements through domain adap-
		tation
	</Extractive Summary>
</Paper ID=ument429>


<Paper ID=ument429> <Table ID =5>
	<Abstractive Summary> =
		92
		Table 5: Results of the strongly/weakly relevant ele-
		ments on the Twitter test set
	</Abstractive Summary>
</Paper ID=ument429>


<Paper ID=ument429> <Table ID =6>
	<Abstractive Summary> =
		However, only FGKF
		4204
		Tasks
		POS
		NER
		Sentence
		I
		got
		u
		next
		week
		Louis
		interview
		with
		The
		Sun
		Source only
		PN
		VBD
		NN
		JJ
		NN
		B-PER
		O
		O
		O
		O
		Target only
		PN
		VBZ
		PN
		JJ
		NN
		O
		O
		O
		B-ORG
		I-ORG
		BasicKD
		PN
		VBD
		NN
		JJ
		NN
		B-PER
		O
		O
		O
		O
		FGKF
		PN
		VBD
		PN
		JJ
		NN
		B-PER
		O
		O
		B-ORG
		I-ORG
		Table 6: Two cases of domain adaptation, where the underlined tags are wrong
	</Abstractive Summary>
	<Extractive Summary> =
		Results in Table 6 show that
		both basicKD and FGKF can improve the perfor-
		mance of strongly relevant elements, e
	</Extractive Summary>
</Paper ID=ument429>


<Paper ID=ument429> <Table ID =7>
	<Abstractive Summary> =
		64
		Table 7: Ablation results of the Twitter test set
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Ablation Study
		We conduct the ablation study on Twitter dataset
		(Table 7)
	</Extractive Summary>
</Paper ID=ument429>


<Paper ID=ument43> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Experimental results of our model compared with other models
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Experiments
		We compare our method with the widely used text
		classiﬁcation methods and baseline models (listed
		in Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results and Discussions
		Table 1 reports the results of our model on dif-
		ferent datasets comparing with the widely used
		text classiﬁcation methods and state-of-the-art ap-
		proaches
	</Extractive Summary>
</Paper ID=ument43>


<Paper ID=ument43> <Table ID =2>
	<Abstractive Summary> =
		6k
		Table 2: Summary statistics for the datasets
	</Abstractive Summary>
</Paper ID=ument43>


<Paper ID=ument43> <Table ID =3>
	<Abstractive Summary> =
		0391]
		N×
		P✓
		Table 3: Projected primary capsule’s representations for polysemic words
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Case Study
		Table 3 shows some sample cases from SST val-
		idation dataset, which are movie reviews for sen-
		timent analysis
	</Extractive Summary>
</Paper ID=ument43>


<Paper ID=ument43> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Ablation study on MR dataset
	</Abstractive Summary>
</Paper ID=ument43>


<Paper ID=ument430> <Table ID =1>
	<Abstractive Summary> =
		90
		Table 1: De-tokenized case-sensitive SacreBLEU on WMT En$De newstest2016, newstest2017, newstest2018,
		newstest2019 and the average score
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		We summarize the results of our training strategy
		for En$De and De$Fr translations in Table 1 and
		Table 2
	</Extractive Summary>
	<Extractive Summary> =
		As can be seen from Table 1, the Paracrawl
		dataset improves the model accuracy by a large
		margin
	</Extractive Summary>
</Paper ID=ument430>


<Paper ID=ument430> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: De-tokenized case-sensitive SacreBLEU on
		WMT De$Fr newstest2019
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we show the results of De$Fr trans-
		lation tasks
	</Extractive Summary>
</Paper ID=ument430>


<Paper ID=ument430> <Table ID =3>
	<Abstractive Summary> =
		We list sev-
		eral widely acknowledged systems on En!De
		translation in Table 3:
		MS-Marian (Junczys-
		Dowmunt, 2018), which is the champion of
		4212
		4203
		En!De
		De!En
		Model
		2016
		2017
		2018
		2019
		Avg
		2016
		2017
		2018
		2019
		Avg
		WMT
		342
		Table 3: De-tokenized case-sensitive SacreBLEU on
		WMT En!De newstest2016, newstest2017 and new-
		stest2018
	</Abstractive Summary>
</Paper ID=ument430>


<Paper ID=ument430> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: De-tokenized case-sensitive SacreBLEU on
		WMT De!En newstest2016, newstest2017 and new-
		stest2018 We also list
		the WMT18 top-2 systems for De!En translation
		in Table 4: RWTH (Grac¸a et al
	</Abstractive Summary>
</Paper ID=ument430>


<Paper ID=ument431> <Table ID =1>
	<Abstractive Summary> =
		Training
		Task #1 (CountryCapital)
		Support
		(China, CountryCapital, Beijing)
		Query
		(France, CountryCapital, Paris)
		Task #2 (CEOof)
		Support
		(Satya Nadella, CEOof, Microsoft)
		Query
		(Jack Dorsey, CEOof, Twitter)
		Testing
		Task #1 (OfﬁcialLanguage)
		Support
		(Japan, OfﬁcialLanguage, Japanese)
		Query
		(Spain, OfﬁcialLanguage, Spanish)
		Table 1: The training and testing examples of 1-shot
		link prediction in KGs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 gives a concrete example of the data
		during learning and testing for few-shot link pre-
		diction
	</Extractive Summary>
</Paper ID=ument431>


<Paper ID=ument431> <Table ID =2>
	<Abstractive Summary> =
		4222
		Dataset
		Fit
		# Train
		# Dev
		# Test
		NELL-One
		Y
		321
		5
		11
		Wiki-One
		Y
		589
		16
		34
		NELL-One
		N
		51
		5
		11
		Wiki-One
		N
		133
		16
		34
		Table 2: Statistic of datasets
	</Abstractive Summary>
</Paper ID=ument431>


<Paper ID=ument431> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Three forms of datasets in our experiments
	</Abstractive Summary>
</Paper ID=ument431>


<Paper ID=ument431> <Table ID =4>
	<Abstractive Summary> =
		178
		Table 4: Results of few-shot link prediction in NELL-One and Wiki-One
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, our model performs better with
		all evaluation metrics on both datasets
	</Extractive Summary>
</Paper ID=ument431>


<Paper ID=ument431> <Table ID =5>
	<Abstractive Summary> =
		052
		Table 5: Results of ablation study on Hits@10 of 1-shot
		link prediction in NELL-One
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows that removing gradient meta de-
		creases 29
	</Extractive Summary>
</Paper ID=ument431>


<Paper ID=ument432> <Table ID =1>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Examining examples from the YELP test set
		(Table 1), we identify examples which have sub-
		stantially higher probabilities under MLE than
		topic CVaR (left column) and vice versa (right
		column)
	</Extractive Summary>
</Paper ID=ument432>


<Paper ID=ument433> <Table ID =1>
	<Abstractive Summary> =
		Domain tuning
		Task tuning
		Prediction
		masked tokens
		tags
		Data
		source + target
		source
		Table 1: Overview of domain tuning and task tuning
		Peters et al
	</Abstractive Summary>
</Paper ID=ument433>


<Paper ID=ument433> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Tagging accuracy on PPCEME, using the coarse-grained tagset
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Fine-tuning to the task and domain each yield
		signiﬁcant improvements in performance over the
		Frozen BERT baseline (Table 2, line 1)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Errors on in-vocabulary terms
		The ﬁnal two lines of Table 2 indicate that there re-
		mains a signiﬁcant gap between AdaptaBERT and
		the performance of taggers trained with in-domain
		data: ﬁne-tuning BERT on the PPCEME train-
		4243
		Early Modern English
		PTB
		System
		Accuracy
		In-vocab
		Out-of-vocab
		Accuracy
		Unsupervised domain adaptation
		1
	</Extractive Summary>
</Paper ID=ument433>


<Paper ID=ument433> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Tagging accuracy on PPCEME, using the full PTB tagset to compare with Yang and Eisenstein (2016)
	</Abstractive Summary>
	<Extractive Summary> =
		As a secondary evaluation, we measure perfor-
		mance on the full PTB tagset in Table 3, thereby
		enabling direct comparison with prior work (Yang
		and Eisenstein, 2016)
	</Extractive Summary>
</Paper ID=ument433>


<Paper ID=ument433> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: Named entity segmentation performance on the WNUT test set and CONLL test set A
	</Abstractive Summary>
</Paper ID=ument433>


<Paper ID=ument434> <Table ID =1>
	<Abstractive Summary> =
		81
		Table 1: Dev accuracy results for MT-DNN model with
		different training set sampling strategies
	</Abstractive Summary>
</Paper ID=ument434>


<Paper ID=ument434> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: The easiest and hardest items judged by machine responses for each class in the SNLI test data set
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Analysis
		Qualitative Evaluation of Difﬁculty
		Table 2
		shows examples of premise-hypothesis sentence
		pairs from SNLI with the learned difﬁculty parame-
		ter from the machine RP IRT model
	</Extractive Summary>
</Paper ID=ument434>


<Paper ID=ument434> <Table ID =3>
	<Abstractive Summary> =
		128
		46
		41
		Table 3: Examples from the SNLI and SSTB data sets where the ranking in terms of difﬁculty varies widely
		between human and DNN models
	</Abstractive Summary>
	<Extractive Summary> =
		1): Where are the differences? To
		examine these more closely we identiﬁed those ex-
		amples from the data sets where the rank order was
		most different between the human- and machine-
		response pattern models (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		For SNLI, the easiest sentence pair for the
		LSTM model (which is also very easy for the NSE
		model) is one of the hardest for humans (Table 3,
		row 1)
	</Extractive Summary>
	<Extractive Summary> =
		For SSTB, we see similar patterns (Table 3, rows
		3-4)
	</Extractive Summary>
</Paper ID=ument434>


<Paper ID=ument435> <Table ID =1>
	<Abstractive Summary> =
		, Tk}
		∪ {A(w) : w ∈ Σa}
		∪ {R(w) : w ∈ Σa}
		(1)
		Example 1
		x
		[ Bolt can have run race ]
		y
		[ Bolt could have run the race ]
		diff (C,[) (C,Bolt) (D,can) (I,can,could) (C,have)
		(C,run) (I,run,the) (C,race) (C,])
		e
		C
		C
		R(could)
		C
		A(the)
		C
		C
		↑
		↑
		↑
		↑
		↑
		↑
		↑
		[ Bolt
		can
		have
		run
		race ]
		Example 2
		x
		[ He still won race ! ]
		y
		[ However , he still won ! ]
		diff (C,[) (I,[,However,) (D,He) (I,He,he) (C,still)
		(C,won) (D,-race) (C,!)
		(C,])
		e
		A(However,) T case
		C
		C
		D
		C C
		↑
		↑
		↑
		↑
		↑
		↑ ↑
		[
		He
		still won race ! ]
		Table 1: Two example sentence pairs converted into
		respective edit sequences
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 gives exam-
		ples of converting (x, y) pairs to edit sequences
	</Extractive Summary>
	<Extractive Summary> =
		The complete
		list of transformations appears in Table 10 of the
		Appendix
	</Extractive Summary>
</Paper ID=ument435>


<Paper ID=ument435> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Comparison of non-ensemble model numbers
		for various GEC models
	</Abstractive Summary>
</Paper ID=ument435>


<Paper ID=ument435> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Comparison of recent GEC models trained using publicly available corpus
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Overall Results
		Table 3 compares ensemble model results of PIE
		and other state of the art models, which all hap-
		pen to be seq2seq ED models and also use en-
		semble decoding
	</Extractive Summary>
</Paper ID=ument435>


<Paper ID=ument435> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Statistics on successive rounds of iterative re-
		ﬁnement
	</Abstractive Summary>
</Paper ID=ument435>


<Paper ID=ument435> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Parallel and Iterative edits done by PIE
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 presents some sentences corrected by
		PIE
	</Extractive Summary>
</Paper ID=ument435>


<Paper ID=ument435> <Table ID =6>
	<Abstractive Summary> =
		6
		Table 6: Ablation study on the PIE model
	</Abstractive Summary>
	<Extractive Summary> =
		In
		Table 6 (row 3) we show that compared to the fac-
		torized model (row 2) we get a 1
	</Extractive Summary>
	<Extractive Summary> =
		5
		points poorly (Table 6 row 4 vs row 2)
	</Extractive Summary>
	<Extractive Summary> =
		From Table 6 (row 7 vs 1) we see
		once again that size matters in deep learning!
		x
		I started invoving into Facebook one years ago
	</Extractive Summary>
</Paper ID=ument435>


<Paper ID=ument435> <Table ID =7>
	<Abstractive Summary> =
		6
		Table 7: Comparing PIE with Seq2Seq models (top-
		part) and Ribeiro’s parallel model on two other local
		sequence transduction tasks
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 7 presents whole-word 0/1 accu-
		racy for these tasks on PIE and the following
		methods: Ribeiro et al
	</Extractive Summary>
</Paper ID=ument435>


<Paper ID=ument435> <Table ID =8>
	<Abstractive Summary> =
		02
		Table 8: Wall-clock decoding speed in words/second of
		PIE and a comparable Seq2Seq T2T model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 compares decod-
		ing speed of PIE with Soft-T2T in words/second
	</Extractive Summary>
</Paper ID=ument435>


<Paper ID=ument436> <Table ID =1>
	<Abstractive Summary> =
		Model
		Training Objective of Generator
		RAML
		LGθ = −EX∼Q(X)[log PGθ(X)]
		MaliGAN LGθ = −EX∼PGθ′ (X)[W
		′
		φ(X) log PGθ(X)]
		ARAML
		LGθ = −EX∼Ps(X)[Wφ(X) log PGθ(X)]
		Table 1: Training objectives of generators for RAML,
		MaliGAN and ARAML
	</Abstractive Summary>
</Paper ID=ument436>


<Paper ID=ument436> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Statistics of COCO, EMNLP2017 WMT and
		WeiboDial
	</Abstractive Summary>
</Paper ID=ument436>


<Paper ID=ument436> <Table ID =3>
	<Abstractive Summary> =
		0001
		Table 3: Implementation details of ARAML
	</Abstractive Summary>
</Paper ID=ument436>


<Paper ID=ument436> <Table ID =4>
	<Abstractive Summary> =
		006
		Table 4: Automatic evaluation on COCO and EMNLP2017 WMT
	</Abstractive Summary>
	<Extractive Summary> =
		We also provide standard deviation of each met-
		ric in Table 4, reﬂecting the stability of each
		model’s performance
	</Extractive Summary>
</Paper ID=ument436>


<Paper ID=ument436> <Table ID =5>
	<Abstractive Summary> =
		529
		Table 5: Human evaluation on WeiboDial
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, ARAML performs signif-
		icantly better than other baselines in all the cases
	</Extractive Summary>
</Paper ID=ument436>


<Paper ID=ument436> <Table ID =6>
	<Abstractive Summary> =
		366
		Table 6: PPL-F, PPL-R and S-BLEU of ARAML with
		random sampling (ARAML-R) and constrained sam-
		pling (ARAML-C) on COCO
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the results
	</Extractive Summary>
</Paper ID=ument436>


<Paper ID=ument436> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Examples of generated sentences on COCO
	</Abstractive Summary>
	<Extractive Summary> =
		8
		Case Study
		Table 7 presents the examples generated by the
		models on COCO
	</Extractive Summary>
</Paper ID=ument436>


<Paper ID=ument436> <Table ID =8>
	<Abstractive Summary> =
		I won’t be late on the weekend!
		Table 8: Examples of generated responses on Weibo-
		Dial
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows the generated examples on Wei-
		boDial
	</Extractive Summary>
</Paper ID=ument436>


<Paper ID=ument437> <Table ID =1>
	<Abstractive Summary> =
		72
		–
		Table 1: BLEU scores on three MT benchmark datasets
		for FlowSeq with argmax decoding and baselines with
		purely non-autoregressive decoding method
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 provides the BLEU scores of FlowSeq
		with argmax decoding, together with baselines
		with purely non-autoregressive decoding methods
		that generate output sequence in one parallel pass
	</Extractive Summary>
</Paper ID=ument437>


<Paper ID=ument437> <Table ID =2>
	<Abstractive Summary> =
		84
		Table 2: BLEU scores on two WMT datasets of models
		using advanced decoding methods
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 illustrates the BLEU scores of FlowSeq
		and baselines with advanced decoding methods
		such as iterative reﬁnement, IWD and NPD
		rescoring
	</Extractive Summary>
	<Extractive Summary> =
		The ﬁrst block in Table 2 includes
		the baseline results from autoregressive Trans-
		former
	</Extractive Summary>
</Paper ID=ument437>


<Paper ID=ument438> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of SCAN input commands (left) and output action sequences (right)
	</Abstractive Summary>
	<Extractive Summary> =
		A primitive can be
		“jump” (Jump, see Table 1a for examples) or “turn
		left” (TurnLeft, see Table 1b for examples)
	</Extractive Summary>
	<Extractive Summary> =
		For the Jump and TurnLeft tasks, “jump” and “turn
		left” appear only as a single command in their re-
		spective training sets, but they appear with other
		words at test time (see Table 1a and Table 1b, re-
		spectively)
	</Extractive Summary>
	<Extractive Summary> =
		Further, we ﬁnd that other pre-
		dicted outputs are even correct translations though
		they are different from references (Table 10 in Ap-
		pendix)
	</Extractive Summary>
</Paper ID=ument438>


<Paper ID=ument438> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Test accuracy (mean ± std %) for SCAN Prim-
		itive tasks
	</Abstractive Summary>
	<Extractive Summary> =
		The result in Table 2
		shows that the proposed method boosts accuracies
		from 14
	</Extractive Summary>
</Paper ID=ument438>


<Paper ID=ument438> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Test accuracy (mean ± std %) for SCAN
		template-matching tasks
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 summarizes the results on SCAN
		template-matching tasks
	</Extractive Summary>
</Paper ID=ument438>


<Paper ID=ument438> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Test accuracy (mean ± std %) for SCAN-ADJ
		task
	</Abstractive Summary>
</Paper ID=ument438>


<Paper ID=ument438> <Table ID =5>
	<Abstractive Summary> =
		5
		Table 5: Test accuracy (mean ± std %) for few-shot
		learning task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results that the proposed
		method achieves high accuracy on both test sets
	</Extractive Summary>
</Paper ID=ument438>


<Paper ID=ument438> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6: Test accuracy (%) for machine translation task
	</Abstractive Summary>
</Paper ID=ument438>


<Paper ID=ument438> <Table ID =7>
	<Abstractive Summary> =
		4
		Table 7: Ablation: Test accuracy (mean ± std %) for
		SCAN Primitive tasks
	</Abstractive Summary>
</Paper ID=ument438>


<Paper ID=ument438> <Table ID =8>
	<Abstractive Summary> =
		1
		Table 8: Test accuracy (mean ± std %) for other SCAN
		tasks
	</Abstractive Summary>
	<Extractive Summary> =
		The re-
		sults in Table 8 conﬁrms that the approach does not
		signiﬁcantly reduce performance in other tasks
	</Extractive Summary>
</Paper ID=ument438>


<Paper ID=ument438> <Table ID =9>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Please see Table 9 in Appendix for more details
	</Extractive Summary>
</Paper ID=ument438>


<Paper ID=ument438> <Table ID =10>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Further, we ﬁnd that other pre-
		dicted outputs are even correct translations though
		they are different from references (Table 10 in Ap-
		pendix)
	</Extractive Summary>
</Paper ID=ument438>


<Paper ID=ument439> <Table ID =1>
	<Abstractive Summary> =
		7%
		Table 1: Evaluation of trained models on all test sets
	</Abstractive Summary>
</Paper ID=ument439>


<Paper ID=ument44> <Table ID =1>
	<Abstractive Summary> =
		com/competition/
		zhihu/data/
		470
		Table 1: Summary of Experimental Datasets
	</Abstractive Summary>
</Paper ID=ument44>


<Paper ID=ument44> <Table ID =2>
	<Abstractive Summary> =
		54%
		Table 2: Comparing LSAN with ﬁve baselines in terms of P@K (K=1,3,5)on four benchmark datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 show the averaged
		performance of all test documents
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2 and 3, we can make a number
		of observations about these results
	</Extractive Summary>
</Paper ID=ument44>


<Paper ID=ument44> <Table ID =3>
	<Abstractive Summary> =
		36%
		Table 3: Comparing LSAN with ﬁve baselines in terms of nDCG@K (K=3,5) on four benchmark datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 show the averaged
		performance of all test documents
	</Extractive Summary>
</Paper ID=ument44>


<Paper ID=ument440> <Table ID =1>
	<Abstractive Summary> =
		60
		Table 1: Performance of EDAM in contrast to baselines for discriminative attribute identiﬁcation
	</Abstractive Summary>
</Paper ID=ument440>


<Paper ID=ument440> <Table ID =2>
	<Abstractive Summary> =
		60
		EDAM gain
		27%
		26%
		25%
		48%
		17%
		30%
		Table 2: Performance comparison (recall) against a random sample of categorized triples
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 depicts the out-
		come of the evaluation of the combined model
	</Extractive Summary>
	<Extractive Summary> =
		It is important to em-
		phasize that the models are complementary at each
		dimension: the combined model (EDAM) signif-
		icantly outperforms the best individual models at
		each dimension (as it can be observed at the gain
		row on Table 2)
	</Extractive Summary>
</Paper ID=ument440>


<Paper ID=ument440> <Table ID =3>
	<Abstractive Summary> =
		02
		Table 3: Overlap between the components relative to the combined model
	</Abstractive Summary>
	<Extractive Summary> =
		In order to observe the proportion on which
		each model contributes (and conversely,
		the
		amount of redundancy for each model), we ana-
		lyze for each individual model, the pairwise over-
		lap, in terms of true positives and false positives
		(Table 3)
	</Extractive Summary>
</Paper ID=ument440>


<Paper ID=ument440> <Table ID =4>
	<Abstractive Summary> =
		09
		Table 4: Categorical model overlap breakdown, relative to all true positives identiﬁed by the combined model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 further breaks down the true positives
		for each dimension, where we can analyse the
		pairwise contribution of each model for each at-
		tribute category
	</Extractive Summary>
</Paper ID=ument440>


<Paper ID=ument441> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Results from the GLUE test server
	</Abstractive Summary>
	<Extractive Summary> =
		Although both
		strategies achieved improvement over the vanilla
		KD baseline (see Table 1), PKD-Skip performs
		slightly better than PKD-Last
	</Extractive Summary>
</Paper ID=ument441>


<Paper ID=ument441> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Performance comparison between PKD-Last and PKD-Skip on GLUE benchmark
	</Abstractive Summary>
</Paper ID=ument441>


<Paper ID=ument441> <Table ID =3>
	<Abstractive Summary> =
		78
		Table 3:
		Results on RACE test set
	</Abstractive Summary>
	<Extractive Summary> =
		Results on RACE are reported in Table 3, which
		shows that the Vanilla KD method outperforms di-
		rect ﬁne-tuning by 4
	</Extractive Summary>
</Paper ID=ument441>


<Paper ID=ument441> <Table ID =4>
	<Abstractive Summary> =
		89 (1×)
		Table 4: The number of parameters and inference time for BERT3, BERT6 and BERT12
	</Abstractive Summary>
	<Extractive Summary> =
		Results in Table 4 show that the
		proposed Patient-KD approach achieves an almost
		linear speedup, 1
	</Extractive Summary>
</Paper ID=ument441>


<Paper ID=ument441> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Performance comparison with different teacher and student models
	</Abstractive Summary>
	<Extractive Summary> =
		This can also be observed on the performance gap
		between teacher and student on RTE in Table 5,
		which also has a small training set
	</Extractive Summary>
</Paper ID=ument441>


<Paper ID=ument442> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Qualitative comparison between VAE and our proposed approach
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Generation
		Table 1 presents the results of text generation task
	</Extractive Summary>
</Paper ID=ument442>


<Paper ID=ument442> <Table ID =2>
	<Abstractive Summary> =
		1
		Datasets
		Data
		Train
		Valid
		Test
		Vocab
		Yelp13
		62522
		7773
		8671
		15K
		PTB
		42068
		3370
		3761
		10K
		Yahoo
		100K
		10K
		10K
		20K
		Table 2: Size and vocabulary size for each dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the statis-
		tics, vocabulary size, and number of samples in
		Train/Validation/Test for each dataset
	</Extractive Summary>
</Paper ID=ument442>


<Paper ID=ument442> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Variational language modeling on PTB
		Model
		NLL
		KL
		PPL
		VAE (Bowman et al
	</Abstractive Summary>
</Paper ID=ument442>


<Paper ID=ument442> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Variational language modeling on Yelp Re-
		views 13
		Model
		NLL
		KL
		PPL
		VAE (Bowman et al
	</Abstractive Summary>
</Paper ID=ument442>


<Paper ID=ument442> <Table ID =5>
	<Abstractive Summary> =
		4
		Table 5: Variational language modeling on Yahoo
		5
	</Abstractive Summary>
</Paper ID=ument442>


<Paper ID=ument443> <Table ID =1>
	<Abstractive Summary> =
		4350
		Table 1: Incorporating Positional Embedding (PE)
	</Abstractive Summary>
</Paper ID=ument443>


<Paper ID=ument443> <Table ID =2>
	<Abstractive Summary> =
		28
		Table 2: Kernel Types
	</Abstractive Summary>
</Paper ID=ument443>


<Paper ID=ument443> <Table ID =3>
	<Abstractive Summary> =
		In
		other words, previous work did not consider the
		4351
		Table 3: Order-Invariance in Attention
	</Abstractive Summary>
</Paper ID=ument443>


<Paper ID=ument443> <Table ID =4>
	<Abstractive Summary> =
		92
		Table 4: Positional Embedding in Value Function
	</Abstractive Summary>
</Paper ID=ument443>


<Paper ID=ument444> <Table ID =1>
	<Abstractive Summary> =
		4359
		Datasets
		FIGER
		OntoNotes
		BBN
		#Types
		128
		89
		47
		Max Hierarchy Depth
		2
		3
		2
		#Training
		2,690,286
		220,398
		86,078
		#Testing
		563
		9,603
		13,187
		Table 1: The statistics of entity typing datasets
	</Abstractive Summary>
</Paper ID=ument444>


<Paper ID=ument444> <Table ID =2>
	<Abstractive Summary> =
		775
		Table 2: The experimental results on ﬁne-grained entity typing datasets
	</Abstractive Summary>
</Paper ID=ument444>


<Paper ID=ument444> <Table ID =3>
	<Abstractive Summary> =
		0036
		Table 3: The robustness analysis on the FIGER dataset
	</Abstractive Summary>
</Paper ID=ument444>


<Paper ID=ument444> <Table ID =4>
	<Abstractive Summary> =
		24
		#Training
		7,769
		781,265
		#Testing
		3,019
		23,149
		Table 4: The statistics of text classiﬁcation datasets
	</Abstractive Summary>
</Paper ID=ument444>


<Paper ID=ument444> <Table ID =5>
	<Abstractive Summary> =
		864
		Table 5: The experimental results on text classiﬁcation datasets
	</Abstractive Summary>
</Paper ID=ument444>


<Paper ID=ument444> <Table ID =6>
	<Abstractive Summary> =
		0033
		Table 6: The robustness analysis on the RCV1 dataset
	</Abstractive Summary>
</Paper ID=ument444>


<Paper ID=ument445> <Table ID =1>
	<Abstractive Summary> =
		6
		Acc
		440K
		Table 1: GLUE task performance of BERT models
		with different initialization
	</Abstractive Summary>
	<Extractive Summary> =
		At the same time, Table 1 shows that ﬁne-
		tuned BERT outperforms pre-trained BERT by a
		signiﬁcant margin on all the tasks (with an aver-
		age of 35
	</Extractive Summary>
</Paper ID=ument445>


<Paper ID=ument446> <Table ID =1>
	<Abstractive Summary> =
		com/attardi/
		wikiextractor
		4379
		Language
		#Wiki Documents
		English
		5,684,240
		German
		2,201,782
		Spanish
		1,389,469
		Romanian
		387,627
		Task
		#Document Pairs
		English-German
		948,631
		English-Spanish
		836,564
		English-Romanian
		87,289
		Table 1: Statistics of Wikipedia data, including num-
		bers of documents and weakly paired documents
	</Abstractive Summary>
</Paper ID=ument446>


<Paper ID=ument446> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: BLEU scores compared with previous approaches
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2, our approach achieves BLEU s-
		core of 24
	</Extractive Summary>
</Paper ID=ument446>


<Paper ID=ument446> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Ablation study of our approach
	</Abstractive Summary>
</Paper ID=ument446>


<Paper ID=ument446> <Table ID =4>
	<Abstractive Summary> =
		75
		100,497
		84,271
		58,814
		Table 4: Number of the mined sentence pairs w
	</Abstractive Summary>
	<Extractive Summary> =
		We present the paired sentence number and the
		BLEU scores with respect to these two threshold-
		s in Table 4 and Figure 1
	</Extractive Summary>
</Paper ID=ument446>


<Paper ID=ument446> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Cases-studies of the sentence pairs mined by our approach for English-German
	</Abstractive Summary>
</Paper ID=ument446>


<Paper ID=ument447> <Table ID =1>
	<Abstractive Summary> =
		One option is to pre-train in a supervised fashion
		with human language, but even then it is found that
		the protocols diverge quickly when the agents are
		Intended message:
		2 elephants and 1 lion
		No constraints
		ﬂoopy globber
		Syntactic
		democracy is a politi-
		cal system
		Syntactic+Semantic
		a pair of elephants and
		a large feline
		Table 1: Examples of valid messages under different con-
		straints
	</Abstractive Summary>
	<Extractive Summary> =
		See
		Table 1 for an illustration of different constraints
	</Extractive Summary>
</Paper ID=ument447>


<Paper ID=ument447> <Table ID =2>
	<Abstractive Summary> =
		73
		Table 2: Results in BLEU score on Multi30k Task 1
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Quantitative Results
		In Table 2, the top three rows are the baselines
		described above
	</Extractive Summary>
</Paper ID=ument447>


<Paper ID=ument447> <Table ID =3>
	<Abstractive Summary> =
		PG+LM
		No LM
		WikiText103
		MS COCO
		Flickr30k
		All
		+G
		�
		�
		�
		�
		�
		Table 3: Using the bootstrapped Wilcoxon signed-rank test (Wilcoxon, 1945), Fr→En results of PG+LM+G are found to be
		signiﬁcantly different from its baselines in all cases considered (on all LM datasets) within the threshold of p = 0
	</Abstractive Summary>
</Paper ID=ument447>


<Paper ID=ument447> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Two communication examples on the data from the Multi30k development set with different models (PG, PG+LM,
		PG+LM+G)
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Qualitative Results
		In the ﬁrst example of Table 4 (previous page), it
		is clear that PG’s communication messages have
		signiﬁcantly diverged from English: the model is
		highly repetitive (“table table table table table”)
		and misses some key content words such as “man”
	</Extractive Summary>
</Paper ID=ument447>


<Paper ID=ument447> <Table ID =5>
	<Abstractive Summary> =
		of a and to in is i this es we for that
		you at what
		Table 5: Top 20 most frequent tokens (sorted) in English
		reference (Reference) or the output from Fr→En models
	</Abstractive Summary>
	<Extractive Summary> =
		Investigating the
		top 20 most frequent words shows that PG+LM dis-
		proportionately favors quotation marks, which are
		very common in many language modeling datasets
		but rare in Multi30k (see Table 5)
	</Extractive Summary>
</Paper ID=ument447>


<Paper ID=ument447> <Table ID =6>
	<Abstractive Summary> =
		29
		Table 6: Exact-match word recall by POS-tag on IWSLT
		development set: when the English reference contains a word
		of a certain POS tag, how often does the agent produce it
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 compares the degree of drift by part-of-
		speech, and shows that the PG model has very low
		recall on function words, such as periods and in-
		ﬁnitives
	</Extractive Summary>
</Paper ID=ument447>


<Paper ID=ument447> <Table ID =7>
	<Abstractive Summary> =
		84
		Table 7: Additional token frequency analysis
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 corroborates the ﬁnding that vanilla PG
		ﬁne-tuning leads to ﬂatter token frequency distri-
		butions, as the number of unique tokens used by
		PG is greater than that of the pretrained model
	</Extractive Summary>
</Paper ID=ument447>


<Paper ID=ument447> <Table ID =8>
	<Abstractive Summary> =
		Table 8: Evidence of token ﬂipping
	</Abstractive Summary>
	<Extractive Summary> =
		For example, one par-
		ticular model uses “punk” to describe “child” (see
		Table 8)
	</Extractive Summary>
</Paper ID=ument447>


<Paper ID=ument449> <Table ID =1>
	<Abstractive Summary> =
		, 2018b) w/o dropout
		S1-S4 (FULL)
		FULL+SL+SYM
		provided
		symmetric: mutual nearest neighbours, w/o dropout
		S1-S4 (FULL)
		Table 1: Conﬁgurations obtained by varying components C1, C2, and C3 used in our empirical comparison in §4
	</Abstractive Summary>
	<Extractive Summary> =
		Again, Table 1 lists the main model conﬁgura-
		tions in our comparison
	</Extractive Summary>
	<Extractive Summary> =
		See Table 1 for the brief description of all model conﬁgurations in the
		comparison
	</Extractive Summary>
</Paper ID=ument449>


<Paper ID=ument449> <Table ID =2>
	<Abstractive Summary> =
		Language
		Family
		Type
		ISO 639-1
		Bulgarian
		IE: Slavic
		fusional
		BG
		Catalan
		IE: Romance
		fusional
		CA
		Esperanto
		– (constructed)
		agglutinative
		EO
		Estonian
		Uralic
		agglutinative
		ET
		Basque
		– (isolate)
		agglutinative
		EU
		Finnish
		Uralic
		agglutinative
		FI
		Hebrew
		Afro-Asiatic
		introﬂexive
		HE
		Hungarian
		Uralic
		agglutinative
		HU
		Indonesian
		Austronesian
		isolating
		ID
		Georgian
		Kartvelian
		agglutinative
		KA
		Korean
		Koreanic
		agglutinative
		KO
		Lithuanian
		IE: Baltic
		fusional
		LT
		Bokmål
		IE: Germanic
		fusional
		NO
		Thai
		Kra-Dai
		isolating
		TH
		Turkish
		Turkic
		agglutinative
		TR
		Table 2:
		The list of 15 languages from our main
		BLI experiments along with their corresponding lan-
		guage family (IE = Indo-European), broad morpholog-
		ical type, and their ISO 639-1 code
	</Abstractive Summary>
	<Extractive Summary> =
		The ﬁnal list of 15 diverse test languages
		is provided in Table 2, and includes samples from
		different languages types and families
	</Extractive Summary>
	<Extractive Summary> =
		, CA-* means that the translation direction is from Catalan (CA)
		as source (L1) to each of the remaining 14 languages listed in Table 2 as targets (L2), and we average over the
		corresponding 14 CA-* BLI setups
	</Extractive Summary>
	<Extractive Summary> =
		A substantial
		number of unsuccessful setups is also observed
		with other two language outliers from our set (see
		Table 2 again), Georgian and Indonesian, as well
		as with morphologically-rich languages such as
		Estonian or Turkish
	</Extractive Summary>
</Paper ID=ument449>


<Paper ID=ument449> <Table ID =3>
	<Abstractive Summary> =
		274
		Table 3: BLI scores (MRR) for all model conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results and Discussion
		Main BLI results averaged over each source lan-
		guage (L1) are provided in Table 3 and Table 4
	</Extractive Summary>
	<Extractive Summary> =
		This property is reﬂected in the results
		reported in Table 3, the number of unsuccessful
		setups in Table 4, as well as later in Figure 4
	</Extractive Summary>
</Paper ID=ument449>


<Paper ID=ument449> <Table ID =4>
	<Abstractive Summary> =
		Win
		UNSUPERVISED
		87 (94)
		0
		87 (94)
		0
		ORTHG-SUPER
		0 (2)
		0
		2 (82)
		0
		ORTHG+SL+SYM
		0 (1)
		0
		1 (34)
		0
		FULL-SUPER
		0 (0)
		46
		0 (41)
		0
		FULL+SL
		0 (7)
		0
		0 (9)
		0
		FULL+SL+NOD
		0 (1)
		7
		0 (3)
		33
		FULL+SL+SYM
		0 (0)
		157
		0 (0)
		177
		Table 4:
		Summary statistics computed over all
		15×14=210 BLI setups
	</Abstractive Summary>
	<Extractive Summary> =
		The scores also reveal that the choice of
		self-learning (C2) does matter: all best performing
		BLI runs with |D0| = 1k are obtained by two con-
		ﬁgs with self-learning, and FULL+SL+SYM is the
		best conﬁguration for 177/210 setups (see Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		This property is reﬂected in the results
		reported in Table 3, the number of unsuccessful
		setups in Table 4, as well as later in Figure 4
	</Extractive Summary>
	<Extractive Summary> =
		For instance,
		the underlying assumption of all projection-based
		methods (both supervised and unsupervised) is the
		topological similarity between monolingual spaces,
		which is why standard simple linear projections
		result in lower absolute BLI scores for distant pairs
		(see Table 4 and results in the supplemental ma-
		terial)
	</Extractive Summary>
</Paper ID=ument449>


<Paper ID=ument449> <Table ID =5>
	<Abstractive Summary> =
		109
		Table 5: Results for a selection of BLI setups which were unsuccessful with the UNSUPERVIED CLWE method
	</Abstractive Summary>
</Paper ID=ument449>


<Paper ID=ument45> <Table ID =1>
	<Abstractive Summary> =
		layer name
		kernel size
		stride
		output size
		pool
		T × 1
		1 × 1
		K × d × 1
		conv 1
		K × 1
		1 × 1
		K × d × 32
		ReLU
		conv 2
		K × 1
		1 × 1
		K × d × 64
		ReLU
		conv 3
		K × 1
		K × 1
		1 × d × 1
		ReLU
		Table 1: Class feature extractor architecture
		So we get a new distance calculation method as
		follow
		d(ci,q
		′) = (ci − q
		′)2 ⋅ λi
		(11)
		where q
		′ is the query vector passed through the
		word level attention mechanism which will be in-
		troduced in the next subsection
	</Abstractive Summary>
</Paper ID=ument45>


<Paper ID=ument45> <Table ID =2>
	<Abstractive Summary> =
		13
		Table 2: Accuracies (%) of different models on the CSID dataset on four different settings
	</Abstractive Summary>
</Paper ID=ument45>


<Paper ID=ument45> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Visualization of word level and instance level multi cross attention scores (IMCAS) for 2 way 3 shot
		setting, the bold words are head entities and tail entities
	</Abstractive Summary>
</Paper ID=ument45>


<Paper ID=ument45> <Table ID =4>
	<Abstractive Summary> =
		18
		Table 4: Accuracies (%) for 5 way 5 shot and 10 way
		5 shot settings on FewRel test set
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results and analysis
		The experimental accuracies on CSID and FewRel
		are shown in Tabel 2 and Table 4 respectively
	</Extractive Summary>
</Paper ID=ument45>


<Paper ID=ument450> <Table ID =1>
	<Abstractive Summary> =
		35M
		Table 1: Statistics of Wikipedia concept-aligned arti-
		cles for our experimentations (M=millions)
	</Abstractive Summary>
	<Extractive Summary> =
		Sampling Procedure
		As the statistics show in
		Table 1, even after ﬁltering, the vocabulary of
		concept-aligned articles is still large
	</Extractive Summary>
	<Extractive Summary> =
		Following the trend of per-
		formance change shown in Figure 3, we see that,
		when the number of shared concepts reaches 2500,
		the improvement in accuracy is already very close
		to the result of the model trained from the total
		number of concepts (reported in Table 1), thus in-
		dicating a direction for future optimization
	</Extractive Summary>
</Paper ID=ument450>


<Paper ID=ument450> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Results of bilingual lexicon induction (accuracy % P@1) for similar and distant language pairs on the
		dataset BLI-1
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results and Discussion
		Table 2 summarizes the results of bilingual lexi-
		con induction on the BLI-1 dataset
	</Extractive Summary>
	<Extractive Summary> =
		The
		results from Table 2 clearly conﬁrm this
	</Extractive Summary>
</Paper ID=ument450>


<Paper ID=ument450> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Results of bilingual lexicon induction (accu-
		racy % P@1) on BLI-2 dataset, all the results of previ-
		ous methods come from (Artetxe et al
	</Abstractive Summary>
	<Extractive Summary> =
		Comparison with the state-of-the-art
		From
		the results shown in Table 3, we can see that in
		most cases, our method works better than previous
		supervised and unsupervised approaches
	</Extractive Summary>
</Paper ID=ument450>


<Paper ID=ument450> <Table ID =4>
	<Abstractive Summary> =
		As far
		as we know, Miceli Barone (2016) was the ﬁrst
		4427
		MUSE
		Our Method
		English to Chinese
		ﬁlm
		电
		电
		电影
		影
		影(ﬁlm), 该片(this ﬁlm),
		本片(this ﬁlm)
		电
		电
		电影
		影
		影(ﬁlm), 影
		影
		影片
		片
		片(ﬁlm),
		本片(this ﬁlm)
		debate
		辩
		辩
		辩论
		论
		论(debate), 争论(dispute),
		讨论(discuss)
		辩
		辩
		辩论
		论
		论(debate), 争论(dispute),
		议题(topic)
		battery
		火炮(artillery), 炮(cannon),
		装甲车辆(armored car)
		电
		电
		电池
		池
		池(battery), 锂电池(lithium battery),
		控制器(controller)
		English to French
		january
		novembre(november), f´evrier(february),
		janvier(january)
		janvier(january), f´evrier(february),
		septembre(september)
		author
		auteur(author), ´ecrivain(writer),
		romancier(novelist)
		auteur(author), ´ecrivain(writer),
		romancier(novelist)
		polytechnic
		universit´e(university),
		polytechnique(polytechnic),
		coll`ege(high school)
		universit´e(university),
		polytechnique(polytechnic),
		facult´e(faculty)
		Table 4: Top 3 English to Chinese and English to French translation candidates selected form our bilingual lexicon
		induction experiment on the dataset of BLI-1
	</Abstractive Summary>
	<Extractive Summary> =
		Mapping examples
		Table 4 lists some examples
		of mapping, randomly selected from our experi-
		ment results on the dataset of BLI-1
	</Extractive Summary>
</Paper ID=ument450>


<Paper ID=ument451> <Table ID =1>
	<Abstractive Summary> =
		triples
		ZH-EN
		Chinese
		106,517
		4,431
		16,152
		329,890
		1,404,615
		English
		185,022
		3,519
		14,459
		453,248
		1,902,725
		JA-EN
		Japanese
		117,836
		2,888
		12,305
		413,558
		1,474,721
		English
		118,570
		2,631
		13,238
		494,087
		1,738,803
		FR-EN
		French
		105,724
		1,775
		8,029
		409,399
		1,361,509
		English
		107,231
		2,504
		13,170
		513,382
		1,957,813
		Table 1: Statistics of DBP15K and DBP100K
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 outlines the statistics of both
		datasets, which contain 15,000 and 100,000 ILLs,
		respectively
	</Extractive Summary>
</Paper ID=ument451>


<Paper ID=ument451> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Results of using graph information on DBP15K and DBP100K
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 2, MAN and HMAN consistently outperform
		all baselines in all scenarios, especially HMAN
	</Extractive Summary>
	<Extractive Summary> =
		As
		reported in Table 2, we observe that after removing
		relation or attribute features, the performance of
		HMAN and MAN drops across all datasets
	</Extractive Summary>
</Paper ID=ument451>


<Paper ID=ument451> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Results of using both graph and textual information on DBP15K and DBP100K
	</Abstractive Summary>
</Paper ID=ument451>


<Paper ID=ument451> <Table ID =4>
	<Abstractive Summary> =
		For those entities whose descriptions
		English
		Chinese
		ILL pair
		Casino Royale (2006 ﬁlm) (3)
		007大戰皇家賭場 (3)
		Features
		starring, starring, distributor
		starring, starring, language
		Neighbors Daniel Craig (1), Eva Green (4),
		Columbia Pictures (9)
		丹尼爾·克雷格 (1), 伊娃·格
		蓮 (4), 英語 (832)
		Table 4: Case study of the noise introduced by the prop-
		agation mechanism
	</Abstractive Summary>
	<Extractive Summary> =
		The example in Table 4 provides insights
		potentially explaining this performance gap
	</Extractive Summary>
</Paper ID=ument451>


<Paper ID=ument452> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Cross-lingual stance detection model
	</Abstractive Summary>
	<Extractive Summary> =
		As Figure 1 shows, each source and tar-
		get pairs are passed to the source and to the target
		memory networks to obtain their corresponding
		representations (Lines 3-4 in Table 1)
	</Extractive Summary>
</Paper ID=ument452>


<Paper ID=ument452> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Evaluation results on the target Arabic test dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the performance of all
		models on the target Arabic test set
	</Extractive Summary>
	<Extractive Summary> =
		Row 10 in Table 2 shows the results for adver-
		sarial memory network (ADMN)
	</Extractive Summary>
	<Extractive Summary> =
		The last column in Table 2 shows that unrelated
		examples are the easiest ones
	</Extractive Summary>
	<Extractive Summary> =
		2
		Assessment of Model Transferability
		The improvements of CLMN model over the mono-
		lingual MN models that use the target only, the
		source only, or both the target and the source (rows
		7–9 in Table 2 respectively) indicate its transfer-
		ability
	</Extractive Summary>
</Paper ID=ument452>


<Paper ID=ument452> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: CLMN results on the target test dataset
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Effect of Pretraining
		Table 3 shows CLMN without pretraining (α =
	</Extractive Summary>
</Paper ID=ument452>


<Paper ID=ument453> <Table ID =1>
	<Abstractive Summary> =
		Table 1:
		Number of sentences for three datasets:
		German→English
		(DeEn),
		Romanian→English
		(RoEn) and English→French (EnFr)
	</Abstractive Summary>
</Paper ID=ument453>


<Paper ID=ument453> <Table ID =2>
	<Abstractive Summary> =
		com/moses-smt/mgiza/
		4458
		Table 2:
		AER
		[%] per layer for all three language
		pairs: German→English (DeEn), Romanian→English
		(RoEn) and English→French (EnFr)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2,
		all three language pairs exhibit a very similar pat-
		tern, wherein the attentions do not seem to learn
		meaningful alignments in the initial layers and
		show a remarkable improvement in the higher lay-
		ers
	</Extractive Summary>
</Paper ID=ument453>


<Paper ID=ument453> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Results on the alignment task (in AER
		[%])
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Alignment Task Results
		Table 3 compares the performance of our meth-
		ods against statistical baselines and previous neu-
		ral approaches
	</Extractive Summary>
</Paper ID=ument453>


<Paper ID=ument453> <Table ID =4>
	<Abstractive Summary> =
		4459
		Table 4: Results on the align and translate task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 summarizes the
		results on alignment and translation tasks
	</Extractive Summary>
	<Extractive Summary> =
		Table 4)
	</Extractive Summary>
</Paper ID=ument453>


<Paper ID=ument454> <Table ID =1>
	<Abstractive Summary> =
		47
		Table 1: Data statistics for SOCIAL IQA
	</Abstractive Summary>
	<Extractive Summary> =
		Shown in Table 1 (top), this yields a
		8Agreement on this task was high (Cohen’s κ=
	</Extractive Summary>
</Paper ID=ument454>


<Paper ID=ument454> <Table ID =2>
	<Abstractive Summary> =
		4*
		Table 2: Experimental results
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Our results (Table 2) show that SOCIAL IQA is
		still a challenging benchmark for existing com-
		putational models, compared to human perfor-
		mance
	</Extractive Summary>
	<Extractive Summary> =
		1
		Sequential Finetuning
		We ﬁrst ﬁnetune BERT-large on SOCIAL IQA,
		which reaches 66% on our dev set (Table 2)
	</Extractive Summary>
</Paper ID=ument454>


<Paper ID=ument454> <Table ID =3>
	<Abstractive Summary> =
		How would
		Alex feel as a
		result?
		(a) they need to practice more
		(b) ashamed
		✓
		(c) boastful
		Table 3: Example CQA triples from the SOCIAL IQA dev set with BERT-large’s predictions (
		: BERT’s predic-
		tion, ✓: true correct answer)
	</Abstractive Summary>
	<Extractive Summary> =
		Examples of errors in Table 3 further indicate
		that, instead of doing advanced reasoning about
		situations, models may only be learning lexical as-
		sociations between the context, question, and an-
		swers, as hinted at by Marcus (2018) and Zellers
		et al
	</Extractive Summary>
</Paper ID=ument454>


<Paper ID=ument454> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Sequential ﬁnetuning of BERT-large on SO-
		CIAL IQA before the task yields state of the art results
		(bolded) on COPA (Roemmele et al
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Shown in Table 4, sequential ﬁnetun-
		ing on SOCIAL IQA yields substantial improve-
		ments over the BERT-only baseline (between 2
	</Extractive Summary>
</Paper ID=ument454>


<Paper ID=ument455> <Table ID =1>
	<Abstractive Summary> =
		4480
		ORIGINAL
		MUTATED
		question
		answer
		question
		answer
		Were Pavel Urysohn and Levin known
		No
		Were Henry Cavill and Li Na known
		Yes
		for the same type of work?
		for the same type of work?
		Were Pavel Urysohn and Levin known
		No
		Were Pavel Urysohn and Levin known
		Yes
		for the same type of work?
		for the different type of work?
		Is Rohan Bopanna older
		Yes
		Is Rohan Bopanna younger
		No
		than Sherwood Stewart?
		than Sherwood Stewart?
		Was Howard Hawks a screenwriter of more
		Yes
		Was Arthur Berthelet a screenwriter of more
		No
		productions than Arthur Berthelet?
		productions than Howard Hawks?
		Table 1: Examples of mutated comparison-type questions and answers from HotpotQA training set
	</Abstractive Summary>
	<Extractive Summary> =
		For each
		question whose answer is either “yes” or “no”, we
		generate a new question by randomly sampling
		two titles of the same type (based on POS and
		NER) from the training set to substitute the
		original entities in the question and corresponding
		supporting documents (1st row in Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		First, if the question contains the word “same” and
		the answer is yes or no, we substitute “same” with
		“different” and vice versa (2nd row in Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Second, we detect the comparative and superlative
		adjectives/adverbs in the original question, trans-
		form them into their antonyms using wordnet,
		and then transform the antonyms back to their
		comparative form (3rd row in Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		, we ﬂip
		the order of the two entities compared (4th row
		in Table 1)
	</Extractive Summary>
</Paper ID=ument455>


<Paper ID=ument455> <Table ID =2>
	<Abstractive Summary> =
		10
		Table 2: EM and F1 scores on HotpotQA dev set and
		test set
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in the ﬁrst three rows
		of Table 2, our modular network achieves signiﬁ-
		cant improvements over both the baseline and the
		convolution-based NMN (Hu et al
	</Extractive Summary>
	<Extractive Summary> =
		beneﬁcial for the modular network to achieve
		good performance (row 5 in Table 2 and Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		As shown
		in Table 2 and Table 3, our adapted modular net-
		work outperforms the original NMN signiﬁcantly
	</Extractive Summary>
</Paper ID=ument455>


<Paper ID=ument455> <Table ID =3>
	<Abstractive Summary> =
		26
		Table 3:
		EM and F1 scores on bridge-type and
		comparison-type questions from HotpotQA dev set
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we further break down the dev-
		set performance on different question types,4 and
		our modular network obtains higher scores in both
		question types compared to the BiDAF baseline
	</Extractive Summary>
	<Extractive Summary> =
		Augmenting
		the comparison-type questions in the training set
		boosts the performance on the comparison-type
		questions in the dev set (comparing row 3 and 4 in
		Table 3) without harming the scores on the bridge-
		type questions too much
	</Extractive Summary>
	<Extractive Summary> =
		beneﬁcial for the modular network to achieve
		good performance (row 5 in Table 2 and Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		As shown
		in Table 2 and Table 3, our adapted modular net-
		work outperforms the original NMN signiﬁcantly
	</Extractive Summary>
</Paper ID=ument455>


<Paper ID=ument455> <Table ID =4>
	<Abstractive Summary> =
		25
		Table 4: EM scores after training on the regular data or
		on the adversarial data from Jiang and Bansal (2019),
		and evaluation on the regular dev set or the adv-dev set
	</Abstractive Summary>
	<Extractive Summary> =
		The second col-
		umn of Table 4 shows that our NMN outperforms
		the baseline signiﬁcantly (+10 points in EM score)
		on the adversarial evaluation, suggesting that our
		NMN is indeed learning stronger compositional
		reasoning skills compared to the BiDAF baseline
	</Extractive Summary>
</Paper ID=ument455>


<Paper ID=ument456> <Table ID =1>
	<Abstractive Summary> =
		Train
		Test
		T ⇒ ε F
		T ⇒ ¬ T
		T ⇒ ¬ F
		T ⇒ ε T
		F ⇒ ¬ T
		F ⇒ ¬ F
		F ⇒ ε T
		F ⇒ ε F
		Table 1: A fair train/test split for the evaluation prob-
		lem deﬁned by the composition tree in Figure 1
	</Abstractive Summary>
</Paper ID=ument456>


<Paper ID=ument456> <Table ID =2>
	<Abstractive Summary> =
		An example fair
		symbol
		example
		set theoretic deﬁnition
		x ≡ y
		couch ≡ sofa
		x = y
		x ⊏ y
		crow ⊏ bird
		x ⊂ y
		x ⊐ y
		bird ⊐ crow
		x ⊃ y
		x ∧ y
		human ∧ nonhuman
		x ∩ y = ∅ ∧ x ∪ y = U
		x | y
		cat | dog
		x ∩ y = ∅ ∧ x ∪ y ̸= U
		x ⌣ y
		animal ⌣ nonhuman
		x ∩ y ̸= ∅ ∧ x ∪ y = U
		x # y
		hungry # hippo
		(all other cases)
		Table 2: The seven basic semantic relations of Mac-
		Cartney and Manning (2009): B = {# = independence,
		⊏ = entailment, ⊐ = reverse entailment, | = alternation,
		⌣ = cover, ∧ = negation, ≡ = equivalence}
	</Abstractive Summary>
</Paper ID=ument456>


<Paper ID=ument456> <Table ID =3>
	<Abstractive Summary> =
		71
		Table 3: Mean accuracy of 5 runs on our difﬁcult but fair generalization task, with standard 95% conﬁdence
		intervals
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results and Analysis
		Table 3 summarizes our ﬁndings on the hardest
		of our fair generalization tasks, where the train-
		ing sets are minimal ones required for fairness
	</Extractive Summary>
</Paper ID=ument456>


<Paper ID=ument457> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Results on the dependency task (test set)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 reports results of all models on the new
		dependency task
	</Extractive Summary>
</Paper ID=ument457>


<Paper ID=ument457> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Eﬀect of ablating gedge and gkb from XPAD
		(test set)
	</Abstractive Summary>
	<Extractive Summary> =
		Impact of XPAD components:
		Table 2 shows the impact of removing dependency
		graph scores from XPAD
	</Extractive Summary>
</Paper ID=ument457>


<Paper ID=ument457> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Results on the state-change task (test set),
		comparing with the best published prior result
	</Abstractive Summary>
	<Extractive Summary> =
		7
		points F1) than the best published state-of-the-art
		system ProStruct3, even though Equation 1 is not
		optimized solely for that task (Table 3)
	</Extractive Summary>
</Paper ID=ument457>


<Paper ID=ument458> <Table ID =1>
	<Abstractive Summary> =
		Q: What is [Brett] and [Jim]’s relationship?
		Thus, instead of simply collecting paraphrases for a
		ﬁxed number of stories, we instead obtain a diverse
		4510
		Table 1: Statistics of the AMT paraphrases
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 contains sum-
		mary statistics of the collected paraphrases
	</Extractive Summary>
</Paper ID=ument458>


<Paper ID=ument458> <Table ID =2>
	<Abstractive Summary> =
		To further conﬁrm this trend, we ran experiments
		4513
		Table 2: Testing the robustness of the various models when training and testing on stories containing various types
		of noise facts
	</Abstractive Summary>
	<Extractive Summary> =
		In all the
		experiments we provide a combination of k = 2
		and k = 3 length clauses in training and testing,
		with noise facts being added to the train and/or test
		set depending on the setting (Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Overall, we ﬁnd that the GAT baseline outper-
		forms the unstructured text-based models across
		most testing scenarios (Table 2), which showcases
		the beneﬁt of a structured feature space for robust
		reasoning
	</Extractive Summary>
	<Extractive Summary> =
		Moreover, when we trained on noisy examples, we
		found that only the GAT model was able to consis-
		tently improve its performance (Table 2)
	</Extractive Summary>
</Paper ID=ument458>


<Paper ID=ument459> <Table ID =1>
	<Abstractive Summary> =
		02
		Table 1: Statistics comparison: Self-dialogs vs Multi-
		WOZ corpus both containing approximately 10k dia-
		logues each
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that Taskmaster-1 has more
		unique words and is more difﬁcult for language
		models to ﬁt
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows that
		our dataset has more unique words, and has al-
		most twice the number of utterances per dialog
		than the MultiWOZ corpus
	</Extractive Summary>
	<Extractive Summary> =
		Finally, Table 1 also
		shows that our dataset contains close to 10 times
		more real-world named entities than MultiWOZ
		and thus, could potentially serve as a realistic base-
		line when designing goal oriented dialog systems
	</Extractive Summary>
</Paper ID=ument459>


<Paper ID=ument459> <Table ID =2>
	<Abstractive Summary> =
		09
		Table 2: Statistics comparison: Self-dialogs vs two
		person corpus both containing 5k dialogs
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2, we ﬁnd
		that self-dialogs exhibit higher perplexity ( almost
		3 times) compared to the two-person conversa-
		tions suggesting that self-dialogs are more diverse
		and contains more non-conventional conversational
		ﬂows which is inline with the observations in
		Section-3
	</Extractive Summary>
</Paper ID=ument459>


<Paper ID=ument459> <Table ID =3>
	<Abstractive Summary> =
		22
		1
		Table 3: Evaluation of various seq2seq architectures
		(Sutskever et al
	</Abstractive Summary>
	<Extractive Summary> =
		We evaluate all the models with perplexity and
		BLEU scores (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		From Table 3, we see that Trans-
		former is the best performing model on automatic
		evaluation metrics
	</Extractive Summary>
</Paper ID=ument459>


<Paper ID=ument459> <Table ID =4>
	<Abstractive Summary> =
		29
		Table 4: Inter-Annotator Reliability scores of seq2seq
		model responses computed for 500 self-dialogs from
		the test set, each annotated by 3 crowdsourced workers
	</Abstractive Summary>
</Paper ID=ument459>


<Paper ID=ument459> <Table ID =5>
	<Abstractive Summary> =
		79
		Table 5: API Argument prediction accuracy for Self-
		dialogs
	</Abstractive Summary>
</Paper ID=ument459>


<Paper ID=ument46> <Table ID =1>
	<Abstractive Summary> =
		,
		ingredients,
		rela-
		tively,
		quick,
		places,
		enough,
		dinner,
		typi-
		cally, me, i
		LIME
		the, dinner, be, quick,
		and, even, you, always,
		fresh, favorite
		you, to, fresh, quick,
		at, can, even, always,
		and, favorite
		dinner, ingredients, typ-
		ically,
		fresh,
		places,
		cause, quick, and, fa-
		vorite, always
		one, watch, to, enough,
		limited,
		cause,
		and,
		fresh, hot, favorite
		Table 1: 10 most important features (separated by comma) identiﬁed by different methods for different models for
		the given review
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows important features for a Yelp
		review in sentiment classiﬁcation
	</Extractive Summary>
</Paper ID=ument46>


<Paper ID=ument46> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Accuracy on the test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the accuracy
		on the test set and our results are comparable to
		prior work
	</Extractive Summary>
</Paper ID=ument46>


<Paper ID=ument460> <Table ID =1>
	<Abstractive Summary> =
		IC = { contentonly },
		SL = { Conﬁrmation Number : AMZ685 }
		· · ·
		· · ·
		· · ·
		Table 1: A segment of a dialogue from the airline domain annotated at the turn level
	</Abstractive Summary>
</Paper ID=ument460>


<Paper ID=ument460> <Table ID =2>
	<Abstractive Summary> =
		695
		Table 2: Dialogue act (DA), Intent class (IC), and
		slot labeling (SL) Inter Source Annotation Agreement
		(ISAA) scores quantifying the agreement of crowd
		sourced and professional annotations
	</Abstractive Summary>
</Paper ID=ument460>


<Paper ID=ument460> <Table ID =3>
	<Abstractive Summary> =
		Pre-processing:
		We pre-process the corpus of
		dialogues for each domain to remove duplicate
		4532
		Domain
		Elicited
		Good/Excellent
		IC/SL
		DA/IC/SL
		Airline
		15100
		14205
		7598
		6287
		Fast Food
		9639
		8674
		7712
		4507
		Finance
		8814
		8160
		8002
		6704
		Insurance
		14262
		13400
		7799
		7434
		Media
		33321
		32231
		19877
		12891
		Software
		5562
		4924
		3830
		2753
		Total
		86698
		81594
		54818
		40576
		Table 3: Total number of conversations per domain: raw conversations Elicited; Good/Excellent is the total number
		of conversations rated as such by the agent annotators; (IC/SL) is the number of conversations annotated for Intent
		Classes and Slot Labels only; (DA/IC/SL) is the total number of conversations annotated for Dialogue Acts, Intent
		Classes, and Slot Labels
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3
		shows the statistics for MultiDoGO raw conver-
		sations harvested, rated as Excellent or Good, and
		annotated for DA, IC and SL
	</Extractive Summary>
</Paper ID=ument460>


<Paper ID=ument460> <Table ID =4>
	<Abstractive Summary> =
		Bias
		Airlines
		Fast Food
		Finance
		Insurance
		Media
		Software
		IntentChange
		1443
		MultiIntent
		2200
		1913
		1799
		1061
		607
		2295
		MultiValue
		354
		Overﬁll
		1486
		2763
		SlotChange
		4207
		2011
		2506
		3321
		570
		2085
		SlotDeletion
		333
		Total
		6407
		6054
		5791
		7145
		1177
		4380
		Table 4: Number of conversations per domain collected with speciﬁc biases
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the number of conversations per
		domain reﬂecting the speciﬁc biases used
	</Extractive Summary>
	<Extractive Summary> =
		Recall that Fastfood had
		the most diverse dialogues (biases) as per Table 4
		and the lowest IAA as per Table 6
	</Extractive Summary>
</Paper ID=ument460>


<Paper ID=ument460> <Table ID =5>
	<Abstractive Summary> =
		16
		Total Unique Tokens
		986
		2,142
		12,043
		2,842
		1,008
		24,071
		70,003
		Number of Unique Slots
		8
		4
		61
		13
		14
		25
		73
		Number of Slot Values
		212
		99
		3,871
		1,363
		138
		4,510
		55,816
		Number of Domains
		1
		1
		1
		3
		1
		7
		6
		Number of Tasks
		1
		1
		1
		2
		2
		2
		3
		Table 5: MultiDoGO is several times larger in nearly every dimension to the pertinent datasets as selected by
		Budzianowski et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 illustrates a comparative statistics to exist-
		ing data sets
	</Extractive Summary>
</Paper ID=ument460>


<Paper ID=ument460> <Table ID =6>
	<Abstractive Summary> =
		698
		Table 6: Data statistics by domain
	</Abstractive Summary>
</Paper ID=ument460>


<Paper ID=ument460> <Table ID =7>
	<Abstractive Summary> =
		48
		Table 7: Dialogue act (DA), Intent class (IC), and slot labeling (SL) F1 scores by domain for the majority class,
		LSTM, and ELMO baselines on data annotated at the sentence (S) and turn (T) level
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 presents the MFC,
		LSTM, and ELMO results for each domain, on
		the subset of 15,000 conversations annotated at
		both the turn and sentence levels
	</Extractive Summary>
</Paper ID=ument460>


<Paper ID=ument460> <Table ID =8>
	<Abstractive Summary> =
		76
		Table 8: Joint training of ELMo on all agent DA data leads to a slight increase in test performance
	</Abstractive Summary>
</Paper ID=ument460>


<Paper ID=ument461> <Table ID =1>
	<Abstractive Summary> =
		1%
		Total
		114656
		15958
		15957
		Table 1: Dataset statistics for our splits of Wikipedia
		Toxic Comments
	</Abstractive Summary>
	<Extractive Summary> =
		be found in Table 11 in the appendix
	</Extractive Summary>
	<Extractive Summary> =
		2
		Fix it Phase
		Results comparing the performance of models
		trained on the adversarial (Ai) and standard (Si)
		tasks are summarized in Table 6, with further re-
		sults in Table 13 in Appendix A
	</Extractive Summary>
	<Extractive Summary> =
		The
		results of these experiments are given in Table 10
	</Extractive Summary>
	<Extractive Summary> =
		8
		Table 10: Results of experiments on the multi-turn ad-
		versarial task
	</Extractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =2>
	<Abstractive Summary> =
		4%
		Table 2: Comparison between our models based on
		fastText and BERT with the BiLSTM used by (Khatri
		et al
	</Abstractive Summary>
	<Extractive Summary> =
		Results are listed in Table 2 and we com-
		pare them using the weighted-F1, i
	</Extractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =3>
	<Abstractive Summary> =
		4541
		Single-Turn Adversarial (Round 1) and Standard Task OFFENSIVE Examples
		contains
		non-profane
		contains
		contains
		requires
		contains
		profanity
		offending words
		negation
		ﬁgurative language
		world knowledge
		sarcasm
		Standard
		13%
		12%
		12%
		11%
		8%
		3%
		Adversarial
		0%
		5%
		23%
		19%
		14%
		6%
		Table 3: Language analysis of the single-turn standard and adversarial (round 1) tasks by human annotation of
		various language properties
	</Abstractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Percent of OFFENSIVE examples in each task
		containing profanity, the token “not”, as well as the av-
		erage number of characters and tokens in each exam-
		ple
	</Abstractive Summary>
	<Extractive Summary> =
		Using a
		list of common English obscenities and otherwise
		bad words2, in Table 4 we calculate the percentage
		of examples in each task containing such obscen-
		ities, and see that the standard examples contain
		2https://github
	</Extractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =5>
	<Abstractive Summary> =
		Rounds {1, 2 and 3}
		Train
		Valid
		Test
		SAFE Examples
		21,600
		2700
		2700
		OFFENSIVE Examples
		2400
		300
		300
		Total Examples
		24,000
		3,000
		3,000
		Table 5: Dataset statistics for the single-turn rounds of
		the adversarial task data collection
	</Abstractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =6>
	<Abstractive Summary> =
		6
		Table 6: Test performance of best standard models trained on standard task rounds (models Si for each round
		i) and best adversarial models trained on adversarial task rounds (models Ai)
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Fix it Phase
		Results comparing the performance of models
		trained on the adversarial (Ai) and standard (Si)
		tasks are summarized in Table 6, with further re-
		sults in Table 13 in Appendix A
	</Extractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =7>
	<Abstractive Summary> =
		89
		Table 7: Adversarial data collection worker scores
	</Abstractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =8>
	<Abstractive Summary> =
		i work at a gas station
		OFFENSIVE Response
		[P2:] i need to follow my dream to not work in a gas station
		Table 8: Examples from the multi-turn adversarial task
	</Abstractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =9>
	<Abstractive Summary> =
		Multi-Turn Adversarial Task
		Train
		Valid
		Test
		SAFE Examples
		21,600
		2,700
		2,700
		OFFENSIVE Examples
		2,400
		300
		300
		Total Examples
		24,000
		3,000
		3,000
		Table 9: Multi-turn adversarial task data statistics
	</Abstractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =10>
	<Abstractive Summary> =
		8
		Table 10: Results of experiments on the multi-turn ad-
		versarial task
	</Abstractive Summary>
</Paper ID=ument461>


<Paper ID=ument461> <Table ID =11>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		be found in Table 11 in the appendix
	</Extractive Summary>
</Paper ID=ument461>


<Paper ID=ument462> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of ellipsis and co-reference resolu-
		tion
		2018; Jin et al
	</Abstractive Summary>
	<Extractive Summary> =
		, Q2 and Q3 in Table 1) based on
		the dialogue context while dialogue systems often
		fail to understand such utterances correctly, which
		may result in false or incoherent responses
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 provides
		examples of reconstructed utterances in which the
		omitted information is recovered or the anaphor is
		substituted with referred expressions
	</Extractive Summary>
</Paper ID=ument462>


<Paper ID=ument462> <Table ID =2>
	<Abstractive Summary> =
		Table 2: An example of the ellipsis / co-reference an-
		notation
		ation probability distribution over the vocabulary
	</Abstractive Summary>
	<Extractive Summary> =
		Annotation Speciﬁcation Annotation cases for
		user utterances can be summarized into the follow-
		ing three conventions:
		• As shown in Table 2, if a user utterance con-
		tains an ellipsis or anaphor, we manually re-
		solve the ambiguity of ellipsis or anaphor and
		supplement the user utterance with a correct
		expression by checking the dialogue context
	</Extractive Summary>
</Paper ID=ument462>


<Paper ID=ument462> <Table ID =3>
	<Abstractive Summary> =
		16
		Table 3: Results of the resolution task on the dataset
	</Abstractive Summary>
	<Extractive Summary> =
		For these cases, we tested our mod-
		els and made statistical analysis on the three ver-
		sions of data as shown in column 3, 4 and 5 of
		Table 3 (EM, EM 1, EM 2)
	</Extractive Summary>
</Paper ID=ument462>


<Paper ID=ument462> <Table ID =4>
	<Abstractive Summary> =
		05
		Table 4: Results of the multi-task learning model
	</Abstractive Summary>
</Paper ID=ument462>


<Paper ID=ument463> <Table ID =1>
	<Abstractive Summary> =
		4562
		Table 1: The statistics of the bAbI-3, 4, 5, DSTC2 and Key-Value Retrieval datasets
		Datasets
		Key-Value Retrieval dataset
		DSTC 2
		bAbI-3
		bAbI-4
		bAbI-5
		Avg
	</Abstractive Summary>
</Paper ID=ument463>


<Paper ID=ument463> <Table ID =2>
	<Abstractive Summary> =
		4563
		Table 2: Results on Key-Value Retrieval dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2
		show the result of models on Key-Value Re-
		trieval dataset
	</Extractive Summary>
	<Extractive Summary> =
		2
		Shortcomings
		From the results in Table 2, we note that HMNs
		and Mem2Seq failed on weather forecasting task
	</Extractive Summary>
</Paper ID=ument463>


<Paper ID=ument463> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Results of Per-response accuracy and Per-dialog accuracy (in brackets) on bAbI dialogues
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows results of models on bAbI tasks
	</Extractive Summary>
	<Extractive Summary> =
		From Table 3,
		we observe that HMNs-CFO is signiﬁcantly bet-
		ter than original Mem2Seq as well as SEQ2SEQ +
		attention in several results and only loses slightly
		on task 4 (89
	</Extractive Summary>
</Paper ID=ument463>


<Paper ID=ument463> <Table ID =4>
	<Abstractive Summary> =
		6)
		Table 4: The results on the DSTC 2
		Model name
		F1
		BLEU
		SEQ2SEQ
		69
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows our model gets the best F1 score
		on dataset DSTC 2, while SEQ2SEQ with atten-
		tion gets the best BLEU result
	</Extractive Summary>
</Paper ID=ument463>


<Paper ID=ument463> <Table ID =5>
	<Abstractive Summary> =
		4564
		Table 5: A generated example from Key-Value Retrieval dataset with correct knowledge entities in bold
	</Abstractive Summary>
</Paper ID=ument463>


<Paper ID=ument464> <Table ID =1>
	<Abstractive Summary> =
		TWITTER
		Train
		1561
		3127
		1560
		Test
		173
		346
		173
		LAP14
		Train
		994
		464
		870
		Test
		341
		169
		128
		REST14
		Train
		2164
		637
		807
		Test
		728
		196
		196
		REST15
		Train
		912
		36
		256
		Test
		326
		34
		182
		REST16
		Train
		1240
		69
		439
		Test
		469
		30
		117
		Table 1: Dataset statistics
	</Abstractive Summary>
</Paper ID=ument464>


<Paper ID=ument464> <Table ID =2>
	<Abstractive Summary> =
		48†
		Table 2: Model comparison results (%)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		As is shown in Table 2, ASGCN-DG consistently
		outperforms all compared models on LAP14 and
		REST15 datasets, and achieves comparable results
		on TWITTER and REST16 datasets compared with
		baseline TNet-LF and on REST14 compared with
		ASCNN
	</Extractive Summary>
</Paper ID=ument464>


<Paper ID=ument464> <Table ID =3>
	<Abstractive Summary> =
		19
		Table 3: Ablation study results (%)
	</Abstractive Summary>
</Paper ID=ument464>


<Paper ID=ument464> <Table ID =4>
	<Abstractive Summary> =
		negative
		negative
		Table 4: Case study
	</Abstractive Summary>
	<Extractive Summary> =
		Particularly, we visualize the attention scores of-
		fered by MemNet, IAN, ASCNN and ASGCN-
		DG in Table 4, along with their predictions on
		these examples and the corresponding ground
		truth labels
	</Extractive Summary>
</Paper ID=ument464>


<Paper ID=ument465> <Table ID =1>
	<Abstractive Summary> =
		[R2]: If you have the money, I suggest going ::
		for ::
		the::i7
		Table 1: Two sample laptop review sentences
	</Abstractive Summary>
	<Extractive Summary> =
		To illustrate why global and local context can
		work together to indicate aspect words, Table 1
		4580
		shows two sentences from laptop review bench-
		mark (Pontiki et al
	</Extractive Summary>
	<Extractive Summary> =
		We take the two samples in Table 1 as in-
		put
	</Extractive Summary>
</Paper ID=ument465>


<Paper ID=ument465> <Table ID =2>
	<Abstractive Summary> =
		41
		Table 2:
		Statistics of laptop and restaurant (rest)
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		It might be because restaurant contains
		more distinct aspects (shown with the larger |Apt|
		in Table 2)
	</Extractive Summary>
</Paper ID=ument465>


<Paper ID=ument465> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3:
		Precision (pre), Recall (rec), F1, and Ac-
		curacy (Acc) produced by various unsupervised mod-
		els
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we re-
		port the aspect extraction results on two datasets
	</Extractive Summary>
	<Extractive Summary> =
		This demonstrates the sparse aspect oc-
		currence patterns in laptop dataset (probably ow-
		ing to the broad range of aspects discussed there),
		also explains the general worse performance on it
		(compared to restaurant and shown in Table 3)
	</Extractive Summary>
</Paper ID=ument465>


<Paper ID=ument465> <Table ID =4>
	<Abstractive Summary> =
		1,
		laptops,
		tapping,
		speakers, noon
		Table 4: Top 30 words of sample latent aspects learned
		by our variant GBC ONLY (on the left) and full model
		LCC+GBC (on the right)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the top 30 words (ranked by likeli-
		hood) of the sample latent aspects
	</Extractive Summary>
	<Extractive Summary> =
		Recall that the
		sample aspects in Table 4 suggest more rare as-
		pects discovered by LCC+GBC compared with
		GBC ONLY
	</Extractive Summary>
</Paper ID=ument465>


<Paper ID=ument465> <Table ID =5>
	<Abstractive Summary> =
		Aspects
		Non-aspects
		easy to, and the, of the, for
		the, with the, a great, that
		the, it ’s, all the, love the
		of the, it is, and the, is a,
		battery life, to use, it ’s,
		with the, i have, for the
		Table 5: Top 10 neighboring bigrams in left context of
		aspects and non-aspects (laptop dataset)
	</Abstractive Summary>
	<Extractive Summary> =
		In addition, we display in Table 5 the top 10
		neighboring bigrams in left context
	</Extractive Summary>
</Paper ID=ument465>


<Paper ID=ument465> <Table ID =6>
	<Abstractive Summary> =
		1
		Table 6: F1 score of our variants with varying encoders
		for local context modeling
	</Abstractive Summary>
</Paper ID=ument465>


<Paper ID=ument466> <Table ID =1>
	<Abstractive Summary> =
		, the features are
		Dataset
		Domain
		Sentences
		Training
		Testing
		L
		Laptop
		1,869
		1,458
		411
		R
		Restaurant
		3,900
		2,481
		1,419
		D
		Device
		1,437
		954
		483
		S
		Service
		2,153
		1,433
		720
		Table 1: Statistics of the datasets
	</Abstractive Summary>
</Paper ID=ument466>


<Paper ID=ument466> <Table ID =2>
	<Abstractive Summary> =
		62)
		-
		-
		Table 2: Main results (%)
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Main Results
		Based on the results in Table 2, we have the fol-
		lowing observations:
		• Our model consistently and signiﬁcantly
		achieves the best results on almost all trans-
		fer pairs, outperforming the strongest base-
		line RNSCN+ by 3
	</Extractive Summary>
</Paper ID=ument466>


<Paper ID=ument466> <Table ID =3>
	<Abstractive Summary> =
		94)
		-
		-
		-
		-
		Table 3: Ablation results (%)
	</Abstractive Summary>
	<Extractive Summary> =
		Based on the Table 3, we have
		the following observations to give us evidences:
		• No DMI v
	</Extractive Summary>
</Paper ID=ument466>


<Paper ID=ument466> <Table ID =4>
	<Abstractive Summary> =
		battery (),
		display,
		apps ()
		[battery]POS (),
		[display]POS,
		[apps]POS ()
		battery (),
		display,
		apps ()
		[battery]POS (),
		[display]POS,
		[apps]POS ()
		battery life,
		display,
		downloading apps
		[battery life]POS,
		[display]POS,
		[downloading apps]POS
		Table 4: Case analysis for the R→L pair
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Case Analysis
		As illustrated in Table 4, we perform case anal-
		ysis of the R→L pair for the Base model+DMI,
		AD-AL, and the full model AD-SAL to demon-
		strate the necessity to conduct selective align-
		ment for the ﬁne-grained adaptation
	</Extractive Summary>
</Paper ID=ument466>


<Paper ID=ument467> <Table ID =1>
	<Abstractive Summary> =
		The parameters in our model are then trained by
		minimizing the combined loss function:
		L = La + 1
		N Lb + λR
		(9)
		Dataset
		#Single
		#Multi
		#Total
		OL NOL Total
		Rest14 Train
		2053
		67
		415
		482
		2535
		Rest14 Val
		412
		19
		75
		94
		506
		Rest14 Test
		611
		27
		162
		189
		800
		Rest15 Train
		622
		47
		262
		309
		931
		Rest15 Val
		137
		13
		39
		52
		189
		Rest15 Test
		390
		30
		162
		192
		582
		Table 1: The numbers of single- and multi-aspect sen-
		tences
	</Abstractive Summary>
</Paper ID=ument467>


<Paper ID=ument467> <Table ID =2>
	<Abstractive Summary> =
		91
		Table 2: Results of the ALSC task in single-task settings in terms of accuracy (%) and Macro-F1 (%)
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 2 and 3 show our experimental results on
		the two public datasets for single-task and multi-
		task settings respectively
	</Extractive Summary>
	<Extractive Summary> =
		Single-task Settings Table 2 shows our exper-
		imental results of ALSC in single-task settings
	</Extractive Summary>
</Paper ID=ument467>


<Paper ID=ument467> <Table ID =3>
	<Abstractive Summary> =
		58
		Table 3: Results of the ALSC task in multi-task settings in terms of accuracy (%) and Macro-F1 (%)
	</Abstractive Summary>
	<Extractive Summary> =
		Multi-task Settings Table 3 shows experimen-
		tal results of ALSC in multi-task settings
	</Extractive Summary>
</Paper ID=ument467>


<Paper ID=ument467> <Table ID =4>
	<Abstractive Summary> =
		5782
		Table 4: Results of the ACD task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results of the ACD task in
		multi-task settings
	</Extractive Summary>
</Paper ID=ument467>


<Paper ID=ument468> <Table ID =1>
	<Abstractive Summary> =
		Here, we investigate whether neural networks
		can be effectively trained under this challenging
		setting when only a small number of descriptive
		keywords, or seed words, are available for each
		4612
		Aspect
		Seed Words
		Price (EN)
		price, value, money, worth, paid
		Image (EN)
		picture, color, quality, black, bright
		Food (EN)
		food, delicious, pizza, cheese, sushi
		Drinks (FR)
		vin, bière, verre, bouteille, cocktail
		Ambience (SP)
		ambiente, mesas, terraza, acogedor, ruido
		Table 1: Examples of aspects and ﬁve of their corre-
		sponding seed words in various domains (electronic
		products, restaurants) and languages (“EN” for En-
		glish, “FR” for French, “SP” for Spanish)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows examples of aspects
		and ﬁve of their corresponding seed words from
		our experimental datasets (described later in more
		detail)
	</Extractive Summary>
</Paper ID=ument468>


<Paper ID=ument468> <Table ID =2>
	<Abstractive Summary> =
		, qK⟩
		Teacher’s aspect predictions
		Table 2: Notation
	</Abstractive Summary>
</Paper ID=ument468>


<Paper ID=ument468> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Micro-averaged F1 reported for 9-class EDU-level aspect detection in product reviews
	</Abstractive Summary>
</Paper ID=ument468>


<Paper ID=ument468> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Micro-averaged F1 reported for 12-class sentence-level aspect detection in restaurant reviews
	</Abstractive Summary>
</Paper ID=ument468>


<Paper ID=ument468> <Table ID =5>
	<Abstractive Summary> =
		4
		Table 5: Micro-averaged F1 scores during the ﬁrst
		round (middle column) and after iterative co-training
		(right column) in product reviews (top) and restaurant
		reviews (bottom)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 re-
		ports the performance of Teacher and Student-*
		after co-training for both product reviews (top) and
		English restaurant reviews (bottom)
	</Extractive Summary>
	<Extractive Summary> =
		Co-
		training leads to improved student performance
		in both datasets (Table 5)
	</Extractive Summary>
</Paper ID=ument468>


<Paper ID=ument469> <Table ID =1>
	<Abstractive Summary> =
		We slightly generalize the three
		4625
		Intent
		Category
		# Samples
		Provocative
		84
		Informative
		119
		Advocative
		97
		Entertainment
		310
		Expositive
		237
		Expressive
		95
		Promotive
		162
		Semiotic
		Category
		# Samples
		Divergent
		115
		Additive
		277
		Parallel
		712
		Contextual Relationship
		Category
		# Samples
		Minimal
		372
		Close
		585
		Transcendent
		147
		Table 1: Counts of different labels in the Multimodal Document Intent Dataset (MDID)
	</Abstractive Summary>
	<Extractive Summary> =
		Dataset statistics are
		shown in Table 1; see https://www
	</Extractive Summary>
</Paper ID=ument469>


<Paper ID=ument469> <Table ID =2>
	<Abstractive Summary> =
		4)
		Table 2: Table showing results with various DCNN models– image-only (Img), text-only (Txt-emb and Txt-
		ELMo), and combined model (Img + Txt-emb and Img + Txt-ELMo)
	</Abstractive Summary>
	<Extractive Summary> =
		Except for the semiotic taxonomy we used ELMo text representations (based on the performance in Table 2)
	</Extractive Summary>
</Paper ID=ument469>


<Paper ID=ument469> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Class-wise results (AUC) for the three taxonomies with different DCNN models on the MDID dataset
	</Abstractive Summary>
</Paper ID=ument469>


<Paper ID=ument47> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Topic classiﬁcation examples for Technology
		and Health, where Apple is ambiguous within local
		context
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 1, the real meaning of Apple can only be
		correctly recognized from overall view instead of
		narrow window
	</Extractive Summary>
</Paper ID=ument47>


<Paper ID=ument47> <Table ID =2>
	<Abstractive Summary> =
		4M
		60k
		QA
		DBP
		14
		55
		560k
		70k
		Ontology
		Table 2:
		Datasets summary
	</Abstractive Summary>
</Paper ID=ument47>


<Paper ID=ument47> <Table ID =3>
	<Abstractive Summary> =
		400k
		256
		20
		DBP
		400k
		128
		15
		Table 3: Model settings
	</Abstractive Summary>
	<Extractive Summary> =
		Other settings are shown
		in Table 3, all trainable parameters including em-
		beddings of words are initialized randomly with-
		out any pre-trained techniques (Mikolov et al
	</Extractive Summary>
</Paper ID=ument47>


<Paper ID=ument47> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Test accuracy [%] on several Datasets
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results and Analysis
		Table 4 is the summary of the experimental re-
		sults
	</Extractive Summary>
</Paper ID=ument47>


<Paper ID=ument47> <Table ID =5>
	<Abstractive Summary> =
		9
		Table 5: Effect of Encoder1 and Interaction Mode
	</Abstractive Summary>
</Paper ID=ument47>


<Paper ID=ument47> <Table ID =6>
	<Abstractive Summary> =
		Negative 3
		Table 6: Visualization of chosen samples on AG News and Yelp Review Polarity dataset
	</Abstractive Summary>
</Paper ID=ument47>


<Paper ID=ument47> <Table ID =7>
	<Abstractive Summary> =
		0
		Table 7: Semi-supervised generalization of our archi-
		tecture and comparison with popular pre-trained mod-
		els
	</Abstractive Summary>
</Paper ID=ument47>


<Paper ID=ument470> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		The statistics of
		two datasets are shown in Table 1, with more in-
		formation in Figure 4
	</Extractive Summary>
</Paper ID=ument470>


<Paper ID=ument470> <Table ID =2>
	<Abstractive Summary> =
		38
		Table 2: Different structures and the future trend of
		conversations
	</Abstractive Summary>
	<Extractive Summary> =
		tion’s structure on its future development, we cal-
		culate the likelihoods of (1) new users joining the
		discussion, and (2) current participants continuing
		the conversation, grouped by different conversa-
		tion structures (Table 2)
	</Extractive Summary>
</Paper ID=ument470>


<Paper ID=ument470> <Table ID =3>
	<Abstractive Summary> =
		533
		Table 3: Results of our model variants on development
		set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 displays their re-
		sults on development set
	</Extractive Summary>
</Paper ID=ument470>


<Paper ID=ument470> <Table ID =4>
	<Abstractive Summary> =
		590
		Table 4: Main results on conversation recommenda-
		tion
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the conversation
		recommendation results with baselines and state
		of the arts
	</Extractive Summary>
</Paper ID=ument470>


<Paper ID=ument470> <Table ID =5>
	<Abstractive Summary> =
		538
		Table 5: MAP scores obtained by our ablations
	</Abstractive Summary>
</Paper ID=ument470>


<Paper ID=ument470> <Table ID =6>
	<Abstractive Summary> =
		737
		Table 6: Cosine similarity between U0’s user embed-
		dings in RF and CI modeling with others’
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the similarity of user factors
		learned by our RF (replying factor modeling in
		Section 3
	</Extractive Summary>
</Paper ID=ument470>


<Paper ID=ument470> <Table ID =7>
	<Abstractive Summary> =
		212
		Table 7: MAP scores for ﬁrst time replies prediction
	</Abstractive Summary>
</Paper ID=ument470>


<Paper ID=ument471> <Table ID =1>
	<Abstractive Summary> =
		, 2017) employs pre-trained word em-
		beddings based on Word2Vec as input embeddings
		4649
		Datasets
		Threads
		Tweets
		True
		False
		Unveriﬁed
		Support
		Deny
		Query
		Comment
		RumourEval
		325
		5,568
		145
		74
		106
		1,004
		415
		464
		3,685
		PHEME
		6,425
		105,354
		1,067
		638
		697
		891
		335
		353
		2,855
		Table 1: Statistics of the two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		There are two reasons for our analysis according
		to Table 1 and Table 2
	</Extractive Summary>
</Paper ID=ument471>


<Paper ID=ument471> <Table ID =2>
	<Abstractive Summary> =
		09
		Table 2: Performance comparison of our sifted multi-task learning method against the baselines
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Error Analysis
		Although the sifted multi-task learning method out-
		performs previous state-of-the-art methods on t-
		wo datasets (From Table 2), we observe that the
		proposed method achieves more remarkable per-
		formance boosts on PHEME than on RumourEval
	</Extractive Summary>
</Paper ID=ument471>


<Paper ID=ument471> <Table ID =3>
	<Abstractive Summary> =
		09
		Table 3: Ablation analysis of the sifted multi-task learning method
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 provides the experimental results of
		these methods on RumourEval and PHEME
		datasets
	</Extractive Summary>
</Paper ID=ument471>


<Paper ID=ument472> <Table ID =1>
	<Abstractive Summary> =
		We evaluate the following four models:
		• A Centroid model summarizes each set of
		seed words by its expected vector in embed-
		ding space, and classiﬁes concepts into the
		class of closest expected embedding in Eu-
		clidean distance following a softmax rule;
		• A Na¨ıve Bayes model considers both mean
		and variance, under the assumption of inde-
		pendence among embedding dimensions, by
		ﬁtting a normal distribution with mean vector
		and diagonal covariance matrix to the set of
		seed words of each class;
		• A k-Nearest Neighbors (kNN) model exploits
		local density estimation and classiﬁes con-
		cepts according to the majority vote of the k
		seed words closest to the query vector;
		• A Kernel Density Estimation (KDE) model
		performs density estimation at a broader
		scale by considering the contribution of each
		seed word toward the total likelihood of each
		4657
		Model
		Parameter
		Posterior Inference
		Centroid
		–
		p(c|q) ∝ exp(−∥q−E[Sc]∥)
		Na¨ıve Bayes
		–
		p(c|q) ∝ ∏d
		j=1 fN
		�
		q j;µ = E[Sc,j],σ2 = Var[Sc,j]
		�
		k-Nearest Neighbors (kNN)
		k
		p(c|q) ∝
		��{k nearest seed words to q}∩Sc
		��
		Kernel Density Estimation (KDE)
		h
		p(c|q) ∝
		1
		|Sc| ∑w∈Sc fMN (q;µ = w,Σ = diag(h))
		Table 1: Summary of models for moral sentiment classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 speciﬁes the formulation of each model
	</Extractive Summary>
</Paper ID=ument472>


<Paper ID=ument472> <Table ID =2>
	<Abstractive Summary> =
		33
		Table 2: Classiﬁcation accuracy of moral seed words for moral relevance, moral polarity, and ﬁne-grained moral
		categories based on 1990–1999 word embeddings for two independent corpora, Google N-grams and COHA
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows classiﬁcation accuracy for all
		models and corpora on each tier for the 1990–1999
		period
	</Extractive Summary>
</Paper ID=ument472>


<Paper ID=ument472> <Table ID =3>
	<Abstractive Summary> =
		0001)
		Table 3: Pearson correlations between model predicted
		moral sentiment polarities and human valence ratings
	</Abstractive Summary>
</Paper ID=ument472>


<Paper ID=ument472> <Table ID =4>
	<Abstractive Summary> =
		26***
		authority+
		1860
		Table 4: Top 10 changing words towards moral relevance during 1800–2000, with model-inferred moral category
		and switching period
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the words with steepest predicted
		change toward moral relevance, along with their
		predicted ﬁne-grained moral categories in mod-
		4661
		ern times (i
	</Extractive Summary>
</Paper ID=ument472>


<Paper ID=ument472> <Table ID =5>
	<Abstractive Summary> =
		26 ***
		authority+
		degradation−
		1800
		Table 5: Top 10 changing words towards moral positive (upper panel) and negative (lower panel) polarities, with
		model-inferred most representative moral categories during historical and modern periods and the switching peri-
		ods
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the
		words with steepest predicted change toward the
		positive and negative moral poles
	</Extractive Summary>
</Paper ID=ument472>


<Paper ID=ument472> <Table ID =6>
	<Abstractive Summary> =
		002
		Table 6: Results from multiple regression that regresses
		rate of change in moral relevance against the factors of
		word frequency, length, and concreteness (n = 606)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the results of multiple linear re-
		gression
	</Extractive Summary>
</Paper ID=ument472>


<Paper ID=ument473> <Table ID =1>
	<Abstractive Summary> =
		Table 1: A taxonomy of relation types and examples of commonly used language cues
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists
		the category deﬁnitions and some common lan-
		guage cues used to identify the relation type for
		each category
	</Extractive Summary>
</Paper ID=ument473>


<Paper ID=ument473> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Examples of sentences and annotated relation types
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the examples of sen-
		tences with different relation types
	</Extractive Summary>
	<Extractive Summary> =
		Sometimes a
		sentence may contain language cues indicating a
		causal relationship, such as “a reliable way to de-
		termine”; however, if the sentence describes the
		function of certain tools or diagnoses rather than
		the explicit relations between independent vari-
		able and dependent variables, it should be labelled
		as “no relationship” (as shown in Example 6 of
		Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Meanwhile, sentences that discuss study
		limitations, as shown in Example 7 of Table 2,
		should be labelled as “no relationship”
	</Extractive Summary>
</Paper ID=ument473>


<Paper ID=ument473> <Table ID =3>
	<Abstractive Summary> =
		0%
		Table 3: Distribution of sentence types in the corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the
		relation type distribution of single-class sentences
		in the corpus
	</Extractive Summary>
</Paper ID=ument473>


<Paper ID=ument473> <Table ID =4>
	<Abstractive Summary> =
		881
		Table 4: Model performance: BioBERT> BERT >
		BiRNN > LinearSVM
	</Abstractive Summary>
	<Extractive Summary> =
		The result in Table 4 shows that
		BioBERT performed the best by all measures, fol-
		lowed by BERT, BiRNN and then LinearSVM
	</Extractive Summary>
</Paper ID=ument473>


<Paper ID=ument473> <Table ID =5>
	<Abstractive Summary> =
		911
		Table 5: Detailed performance of BioBERT on each
		category of sentence relationship
	</Abstractive Summary>
	<Extractive Summary> =
		See the
		performance scores per category in Table 5 and the
		confusion matrix in Fig
	</Extractive Summary>
</Paper ID=ument473>


<Paper ID=ument473> <Table ID =6>
	<Abstractive Summary> =
		194
		Table 6: Ratios of causal language use in studies pub-
		lished in English and other languages (sorted by causal
		only, reversely)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the ratios of causal language use
		in the non-English subset
	</Extractive Summary>
	<Extractive Summary> =
		Because some languages are used in multiple
		countries and some countries did not publish many
		papers, not all results in Table 6 and Table 7 are
		comparable
	</Extractive Summary>
</Paper ID=ument473>


<Paper ID=ument473> <Table ID =7>
	<Abstractive Summary> =
		174
		Table 7: Ratios of causal language use in different
		countries (sorted by causal only, reversely)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the ratios in the English subset,
		broken down by the authors’ countries
	</Extractive Summary>
	<Extractive Summary> =
		Because some languages are used in multiple
		countries and some countries did not publish many
		papers, not all results in Table 6 and Table 7 are
		comparable
	</Extractive Summary>
</Paper ID=ument473>


<Paper ID=ument474> <Table ID =1>
	<Abstractive Summary> =
		(2018)
		28,608
		directed, generalized + target = archaic, class, disability,
		3
		ethnicity, gender, nationality, religion, sexual orientation
		Ours
		13,000
		Labels for ﬁve different aspects
		5
		Table 1: Comparative table of some of the available hate speech and abusive language corpora in terms of labels
		and sizes
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 compares different la-
		belsets that exist in the literature
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the counts of the ﬁve categories out of 16 that
		commonly occur in the three languages
	</Extractive Summary>
</Paper ID=ument474>


<Paper ID=ument474> <Table ID =2>
	<Abstractive Summary> =
		The average Krippendorff scores for inter-
		Attribute
		Label
		En
		Fr
		Ar
		Directness
		Direct
		530
		2,198
		1,684
		Indirect
		4,456
		997
		754
		Hostility
		Abusive
		671
		1,056
		610
		Hateful
		1,278
		399
		755
		Offensive
		4,020
		1,690
		1,151
		Disrespectful
		782
		396
		615
		Fearful
		562
		388
		41
		Normal
		1,359
		1,124
		1,197
		Target
		Origin
		2,448
		2,266
		877
		Gender
		638
		27
		548
		SexOrient
		514
		12
		0
		Religion
		68
		146
		145
		Disability
		1,089
		177
		1
		Other
		890
		1,386
		1,782
		Group
		Individual
		497
		918
		915
		Other
		1,590
		1,085
		1,470
		Women
		878
		62
		722
		SpecNeeds
		1,571
		174
		2
		African
		86
		311
		51
		Annotator
		Disgust
		3,469
		602
		778
		Shock
		2,151
		1,179
		917
		Anger
		2,955
		531
		356
		Sadness
		2,775
		1,457
		388
		Fear
		1,304
		378
		35
		Confusion
		1,747
		446
		115
		Indifference
		2,878
		2,035
		1,825
		Total number of tweets
		5,647
		4,014
		3,353
		Table 2: The label distributions of each task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows
		that even when partly using equivalent keywords
		to search for candidate tweets, there are still sig-
		niﬁcant differences in the resulting data
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows there are
		fewer tweets targeting disability in Arabic com-
		pared to English and French and no tweets insult-
		ing people based on their sexual orientation which
		may be due to the fact that the labels of gender,
		gender identity, and sexual orientation use almost
		the same wording
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows more tweets making the anno-
		tators feel disgusted and angry in English, while
		annotators show more indifference in both French
		and Arabic
	</Extractive Summary>
</Paper ID=ument474>


<Paper ID=ument474> <Table ID =3>
	<Abstractive Summary> =
		73
		Table 3: Full evaluation scores of the only binary classiﬁcation task where the single task single language model
		consistently outperforms multilingual multitask models
	</Abstractive Summary>
</Paper ID=ument474>


<Paper ID=ument474> <Table ID =4>
	<Abstractive Summary> =
		49
		Table 4: Full evaluation of tasks where multilingual and multitask models outperform on average single task single
		language model on four different tasks
	</Abstractive Summary>
</Paper ID=ument474>


<Paper ID=ument475> <Table ID =1>
	<Abstractive Summary> =
		org/2018/
		10/factchecking-trump-on-trade/
		Reason
		None
		Category
		the-factcheck-wire
		Speaker
		Donald Trump
		Checker
		Eugene Kiely
		Tags
		North
		American
		Free
		Trade
		Agreement
		Claim Entities
		United States, Canada, Mexico
		Article Title
		FactChecking Trump on Trade
		Publish Date
		October 3, 2018
		Claim Date
		Monday, October 1, 2018
		Table 1: An example of a claim instance
	</Abstractive Summary>
	<Extractive Summary> =
		1 It consists of 34,918 claims,
		collected from 26 fact checking websites in
		English; evidence pages to verify the claims; the
		context in which they occurred; and rich metadata
		(see Table 1 for an example)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Dataset Construction
		We crawled a total of 43,837 claims with their
		metadata (see details in Table 11)
	</Extractive Summary>
	<Extractive Summary> =
		38 websites in total (shown in Table 11)
	</Extractive Summary>
	<Extractive Summary> =
		The
		number of instances, as well as labels per domain,
		are shown in Table 6 and label names in Table 10
		in the appendix
	</Extractive Summary>
	<Extractive Summary> =
		1
		Multi-Domain Claim Veracity Prediction
		with Disparate Label Spaces
		Since not all fact checking websites use the same
		claim labels (see Table 6, and Table 10 in the ap-
		pendix), training a claim veracity prediction model
		is not entirely straight-forward
	</Extractive Summary>
	<Extractive Summary> =
		Domain names are from hereon af-
		ter abbreviated for brevity, see Table 11 in the ap-
		pendix for correspondences to full website names
	</Extractive Summary>
	<Extractive Summary> =
		Domain # Insts # Labels Labels
		abbc
		436
		3
		in-between, in-the-red, in-the-green
		afck
		433
		7
		correct, incorrect, mostly-correct, unproven, misleading, understated, exagger-
		ated
		bove
		295
		2
		none, rating: false
		chct
		355
		4
		verdict: true, verdict: false, verdict: unsubstantiated, none
		clck
		38
		3
		incorrect, unsupported, misleading
		faan
		111
		3
		factscan score: false, factscan score: true, factscan score: misleading
		faly
		71
		5
		true, none, partly true, unveriﬁed, false
		fani
		20
		3
		conclusion: accurate, conclusion: false, conclusion: unclear
		farg
		485
		11
		false, none, distorts the facts, misleading, spins the facts, no evidence, not the
		whole story, unsupported, cherry picks, exaggerates, out of context
		goop
		2943
		6
		0, 1, 2, 3, 4, 10
		hoer
		1310
		7
		facebook scams, true messages, bogus warning, statirical reports, fake news,
		unsubstantiated messages, misleading recommendations
		huca
		34
		3
		a lot of baloney, a little baloney, some baloney
		mpws
		47
		3
		accurate, false, misleading
		obry
		59
		4
		mostly true, veriﬁed, unobservable, mostly false
		para
		222
		7
		mostly false, mostly true, half-true, false, true, pants on ﬁre!, half ﬂip
		peck
		65
		3
		false, true, partially true
		pomt
		15390 9
		half-true, false, mostly true, mostly false, true, pants on ﬁre!, full ﬂop, half ﬂip,
		no ﬂip
		pose
		1361
		6
		promise kept, promise broken, compromise, in the works, not yet rated, stalled
		ranz
		21
		2
		fact, ﬁction
		snes
		6455
		12
		false, true, mixture, unproven, mostly false, mostly true, miscaptioned, legend,
		outdated, misattributed, scam, correct attribution
		thet
		79
		6
		none, mostly false, mostly true, half true, false, true
		thal
		74
		2
		none, we rate this claim false
		tron
		3423
		27
		ﬁction!, truth!, unproven!, truth! & ﬁction!, mostly ﬁction!, none, disputed!,
		truth! & misleading!, authorship conﬁrmed!, mostly truth!, incorrect attribu-
		tion!, scam!, investigation pending!, conﬁrmed authorship!, commentary!, pre-
		viously truth! now resolved!, outdated!, truth! & outdated!, virus!, ﬁction! &
		satire!, truth! & unproven!, misleading!, grass roots movement!, opinion!, cor-
		rect attribution!, truth! & disputed!, inaccurate attribution!
		vees
		504
		4
		none, fake, misleading, false
		vogo
		653
		8
		none, determination: false, determination: true, determination: mostly true,
		determination: misleading, determination: barely true, determination: huckster
		propaganda, determination: false, determination: a stretch
		wast
		201
		7
		4 pinnochios, 3 pinnochios, 2 pinnochios, false, not the whole story, needs
		context, none
		Table 10: Number of instances, and labels per domain sorted by number of occurrences
		4697
		Website
		Domain Claims Labels Category Speaker Checker Tags
		Article Claim date Publish date Full text Outlinks
		abc
		abbc
		436
		436
		436
		-
		-
		436
		436
		-
		436
		436
		7676
		africacheck
		afck
		436
		436
		-
		-
		-
		-
		436
		-
		436
		436
		2325
		altnews
		-
		496
		-
		-
		-
		496
		-
		496
		-
		496
		496
		6389
		boomlive
		-
		302
		302
		-
		-
		-
		-
		302
		-
		302
		302
		6054
		checkyourfact
		chht
		358
		358
		-
		-
		358
		-
		-
		-
		358
		358
		5271
		climatefeedback
		clck
		45
		45
		-
		-
		-
		-
		45
		-
		45
		45
		489
		crikey
		-
		18
		18
		18
		-
		18
		18
		18
		-
		18
		18
		212
		factcheckni
		-
		36
		36
		36
		-
		-
		-
		36
		-
		-
		36
		151
		factcheckorg
		farg
		512
		512
		512
		512
		512
		512
		512
		512
		512
		512
		8282
		factly
		-
		77
		77
		-
		-
		-
		-
		77
		-
		-
		77
		658
		factscan
		-
		115
		115
		-
		115
		-
		-
		-
		115
		115
		115
		1138
		fullfact
		-
		336
		336
		336
		-
		336
		-
		336
		-
		336
		336
		3838
		gossipcop
		goop
		2947
		2947
		-
		-
		2947
		-
		2947
		-
		2947
		2947
		12583
		hoaxslayer
		hoer
		1310
		1310
		-
		-
		1310
		-
		1310
		-
		1310
		1310
		14499
		hufﬁngtonpostca
		huca
		38
		38
		-
		38
		38
		-
		38
		38
		38
		38
		78
		leadstories
		-
		1547
		1547
		-
		-
		1547
		-
		1547
		-
		1547
		1547
		12015
		mprnews
		mpws
		49
		49
		-
		-
		49
		-
		49
		-
		49
		49
		319
		nytimes
		-
		17
		17
		-
		-
		17
		-
		17
		-
		17
		17
		271
		observatory
		obry
		60
		60
		-
		-
		60
		-
		60
		-
		60
		60
		592
		pandora
		para
		225
		225
		225
		225
		225
		-
		225
		-
		225
		225
		114
		pesacheck
		peck
		67
		67
		-
		-
		67
		-
		67
		-
		67
		67
		521
		politico
		-
		102
		102
		-
		-
		102
		-
		102
		-
		102
		102
		150
		politifact promise pose
		1361
		1361
		1361
		1361
		-
		-
		1361
		-
		1361
		1361
		6279
		politifact stmt
		pomt
		15390
		15390
		-
		15390
		-
		-
		-
		15390
		15390
		15390
		78543
		politifact story
		-
		5460
		-
		-
		-
		5460
		-
		-
		-
		5460
		5460
		24836
		radionz
		ranz
		32
		32
		32
		32
		-
		-
		32
		32
		32
		32
		44
		snopes
		snes
		6457
		6457
		6457
		-
		6457
		-
		6457
		-
		6457
		6457
		46735
		swissinfo
		-
		20
		20
		20
		20
		20
		-
		20
		-
		20
		20
		40
		theconversation
		-
		62
		62
		62
		62
		62
		62
		62
		-
		62
		62
		723
		theferret
		thet
		81
		81
		81
		81)
		-
		-
		81
		-
		81(81)
		81
		885
		theguardian
		-
		155
		155
		155
		-
		155
		-
		155
		-
		155
		155
		2600
		thejournal
		thal
		179
		179
		-
		-
		-
		-
		179
		-
		179
		179
		2375
		truthorﬁction
		tron
		3674
		3674
		3674
		-
		-
		3674
		3674
		-
		3674
		3674
		8268
		veraﬁles
		vees
		509
		509
		-
		-
		-
		509
		509
		-
		509
		509
		23
		voiceofsandiego
		vogo
		660
		660
		-
		-
		-
		-
		660
		-
		660
		660
		2352
		washingtonpost
		wast
		227
		227
		-
		227
		227
		-
		227
		-
		227
		227
		2470
		wral
		-
		20
		20
		-
		-
		20
		20
		20
		-
		20
		20
		355
		zimfact
		-
		21
		21
		21
		21
		21
		-
		21
		-
		21
		21
		179
		Total
		43837
		43837
		43837
		43837
		43837
		43837 43837
		43837
		43837
		43837
		260330
		Table 11: Summary statistics for claim collection
	</Extractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =2>
	<Abstractive Summary> =
		(2018)†
		185,445
		3
		No
		Wikipedia
		V: Veracity + evidence relevancy
		MultiFC
		36,534
		2-40
		Yes
		Fact Checking Websites
		Table 2: Comparison of fact checking datasets
	</Abstractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =3>
	<Abstractive Summary> =
		419
		Table 3: The top 30 most frequently occurring URL
		domains
	</Abstractive Summary>
	<Extractive Summary> =
		The resulting evidence pages
		are from a wide variety of URL domains, though
		with a predictable skew towards popular websites,
		such as Wikipedia or The Guardian (see Table 3
		for detailed statistics)
	</Extractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =4>
	<Abstractive Summary> =
		164
		Jennifer Aniston
		163
		Mexico
		158
		Ted Cruz
		152
		Federal Bureau of Investigation
		146
		Syria
		130
		Table 4: Top 30 most frequent entities listed by their
		Wikipedia URL with preﬁx omitted
		only consider the claims themselves, and those
		that encode evidence pages as well
	</Abstractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =5>
	<Abstractive Summary> =
		492
		Table 5: Results with different model variants on the
		test set, ‘meta’ means all metadata is used
	</Abstractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =6>
	<Abstractive Summary> =
		492
		Table 6: Total number of instances and unique labels
		per domain, as well as per-domain results with model
		crawled ranked + meta, sorted by label size
		Metadata
		Micro F1
		Macro F1
		None
		0
	</Abstractive Summary>
	<Extractive Summary> =
		The
		number of instances, as well as labels per domain,
		are shown in Table 6 and label names in Table 10
		in the appendix
	</Extractive Summary>
	<Extractive Summary> =
		1
		Multi-Domain Claim Veracity Prediction
		with Disparate Label Spaces
		Since not all fact checking websites use the same
		claim labels (see Table 6, and Table 10 in the ap-
		pendix), training a claim veracity prediction model
		is not entirely straight-forward
	</Extractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =7>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =8>
	<Abstractive Summary> =
		492
		Table 8: Ablation results with crawled ranked + meta
		encoding for STL vs
	</Abstractive Summary>
	<Extractive Summary> =
		We ﬁrst experiment with
		different base model variants and ﬁnd that label
		embeddings improve results, and that the best pro-
		posed models utilising multiple domains outper-
		form single-task models (see Table 8)
	</Extractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =9>
	<Abstractive Summary> =
		Table 9: The list of websites that we did not crawl and reasons for not crawling them
	</Abstractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =10>
	<Abstractive Summary> =
		Domain # Insts # Labels Labels
		abbc
		436
		3
		in-between, in-the-red, in-the-green
		afck
		433
		7
		correct, incorrect, mostly-correct, unproven, misleading, understated, exagger-
		ated
		bove
		295
		2
		none, rating: false
		chct
		355
		4
		verdict: true, verdict: false, verdict: unsubstantiated, none
		clck
		38
		3
		incorrect, unsupported, misleading
		faan
		111
		3
		factscan score: false, factscan score: true, factscan score: misleading
		faly
		71
		5
		true, none, partly true, unveriﬁed, false
		fani
		20
		3
		conclusion: accurate, conclusion: false, conclusion: unclear
		farg
		485
		11
		false, none, distorts the facts, misleading, spins the facts, no evidence, not the
		whole story, unsupported, cherry picks, exaggerates, out of context
		goop
		2943
		6
		0, 1, 2, 3, 4, 10
		hoer
		1310
		7
		facebook scams, true messages, bogus warning, statirical reports, fake news,
		unsubstantiated messages, misleading recommendations
		huca
		34
		3
		a lot of baloney, a little baloney, some baloney
		mpws
		47
		3
		accurate, false, misleading
		obry
		59
		4
		mostly true, veriﬁed, unobservable, mostly false
		para
		222
		7
		mostly false, mostly true, half-true, false, true, pants on ﬁre!, half ﬂip
		peck
		65
		3
		false, true, partially true
		pomt
		15390 9
		half-true, false, mostly true, mostly false, true, pants on ﬁre!, full ﬂop, half ﬂip,
		no ﬂip
		pose
		1361
		6
		promise kept, promise broken, compromise, in the works, not yet rated, stalled
		ranz
		21
		2
		fact, ﬁction
		snes
		6455
		12
		false, true, mixture, unproven, mostly false, mostly true, miscaptioned, legend,
		outdated, misattributed, scam, correct attribution
		thet
		79
		6
		none, mostly false, mostly true, half true, false, true
		thal
		74
		2
		none, we rate this claim false
		tron
		3423
		27
		ﬁction!, truth!, unproven!, truth! & ﬁction!, mostly ﬁction!, none, disputed!,
		truth! & misleading!, authorship conﬁrmed!, mostly truth!, incorrect attribu-
		tion!, scam!, investigation pending!, conﬁrmed authorship!, commentary!, pre-
		viously truth! now resolved!, outdated!, truth! & outdated!, virus!, ﬁction! &
		satire!, truth! & unproven!, misleading!, grass roots movement!, opinion!, cor-
		rect attribution!, truth! & disputed!, inaccurate attribution!
		vees
		504
		4
		none, fake, misleading, false
		vogo
		653
		8
		none, determination: false, determination: true, determination: mostly true,
		determination: misleading, determination: barely true, determination: huckster
		propaganda, determination: false, determination: a stretch
		wast
		201
		7
		4 pinnochios, 3 pinnochios, 2 pinnochios, false, not the whole story, needs
		context, none
		Table 10: Number of instances, and labels per domain sorted by number of occurrences
		4697
		Website
		Domain Claims Labels Category Speaker Checker Tags
		Article Claim date Publish date Full text Outlinks
		abc
		abbc
		436
		436
		436
		-
		-
		436
		436
		-
		436
		436
		7676
		africacheck
		afck
		436
		436
		-
		-
		-
		-
		436
		-
		436
		436
		2325
		altnews
		-
		496
		-
		-
		-
		496
		-
		496
		-
		496
		496
		6389
		boomlive
		-
		302
		302
		-
		-
		-
		-
		302
		-
		302
		302
		6054
		checkyourfact
		chht
		358
		358
		-
		-
		358
		-
		-
		-
		358
		358
		5271
		climatefeedback
		clck
		45
		45
		-
		-
		-
		-
		45
		-
		45
		45
		489
		crikey
		-
		18
		18
		18
		-
		18
		18
		18
		-
		18
		18
		212
		factcheckni
		-
		36
		36
		36
		-
		-
		-
		36
		-
		-
		36
		151
		factcheckorg
		farg
		512
		512
		512
		512
		512
		512
		512
		512
		512
		512
		8282
		factly
		-
		77
		77
		-
		-
		-
		-
		77
		-
		-
		77
		658
		factscan
		-
		115
		115
		-
		115
		-
		-
		-
		115
		115
		115
		1138
		fullfact
		-
		336
		336
		336
		-
		336
		-
		336
		-
		336
		336
		3838
		gossipcop
		goop
		2947
		2947
		-
		-
		2947
		-
		2947
		-
		2947
		2947
		12583
		hoaxslayer
		hoer
		1310
		1310
		-
		-
		1310
		-
		1310
		-
		1310
		1310
		14499
		hufﬁngtonpostca
		huca
		38
		38
		-
		38
		38
		-
		38
		38
		38
		38
		78
		leadstories
		-
		1547
		1547
		-
		-
		1547
		-
		1547
		-
		1547
		1547
		12015
		mprnews
		mpws
		49
		49
		-
		-
		49
		-
		49
		-
		49
		49
		319
		nytimes
		-
		17
		17
		-
		-
		17
		-
		17
		-
		17
		17
		271
		observatory
		obry
		60
		60
		-
		-
		60
		-
		60
		-
		60
		60
		592
		pandora
		para
		225
		225
		225
		225
		225
		-
		225
		-
		225
		225
		114
		pesacheck
		peck
		67
		67
		-
		-
		67
		-
		67
		-
		67
		67
		521
		politico
		-
		102
		102
		-
		-
		102
		-
		102
		-
		102
		102
		150
		politifact promise pose
		1361
		1361
		1361
		1361
		-
		-
		1361
		-
		1361
		1361
		6279
		politifact stmt
		pomt
		15390
		15390
		-
		15390
		-
		-
		-
		15390
		15390
		15390
		78543
		politifact story
		-
		5460
		-
		-
		-
		5460
		-
		-
		-
		5460
		5460
		24836
		radionz
		ranz
		32
		32
		32
		32
		-
		-
		32
		32
		32
		32
		44
		snopes
		snes
		6457
		6457
		6457
		-
		6457
		-
		6457
		-
		6457
		6457
		46735
		swissinfo
		-
		20
		20
		20
		20
		20
		-
		20
		-
		20
		20
		40
		theconversation
		-
		62
		62
		62
		62
		62
		62
		62
		-
		62
		62
		723
		theferret
		thet
		81
		81
		81
		81)
		-
		-
		81
		-
		81(81)
		81
		885
		theguardian
		-
		155
		155
		155
		-
		155
		-
		155
		-
		155
		155
		2600
		thejournal
		thal
		179
		179
		-
		-
		-
		-
		179
		-
		179
		179
		2375
		truthorﬁction
		tron
		3674
		3674
		3674
		-
		-
		3674
		3674
		-
		3674
		3674
		8268
		veraﬁles
		vees
		509
		509
		-
		-
		-
		509
		509
		-
		509
		509
		23
		voiceofsandiego
		vogo
		660
		660
		-
		-
		-
		-
		660
		-
		660
		660
		2352
		washingtonpost
		wast
		227
		227
		-
		227
		227
		-
		227
		-
		227
		227
		2470
		wral
		-
		20
		20
		-
		-
		20
		20
		20
		-
		20
		20
		355
		zimfact
		-
		21
		21
		21
		21
		21
		-
		21
		-
		21
		21
		179
		Total
		43837
		43837
		43837
		43837
		43837
		43837 43837
		43837
		43837
		43837
		260330
		Table 11: Summary statistics for claim collection
	</Abstractive Summary>
	<Extractive Summary> =
		The
		number of instances, as well as labels per domain,
		are shown in Table 6 and label names in Table 10
		in the appendix
	</Extractive Summary>
	<Extractive Summary> =
		1
		Multi-Domain Claim Veracity Prediction
		with Disparate Label Spaces
		Since not all fact checking websites use the same
		claim labels (see Table 6, and Table 10 in the ap-
		pendix), training a claim veracity prediction model
		is not entirely straight-forward
	</Extractive Summary>
</Paper ID=ument475>


<Paper ID=ument475> <Table ID =11>
	<Abstractive Summary> =
		Domain # Insts # Labels Labels
		abbc
		436
		3
		in-between, in-the-red, in-the-green
		afck
		433
		7
		correct, incorrect, mostly-correct, unproven, misleading, understated, exagger-
		ated
		bove
		295
		2
		none, rating: false
		chct
		355
		4
		verdict: true, verdict: false, verdict: unsubstantiated, none
		clck
		38
		3
		incorrect, unsupported, misleading
		faan
		111
		3
		factscan score: false, factscan score: true, factscan score: misleading
		faly
		71
		5
		true, none, partly true, unveriﬁed, false
		fani
		20
		3
		conclusion: accurate, conclusion: false, conclusion: unclear
		farg
		485
		11
		false, none, distorts the facts, misleading, spins the facts, no evidence, not the
		whole story, unsupported, cherry picks, exaggerates, out of context
		goop
		2943
		6
		0, 1, 2, 3, 4, 10
		hoer
		1310
		7
		facebook scams, true messages, bogus warning, statirical reports, fake news,
		unsubstantiated messages, misleading recommendations
		huca
		34
		3
		a lot of baloney, a little baloney, some baloney
		mpws
		47
		3
		accurate, false, misleading
		obry
		59
		4
		mostly true, veriﬁed, unobservable, mostly false
		para
		222
		7
		mostly false, mostly true, half-true, false, true, pants on ﬁre!, half ﬂip
		peck
		65
		3
		false, true, partially true
		pomt
		15390 9
		half-true, false, mostly true, mostly false, true, pants on ﬁre!, full ﬂop, half ﬂip,
		no ﬂip
		pose
		1361
		6
		promise kept, promise broken, compromise, in the works, not yet rated, stalled
		ranz
		21
		2
		fact, ﬁction
		snes
		6455
		12
		false, true, mixture, unproven, mostly false, mostly true, miscaptioned, legend,
		outdated, misattributed, scam, correct attribution
		thet
		79
		6
		none, mostly false, mostly true, half true, false, true
		thal
		74
		2
		none, we rate this claim false
		tron
		3423
		27
		ﬁction!, truth!, unproven!, truth! & ﬁction!, mostly ﬁction!, none, disputed!,
		truth! & misleading!, authorship conﬁrmed!, mostly truth!, incorrect attribu-
		tion!, scam!, investigation pending!, conﬁrmed authorship!, commentary!, pre-
		viously truth! now resolved!, outdated!, truth! & outdated!, virus!, ﬁction! &
		satire!, truth! & unproven!, misleading!, grass roots movement!, opinion!, cor-
		rect attribution!, truth! & disputed!, inaccurate attribution!
		vees
		504
		4
		none, fake, misleading, false
		vogo
		653
		8
		none, determination: false, determination: true, determination: mostly true,
		determination: misleading, determination: barely true, determination: huckster
		propaganda, determination: false, determination: a stretch
		wast
		201
		7
		4 pinnochios, 3 pinnochios, 2 pinnochios, false, not the whole story, needs
		context, none
		Table 10: Number of instances, and labels per domain sorted by number of occurrences
		4697
		Website
		Domain Claims Labels Category Speaker Checker Tags
		Article Claim date Publish date Full text Outlinks
		abc
		abbc
		436
		436
		436
		-
		-
		436
		436
		-
		436
		436
		7676
		africacheck
		afck
		436
		436
		-
		-
		-
		-
		436
		-
		436
		436
		2325
		altnews
		-
		496
		-
		-
		-
		496
		-
		496
		-
		496
		496
		6389
		boomlive
		-
		302
		302
		-
		-
		-
		-
		302
		-
		302
		302
		6054
		checkyourfact
		chht
		358
		358
		-
		-
		358
		-
		-
		-
		358
		358
		5271
		climatefeedback
		clck
		45
		45
		-
		-
		-
		-
		45
		-
		45
		45
		489
		crikey
		-
		18
		18
		18
		-
		18
		18
		18
		-
		18
		18
		212
		factcheckni
		-
		36
		36
		36
		-
		-
		-
		36
		-
		-
		36
		151
		factcheckorg
		farg
		512
		512
		512
		512
		512
		512
		512
		512
		512
		512
		8282
		factly
		-
		77
		77
		-
		-
		-
		-
		77
		-
		-
		77
		658
		factscan
		-
		115
		115
		-
		115
		-
		-
		-
		115
		115
		115
		1138
		fullfact
		-
		336
		336
		336
		-
		336
		-
		336
		-
		336
		336
		3838
		gossipcop
		goop
		2947
		2947
		-
		-
		2947
		-
		2947
		-
		2947
		2947
		12583
		hoaxslayer
		hoer
		1310
		1310
		-
		-
		1310
		-
		1310
		-
		1310
		1310
		14499
		hufﬁngtonpostca
		huca
		38
		38
		-
		38
		38
		-
		38
		38
		38
		38
		78
		leadstories
		-
		1547
		1547
		-
		-
		1547
		-
		1547
		-
		1547
		1547
		12015
		mprnews
		mpws
		49
		49
		-
		-
		49
		-
		49
		-
		49
		49
		319
		nytimes
		-
		17
		17
		-
		-
		17
		-
		17
		-
		17
		17
		271
		observatory
		obry
		60
		60
		-
		-
		60
		-
		60
		-
		60
		60
		592
		pandora
		para
		225
		225
		225
		225
		225
		-
		225
		-
		225
		225
		114
		pesacheck
		peck
		67
		67
		-
		-
		67
		-
		67
		-
		67
		67
		521
		politico
		-
		102
		102
		-
		-
		102
		-
		102
		-
		102
		102
		150
		politifact promise pose
		1361
		1361
		1361
		1361
		-
		-
		1361
		-
		1361
		1361
		6279
		politifact stmt
		pomt
		15390
		15390
		-
		15390
		-
		-
		-
		15390
		15390
		15390
		78543
		politifact story
		-
		5460
		-
		-
		-
		5460
		-
		-
		-
		5460
		5460
		24836
		radionz
		ranz
		32
		32
		32
		32
		-
		-
		32
		32
		32
		32
		44
		snopes
		snes
		6457
		6457
		6457
		-
		6457
		-
		6457
		-
		6457
		6457
		46735
		swissinfo
		-
		20
		20
		20
		20
		20
		-
		20
		-
		20
		20
		40
		theconversation
		-
		62
		62
		62
		62
		62
		62
		62
		-
		62
		62
		723
		theferret
		thet
		81
		81
		81
		81)
		-
		-
		81
		-
		81(81)
		81
		885
		theguardian
		-
		155
		155
		155
		-
		155
		-
		155
		-
		155
		155
		2600
		thejournal
		thal
		179
		179
		-
		-
		-
		-
		179
		-
		179
		179
		2375
		truthorﬁction
		tron
		3674
		3674
		3674
		-
		-
		3674
		3674
		-
		3674
		3674
		8268
		veraﬁles
		vees
		509
		509
		-
		-
		-
		509
		509
		-
		509
		509
		23
		voiceofsandiego
		vogo
		660
		660
		-
		-
		-
		-
		660
		-
		660
		660
		2352
		washingtonpost
		wast
		227
		227
		-
		227
		227
		-
		227
		-
		227
		227
		2470
		wral
		-
		20
		20
		-
		-
		20
		20
		20
		-
		20
		20
		355
		zimfact
		-
		21
		21
		21
		21
		21
		-
		21
		-
		21
		21
		179
		Total
		43837
		43837
		43837
		43837
		43837
		43837 43837
		43837
		43837
		43837
		260330
		Table 11: Summary statistics for claim collection
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Dataset Construction
		We crawled a total of 43,837 claims with their
		metadata (see details in Table 11)
	</Extractive Summary>
	<Extractive Summary> =
		38 websites in total (shown in Table 11)
	</Extractive Summary>
	<Extractive Summary> =
		Domain names are from hereon af-
		ter abbreviated for brevity, see Table 11 in the ap-
		pendix for correspondences to full website names
	</Extractive Summary>
</Paper ID=ument475>


<Paper ID=ument476> <Table ID =1>
	<Abstractive Summary> =
		,
		Datasets
		Zhihu
		Cora
		HepTh
		Vertices
		10000
		2277
		1038
		Edges
		43894
		5214
		1990
		#(Edges)
		2191
		93
		24
		*(Text)
		190
		90
		54
		Labels
		-
		7
		-
		Table 1: Statistics of datasets, where #(Edges) denotes
		the max number of the connective relationship of a
		node, and *(Text) denotes the average lengths of the
		text
	</Abstractive Summary>
</Paper ID=ument476>


<Paper ID=ument476> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: AUC scores for link prediction on Zhihu
	</Abstractive Summary>
	<Extractive Summary> =
		The AUC scores of
		different models under proportions ranging from
		15% to 95% on Zhihu, Cora and HepTh datasets
		are shown in Table 2, Table 3 and Table 4, re-
		spectively, with the best performance highlighted
		in bold
	</Extractive Summary>
	<Extractive Summary> =
		As can be seen from Table 2, our proposed
		method outperforms all other baselines in Zhihu
		dataset substantially, with approximately a 10 per-
		cent improvement over the current state-of-the-art
		WANE model
	</Extractive Summary>
</Paper ID=ument476>


<Paper ID=ument476> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: AUC scores for link prediction on Cora
	</Abstractive Summary>
	<Extractive Summary> =
		The AUC scores of
		different models under proportions ranging from
		15% to 95% on Zhihu, Cora and HepTh datasets
		are shown in Table 2, Table 3 and Table 4, re-
		spectively, with the best performance highlighted
		in bold
	</Extractive Summary>
</Paper ID=ument476>


<Paper ID=ument476> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: AUC scores for link prediction on HepTh
	</Abstractive Summary>
	<Extractive Summary> =
		The AUC scores of
		different models under proportions ranging from
		15% to 95% on Zhihu, Cora and HepTh datasets
		are shown in Table 2, Table 3 and Table 4, re-
		spectively, with the best performance highlighted
		in bold
	</Extractive Summary>
	<Extractive Summary> =
		For the other two datasets, as shown in Table
		3 and Table 4, our proposed method outperforms
		baseline methods overall
	</Extractive Summary>
</Paper ID=ument476>


<Paper ID=ument477> <Table ID =1>
	<Abstractive Summary> =
		004
		# components
		1
		1
		1
		homophily
		38%
		60%
		68%
		Table 1: Statistics for each dataset: Number of tweets;
		percentage of tweets for which we are able to retrieve
		information about the author; number of nodes; num-
		ber of edges; density; number of connected compo-
		nents; and amount of homophily as percentage of con-
		nected authors whose tweets share the same label
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarises the main statistics of the
		datasets and their respective graphs
	</Extractive Summary>
</Paper ID=ument477>


<Paper ID=ument477> <Table ID =2>
	<Abstractive Summary> =
		674∗⋄†
		Table 2: Results for all the models on the three datasets
		in our experiment
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2 we report the results,
		that we compute as the average of ten runs with
		random parameter initialization
	</Extractive Summary>
</Paper ID=ument477>


<Paper ID=ument477> <Table ID =3>
	<Abstractive Summary> =
		Your love never fails
		AGAINST atheism
		(3) Why are Tumblr feminists so territorial?
		Pro-lifers can’t voice their opinions without
		being attacked
		AGAINST abortion
		(4) @user No, just pointed out how idiotic your
		statement was
		HATEFUL
		Table 3: Examples from Stance (2, 3) and Hate Speech
		(1, 4) datasets, and their gold label
	</Abstractive Summary>
	<Extractive Summary> =
		For example, tweet (1)
		in Table 3 is incorrectly labelled as NORMAL by
		the LING model
	</Extractive Summary>
	<Extractive Summary> =
		For example, tweet (2) in Table 3 is authored by a
		user who is socially connected to other users who
		tweet against atheism (the orange cluster in the
		right-hand side plot of Figure 2)
	</Extractive Summary>
	<Extractive Summary> =
		This is illustrated by how the models deal
		with tweet (3) in Table 3, which expresses a nega-
		13The plot showing N2V user representations for the Hate
		Speech dataset is in the supplementary material
	</Extractive Summary>
	<Extractive Summary> =
		For example, the
		LING+N2V model correctly classiﬁes tweet (4) in
		Table 3, as the N2V vector of its author is close in
		social space to that of other users who post hate-
		ful tweets (7 out of 10 closest neighbours)
	</Extractive Summary>
</Paper ID=ument477>


<Paper ID=ument478> <Table ID =1>
	<Abstractive Summary> =
		, center
		Topic 2
		two, samour, family, veronica, son
		Topic 3
		would, hospital, also, car, hyundai
		Topic 4
		said, people, one, years, think
		Topic 5
		city, 6-4, last, wine, york
		Table 1: Randomly sampled topics and top keywords
		derived from a 50-topic LDA model trained on a sam-
		ple of COHA documents
	</Abstractive Summary>
	<Extractive Summary> =
		These cor-
		pora are largely untouched by political scientists;
		to illustrate some problems that arise with study-
		ing such data, Table 1 shows a sample of topics
		Topic 1
		like, day, would, a
	</Extractive Summary>
	<Extractive Summary> =
		2
		Datasets
		Does adaptive ensembling yield better topics?
		In Table 1, we showed that applying LDA di-
		rectly on COHA yields noisy, unrecognizable top-
		ics
	</Extractive Summary>
</Paper ID=ument478>


<Paper ID=ument478> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Framework results for the binary label task
		(left) and multi-label task (right)
	</Abstractive Summary>
</Paper ID=ument478>


<Paper ID=ument478> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Model results with adaptive ensembling for
		the binary label task (left) and multi-label task (right)
	</Abstractive Summary>
</Paper ID=ument478>


<Paper ID=ument478> <Table ID =4>
	<Abstractive Summary> =
		Topic 1
		dr, women, week, medical, doctors
		Topic 2
		city, police, street, car, avenue
		Topic 3
		trial, years, police, prison, court
		Topic 4
		union, strike, workers, lewis, service
		Topic 5
		like, man, years, little, week
		Table 4: Randomly sampled topics and top keywords
		derived from a 50-topic LDA model trained on 28K
		COHA articles identiﬁed as political using the SOURCE
		ONLY model
	</Abstractive Summary>
	<Extractive Summary> =
		The SOURCE ONLY results are
		shown in Table 4 and the adaptive ensembling re-
		sults are shown in Table 5
	</Extractive Summary>
</Paper ID=ument478>


<Paper ID=ument478> <Table ID =5>
	<Abstractive Summary> =
		Topic 1
		vietnam, hanoi, atomic, bombing, south
		Topic 2
		germany, britain, france, europe, soviet
		Topic 3
		court, justice, commission, law, attorney
		Topic 4
		tax, oil, prices, petroleum, industry
		Topic 5
		coal, union, strike, workers, miners
		Table 5: Randomly sampled topics and top keywords
		derived from a 50-topic LDA model trained on 28K
		COHA articles identiﬁed as political using ADAPTIVE
		ENSEMBLING
	</Abstractive Summary>
</Paper ID=ument478>


<Paper ID=ument478> <Table ID =6>
	<Abstractive Summary> =
		com/gensim/
		Political
		Non-Political
		AG
		PE
		IR
		Train
		333
		8
		156
		497
		Dev
		82
		1
		33
		116
		Test
		125
		8
		47
		208
		Table 6: Distribution of train (In-Domain benchmark
		only), dev, test documents in our expert-annotated
		COHA subcorpus
	</Abstractive Summary>
</Paper ID=ument478>


<Paper ID=ument478> <Table ID =7>
	<Abstractive Summary> =
		Area Label
		Topic
		AG
		PE
		IR
		International Relations
		✓
		Presidents and Presidency (US)
		✓
		Presidential Elections (US)
		✓
		War and Revolution
		✓
		Presidential Election of 2000
		✓
		Presidential Election of 2004
		✓
		Law and Legislation
		✓
		Civil War and Guerrilla Warfare
		✓
		International Trade and World Market
		✓
		Presidential Election of 1996
		✓
		Public Opinion
		Economic Conditions and Trends
		✓
		Bombs and Explosives
		✓
		Arms Sales Abroad
		✓
		United States Economy
		✓
		Missiles and Missile Defense Systems
		✓
		Oil (Petroleum) and Gasoline
		✓
		Appointments and Executive Changes
		✓
		Foreign Service
		✓
		Prisoners of War
		✓
		War Crimes, Genocide and Crimes
		Against Humanity
		✓
		Vice Presidents and Vice Presidency
		(US)
		✓
		Arms Control and Limitation and Dis-
		armament
		✓
		Military Bases and Installations
		✓
		Presidential Election of 2008
		✓
		Whitewater Case
		✓
		Vietnam War
		✓
		✓
		Governors (US)
		✓
		Energy and Power
		✓
		Stocks and Bonds
		✓
		State of the Union Message (US)
		✓
		Wages and Salaries
		✓
		Church-State Relations
		✓
		Shiite Muslims
		✓
		Special
		Prosecutors
		(Independent
		Counsel)
		✓
		White House (Washington, DC)
		✓
		Federal Taxes (US)
		✓
		Illegal Aliens
		✓
		Social Security (US)
		✓
		Political Prisoners
		✓
		✓
		Watergate Affair
		✓
		Government Employees
		✓
		Sunni Muslims
		✓
		Third World and Developing Countries
		✓
		Customs (Tariff)
		✓
		Welfare (US)
		✓
		Gun Control
		✓
		Global Warming
		✓
		Interest Rates
		✓
		Vetoes (US)
		✓
		Futures and Options Trading
		✓
		Attorneys General
		✓
		Layoffs and Job Reductions
		✓
		Nazi Policies Toward Jews and Minori-
		ties
		✓
		Government Bonds
		✓
		Police Brutality and Misconduct
		✓
		Executive Privilege, Doctrine of
		✓
		Table 7: Political descriptors in NYT
	</Abstractive Summary>
</Paper ID=ument478>


<Paper ID=ument479> <Table ID =1>
	<Abstractive Summary> =
		97
		Table 1: A brief summary of our datasets
	</Abstractive Summary>
</Paper ID=ument479>


<Paper ID=ument479> <Table ID =2>
	<Abstractive Summary> =
		CNN
		3,4,5
		3,4,5
		3,4,5
		Filter number
		for each size
		100
		100
		100
		h: number of heads
		10
		10
		10
		L: layers of
		transformer encoder
		3
		3
		3
		λ: initial penalty term
		1
		1
		1
		α: weight for country
		supervision
		1
		1
		1
		Dff: inner
		dimension of FFN
		2400
		2400
		2400
		Max number of
		tweets per user
		100
		50
		20
		Table 2: A summary of hyperparameter settings of our
		model
	</Abstractive Summary>
</Paper ID=ument479>


<Paper ID=ument479> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Comparisons between our method and baselines
	</Abstractive Summary>
</Paper ID=ument479>


<Paper ID=ument479> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: An ablation study on WNUT dataset
	</Abstractive Summary>
</Paper ID=ument479>


<Paper ID=ument48> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: ∆(Lat
	</Abstractive Summary>
</Paper ID=ument48>


<Paper ID=ument48> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Changes in accuracy when adding a directed
		edge from the label to the input, i
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 2 show the
		performance gains after adding this edge, which
		are all positive and sometimes very large
	</Extractive Summary>
</Paper ID=ument48>


<Paper ID=ument48> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Accuracy comparison of standard generative
		(Gen
	</Abstractive Summary>
	<Extractive Summary> =
		PC in Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		PC in Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the results with different conﬁg-
		urations, including the choices mentioned above
		as well as the results from earlier conﬁgurations
		mentioned in the paper
	</Extractive Summary>
</Paper ID=ument48>


<Paper ID=ument48> <Table ID =4>
	<Abstractive Summary> =
		9 (0)
		Table 4: Comparison of the classiﬁcation accuracy and
		convergence speed of the classiﬁers trained with di-
		rect optimization (Direct) of the log marginal likeli-
		hood and the EM algorithm (EM)
	</Abstractive Summary>
	<Extractive Summary> =
		Our results in Table 4 show that the
		direct approach and the EM algorithm have simi-
		lar performance in terms of classiﬁcation accuracy
		and convergence speed in optimizing the parame-
		ters of our latent models
	</Extractive Summary>
</Paper ID=ument48>


<Paper ID=ument48> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Latent variable values (“id”), our manually-deﬁned descriptions, and examples of instances associated to
		them
	</Abstractive Summary>
</Paper ID=ument48>


<Paper ID=ument48> <Table ID =6>
	<Abstractive Summary> =
		com & lt
		; / B & gt ; & lt
		Table 6: Generated examples by controlling the latent variables and labels (world, sport, business, sci/tech) with
		our latent classiﬁer trained on a small subset of the AGNews dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows some gener-
		ated examples
	</Extractive Summary>
</Paper ID=ument48>


<Paper ID=ument480> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1: Comparison of the capabilities of each baseline and our CRAFT models (full and without the Context En-
		coder) with regards to capturing inter-comment (D)ynamics, processing conversations in an (O)nline fashion, and
		automatically (L)earning feature representations, as well as their performance in terms of (A)ccuracy, (P)recision,
		(R)ecall, False Positive Rate (FPR), and F1 score
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 compares CRAFT to the base-
		lines on the test splits (random baseline is 50%)
		and illustrates several key ﬁndings
	</Extractive Summary>
</Paper ID=ument480>


<Paper ID=ument481> <Table ID =1>
	<Abstractive Summary> =
		fm
		85k
		No
		threat/blackmail, insult, defamation,
		sexual talk, curse/exclusion, defense,
		encouragement to the harasser
		No
		Ours
		Reddit
		22k
		Yes
		hate, non-hate
		Yes
		Ours
		Gab
		34k
		Yes
		hate, non-hate
		Yes
		Table 1: Comparison of our datasets with previous hate speech datasets
	</Abstractive Summary>
</Paper ID=ument481>


<Paper ID=ument481> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: Experimental results for the detection task
	</Abstractive Summary>
	<Extractive Summary> =
		The word embeddings are randomly ini-
		tialized (CNN in Table 2) or initialized with pre-
		trained Word2Vec (Mikolov et al
	</Extractive Summary>
	<Extractive Summary> =
		, 2013) embed-
		dings on Google News (CNN∗ in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2 and 3, all the classiﬁcation
		4762
		Dataset
		Gab
		Reddit
		Inp
	</Extractive Summary>
</Paper ID=ument481>


<Paper ID=ument481> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Experimental results for generative intervention task
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results and Discussion
		The experimental results of the detection task and
		the generative intervention task are shown in Ta-
		ble 2 and Table 3 separately
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 3, applying Reinforcement
		Learning does not lead to higher scores on the
		three automatic metrics
	</Extractive Summary>
	<Extractive Summary> =
		The inconsistency be-
		tween the human evaluation results and the auto-
		matic ones indicates the automatic evaluation met-
		rics listed in Table 3 can hardly reﬂect the quality
		of the generated responses
	</Extractive Summary>
</Paper ID=ument481>


<Paper ID=ument481> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Human evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		However, human evalu-
		ation (Table 4) shows that the RL model creates
		responses that are potentially better at mitigating
		hate speech and are more diverse, which is con-
		sistent with Li et al
	</Extractive Summary>
</Paper ID=ument481>


<Paper ID=ument482> <Table ID =1>
	<Abstractive Summary> =
		We also ask
		the annotator to write down thoughts and observa-
		4768
		Tweets
		“@user do you think it’s cool yo ass
		slow ? You sound dumb asf”
		“Rob you fucking goofys that got some to say
		about rage just to show y’all bitches”
		“GlizzyGang Bitch We Got Out Glocks Up
		Table 1:
		Typical aggression tweets in our dataset,
		which includes annotations with human rationales (un-
		derlined)
	</Abstractive Summary>
</Paper ID=ument482>


<Paper ID=ument482> <Table ID =2>
	<Abstractive Summary> =
		1
		Context Label Types
		Yes
		No
		N/A
		Context
		107
		74
		148
		Mention/Retweet
		36
		35
		258
		URL
		9
		3
		317
		Picture
		14
		15
		300
		Table 2: Whether aggressive intent is observed in Con-
		text, Mention/Retweet, URL and/or Picture
	</Abstractive Summary>
</Paper ID=ument482>


<Paper ID=ument482> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Results comparing different model architectures
	</Abstractive Summary>
</Paper ID=ument482>


<Paper ID=ument482> <Table ID =4>
	<Abstractive Summary> =
		14
		Table 4: Results on (averaged) rationale rank (RR),
		fraction of tweets when RR = 0 and RR = 1, respec-
		tively
	</Abstractive Summary>
</Paper ID=ument482>


<Paper ID=ument482> <Table ID =5>
	<Abstractive Summary> =
		96
		Table 5: The number of model’s prediction ﬂip from non-aggressive to aggressive, out of the 800 attacking tweets
		generated by inserting a speciﬁed “neutral” unigram
	</Abstractive Summary>
</Paper ID=ument482>


<Paper ID=ument483> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of code-switched posts in English-Tagalog, English-Romanian, and English-Greek
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents
		some examples of code-switched utterances in the
		dataset, both intra- and inter-sentential
	</Extractive Summary>
</Paper ID=ument483>


<Paper ID=ument483> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Statistics of the main CodeSwitch-Reddit corpus (left) and the additional CS dataset (right)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 (left) reports details on the corpus,
		which has over 135K posts, with an average of
		75 tokens per post, thereby introducing a unique
		resource facilitating further research in this ﬁeld
	</Extractive Summary>
	<Extractive Summary> =
		) Table 2 (right) reports the de-
		tails of the ﬁve additional language pairs that had
		at least 70% precision
	</Extractive Summary>
</Paper ID=ument483>


<Paper ID=ument483> <Table ID =3>
	<Abstractive Summary> =
		, T K
		2 ,
		where N and K are the number of topics in M1
		and M2, respectively, we deﬁne the similarity be-
		4781
		Topics with highest coherence scores in code-switched posts
		Topics with highest coherence scores in monolingual posts
		CS-1
		CS-2
		CS-3
		CS-4
		CS-5
		M-1
		M-2
		M-3
		M-4
		M-5
		student
		friend
		make
		give
		phone
		thing
		vote
		problem
		learn
		plan
		exam
		feel
		thing
		parent
		check
		person
		power
		change
		student
		price
		study
		girl
		happen
		live
		driver
		love
		political
		issue
		study
		cost
		teacher
		love
		talk
		chance
		internet
		call
		support
		reason
		experience
		cheap
		grade
		date
		wrong
		reason
		price
		wrong
		rule
		fact
		teach
		sell
		graduate
		close
		word
		plan
		mall
		happen
		majority
		deal
		education
		stay
		pass
		break
		hate
		late
		shop
		hate
		politician
		current
		skill
		travel
		subject
		relationship
		joke
		decide
		trafﬁc
		understand
		citizen
		control
		teacher
		visit
		program
		happy
		sense
		child
		store
		kind
		election
		matter
		research
		expensive
		review
		meet
		mind
		baby
		brand
		accept
		leader
		point
		test
		compare
		Table 3: Most coherent topics identiﬁed in code-switched and monolingual posts by the same set of authors
	</Abstractive Summary>
</Paper ID=ument483>


<Paper ID=ument483> <Table ID =4>
	<Abstractive Summary> =
		]
		Table 4: Examples of CS posts (with translations) associated with emotion, sentiment, and relationships
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents a few examples of code-
		switched posts associated most with the relation-
		ship topic in the model
	</Extractive Summary>
</Paper ID=ument483>


<Paper ID=ument484> <Table ID =1>
	<Abstractive Summary> =
		8
		105,354
		–
		1,067
		638
		697
		Table 1: Statistics of two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the statistics of two datasets
	</Extractive Summary>
</Paper ID=ument484>


<Paper ID=ument484> <Table ID =2>
	<Abstractive Summary> =
		751
		Table 2: Results of rumor stance classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Performance Comparison
		Table 2 shows the
		results of different methods for rumor stance clas-
		siﬁcation
	</Extractive Summary>
</Paper ID=ument484>


<Paper ID=ument484> <Table ID =3>
	<Abstractive Summary> =
		361
		Table 3: Results of veracity prediction
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Performance Comparison
		Table 3 shows the
		comparisons of different methods
	</Extractive Summary>
</Paper ID=ument484>


<Paper ID=ument484> <Table ID =4>
	<Abstractive Summary> =
		326
		Table 4: Ablation tests of stance features and temporal
		modeling for veracity prediction on PHEME dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows
		that “– stance features” performs poorly, and thus
		the temporal modeling process beneﬁts from the in-
		dicative signals provided by stance features
	</Extractive Summary>
	<Extractive Summary> =
		Results in Table 4 verify that replacing
		or removing the GRU block hurts the performance,
		and thus modeling the stance evolution of public
		reactions towards a rumorous message is indeed
		necessary for effective veracity prediction
	</Extractive Summary>
</Paper ID=ument484>


<Paper ID=ument484> <Table ID =5>
	<Abstractive Summary> =
		438
		Table 5: Stance classiﬁcation results w
	</Abstractive Summary>
	<Extractive Summary> =
		B
		Numerical Results of Figure 4
		Table 5 shows the exact numerical numbers of the
		results in Figure 4
	</Extractive Summary>
</Paper ID=ument484>


<Paper ID=ument485> <Table ID =1>
	<Abstractive Summary> =
		86
		Number of Seen Intents
		5
		24
		Number of Unseen Intents
		2
		6
		Table 1: Dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		summarizes the dataset statistics
	</Extractive Summary>
</Paper ID=ument485>


<Paper ID=ument485> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		dw
		dh
		da
		dp
		R
		nroute
		SNIPS-NLU
		300
		16
		10
		10
		3
		3
		SMP-2018
		300
		32
		30
		10
		8
		3
		Table 2: Network structure hyperparameters
	</Abstractive Summary>
</Paper ID=ument485>


<Paper ID=ument485> <Table ID =3>
	<Abstractive Summary> =
		4769
		Table 3: Results of zero-shot intent classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 sum-
		marizes the average results over 10 runs, where the
		top 2 results are highlighted in bold
	</Extractive Summary>
</Paper ID=ument485>


<Paper ID=ument485> <Table ID =4>
	<Abstractive Summary> =
		5124
		Table 4: Results of generalized zero-shot intent classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the average results over 10 runs,
		where the top 2 results are highlighted in bold
	</Extractive Summary>
</Paper ID=ument485>


<Paper ID=ument486> <Table ID =1>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument486>


<Paper ID=ument486> <Table ID =2>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument486>


<Paper ID=ument486> <Table ID =3>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument486>


<Paper ID=ument486> <Table ID =4>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		In
		Table 4, we compare our model with three recently
		proposed methods for person-job ﬁt
	</Extractive Summary>
</Paper ID=ument486>


<Paper ID=ument486> <Table ID =5>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 presents the performance
		of different comparison methods for domain adap-
		tation
	</Extractive Summary>
</Paper ID=ument486>


<Paper ID=ument486> <Table ID =6>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6,
		all the three factors are useful to improve the
		performance of our model
	</Extractive Summary>
</Paper ID=ument486>


<Paper ID=ument487> <Table ID =1>
	<Abstractive Summary> =
		1 (63%)
		2
		Table 1: Statistics of the datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the statistics of the datasets,
		including the number of documents, the number of
		average tokens and entities, the number of classes,
		and the proportion of texts containing entities in
		parentheses
	</Extractive Summary>
</Paper ID=ument487>


<Paper ID=ument487> <Table ID =2>
	<Abstractive Summary> =
		21∗
		Table 2: Test accuracy (%) of different models on six standard datasets
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results
		Table 2 shows the classiﬁcation accuracy of dif-
		ferent methods on 6 benchmark datasets
	</Extractive Summary>
	<Extractive Summary> =
		We can see from Table 2, HGAT w/o ATT con-
		sistently outperforms GCN-HIN on all datasets,
		demonstrating the effectiveness of our proposed
		heterogeneous graph convolution which consid-
		ers the heterogeneity of various information types
	</Extractive Summary>
</Paper ID=ument487>


<Paper ID=ument487> <Table ID =3>
	<Abstractive Summary> =
		21∗
		Table 3: Test accuracy (%) of our variants
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, we compare our HGAT
		with four variant models
	</Extractive Summary>
</Paper ID=ument487>


<Paper ID=ument489> <Table ID =1>
	<Abstractive Summary> =
		1936
		Table 1: Acc and MCC Results of stock movements in
		the test set
		In Table 1, test results are presented according
		to two measurements, namely the accuracy (Acc)
		and the Matthews Correlation Coefﬁcient (MCC)
	</Abstractive Summary>
</Paper ID=ument489>


<Paper ID=ument489> <Table ID =2>
	<Abstractive Summary> =
		53%
		Table 2: Evaluation results of news recommendation
		From Table 2, it is observed that the success
		rate improves signiﬁcantly after ﬁne-tuning the
		news vectors with respect to the news browsing
		sequences
	</Abstractive Summary>
</Paper ID=ument489>


<Paper ID=ument49> <Table ID =1>
	<Abstractive Summary> =
		8
		Table 1: The mean precision of different baselines with optimal hyperparamters on the NIPS dataset
	</Abstractive Summary>
	<Extractive Summary> =
		The reviewer-submission matching results of
		our model on the NIPS dataset are presented in
		Table 1 alongside those of our chosen baselines
	</Extractive Summary>
</Paper ID=ument49>


<Paper ID=ument49> <Table ID =2>
	<Abstractive Summary> =
		9
		≥2
		100
		Table 2: Percentage of reviewers in levels of expertise
		to the submissions recommended by our model
	</Abstractive Summary>
</Paper ID=ument49>


<Paper ID=ument49> <Table ID =3>
	<Abstractive Summary> =
		Reviewer 1 used
		526
		Reviewer
		TREC
		Research topics
		CT
		HT
		LDA
		Doc2Vec
		WMD
		1
		1
		Speech recognition with Bayesian approach, Neural network
		6
		2
		4
		1
		7
		2
		0
		Online learning, Sequential prediction, Bayes point machine
		10
		10
		8
		9
		1
		3
		2
		Bayesian network, Variational Bayes estimation, Mixture models
		1
		4
		3
		2
		9
		4
		3
		Variational method, Bayesian learning, Markov model
		2
		3
		9
		4
		8
		5
		3
		Bayesian learning, Variational method, Active learning
		3
		5
		1
		7
		6
		Table 3: Examples of reviewers and their relevance to the submission ranked by different algorithms
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3,
		We also present the rank of each reviewer given by
		the models
	</Extractive Summary>
</Paper ID=ument49>


<Paper ID=ument49> <Table ID =4>
	<Abstractive Summary> =
		]
		Table 4: An example of abstract from a submission
	</Abstractive Summary>
</Paper ID=ument49>


<Paper ID=ument490> <Table ID =1>
	<Abstractive Summary> =
		8140
		Table 1: SEQ vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that the
		SEQ model outperforms the current SOTA system
		on all three text genres for binary CWI (statisti-
		cally signiﬁcant using McNemar’s test, p=0
	</Extractive Summary>
	<Extractive Summary> =
		Table 10 presents the entire REC-LS algo-
		rithm including the recursive simpliﬁcation step
	</Extractive Summary>
	<Extractive Summary> =
		For example, wealthy → rich in “could participate
		Table 10: REC-LS Algorithm Steps
		(4)
		Check if complexity of any word in S
		is above the threshold c
		(5)
		Check sentence for complex words
		minus those which are in ignore list
		(6)
		Sort complex words according to
		conﬁdence score
		(8)
		Generate substitutes for the top complex
		word
		(9)
		Convert substitutions to correct
		word form (e
	</Extractive Summary>
	<Extractive Summary> =
		4001
		Table 11: Lexical substitution results for DRESS-LS, P&S and our system (REC-LS) on three genres
		• Finally, correct stands for the proportion of
		instances where the top lexical substitution
		returned by the system is exactly the same as
		the gold standard one
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =2>
	<Abstractive Summary> =
		4460
		Table 2: SEQ results for CWI on the two datasets
		SEQ uses 300-dimensional GloVe embeddings
		as word representations (Pennington et al
	</Abstractive Summary>
	<Extractive Summary> =
		To further assess generalisabil-
		ity of the model, we test it on CEFR-LS, as well
		as BENCHLS for consistency (see Table 2)
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Contextual simplicity scores for 2 possible al-
		ternatives (S1-S2) for the original O
		et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		2 possible substitutions for engulfed, with their re-
		spective simplicity scores
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =4>
	<Abstractive Summary> =
		]
		Table 4: Contextual semantic equivalence scores for 3
		candidate substitutions (S1-S3) for the original O
		O
		Oak is strong and also gives shade
		g
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 provides some
		examples for this score
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =5>
	<Abstractive Summary> =
		Oak gives shade
		Table 5: Original sentence O and the grammatical con-
		text (g
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 provides an example of a full sentence O
		and the grammatical context for the word gives
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =6>
	<Abstractive Summary> =
		]
		bigram
		was able
		able of
		freq
		10281
		0
		Table 6: Bigram frequencies for left and right contexts
		of a candidate substitute
		Here, we use the bigram frequencies as a proxy
		for grammaticality: although “able” and “capable”
		are semantically similar, it is the grammatical con-
		straints, captured by the bigram frequencies, that
		rule one of the alternatives out
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows one such
		example
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =7>
	<Abstractive Summary> =
		6615
		Table 7: Ranking results on BENCHLS dataset
		CEFR-LS
		n=1
		n=2
		n=3
		MRR
		S
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows that ranking with S+ ¯C works best
		according to all measures and across both sets
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =8>
	<Abstractive Summary> =
		4736
		Table 8: Ranking results on CEFR-LS dataset
		r≤n is ranked ﬁrst by the system
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 reports the ranking results on the
		CEFR-LS data
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =9>
	<Abstractive Summary> =
		45
		Simplify CW
		prepare for a dangerous trip
		Simplify all
		arrange for a dangerous trip
		Table 9: Recursive simpliﬁcation compared with sim-
		plifying only complex words and simplifying all words
		We note that the only consequence of perform-
		ing simpliﬁcation recursively is that fewer words,
		or potentially different words, are simpliﬁed, and
		highlight that it does not lead to any error propa-
		gation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 exempliﬁes the beneﬁt of using the re-
		cursive simpliﬁcation approach REC-LS
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =10>
	<Abstractive Summary> =
		For example, wealthy → rich in “could participate
		Table 10: REC-LS Algorithm Steps
		(4)
		Check if complexity of any word in S
		is above the threshold c
		(5)
		Check sentence for complex words
		minus those which are in ignore list
		(6)
		Sort complex words according to
		conﬁdence score
		(8)
		Generate substitutes for the top complex
		word
		(9)
		Convert substitutions to correct
		word form (e
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10 presents the entire REC-LS algo-
		rithm including the recursive simpliﬁcation step
	</Extractive Summary>
</Paper ID=ument490>


<Paper ID=ument490> <Table ID =11>
	<Abstractive Summary> =
		4001
		Table 11: Lexical substitution results for DRESS-LS, P&S and our system (REC-LS) on three genres
		• Finally, correct stands for the proportion of
		instances where the top lexical substitution
		returned by the system is exactly the same as
		the gold standard one
	</Abstractive Summary>
</Paper ID=ument490>


<Paper ID=ument491> <Table ID =1>
	<Abstractive Summary> =
		4867
		Source
		# Sections
		# Sentences
		Textbooks
		807
		7,407
		Guidelines
		4,781
		53,013
		Wikipedia
		2,658
		24,830
		Table 1: Sections and sentences in each MedLit source
	</Abstractive Summary>
</Paper ID=ument491>


<Paper ID=ument491> <Table ID =2>
	<Abstractive Summary> =
		Source
		# Sections # Sentences Ratio
		Train MedLit
		7042
		72191
		10
		i2b2
		1680
		10093
		6
		ClvC
		294
		3467
		12
		Dev
		MedLit
		1204
		13059
		11
		i2b2
		1591
		9373
		6
		ClvC
		404
		3282
		8
		Test
		i2b2
		3098
		19110
		6
		ClvC
		404
		4046
		10
		Table 2: The number of sections and sentences as well
		as the ratio of sentences to sections in each of the three
		datasets for train, dev, and test sets
	</Abstractive Summary>
</Paper ID=ument491>


<Paper ID=ument491> <Table ID =3>
	<Abstractive Summary> =
		92∗
		(b) Testing on i2b2
		Table 3: Section-Level results for testing on the (a) ClvC and (b) i2b2 EHRs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3b shows results using the i2b2 test set
		as target
	</Extractive Summary>
	<Extractive Summary> =
		The bottom rows of Table 3b shows perfor-
		mance if we did not have i2b2 training data
	</Extractive Summary>
	<Extractive Summary> =
		1
		How much do out-of-domain EHRs help?
		One of our most interesting results using the full
		datasets is the improvement we are able to achieve
		training with a small amount of ClvC (out-of-
		domain) EHR data and testing on i2b2 as the tar-
		get domain (the last two rows of Table 3b and Ta-
		ble 4b)
	</Extractive Summary>
</Paper ID=ument491>


<Paper ID=ument491> <Table ID =4>
	<Abstractive Summary> =
		60∗
		(b) Testing on i2b2
		Table 4: Sentence-Level results for testing on the (a) ClvC and (b) i2b2 EHRs in Micro Avg F1-score
	</Abstractive Summary>
	<Extractive Summary> =
		0%
		25%
		50%
		75%
		100%
		ClvC
		i2b2
		medlit + TR i2b2
		Figure 3: F-score trends of our RNN model training
		on smaller portions of the i2b2 data for the i2b2 and
		MedLit + TR i2b2 experiments and testing on ClvC
		The results for testing on the ClvC data are
		shown in Table 4a
	</Extractive Summary>
	<Extractive Summary> =
		The sentence-level results for testing with i2b2
		as target are shown in Table 4b
	</Extractive Summary>
	<Extractive Summary> =
		To do this,
		we explored the training on i2b2 and testing on
		BERT i2b2, and BERT MedLit + TR i2b2 exper-
		iments (See the middle rows in Table 4b) where
		both models performed the same
	</Extractive Summary>
</Paper ID=ument491>


<Paper ID=ument491> <Table ID =5>
	<Abstractive Summary> =
		83
		Table 5: Average F1 score for training on ClvC and
		testing on i2b2 when examining the top 3 predictions
	</Abstractive Summary>
	<Extractive Summary> =
		Both ClvC and MedLit+TR ClvC showed large
		improvements for the RNN and BERT models as
		shown in Table 5 with roughly a 27 point im-
		provement for the RNN models and a 22 point im-
		provement for the BERT models
	</Extractive Summary>
</Paper ID=ument491>


<Paper ID=ument492> <Table ID =1>
	<Abstractive Summary> =
		01
		# positive samples
		503,698
		# negative samples
		9,970,795
		Table 1: Statistics of our dataset
	</Abstractive Summary>
</Paper ID=ument492>


<Paper ID=ument492> <Table ID =2>
	<Abstractive Summary> =
		4076
		Table 2: The results of different methods
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2, we have several observations
	</Extractive Summary>
</Paper ID=ument492>


<Paper ID=ument493> <Table ID =1>
	<Abstractive Summary> =
		1
		Datasets and Experimental Settings
		We conducted experiments on four widely used
		benchmark datasets in different domains and
		4889
		Dataset
		#users
		#items
		#reviews
		Toys
		19,412
		11,924
		167,597
		Kindle
		68,223
		61,935
		982,619
		Movies
		123,960
		50,052
		1,679,533
		Yelp
		199,445
		119,441
		3,072,129
		Table 1: Statistics of the benchmark datasets
	</Abstractive Summary>
</Paper ID=ument493>


<Paper ID=ument493> <Table ID =2>
	<Abstractive Summary> =
		Different from these methods, our RMG
		4890
		Information
		PMF
		NMF
		SVD++
		HFT
		DeepCoNN
		Attn+CNN
		NARRE
		RMG-review
		RMG
		Rating score
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Review text
		✓
		✓
		✓
		✓
		✓
		✓
		Context & word order
		✓
		✓
		✓
		✓
		✓
		Review attention
		✓
		✓
		✓
		Word attention
		✓
		✓
		✓
		Sentence attention
		✓
		✓
		User-item graph
		✓
		Table 2: Comparisons of the information used in different methods
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we present a simple comparison of
		different methods by summarizing the informa-
		tion they incorporated
	</Extractive Summary>
</Paper ID=ument493>


<Paper ID=ument493> <Table ID =3>
	<Abstractive Summary> =
		1113
		Table 3: The performance of different methods on the
		benchmark datasets
	</Abstractive Summary>
</Paper ID=ument493>


<Paper ID=ument494> <Table ID =1>
	<Abstractive Summary> =
		74
		Table 1: Experimental results on hard similarity dataset and transitive sentence similarity dataset
	</Abstractive Summary>
</Paper ID=ument494>


<Paper ID=ument494> <Table ID =2>
	<Abstractive Summary> =
		09
		Table 2: Case study of the cosine similarity score changes with incorporating the intent and sentiment
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Case Study
		To further analyse the effects of intents and emo-
		tions on the event representation learning, we
		present case studies in Table 2, which directly
		shows the changes of similarity scores before and
		after incorporating intent and sentiment
	</Extractive Summary>
</Paper ID=ument494>


<Paper ID=ument494> <Table ID =3>
	<Abstractive Summary> =
		03
		Table 3: Results of script event prediction on the test
		set
	</Abstractive Summary>
</Paper ID=ument494>


<Paper ID=ument495> <Table ID =1>
	<Abstractive Summary> =
		SST-2
		67,349
		1,821
		56
		1
		19
		IMDb
		25,000
		25,000
		2,738
		8
		262
		Table 1: The statistics of datasets
	</Abstractive Summary>
</Paper ID=ument495>


<Paper ID=ument495> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Examples of each type of attack
		ber of embeddings in the embedding corpus C
	</Abstractive Summary>
</Paper ID=ument495>


<Paper ID=ument495> <Table ID =3>
	<Abstractive Summary> =
		5106
		Table 3: Performance of SC and DISP on identifying perpetuated tokens
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the performance of DISP and
		SC in discriminating perturbations
	</Extractive Summary>
	<Extractive Summary> =
		lines, consistent with Table 3, SC performs the
		best for character-level attacks and the worst for
		word-level attacks
	</Extractive Summary>
</Paper ID=ument495>


<Paper ID=ument495> <Table ID =4>
	<Abstractive Summary> =
		9090
		Table 4: The accuracy scores of methods with different adversarial attacks on two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 reports the ac-
		curacy scores of all methods with different types
		of adversarial attacks on two datasets
	</Extractive Summary>
</Paper ID=ument495>


<Paper ID=ument495> <Table ID =5>
	<Abstractive Summary> =
		8005
		Table 5: The accuracy of DISP over different types of
		attacks on the SST-2 dataset with the tokens recovered
		by the perturbation discriminator and the embedding
		estimator trained on the IMDb dataset for robust trans-
		fer defense
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows
		the experimental results of robust transfer de-
		fense
	</Extractive Summary>
</Paper ID=ument495>


<Paper ID=ument495> <Table ID =6>
	<Abstractive Summary> =
		painfully; silly; one
		positive
		negative
		Table 6: A case study of recovered tokens in SST-2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 lists four
		4911
		Number of Attacks
		0
		1
		2
		3
		4
		Accuracy
		 30%
		 40%
		 50%
		 60%
		 70%
		 80%
		 90%
		100%
		BERT
		SC
		ADA
		AT
		DISP
		DISP+SC
		(a) Insertion
		Number of Attacks
		0
		1
		2
		3
		4
		Accuracy
		 30%
		 40%
		 50%
		 60%
		 70%
		 80%
		 90%
		100%
		BERT
		SC
		ADA
		AT
		DISP
		DISP+SC
		(b) Deletion
		Number of Attacks
		0
		1
		2
		3
		4
		Accuracy
		 30%
		 40%
		 50%
		 60%
		 70%
		 80%
		 90%
		100%
		BERT
		SC
		ADA
		AT
		DISP
		DISP+SC
		(c) Swap
		Number of Attacks
		0
		1
		2
		3
		4
		Accuracy
		 20%
		 30%
		 40%
		 50%
		 60%
		 70%
		 80%
		 90%
		100%
		BERT
		SC
		ADA
		AT
		DISP
		DISP+SC
		(d) Random
		Number of Attacks
		0
		1
		2
		3
		4
		Accuracy
		 30%
		 40%
		 50%
		 60%
		 70%
		 80%
		 90%
		100%
		BERT
		SC
		ADA
		AT
		DISP
		DISP+SC
		(e) Embed
		Number of Attacks
		0
		1
		2
		3
		4
		Accuracy
		 30%
		 40%
		 50%
		 60%
		 70%
		 80%
		 90%
		100%
		BERT
		SC
		ADA
		AT
		DISP
		DISP+SC
		(f) Overall
		Figure 4: The accuracy of methods over different numbers and types of attacks
	</Extractive Summary>
</Paper ID=ument495>


<Paper ID=ument495> <Table ID =7>
	<Abstractive Summary> =
		8083
		Table 7: The performance of DISP using ground-truth
		and recovered tokens over different types of attacks in
		SST-2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the accuracy
		scores over different types of attacks in the SST-
		2 dataset
	</Extractive Summary>
</Paper ID=ument495>


<Paper ID=ument495> <Table ID =8>
	<Abstractive Summary> =
		5502
		Table 8: The accuracy scores of BERT and DISP over
		different types of attacks on the CoLA dataset for the
		task of linguistic acceptability classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 presents the
		accuracy scores of BERT and DISP on the CoLA
		dataset with one adversarial attack of each type
	</Extractive Summary>
</Paper ID=ument495>


<Paper ID=ument496> <Table ID =1>
	<Abstractive Summary> =
		Symbols
		Descriptions
		xd
		the non-review features of paper d
		zd
		the ﬁnal review representation of paper d
		cd
		the citation count of paper d
		ad
		the abstract text of paper d
		rk
		the k-th review consisting of multiple review
		sentences
		sA
		j
		the
		learned
		representations
		of
		the
		j-th
		sentence in the abstract text
		sR
		k,j
		the
		learned
		representations
		of
		the
		j-th
		sentence in the k-th review text
		hS
		the dimension of sentence vectors
		nd
		the number of sentences in abstract of paper d
		nk
		the number of sentences in the k-th review of
		paper d
		uR
		t
		the updated representation of the t-th sentence
		in a review after abstract-review match
		hH
		the GRU hidden size
		vR
		k
		the representation of the k-th review
		vR
		k,∥
		the parallel representation of the k-th review
		vR
		k,⊥
		the orthogonal representation of the k-th
		review
		vR
		¬k
		the representation of other reviews excluding
		the k-th review
		ˆvR
		k
		the reﬁned representation of the k-th review
		after cross-review match
		Table 1: Notations used in the paper
	</Abstractive Summary>
</Paper ID=ument496>


<Paper ID=ument496> <Table ID =2>
	<Abstractive Summary> =
		91
		Table 2: Statistics of our datasets after preprocessing
	</Abstractive Summary>
</Paper ID=ument496>


<Paper ID=ument496> <Table ID =3>
	<Abstractive Summary> =
		3026
		Table 3: Performance comparisons of different methods for citation count prediction using two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the performance
		of different methods on citation count prediction
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 6, we are able
		to see that the performance decreases compared
		with the results in Table 3, since we use a training
		set from a different venue
	</Extractive Summary>
</Paper ID=ument496>


<Paper ID=ument496> <Table ID =4>
	<Abstractive Summary> =
		2697
		Table 4: Ablation analysis on ICLR dataset (SR =
		Spearman’s Rank)
	</Abstractive Summary>
</Paper ID=ument496>


<Paper ID=ument496> <Table ID =5>
	<Abstractive Summary> =
		3026
		Table 5: Analysis of the usefulness of peer review text
		on ICLR dataset (SR = Spearman’s Rank)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the
		performance of all the four methods have been
		improved with the text features
	</Extractive Summary>
</Paper ID=ument496>


<Paper ID=ument496> <Table ID =6>
	<Abstractive Summary> =
		2933
		Table 6: Analysis on cross-venue prediction (SR =
		Spearman’s Rank)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6, we are able
		to see that the performance decreases compared
		with the results in Table 3, since we use a training
		set from a different venue
	</Extractive Summary>
</Paper ID=ument496>


<Paper ID=ument496> <Table ID =7>
	<Abstractive Summary> =
		46
		Table 7: Samples of the abstract and reviews from NIPS dataset
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 7, we
		perform the qualitative analysis on a sample paper
		with three reviews for understanding how the two
		mechanisms work
	</Extractive Summary>
</Paper ID=ument496>


<Paper ID=ument497> <Table ID =1>
	<Abstractive Summary> =
		5
		-
		-
		-
		-
		-
		-
		Table 1: Overall, intra- and inter-sentence pairs performance comparison with the state-of-the-art on the CDR test
		set
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 1 depicts the performance of our proposed
		model on the CDR test set, in comparison with
		the state-of-the-art
	</Extractive Summary>
	<Extractive Summary> =
		The observation that
		indirect edges perform slightly better than direct
		for intra-sentence pairs (l ≤ 16) agrees with the
		results of Table 1 where we showed that inter-
		sentence information can act as complementary
		evidence for intra-sentence pairs
	</Extractive Summary>
</Paper ID=ument497>


<Paper ID=ument497> <Table ID =2>
	<Abstractive Summary> =
		8
		-
		-
		Table 2: Performance comparison on the GDA devel-
		opment and test sets
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2 re-
		sults for intra-sentence pairs are consistent with
		the ﬁndings of the CDR dataset for both develop-
		ment and test sets
	</Extractive Summary>
</Paper ID=ument497>


<Paper ID=ument497> <Table ID =3>
	<Abstractive Summary> =
		51
		Table 3: Performance of EoG on the CDR test set with
		different pre-trained word embeddings
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the performance dif-
		ference between domain-speciﬁc (PubMed) (Chiu
		et al
	</Extractive Summary>
</Paper ID=ument497>


<Paper ID=ument497> <Table ID =4>
	<Abstractive Summary> =
		00
		Table 4: Ablation analysis for different edge and node
		types on the CDR development set
	</Abstractive Summary>
</Paper ID=ument497>


<Paper ID=ument497> <Table ID =5>
	<Abstractive Summary> =
		48
		Table 5: Ablation analysis of edge enhancements on
		the CDR development set
	</Abstractive Summary>
</Paper ID=ument497>


<Paper ID=ument497> <Table ID =6>
	<Abstractive Summary> =
		]
		Table 6: Inter-sentence pairs from the CDR develop-
		ment set that EoG fails to detect
	</Abstractive Summary>
</Paper ID=ument497>


<Paper ID=ument497> <Table ID =7>
	<Abstractive Summary> =
		uk/GENIA/tagger/
		Train
		Dev
		Test
		Documents
		500
		500
		500
		Positive pairs
		1,038
		1,012
		1,066
		Intra
		754
		766
		747
		Inter
		284
		246
		319
		Negative pairs
		4,202
		4,075
		4,138
		Entities
		Chemical
		1,467
		1,507
		1,434
		Disease
		1,965
		1,864
		1,988
		Mentions
		Chemical
		5,162
		5,307
		5,370
		Disease
		4,252
		4,328
		4,430
		Table 7: CDR (BioCreative V) dataset statistics
	</Abstractive Summary>
</Paper ID=ument497>


<Paper ID=ument497> <Table ID =8>
	<Abstractive Summary> =
		Train
		Dev
		Test
		Documents
		23,353
		5,839
		1,000
		Positive pairs
		36,079
		8,762
		1,502
		Intra
		30,905
		7,558
		1,305
		Inter
		5,174
		1,204
		197
		Negative pairs
		96,399
		24,362
		3,720
		Entities
		Gene
		46,151
		11,406
		1,903
		Disease
		67,257
		16,703
		2,778
		Mentions
		Gene
		205,457
		51,410
		8,404
		Disease
		226,015
		56,318
		9,524
		Table 8: GDA (DisGeNet) dataset statistics
	</Abstractive Summary>
</Paper ID=ument497>


<Paper ID=ument497> <Table ID =9>
	<Abstractive Summary> =
		8
		Optimizer
		Adam
		Inference iterations
		[0, 5]
		Table 9: Hyper-parameter values used in the reported
		experiments
	</Abstractive Summary>
</Paper ID=ument497>


<Paper ID=ument498> <Table ID =1>
	<Abstractive Summary> =
		en
		2,788
		1,332
		4,891k
		Table 1: Statistics of two style transfer datasets
	</Abstractive Summary>
</Paper ID=ument498>


<Paper ID=ument498> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Case study: the sentences in the brackets are the translation of the corresponding sentence
	</Abstractive Summary>
</Paper ID=ument498>


<Paper ID=ument498> <Table ID =3>
	<Abstractive Summary> =
		61
		Table 3: Automatic evaluation results on four style transfer tasks
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the evalu-
		ation results of automatic metrics on the models
	</Extractive Summary>
</Paper ID=ument498>


<Paper ID=ument498> <Table ID =4>
	<Abstractive Summary> =
		6200
		Table 4: The human annotation results of the S2S
		model and CPLS model from three aspects
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 compares the human evaluation results
		of S2S model and CPLS model on all the datasets,
		which are calculated by the average score of the
		human annotations
	</Extractive Summary>
	<Extractive Summary> =
		As shown in the Table 4,
		the CPLS model outperforms the S2S model in
		4945
		the aspects of the content preservation and style
		strength, and is on par in terms of ﬂuency
	</Extractive Summary>
</Paper ID=ument498>


<Paper ID=ument499> <Table ID =1>
	<Abstractive Summary> =
		, 2017)
		News Articles
		
		
		
		
		Table 1: Comparison of the PRIVACYQA dataset to other question answering datasets
	</Abstractive Summary>
</Paper ID=ument499>


<Paper ID=ument499> <Table ID =2>
	<Abstractive Summary> =
		46
		does the app save the addresses
		that i enter?
		Table 2: Ten most frequent ﬁrst words in questions in
		the PRIVACYQA dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 describes the distribution over ﬁrst
		words of questions posed by crowdworkers
	</Extractive Summary>
</Paper ID=ument499>


<Paper ID=ument499> <Table ID =3>
	<Abstractive Summary> =
		4 %
		how are features personalized?
		Table 3: OPP-115 categories most relevant to the questions collected from users
	</Abstractive Summary>
</Paper ID=ument499>


<Paper ID=ument499> <Table ID =4>
	<Abstractive Summary> =
		62
		Table 4: Statistics of the PRIVACYQA Dataset, where
		# denotes number of questions, policies and sen-
		tences, and average length of questions, policies and
		answers in words, for training and test partitions
	</Abstractive Summary>
</Paper ID=ument499>


<Paper ID=ument499> <Table ID =5>
	<Abstractive Summary> =
		6
		Table 5: Classiﬁer Performance (%) for answerability
		of questions
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results and Discussion
		The results of the answerability baselines are
		presented in Table 5, and on answer sentence
		selection in Table 6
	</Extractive Summary>
</Paper ID=ument499>


<Paper ID=ument499> <Table ID =6>
	<Abstractive Summary> =
		9%
		Table 6:
		Performance of baselines on PRIVACYQA
		dataset
	</Abstractive Summary>
</Paper ID=ument499>


<Paper ID=ument499> <Table ID =7>
	<Abstractive Summary> =
		72
		Table 7: Stratiﬁcation of classiﬁer performance by
		OPP-115 category of questions
	</Abstractive Summary>
</Paper ID=ument499>


<Paper ID=ument499> <Table ID =8>
	<Abstractive Summary> =
		BERT
		# Answerability Mistakes
		137
		% Answerable ->Unanswerable
		124
		% Unanswerable ->Answerable
		13
		Out-of-scope
		2
		Subjective
		46
		Policy Silent
		19
		Unexpected
		6
		Table 8: Analysis of BERT performance at identify-
		ing answerability
	</Abstractive Summary>
</Paper ID=ument499>


<Paper ID=ument5> <Table ID =1>
	<Abstractive Summary> =
		33
		Table 1: Comparison of masked LM perplexity, Wikidata probing MRR, and number of parameters (in millions)
		in the masked LM (word piece embeddings, transformer layers, and output layers), KAR, and entity embeddings
		for BERT and KnowBert
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Intrinsic Evaluation
		Perplexity
		Table 1 compares masked LM per-
		plexity
		for
		KnowBert
		with
		BERTBASE
		and
		BERTLARGE
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 displays a sum-
		mary of the results (see supplementary material
		for results across all relationship types)
	</Extractive Summary>
	<Extractive Summary> =
		of (frozen) parameters in the entity embed-
		dings (Table 1)
	</Extractive Summary>
</Paper ID=ument5>


<Paper ID=ument5> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Fine-grained WSD F1
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 displays ﬁne-grained WSD F1 using the
		evaluation framework from Navigli et al
	</Extractive Summary>
</Paper ID=ument5>


<Paper ID=ument5> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: End-to-end entity linking strong match, micro
		averaged F1
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 reports end-to-end entity linking per-
		formance for the AIDA-A and AIDA-B datasets
	</Extractive Summary>
</Paper ID=ument5>


<Paper ID=ument5> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Single model test set results on the TACRED
		relationship extraction dataset
	</Abstractive Summary>
	<Extractive Summary> =
		For TACRED (Table 4), KnowBert-W+W sig-
		niﬁcantly outperforms the comparable BERTBASE
		systems including ERNIE by 3
	</Extractive Summary>
</Paper ID=ument5>


<Paper ID=ument5> <Table ID =5>
	<Abstractive Summary> =
		1
		Table 5: Test set F1 for SemEval 2010 Task 8 relation-
		ship extraction
	</Abstractive Summary>
	<Extractive Summary> =
		For SemEval
		2010 Task 8 (Table 5), KnowBert-W+W F1 falls
		between the entity aware BERTBASE model from
		Wang et al
	</Extractive Summary>
</Paper ID=ument5>


<Paper ID=ument5> <Table ID =6>
	<Abstractive Summary> =
		9
		Table 6: Test set results for the WiC dataset (v1
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 6, KnowBert-W+W sets a new state of
		the art for this task, improving over BERTLARGE
		by 1
	</Extractive Summary>
</Paper ID=ument5>


<Paper ID=ument5> <Table ID =7>
	<Abstractive Summary> =
		1
		Table 7: Test set results for entity typing using the nine
		general types from (Choi et al
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 7, KnowBert-W+W
		shows an improvement of 0
	</Extractive Summary>
</Paper ID=ument5>


<Paper ID=ument50> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Randomly sampled sentence stimuli from the
		human neuroimaging dataset of Pereira et al
	</Abstractive Summary>
</Paper ID=ument50>


<Paper ID=ument50> <Table ID =2>
	<Abstractive Summary> =
		8k
		Table 2: Details of the tasks used for ﬁne-tuning
	</Abstractive Summary>
	<Extractive Summary> =
		We ﬁne-tune the pretrained BERT model on a
		set of popular shared NLU tasks, shown in Table 2,
		with ﬁxed hyperparameters across tasks (available
		in Appendix B)
	</Extractive Summary>
</Paper ID=ument50>


<Paper ID=ument500> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example of product description generation
	</Abstractive Summary>
	<Extractive Summary> =
		An ex-
		ample case is shown in Table 1, where the good
		product description matches the input information,
		while the bad description mistakes the brand and
		the style of the jeans
	</Extractive Summary>
</Paper ID=ument500>


<Paper ID=ument500> <Table ID =2>
	<Abstractive Summary> =
		25
		Table 2: RQ1: Comparison between baselines
	</Abstractive Summary>
</Paper ID=ument500>


<Paper ID=ument500> <Table ID =3>
	<Abstractive Summary> =
		38▲
		Table 3: RQ1: Human evaluation comparison with
		Pointer-Gen baseline
	</Abstractive Summary>
	<Extractive Summary> =
		The results are shown in Table 3, where
		FPDG outperforms Pointer-Gen by 10
	</Extractive Summary>
</Paper ID=ument500>


<Paper ID=ument500> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Examples of the generated answers by Pointer-
		Gen and FPDG
	</Abstractive Summary>
</Paper ID=ument500>


<Paper ID=ument501> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Statistics of the datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		some statistics about the two datasets
	</Extractive Summary>
</Paper ID=ument501>


<Paper ID=ument501> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Values of the Hyper-Parameters in HMGCN
		model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents detailed information about the
		implementation of our method for each dataset,
		including (1) number of hidden units; (2) dropout
		rate; and (3) learning rate η
	</Extractive Summary>
</Paper ID=ument501>


<Paper ID=ument501> <Table ID =3>
	<Abstractive Summary> =
		Most ex-
		4976
		Table 3: Typing performance on DBpedia
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Overall Comparison Results
		Table 3 and 4 shows the overall performance on
		two datasets and we have the following conclu-
		sions:
		Comparison with Entity Typing methods
	</Extractive Summary>
</Paper ID=ument501>


<Paper ID=ument501> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Typing performance on FIGER
	</Abstractive Summary>
</Paper ID=ument501>


<Paper ID=ument502> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example to illustrate entities, properties
		and their relations
	</Abstractive Summary>
	<Extractive Summary> =
		The “pain” in the example in Table 1 is an-
		notated as symType (sym/msk/pain), where msk
		stands for musculo-skeletal system
	</Extractive Summary>
</Paper ID=ument502>


<Paper ID=ument502> <Table ID =2>
	<Abstractive Summary> =
		1]
		Learning rate
		1e-2
		[1e-4 – 1e-1]
		Table 2: Hyperparameters of our models for model re-
		producibility
	</Abstractive Summary>
	<Extractive Summary> =
		2, the hyperparameters were selected ac-
		cording to Table 2, and the model parameters were
		trained using cross-entropy loss
	</Extractive Summary>
	<Extractive Summary> =
		5
		Parameter Tuning
		Table 2 shows the parameters that were selected
		after evaluating over a range on a development set
	</Extractive Summary>
</Paper ID=ument502>


<Paper ID=ument502> <Table ID =3>
	<Abstractive Summary> =
		52
		Table 3: Comparison of the performance of the proposed R-SAT model with baselines and ablation analysis on
		different components (KG, Context, Buffer, Multi-task) where ‘context’ is the latent representation of the span hs
		ij
		in the memory buffer
	</Abstractive Summary>
</Paper ID=ument502>


<Paper ID=ument502> <Table ID =4>
	<Abstractive Summary> =
		60
		Table 4: Performance of the model when the entities
		and properties are given and it is only required to pre-
		dict existence of relations
	</Abstractive Summary>
</Paper ID=ument502>


<Paper ID=ument504> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example of an edit and its associated comment
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows an example of a comment left
		by an editor in a Wikipedia revision
	</Extractive Summary>
</Paper ID=ument504>


<Paper ID=ument504> <Table ID =2>
	<Abstractive Summary> =
		Field
		Description
		Revision ID
		Wiki revision ID
		Parent ID
		Parent revision ID
		Timestamp
		Timestamp of revision
		Page Title
		Title of Wikipedia page
		Comment
		Revision comment
		Neg-Cmnts
		Negative sampled comments
		Src-Tokens
		Tokens of pre-editing document
		Src-Actions
		Action encoding of pre-editing document
		Tgt-Tokens
		Tokens of post-editing document
		Tgt-Actions
		Action encoding of post-editing docu-
		ment
		Pos-Edits
		Edited sentences in post-editing docu-
		ment
		Neg-Edits
		Negative sampled sentences in post-
		editing document
		Table 2: Data Fields in WikiCmnt Dataset
		and post-edit versions of a text
	</Abstractive Summary>
</Paper ID=ument504>


<Paper ID=ument504> <Table ID =3>
	<Abstractive Summary> =
		796
		Table 3: Performance on Comment Ranking
		comment and edit vectors generated by the
		state-of-the-art sentence embedding method In-
		ferSent (Conneau et al
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Comment Ranking
		Table 3 summarizes results of the Comment Rank-
		ing task
	</Extractive Summary>
</Paper ID=ument504>


<Paper ID=ument504> <Table ID =4>
	<Abstractive Summary> =
		583
		Table 4: Performance on Edit Anchoring
		task
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Edit Anchoring
		Table 4 shows the results for Edit Anchoring
	</Extractive Summary>
</Paper ID=ument504>


<Paper ID=ument504> <Table ID =5>
	<Abstractive Summary> =
		583
		Table 5: Ablation study
		taining a P@1 score of 59
	</Abstractive Summary>
	<Extractive Summary> =
		Results in Table 5 show that each compo-
		nent improves the overall performance on both
		Comment Ranking and Edit Anchoring tasks,
		across our evaluation metrics
	</Extractive Summary>
</Paper ID=ument504>


<Paper ID=ument504> <Table ID =6>
	<Abstractive Summary> =
		479]
		Table 6: Example of the edits and comments matched by the proposed model with one more deceptive distractor for each
		case
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Qualitative Evaluation
		Table 6 provides a few output examples from our
		model on the Comment Ranking task, demonstrat-
		ing its ability to learn abstract connections be-
		tween comments and edits
	</Extractive Summary>
</Paper ID=ument504>


<Paper ID=ument505> <Table ID =1>
	<Abstractive Summary> =
		4M
		60K
		Table 1: Data Set Characteristics
		Parameter
		Description
		B
		Dimension of the projection
		d
		Dimension of the word embedding
		computed on the ﬂy
		N1
		Number of unigram convolution channels
		N2
		Number of bigram convolution channels
		N3
		Number of trigram convolution channels
		N4
		Number of 4gram convolution channels
		N5
		Number of 5gram convolution channels
		S1
		2
		Number of skip 1 bigram convolution channels
		S2
		2
		Number of skip 2 bigram convolution channels
		Table 2: Hyper Parameters Searched
		4
	</Abstractive Summary>
	<Extractive Summary> =
		, 2015)
		with documents contain question title, ques-
		tion context and best answer and ten classes
		such as: Society & Culture; Science & Math-
		ematics; Health; Education & Reference;
		Computers & Internet; Sports; Business &
		Finance; Entertainment & Musical; Family &
		Relationship; Politics & Government;
		Table 1 shows the characteristics of each data set
	</Extractive Summary>
	<Extractive Summary> =
		This dataset has relatively
		few training samples per class (see Table 1) which
		causes the model to overﬁt the training data and
		regularization provided by the operation that sim-
		5019
		ulates quantization during training helps it gener-
		alize better
	</Extractive Summary>
</Paper ID=ument505>


<Paper ID=ument505> <Table ID =2>
	<Abstractive Summary> =
		4M
		60K
		Table 1: Data Set Characteristics
		Parameter
		Description
		B
		Dimension of the projection
		d
		Dimension of the word embedding
		computed on the ﬂy
		N1
		Number of unigram convolution channels
		N2
		Number of bigram convolution channels
		N3
		Number of trigram convolution channels
		N4
		Number of 4gram convolution channels
		N5
		Number of 5gram convolution channels
		S1
		2
		Number of skip 1 bigram convolution channels
		S2
		2
		Number of skip 2 bigram convolution channels
		Table 2: Hyper Parameters Searched
		4
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Setting
		We setup our experimental evaluation, as follows:
		given a long text classiﬁcation task and a data set,
		we construct a model with the hyper-parameters
		listed in Table 2 and we use a hyper-parameter
		search technique to ﬁnd the optimal model for
		each data set
	</Extractive Summary>
	<Extractive Summary> =
		In addition to the parameters listed
		in Table 2, the search method also looks for op-
		timal learning rate schedule and regularization
		scale
	</Extractive Summary>
</Paper ID=ument505>


<Paper ID=ument505> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Evaluation Results
		hoo Answers evaluation, therefore we do not re-
		port their results here
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the obtained
		results for each data set and method
	</Extractive Summary>
	<Extractive Summary> =
		As it can be
		seen in Table 3, our PRADO approach consis-
		tently outperforms all baseline methods with +4
	</Extractive Summary>
	<Extractive Summary> =
		Their SGNN
		approach was targeted towards short text clas-
		siﬁcation tasks and as shown in Table 3, our
		PRADO model achieves upto +40% improvement
		over SGNN demonstrating that PRADO is more
		powerful and suited for long text classiﬁcation
	</Extractive Summary>
	<Extractive Summary> =
		Figure 3 shows that PRADO can
		reach the performance reported in the Table 3 with
		model size of less than 200 Kilobytes
	</Extractive Summary>
	<Extractive Summary> =
		We note that using
		our approach, a PRADO model transfer-learned
		on just 10% of training data achieves very compet-
		itive performance resulting in less than 10% drop
		in relative accuracy on Yelp data set (see Table 3)
	</Extractive Summary>
</Paper ID=ument505>


<Paper ID=ument505> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Model Size vs
	</Abstractive Summary>
</Paper ID=ument505>


<Paper ID=ument505> <Table ID =5>
	<Abstractive Summary> =
		For instance, the skip
		gram “waste * time” captures “ waste your time”
		Bigrams
		5 Star
		1 Star
		highly recommend
		zero stars
		love this
		horrible customer
		hands down
		1 star
		was perfect
		disgusting and
		top notch
		no stars
		amazing service
		better off
		Skip-1-Bigrams
		5 Star
		1 Star
		waste * time
		worst * ever
		was * reasonable
		give * stars
		felt * comfortable
		a * star
		is * delicious
		waste * money
		you * comfortable
		horrible * service
		and * delicious
		worst * experience
		Table 5: Prado Attention Focus on Yelp Data
		or “waste of time”
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows sam-
		ple of the most frequent entries for the 5 and 1
		star Yelp reviews
	</Extractive Summary>
</Paper ID=ument505>


<Paper ID=ument506> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: Results of completion generation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the generation result of MPC, the
		character baseline, and our model variants
	</Extractive Summary>
</Paper ID=ument506>


<Paper ID=ument506> <Table ID =2>
	<Abstractive Summary> =
		com
		national city
		real estate
		real estate
		national city
		nationwide
		remax
		restaurants
		national geographic
		national parks
		realtor
		resources
		national car rental
		national park
		Table 2: Examples of top 5 candidates of completions
		given ”re” and ”nat” as preﬁxes generaed by the char-
		acter baseline and SR model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows examples of decoding results
	</Extractive Summary>
</Paper ID=ument506>


<Paper ID=ument507> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example of a doctor-patient dialogue and
		symptom diagnosis
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, a kid has a cough, the
		doctor asks the patient whether he has a fever
	</Extractive Summary>
</Paper ID=ument507>


<Paper ID=ument507> <Table ID =2>
	<Abstractive Summary> =
		00
		Table 2: Symptom distributions
	</Abstractive Summary>
</Paper ID=ument507>


<Paper ID=ument507> <Table ID =3>
	<Abstractive Summary> =
		18
		Table 3: Statistics of the dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents some statistics of the
		dataset
	</Extractive Summary>
</Paper ID=ument507>


<Paper ID=ument507> <Table ID =4>
	<Abstractive Summary> =
		62%
		Table 4: Performance of various models for symptom
		recognition
	</Abstractive Summary>
</Paper ID=ument507>


<Paper ID=ument507> <Table ID =5>
	<Abstractive Summary> =
		25%
		Table 5: Performance of various models for symptom
		inference
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Performance of Symptom Inference
		Table 5 presents the symptom inference results of
		the classical Bi-LSTM CRF-inference model and
		our proposed joint model (Figure 2)
	</Extractive Summary>
	<Extractive Summary> =
		The results in
		Table 5 show that when incorporating the symp-
		tom graphs for inference, the performance of each
		model can be further boosted
	</Extractive Summary>
</Paper ID=ument507>


<Paper ID=ument507> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Symptom recognition results of the baseline
		and our methods
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Qualitative Analysis
		Table 6 presents a case of symptom recognition
		based on the baseline and our model
	</Extractive Summary>
</Paper ID=ument507>


<Paper ID=ument507> <Table ID =7>
	<Abstractive Summary> =
		Inference:False
		Table 7: Symptom inference results of the Bi-LSTM
		CRF-inference model and our joint model with symp-
		tom graph
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the results of symptom infer-
		ence for a case by using the baseline and our joint
		model
	</Extractive Summary>
</Paper ID=ument507>


<Paper ID=ument508> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples from TIMETRAVEL
		crowdworkers
	</Abstractive Summary>
</Paper ID=ument508>


<Paper ID=ument508> <Table ID =2>
	<Abstractive Summary> =
		Overall, we collect
		Train
		Valid
		Test
		ROCStories data:
		# Stories
		98,159
		1,871
		1,871
		TIMETRAVEL:
		# Counterfactual Context
		98,159
		5,613
		7,484
		# Edited Ending
		16,752
		5,613
		7,484
		Table 2: Dataset statistics
		16,752 training examples of a counterfactual
		context and a rewritten ending
	</Abstractive Summary>
</Paper ID=ument508>


<Paper ID=ument508> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Model Outputs
		with the counterfactual sentence s′
		2, and the model
		must generate the continuation of (S, [s], s1, s′
		2)
	</Abstractive Summary>
	<Extractive Summary> =
		To give a
		sense of the model generation, Table 3 presents
		example outputs by a subset of representative
		models on two test cases
	</Extractive Summary>
</Paper ID=ument508>


<Paper ID=ument508> <Table ID =4>
	<Abstractive Summary> =
		520
		Table 4: Likert scale scores for different models
	</Abstractive Summary>
	<Extractive Summary> =
		We present the results from this study in
		Table 4 and share key observations below
	</Extractive Summary>
	<Extractive Summary> =
		Domain Adaptation
		Another pattern we notice
		is that ﬁne-tuning on the ROCStories data (FT)
		is always helpful for increasing performance on
		counterfactual relevance (CF (3) in Table 4),
		indicating
		adapting
		to
		the
		ROCStories-style
		language distribution helps the model learn to
		produce relevant rewrites for counterfactuals,
		especially for models with fewer parameters
	</Extractive Summary>
	<Extractive Summary> =
		The
		Plot (2) question in Table 4 indicates why this
		might be the case, as the zero-shot models tend to
		produce more creative rewritings that are not at all
		tied to the original story
	</Extractive Summary>
</Paper ID=ument508>


<Paper ID=ument508> <Table ID =5>
	<Abstractive Summary> =
		5
		Human
		Table 5: Pairwise human comparison between the best
		model (GPT2-M + Sup) and comparison models on
		all three questions
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		In Table 5, we present the human
		preference results, showing that the best model
		outperforms the comparison baselines in terms
		of consistency with premise, while being less
		consistently better with regards to the other two
		questions
	</Extractive Summary>
</Paper ID=ument508>


<Paper ID=ument508> <Table ID =6>
	<Abstractive Summary> =
		0995
		Table 6:
		Pearson correlation between automatic
		metrics and human scores
	</Abstractive Summary>
</Paper ID=ument508>


<Paper ID=ument508> <Table ID =7>
	<Abstractive Summary> =
		16
		Table 7: Results on automatic metrics for the cross-product of the models and loss functions proposed in Section 4
	</Abstractive Summary>
	<Extractive Summary> =
		However,
		the correlation is weak, and the results in Table 7
		indicate that the BERTScore metrics are difﬁcult
		to distinguish between models
	</Extractive Summary>
</Paper ID=ument508>


<Paper ID=ument509> <Table ID =1>
	<Abstractive Summary> =
		⟨::::⟩ however
		n’t
		and
		she
		he
		an
		that
		Table 1: The 15 most frequently added phrases in the
		datasets studied in this work, in order of decreasing fre-
		quency
	</Abstractive Summary>
	<Extractive Summary> =
		Using the
		phrase vocabulary of size 500 yields a 31% cover-
		age of the targets from the training set (top phrases
		shown in Table 1)
	</Extractive Summary>
</Paper ID=ument509>


<Paper ID=ument509> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Sentence fusion results on DfWiki
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists
		the results for the DfWiki dataset
	</Extractive Summary>
</Paper ID=ument509>


<Paper ID=ument509> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Results on the WikiSplit dataset
	</Abstractive Summary>
</Paper ID=ument509>


<Paper ID=ument509> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Results on summarization
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 compares our taggers against
		seq2seq baselines and systems from the litera-
		ture
	</Extractive Summary>
</Paper ID=ument509>


<Paper ID=ument509> <Table ID =5>
	<Abstractive Summary> =
		52
		Table 5: Results on grammatical-error correction
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 compares our taggers against two base-
		lines
	</Extractive Summary>
</Paper ID=ument509>


<Paper ID=ument509> <Table ID =6>
	<Abstractive Summary> =
		batch size
		LASERTAGGERFF
		LASERTAGGERAR
		SEQ2SEQBERT
		1
		13
		535
		1,773
		8
		47
		668
		8,279
		32
		149
		1,273
		27,305
		Table 6: Inference time (in ms) across various batch
		sizes on GPU (Nvidia Tesla P100) averaged across 100
		runs with random inputs
	</Abstractive Summary>
</Paper ID=ument509>


<Paper ID=ument509> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Main error patterns observed in the output of the tagging and seq2seq models on their test sets (all tasks)
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 7, we list the error classes and refer to
		Appendix A for more details on our observations
	</Extractive Summary>
</Paper ID=ument509>


<Paper ID=ument51> <Table ID =1>
	<Abstractive Summary> =
		Thus, when the minimum estimate of $300,000
		was not met, a further attempt was made by a former player
		of the Yankees to personally visit the new owner as an
		Table 1: Example summaries collected from human annotators in the constrained (left) and unconstrained (right)
		task
	</Abstractive Summary>
</Paper ID=ument51>


<Paper ID=ument51> <Table ID =2>
	<Abstractive Summary> =
		524
		Table 2: Average number of sentences, per-article,
		which annotators agreed were important
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the average number of sentences,
		per-article, that annotators agreed were important
	</Extractive Summary>
</Paper ID=ument51>


<Paper ID=ument51> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Examples of noisy reference summaries found
		in the CNN/DM and Newsroom datasets
	</Abstractive Summary>
</Paper ID=ument51>


<Paper ID=ument51> <Table ID =4>
	<Abstractive Summary> =
		)
		Table 4: Example of a factually incorrect summary
		generated by an abstractive model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows examples of dis-
		covered inconsistencies
	</Extractive Summary>
</Paper ID=ument51>


<Paper ID=ument51> <Table ID =5>
	<Abstractive Summary> =
		36
		Table 5: Correlations between human annotators and ROUGE scores along different dimensions and multiple
		reference set sizes
	</Abstractive Summary>
</Paper ID=ument51>


<Paper ID=ument51> <Table ID =6>
	<Abstractive Summary> =
		85
		Table 6: ROUGE (R-) scores computed for different models on the test set of the CNN/DM dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Comparing results with the n-gram overlap be-
		tween models and reference summaries (Table 6)
		shows a substantially higher overlap between any
		model pair than between the models and reference
		summaries
	</Extractive Summary>
	<Extractive Summary> =
		7
		Acknowledgements
		We thank all the authors listed in Table 6 for shar-
		ing their model outputs and thus contributing to
		this work
	</Extractive Summary>
</Paper ID=ument51>


<Paper ID=ument510> <Table ID =1>
	<Abstractive Summary> =
		Table 1: A motivating example of CQG task, includ-
		ing the post, question, and answer
	</Abstractive Summary>
	<Extractive Summary> =
		Taking Table 1 as a motivating example, the top-
		ic of this conversation is about cooking and dishes
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows that there exists semantic coher-
		ence in the post-question-answer conversational
		thread
	</Extractive Summary>
	<Extractive Summary> =
		“dish”
		in ground truth question and “cooking”, “beaf” in
		answer of Table 1)
	</Extractive Summary>
</Paper ID=ument510>


<Paper ID=ument510> <Table ID =2>
	<Abstractive Summary> =
		93
		Table 2: Evaluation result of question generation
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, CVAE(qt) incorporates
		question type information, and slightly improves
		the performance compared with CVAE, because
		it could help generate questions with reasonable
		type
	</Extractive Summary>
	<Extractive Summary> =
		The coherence of question and
		answer improved overall performance shown in
		Table 2 by a large margin
	</Extractive Summary>
</Paper ID=ument510>


<Paper ID=ument510> <Table ID =3>
	<Abstractive Summary> =
		826
		Table 3: Evaluation result of GRU-MatchPyramid
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 3, GRU-MatchPyramid model performs bet-
		ter than MatchPyramid (dubbed as MP) model in
		large scale conversational data, because of captur-
		ing higher level semantic of word
	</Extractive Summary>
</Paper ID=ument510>


<Paper ID=ument510> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Two cases comparison among different models
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Case Study
		As shown in Table 4, we list two cases from test
		corpus to compare different methods
	</Extractive Summary>
</Paper ID=ument510>


<Paper ID=ument510> <Table ID =5>
	<Abstractive Summary> =
		512
		Table 5: Evaluation result of question-answer semantic
		coherence
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, our proposed models
		could generate questions coherent to answers in
		inference process
	</Extractive Summary>
</Paper ID=ument510>


<Paper ID=ument510> <Table ID =6>
	<Abstractive Summary> =
		670
		Table 6: Results of human evalution based on criteri-
		a of appropriateness (A), semantic coherence (S) and
		willingness to answer (W)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6, RL-CVAE and A-CVAE
		are consistently in line with the human perspec-
		tive, especially in semantic coherence and willing
		to answer criteria
	</Extractive Summary>
</Paper ID=ument510>


<Paper ID=ument511> <Table ID =1>
	<Abstractive Summary> =
		Table 1: A news example from Yahoo!
		et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 illustrates
		news commenting with an example from Yahoo!
		News
	</Extractive Summary>
	<Extractive Summary> =
		Table 9 and Table 10 show the comments given by the three models
	</Extractive Summary>
	<Extractive Summary> =
		Table 10: A case from Yahoo! News dataset
	</Extractive Summary>
</Paper ID=ument511>


<Paper ID=ument511> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Statistics of the two datasets
	</Abstractive Summary>
</Paper ID=ument511>


<Paper ID=ument511> <Table ID =3>
	<Abstractive Summary> =
		, 2018), we evaluate the
		5083
		Score
		Criteria
		5
		Rich in content; attractive; deep insights; new yet
		relevant viewpoints
		4
		Highly relevant with meaningful ideas
		3
		Less relevant; applied to other articles
		2
		Fluent/grammatical; irrelevant
		1
		Hard to read; Broken language; Only emoji
		Table 3: Human judgment criteria
		performance of different models with both au-
		tomatic metrics and human judgment
	</Abstractive Summary>
</Paper ID=ument511>


<Paper ID=ument511> <Table ID =4>
	<Abstractive Summary> =
		68
		Table 4: Evaluation results on automatic metrics and human judgment
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Evaluation Results
		Table 4 reports evaluation results in terms of both
		automatic metrics and human annotations
	</Extractive Summary>
</Paper ID=ument511>


<Paper ID=ument511> <Table ID =5>
	<Abstractive Summary> =
		665
		Table 5: Model ablation results
		than 0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 reports the results on automatic metrics
	</Extractive Summary>
</Paper ID=ument511>


<Paper ID=ument511> <Table ID =6>
	<Abstractive Summary> =
		0%
		Table 6: Human score distributions
		tion rather than a few single words or bi-grams in
		comment generation, which demonstrates the ad-
		vantage of our learning method
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the re-
		sults
	</Extractive Summary>
</Paper ID=ument511>


<Paper ID=ument511> <Table ID =7>
	<Abstractive Summary> =
		It is others
		that ﬁght with Stephen Chow!)
		Table 7: A Case from Tencent News dataset
	</Abstractive Summary>
</Paper ID=ument511>


<Paper ID=ument511> <Table ID =8>
	<Abstractive Summary> =
		05
		Table 8: Statistics of the two datasets
	</Abstractive Summary>
</Paper ID=ument511>


<Paper ID=ument511> <Table ID =9>
	<Abstractive Summary> =
		It is others that ﬁght
		with Stephen Chow!）
		Table 9: A case from Tencent News dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 and Table 10 show the comments given by the three models
	</Extractive Summary>
</Paper ID=ument511>


<Paper ID=ument511> <Table ID =10>
	<Abstractive Summary> =
		Table 10: A case from Yahoo! News dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 and Table 10 show the comments given by the three models
	</Extractive Summary>
</Paper ID=ument511>


<Paper ID=ument512> <Table ID =1>
	<Abstractive Summary> =
		8
		Table 1: Summary statistics for datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows summary
		statistics of all datasets
	</Extractive Summary>
</Paper ID=ument512>


<Paper ID=ument512> <Table ID =2>
	<Abstractive Summary> =
		34
		Table 2: Language modeling results in the terms of PPL and KL
	</Abstractive Summary>
</Paper ID=ument512>


<Paper ID=ument512> <Table ID =3>
	<Abstractive Summary> =
		113
		Table 3: Topic coherence over the datasets in the term
		of NPMI
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 3, we ﬁnd that
		the discriminator gives little improvement
	</Extractive Summary>
</Paper ID=ument512>


<Paper ID=ument512> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Test split accuracy of classiﬁers trained with
		learned representations
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, our model which com-
		bines the two latent variables achieves the high-
		est accuracy
	</Extractive Summary>
</Paper ID=ument512>


<Paper ID=ument512> <Table ID =5>
	<Abstractive Summary> =
		Table 5: A topic transfer example of one question-answering pair on Yahoo dataset
	</Abstractive Summary>
	<Extractive Summary> =
		From the second column
		of Table 5, we ﬁnd that the generation can ex-
		press a different topic while maintaining the orig-
		inal question structure
	</Extractive Summary>
</Paper ID=ument512>


<Paper ID=ument513> <Table ID =1>
	<Abstractive Summary> =
		18M
		Table 1: Amount of data for pre-training
	</Abstractive Summary>
</Paper ID=ument513>


<Paper ID=ument513> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Test-set results
	</Abstractive Summary>
	<Extractive Summary> =
		5
		LXMERT improves the SotA over-
		all accuracy (‘Accu’ in Table 2) by 2
	</Extractive Summary>
	<Extractive Summary> =
		6% improvement on open-
		domain questions (‘Open’ in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		2% on unreleased test set ‘Test-
		U’, in Table 2) by 22%
	</Extractive Summary>
</Paper ID=ument513>


<Paper ID=ument513> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: Dev-set accuracy of using BERT
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, we discuss sev-
		eral ways to incorporate a BERTBASE pre-trained
		model for vision-language tasks and empirically
		compare it with our LXMERT approach
	</Extractive Summary>
	<Extractive Summary> =
		As shown in the
		second block of Table 3, the result of 1 cross-
		modality layer is better than BUTD, while stack-
		ing more cross-modality layers further improves
		it
	</Extractive Summary>
</Paper ID=ument513>


<Paper ID=ument513> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: Dev-set accuracy showing the importance
		of the image-QA pre-training task
	</Abstractive Summary>
</Paper ID=ument513>


<Paper ID=ument513> <Table ID =5>
	<Abstractive Summary> =
		9
		Table 5:
		Dev-set accuracy of different vision pre-
		training tasks
	</Abstractive Summary>
	<Extractive Summary> =
		, only using the language and
		cross-modality pre-training tasks), the results (row
		1 of Table 5) are similar to BERT+3 CrossAtt in
		Table 3
	</Extractive Summary>
</Paper ID=ument513>


<Paper ID=ument514> <Table ID =1>
	<Abstractive Summary> =
		69
		Table 1: Performance of different phrase grounding methods on Flickr30k Entities (test set)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the performance of previous
		structured prediction models, current state-of-the-
		art models, our baseline models and the Soft-
		Label Chain CRF model
	</Extractive Summary>
</Paper ID=ument514>


<Paper ID=ument514> <Table ID =2>
	<Abstractive Summary> =
		48
		Table 2: Performance of Soft-Label Chain CRF mod-
		els by conditioning transition scores on different sets
		of context features
	</Abstractive Summary>
	<Extractive Summary> =
		Our CRF models has
		transition scores conditioned on features of context in between the two phrases (“M” in Table 2)
	</Extractive Summary>
</Paper ID=ument514>


<Paper ID=ument514> <Table ID =3>
	<Abstractive Summary> =
		73
		Table 3: Decoding algorithms’ impact on performance
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows that in
		both Hard-Label Chain CRF and Soft-Label Chain
		CRF, smoothing decoding gives a prediction accu-
		racy 0
	</Extractive Summary>
</Paper ID=ument514>


<Paper ID=ument515> <Table ID =1>
	<Abstractive Summary> =
		12
		Table 1: Experimental results on VisPro
	</Abstractive Summary>
</Paper ID=ument515>


<Paper ID=ument516> <Table ID =1>
	<Abstractive Summary> =
		✓
		✓
		Makeup
		YouTube
		Table 1: Comparison of different instructional video datasets
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1,
		different instructional video datasets have been
		released
	</Extractive Summary>
</Paper ID=ument516>


<Paper ID=ument516> <Table ID =2>
	<Abstractive Summary> =
		Type
		Terms
		Actions
		apply, use, blend, highlight, draw, curler, ﬁx, clean, pat, shape, empha-
		size, press, sweep, correct, bake, tap, contour
		Products
		eyeshadow, concealer, powder, pencil, foundation, lipstick, eyeliner,
		blush, primer, shadow, highlighter, contour, bronzer, gel, cream, lip-
		gloss, false lashes, blam, spray
		Areas
		face, lips, eyelid, eye, lashes, brow, cheek, nose, under-eye, cheekbone,
		lash line, forehead, chin, mouth, blemish, temple, jaw, hairline, jawline,
		t-zone, nosebone, browbone, eye corner, lash line, philtrum, eyeball
		Tools
		brush, blender, sponge, ﬁnger, puff, tweezer
		Table 2: Top words of four categories in step captions
	</Abstractive Summary>
</Paper ID=ument516>


<Paper ID=ument516> <Table ID =3>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the step captioning
		performance with ground truth step segmentation
		Table 3: Step captioning performance with groundtruth
		step segmentation on the YouMakeup dataset
	</Extractive Summary>
</Paper ID=ument516>


<Paper ID=ument517> <Table ID =1>
	<Abstractive Summary> =
		51
		Table 1: Performance (%) over R@1,IoU@θ and mIoU
		compared with the state-of-the-art NLVL models on
		TACoS, Charades-STA and ActivityNet Captions
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 1, we can observe that the DEBUG
		achieves a new state-of-the-art performance un-
		der all evaluation metrics and benchmarks
	</Extractive Summary>
</Paper ID=ument517>


<Paper ID=ument517> <Table ID =2>
	<Abstractive Summary> =
		51
		Table 2: Performance (%) over R@1,IoU@θ and mIoU
		compared with QANet-SE on TACoS, Charades-STA
		and ActivityNet Captions
	</Abstractive Summary>
</Paper ID=ument517>


<Paper ID=ument517> <Table ID =3>
	<Abstractive Summary> =
		51
		Table 3: Performance (%) over R@1,IoU@θ and mIoU in ablative experiments of each component of DEBUG
		model on TACoS, Charades-STA and ActivityNet Captions
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		are shown in Table 3 and discussed in detail next
	</Extractive Summary>
</Paper ID=ument517>


<Paper ID=ument517> <Table ID =4>
	<Abstractive Summary> =
		02
		Table 4: Average time (s) to localize
		one sentence for different methods on
		TACoS and ActivityNet Captions
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, the
		DEBUG signiﬁcantly reduce the localization time
		compared to all top-down models (MCN, VSA-
		Methods
		Charades-STA
		TACoS
		100
		200
		300
		100
		200
		300
		L
		QANet-SE
		54
	</Extractive Summary>
</Paper ID=ument517>


<Paper ID=ument517> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Accuracy (%) of multiple keypoints at different
		thresholds on Charades-STA and TACoS
	</Abstractive Summary>
</Paper ID=ument517>


<Paper ID=ument518> <Table ID =1>
	<Abstractive Summary> =
		[Nats]{PER}, [NZ]{LOC}
		[Nats]{ORG}, [NZ First]{ORG}
		Seagramd ace 20/11/96 5,000 Japan
		[Seagramd] {MISC}, [Japan]{LOC}
		[Seagramd ace]{MISC}, [Japan] {LOC}
		Table 1: Typical Examples of Our Corrections on the CoNLL03 NER dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents some typical examples of our
		corrections
	</Extractive Summary>
	<Extractive Summary> =
		The results averaged across 5 runs are reported
		in Table 10
	</Extractive Summary>
	<Extractive Summary> =
		21)
		Table 10: Applying CrossWeigh on other datasets
		Training with CrossWeigh leads to a signiﬁcantly
		higher F1 and a smaller standard deviation
	</Extractive Summary>
	<Extractive Summary> =
		For example, in Table 11, the origi-
		nal training sentence “Hapoel Haifa 3 Maccabi Tel
		Aviv 1” contains a label mistake, because “Maccabi
		Tel Aviv” is a sports team but was not annotated
		completely
	</Extractive Summary>
	<Extractive Summary> =
		[Hapoel Jerusalem]{ORD}, [Maccabi Tel Aviv]{ORG}
		Table 11: Case Study on the CoNLL03 dataset
	</Extractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =2>
	<Abstractive Summary> =
		11)
		Table 2:
		CoNLL03 Re-Evaluation:
		Test F1 scores
		and standard deviations on both original and corrected
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Flair) and two best-performing NER algorithms
		with or without language models in Table 2 (i
	</Extractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =3>
	<Abstractive Summary> =
		05)
		Table 3: Test F1 scores and its standard deviations of models trained without or with CrossWeigh
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, compared
		with the three algorithms, applying CrossWeigh
		always leads to a higher F1 score and a compa-
		rable, sometimes even smaller, standard deviation
	</Extractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =4>
	<Abstractive Summary> =
		10)
		Table 4: Importance of Entity Disjoint Filtering
	</Abstractive Summary>
	<Extractive Summary> =
		Experiments in Table 4 show
		that this step is crucial to our performance gain
	</Extractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =5>
	<Abstractive Summary> =
		06)
		Table 5: Different ci Estimation Heuristics
	</Abstractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =6>
	<Abstractive Summary> =
		06)
		Table 6: Different Numbers of Iterations t
	</Abstractive Summary>
	<Extractive Summary> =
		In our exper-
		iments (see Table 6), we ﬁnd that t = 3 provides a
		good enough result
	</Extractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =7>
	<Abstractive Summary> =
		06)
		Table 7: Different Numbers of Folds k
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 7, we
		observe that k = 5, k = 10 are signiﬁcantly better
		than k = 2
	</Extractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =8>
	<Abstractive Summary> =
		06)
		Table 8: Different Weight Adjustments ϵ
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 presents some results when other values
		are used
	</Extractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =9>
	<Abstractive Summary> =
		2904
		Table 9: Quality of noise estimation
	</Abstractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =10>
	<Abstractive Summary> =
		21)
		Table 10: Applying CrossWeigh on other datasets
		Training with CrossWeigh leads to a signiﬁcantly
		higher F1 and a smaller standard deviation
	</Abstractive Summary>
</Paper ID=ument518>


<Paper ID=ument518> <Table ID =11>
	<Abstractive Summary> =
		[Hapoel Jerusalem]{ORD}, [Maccabi Tel Aviv]{ORG}
		Table 11: Case Study on the CoNLL03 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		For example, in Table 11, the origi-
		nal training sentence “Hapoel Haifa 3 Maccabi Tel
		Aviv 1” contains a label mistake, because “Maccabi
		Tel Aviv” is a sports team but was not annotated
		completely
	</Extractive Summary>
</Paper ID=ument518>


<Paper ID=ument519> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1: Variance analysis for signiﬁcance testing of
		different active learning systems using paired bootstrap
		resampling
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 1
		we see that the baselines are signiﬁcantly worse
		than ETAL at 600 and 1200 annotated tokens
	</Extractive Summary>
</Paper ID=ument519>


<Paper ID=ument519> <Table ID =2>
	<Abstractive Summary> =
		6 (2134)
		Table 2: Annotator performance measures F1 of each
		annotator with respect to the oracle
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		Table 2 records the results of the human annota-
		tion experiments
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2 we see that under
		this setting ETAL outperforms SAL, similar to the
		simulation results
	</Extractive Summary>
</Paper ID=ument519>


<Paper ID=ument52> <Table ID =1>
	<Abstractive Summary> =
		07
		Table 1: Accuracy of Discourse Ordering, Text Struc-
		turing and Referring Expression models, as well as
		BLEU score of Lexicalization approaches
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 1 shows the results for our models for each
		of the 4 evaluated pipeline steps
	</Extractive Summary>
</Paper ID=ument52>


<Paper ID=ument52> <Table ID =2>
	<Abstractive Summary> =
		63A
		Table 2: (1) BLEU and METEOR scores of the models in the automatic evaluation, and (2) Fluency and Semantic
		obtained in the human evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 2 depicts the results of automatic and human
		evaluations, whereas Table 3 shows the results of
		the qualitative analysis
	</Extractive Summary>
</Paper ID=ument52>


<Paper ID=ument52> <Table ID =3>
	<Abstractive Summary> =
		92
		Table 3: Qualitative analysis
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 2 depicts the results of automatic and human
		evaluations, whereas Table 3 shows the results of
		the qualitative analysis
	</Extractive Summary>
</Paper ID=ument52>


<Paper ID=ument520> <Table ID =1>
	<Abstractive Summary> =
		Table 1: The agreements between pairs of expert
		judges at different annotation depth
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, the
		judge agreements is substantially higher on Uni-
		gram overlaps than on Exact matches, indicating
		that they may select chunks that overlap with each
		other but not exactly the same
	</Extractive Summary>
</Paper ID=ument520>


<Paper ID=ument520> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Statistics of OpenKP used in our experi-
		ments
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Data Characteristics
		Table 2 lists the statistics of OpenKP
	</Extractive Summary>
</Paper ID=ument520>


<Paper ID=ument520> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Visual Features
	</Abstractive Summary>
</Paper ID=ument520>


<Paper ID=ument520> <Table ID =4>
	<Abstractive Summary> =
		5180
		Table 4: Statistics of Query Prediction Dataset
	</Abstractive Summary>
</Paper ID=ument520>


<Paper ID=ument520> <Table ID =5>
	<Abstractive Summary> =
		OpenKP and Query Prediction ex-
		periments compare BLING-KPE with: traditional
		KPE methods, production systems, and a neural
		Table 5: Parameters to learn in BLING-KPE
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 lists BLING-
		KPE parameters
	</Extractive Summary>
</Paper ID=ument520>


<Paper ID=ument520> <Table ID =6>
	<Abstractive Summary> =
		5181
		Table 6: Keyphrase Extraction Accuracy
	</Abstractive Summary>
</Paper ID=ument520>


<Paper ID=ument520> <Table ID =7>
	<Abstractive Summary> =
		729
		Table 7: Performance of BLING-KPE ablations
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Ablation Study
		Table 7 shows ablation results on BLING-KPE’s
		variations
	</Extractive Summary>
	<Extractive Summary> =
		282
		Table 7 studies the contribution of Transformer
		and position embedding
	</Extractive Summary>
	<Extractive Summary> =
		As shown
		in the second part of Table 7, both visual features
		and search pretraining contribute signiﬁcantly to
		BLING-KPE’s effectiveness
	</Extractive Summary>
</Paper ID=ument520>


<Paper ID=ument520> <Table ID =8>
	<Abstractive Summary> =
		The second part of
		5182
		Table 8: Performance on DUC-2001
	</Abstractive Summary>
</Paper ID=ument520>


<Paper ID=ument521> <Table ID =1>
	<Abstractive Summary> =
		, 2019)
		f(vec(es ∗ vec−1(wrH))W)eo
		wr ∈ Rdr
		O(nede + nrdr)
		TuckER (ours)
		W ×1 es ×2 wr ×3 eo
		wr ∈ Rdr
		O(nede + nrdr)
		Table 1: Scoring functions of state-of-the-art link prediction models, the dimensionality of their relation param-
		eters, and signiﬁcant terms of their space complexity
	</Abstractive Summary>
	<Extractive Summary> =
		2, the RESCAL
		scoring function (see Table 1) has the form:
		X ≈ Z ×1 A ×3 C
	</Extractive Summary>
	<Extractive Summary> =
		, 2015)
		The scoring func-
		tion of DistMult (see Table 1) can be viewed as
		equivalent to that of TuckER (see Equation 1) with
		a core tensor Z ∈ RP×Q×R, P = Q = R = de,
		which is superdiagonal with 1s on the superdiag-
		onal, i
	</Extractive Summary>
	<Extractive Summary> =
		Similarly to DistMult, we can regard the scoring
		function of ComplEx (see Table 1) as equivalent
		to the scoring function of TuckER (see Equation
		1), with core tensor Z ∈ RP×Q×R, P = Q =
		R = 2de, where 3de elements on different tensor
		diagonals are set to 1, de elements on one tensor
		diagonal are set to -1 and all other elements are set
		to 0 (see Figure 2b)
	</Extractive Summary>
	<Extractive Summary> =
		The SimplE scoring function (see
		Table 1) is therefore equivalent to that of TuckER
		(see Equation 1), with core tensor Z ∈ RP×Q×R,
		P = Q = R = 2de, where 2de elements on two
		tensor diagonals are set to 1
		2 and all other elements
		are set to 0 (see Figure 2c)
	</Extractive Summary>
</Paper ID=ument521>


<Paper ID=ument521> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		# Entities (ne)
		# Relations (nr)
		FB15k
		14,951
		1,345
		FB15k-237
		14,541
		237
		WN18
		40,943
		18
		WN18RR
		40,943
		11
		Table 2: Dataset statistics
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Datasets
		We evaluate TuckER using four standard link pre-
		diction datasets (see Table 2):
		FB15k (Bordes et al
	</Extractive Summary>
</Paper ID=ument521>


<Paper ID=ument521> <Table ID =3>
	<Abstractive Summary> =
		266
		Table 3: Link prediction results on WN18RR and FB15k-237
	</Abstractive Summary>
</Paper ID=ument521>


<Paper ID=ument521> <Table ID =4>
	<Abstractive Summary> =
		741
		Table 4: Link prediction results on WN18 and FB15k
	</Abstractive Summary>
</Paper ID=ument521>


<Paper ID=ument521> <Table ID =5>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		0) for
		WN18RR (see Table 5 in the Appendix A for a
		complete list of hyper-parameter values on each
		dataset)
	</Extractive Summary>
</Paper ID=ument521>


<Paper ID=ument521> <Table ID =6>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		(see Table 6 in the Appendix A for exact hyper-
		parameter values used) and train all three models
		on FB15k-237 with embedding sizes de = dr ∈
		{20, 50, 100, 200}
	</Extractive Summary>
</Paper ID=ument521>


<Paper ID=ument522> <Table ID =1>
	<Abstractive Summary> =
		5: (In)correct, unconﬁdent
		Table 1: A summary of the proposed human-grounded evaluation tasks
	</Abstractive Summary>
	<Extractive Summary> =
		Also,
		the humans have the option to state no preference,
		for which the explanation method will get a zero
		score (See the last row of Table 1)
	</Extractive Summary>
</Paper ID=ument522>


<Paper ID=ument522> <Table ID =2>
	<Abstractive Summary> =
		For
		Method Name
		Approach
		Granularity
		Random (W)
		Random
		Baselines
		Words
		Random (N)
		N-grams
		LIME
		Perturbation
		Words
		LRP (W)
		Relevance
		Propagation
		Words
		LRP (N)
		N-grams
		DeepLIFT (W)
		Words
		DeepLIFT (N)
		N-grams
		Grad-CAM-Text
		Gradient
		N-grams
		Decision Trees
		(DTs)
		Model
		Extraction
		N-grams
		Table 2: Nine explanation methods evaluated
	</Abstractive Summary>
</Paper ID=ument522>


<Paper ID=ument522> <Table ID =3>
	<Abstractive Summary> =
		I wanted / would be ok
		Table 3: Examples of evidence and counter-evidence texts generated by some of the explanation methods
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results and Discussion
		Examples of the generated explanations are shown
		in Table 3 and a separate appendix
	</Extractive Summary>
</Paper ID=ument522>


<Paper ID=ument522> <Table ID =4>
	<Abstractive Summary> =
		499
		N/A
		Table 4: The average scores of the three evaluation tasks
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows
		the average scores of each explanation method
		for each task and dataset, while Figure 3 displays
		3The code and datasets of this paper are available at
		https://github
	</Extractive Summary>
</Paper ID=ument522>


<Paper ID=ument522> <Table ID =5>
	<Abstractive Summary> =
		84
		Computer Science
		107
		17
		54
		Mathematics
		263
		28
		132
		Physics
		237
		29
		119
		Table 5: Metadata of the DTs in the experiments
	</Abstractive Summary>
</Paper ID=ument522>


<Paper ID=ument523> <Table ID =1>
	<Abstractive Summary> =
		A citation context is deﬁned as a sequence
		5208
		#Bodytext #Footnote
		Total
		Annotation
		ARC
		3,718
		15,043
		18,761
		1,026
		NeurlPS
		686
		896
		1,582
		1,056
		PUBMED
		31,852
		510
		32,362
		1,006
		Total
		36,256
		16,449
		52,705
		3,088
		Table 1: A description for the collection of 52,705 data
		sample in the scientiﬁc resource corpus
	</Abstractive Summary>
</Paper ID=ument523>


<Paper ID=ument523> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: The 3 1-st category role types and 9 2-nd category role types along with their statistics in SciRes from
		three scientiﬁc literature sources
	</Abstractive Summary>
	<Extractive Summary> =
		All
		the function and role types along with their statis-
		tics are shown in Table 2 and Table 3
	</Extractive Summary>
</Paper ID=ument523>


<Paper ID=ument523> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: The 6 resource function types along with
		their statistics in SciRes from three scientiﬁc literature
		sources
	</Abstractive Summary>
</Paper ID=ument523>


<Paper ID=ument523> <Table ID =4>
	<Abstractive Summary> =
		715
		Table 4: Comparison results of SciResCLF for the three classiﬁcation tasks
	</Abstractive Summary>
</Paper ID=ument523>


<Paper ID=ument523> <Table ID =5>
	<Abstractive Summary> =
		448
		Table 5: Results (F1-score) on each category predicted
		by the best model for each of the three tasks
	</Abstractive Summary>
</Paper ID=ument523>


<Paper ID=ument523> <Table ID =6>
	<Abstractive Summary> =
		497
		Table 6: The Precision@Top3 and the MAP results for
		the ranking list predicted by SciResREC
	</Abstractive Summary>
	<Extractive Summary> =
		Recommendation Results
		As Table 6 shows, our SciResREC framework
		outperforms the two baselines
	</Extractive Summary>
</Paper ID=ument523>


<Paper ID=ument524> <Table ID =1>
	<Abstractive Summary> =
		8
		Table 1: Summary of datasets
	</Abstractive Summary>
</Paper ID=ument524>


<Paper ID=ument524> <Table ID =2>
	<Abstractive Summary> =
		32
		Table 2: Test accuracy of various classiﬁcation mod-
		els
	</Abstractive Summary>
</Paper ID=ument524>


<Paper ID=ument524> <Table ID =3>
	<Abstractive Summary> =
		25
		Table 3: Adversarial Reprogramming Experiments:
		The accuracies of white-box and black-box reprogram-
		ming experiments on different combinations of original
		task, adversarial task and model
	</Abstractive Summary>
	<Extractive Summary> =
		We can observe that our white-box attack on pre-
		trained networks, outperforms this classiﬁer in all
		scenarios (refer to Table 3)
	</Extractive Summary>
</Paper ID=ument524>


<Paper ID=ument525> <Table ID =1>
	<Abstractive Summary> =
		7941
		Table 1: The precisions of the top 100 retrieved documents on three datasets with different numbers of hashing
		bits in unsupervised hashing
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Performance Evaluation of Unsupervised
		Semantic Hashing
		Table 1 shows the performance of the proposed
		and baseline models on three datasets under the
		unsupervised setting, with the number of hashing
		bits ranging from 16 to 128
	</Extractive Summary>
</Paper ID=ument525>


<Paper ID=ument525> <Table ID =2>
	<Abstractive Summary> =
		9590
		Table 2: The performances of different supervised hashing models on three datasets under different lengths of
		hashing codes
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the perfor-
		mances of different supervised hashing models on
		three datasets under different lengths of hashing
		codes
	</Extractive Summary>
</Paper ID=ument525>


<Paper ID=ument525> <Table ID =3>
	<Abstractive Summary> =
		8279
		Table 3: Precisions of top 100 retrieved documentswith different numberer of clusters, K denotes the number of
		components, D represents datasets, GT represents the ground truth number of classes for each dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the precisions of top 100
		retrieved documents when the number of compo-
		nents K is set to different values
	</Extractive Summary>
	<Extractive Summary> =
		However, as
		seen from Table 3, as long as the number K is not
		too small, the performance loss is still acceptable
	</Extractive Summary>
</Paper ID=ument525>


<Paper ID=ument526> <Table ID =1>
	<Abstractive Summary> =
		Datasets
		Dimension
		# Base Data
		fastTextEn
		300
		989873
		fastTextFr
		300
		1142501
		GloVe
		50
		1183514
		Amovie
		64
		104708
		Yelp
		64
		25815
		Netﬂix
		50
		17770
		Table 1: Statistics of the datasets
	</Abstractive Summary>
</Paper ID=ument526>


<Paper ID=ument526> <Table ID =2>
	<Abstractive Summary> =
		7%)
		Table 2: Number and percentage of nodes with incoming
		edges for graphs built by ip-NSW and IPDG
	</Abstractive Summary>
</Paper ID=ument526>


<Paper ID=ument527> <Table ID =1>
	<Abstractive Summary> =
		78†
		Table 1: Results on different SP acquisition evaluation sets
	</Abstractive Summary>
</Paper ID=ument527>


<Paper ID=ument527> <Table ID =2>
	<Abstractive Summary> =
		, 2019a)
		2,500
		6,000
		Table 2: Statistics of Human-labeled SP Evaluation
		Sets
	</Abstractive Summary>
</Paper ID=ument527>


<Paper ID=ument527> <Table ID =3>
	<Abstractive Summary> =
		48
		Table 3: Spearman’s correlation of different embeddings for
		the WS measurement
	</Abstractive Summary>
	<Extractive Summary> =
		Results are reported in Table 3 with several ob-
		servations
	</Extractive Summary>
</Paper ID=ument527>


<Paper ID=ument527> <Table ID =4>
	<Abstractive Summary> =
		17
		Table 4: Comparison of MWE against language models on
		the WS task
	</Abstractive Summary>
	<Extractive Summary> =
		We also compare MWE with pre-trained con-
		textualized word embedding models in Table 4
		for this task, with overall performance, embed-
		ding dimensions, and training times reported
	</Extractive Summary>
</Paper ID=ument527>


<Paper ID=ument527> <Table ID =5>
	<Abstractive Summary> =
		476
		Table 5: Comparisons of different training strategies
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, we compare our model
		with several different strategies
	</Extractive Summary>
</Paper ID=ument527>


<Paper ID=ument528> <Table ID =1>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summa-
		rizes the key characteristics of the four data sets
		Table 1: Statistics of data sets
	</Extractive Summary>
</Paper ID=ument528>


<Paper ID=ument528> <Table ID =2>
	<Abstractive Summary> =
		5263
		Table 2: Compressed model evaluation with 3 language models and 1 machine translation model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the comparison between
		the proposed methods and state-of-the-art base-
		lines for the four benchmark data sets and LSTM
		models
	</Extractive Summary>
	<Extractive Summary> =
		5 as shown in Table 2)
	</Extractive Summary>
</Paper ID=ument528>


<Paper ID=ument528> <Table ID =3>
	<Abstractive Summary> =
		66
		Table 3: Results of compressing all input and Softmax embedding layers on three data sets
	</Abstractive Summary>
</Paper ID=ument528>


<Paper ID=ument528> <Table ID =4>
	<Abstractive Summary> =
		To understand how the choices of M, N, K
		would affect the performance of the compressed
		model, we test the proposed model for PTB-small
		5264
		Table 4: Ablation analysis of MulCode
	</Abstractive Summary>
</Paper ID=ument528>


<Paper ID=ument528> <Table ID =5>
	<Abstractive Summary> =
		football),
		Word
		1
		2
		3
		4
		tomorrow
		today
		Sunday
		prompt
		Tuesday
		beautiful
		elegant
		iconic
		great
		ﬁelds
		soccer
		football
		hockey
		Ghana
		basketball
		where
		when
		experiencing
		who
		learn
		bank
		company
		police
		companies
		banks
		halt
		stop
		halted
		afternoon
		cover
		like
		just
		Like
		such
		is
		Table 5: Most similar word computed using Hamming
		distance for each of the N-way codings
	</Abstractive Summary>
	<Extractive Summary> =
		We
		compute the hamming distance of example query
		words (shown in Table 5) for each of the N-way
		codes
	</Extractive Summary>
</Paper ID=ument528>


<Paper ID=ument529> <Table ID =1>
	<Abstractive Summary> =
		72 < 10�2
		Table 1: Direct bias results
		ily analogy subset of the Google Analogy Test set
		(Mikolov et al
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Direct bias
		Table 1 presents the d scores and
		WEAT one-tailed p-values, which indicate whether
		the difference in samples means between targets
		X and Y and attributes A and B is signiﬁcant
	</Extractive Summary>
</Paper ID=ument529>


<Paper ID=ument529> <Table ID =2>
	<Abstractive Summary> =
		367
		Table 2: Word similarity Results
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Word similarity
		Table 2 reports the SimLex-999
		Spearman rank-order correlation coefﬁcients rs
		(all are signiﬁcant, p < 0
	</Extractive Summary>
</Paper ID=ument529>


<Paper ID=ument53> <Table ID =1>
	<Abstractive Summary> =
		Results
		Table 1: In all language pairs, the best
		correlation is achieved by our word mover met-
		rics that use a BERT pretrained on MNLI as the
		embedding generator and PMeans to aggregate
		the embeddings from different BERT layers, i743
		Table 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset
	</Abstractive Summary>
</Paper ID=ument53>


<Paper ID=ument53> <Table ID =2>
	<Abstractive Summary> =
		694
		Table 2: Pearson r and Spearman ρ correlations with summary-level human judgments on TAC 2008 and 2009
	</Abstractive Summary>
</Paper ID=ument53>


<Paper ID=ument53> <Table ID =3>
	<Abstractive Summary> =
		182
		Table 3: Spearman correlation with utterance-level human judgments for BAGEL and SFHOTEL datasets
	</Abstractive Summary>
</Paper ID=ument53>


<Paper ID=ument53> <Table ID =4>
	<Abstractive Summary> =
		Results
		Table 4: Word mover metrics outper-
		form all baselines except for the supervised metric
		LEIC, which uses more information by consider-
		ing both images and texts808
		Table 4: Pearson correlation with system-level human judg-
		ments on MSCOCO dataset
	</Abstractive Summary>
</Paper ID=ument53>


<Paper ID=ument53> <Table ID =5>
	<Abstractive Summary> =
		712
		Table 5: Comparison on hard and soft alignments
	</Abstractive Summary>
</Paper ID=ument53>


<Paper ID=ument53> <Table ID =6>
	<Abstractive Summary> =
		7192
		Table 6: Absolute Pearson correlations with segment-level human judgments on WMT17 to-English translations
	</Abstractive Summary>
	<Extractive Summary> =
		Algorithm 1 Aggregation by Routing
		1: procedure ROUTING(zij, ℓ)
		2: Initialize ∀i, j : γij = 0
		3: while true do
		4:
		foreach representation i and j in layer ℓ and ℓ + 1 do γij ← softmax(γij)
		5:
		foreach representation j in layer ℓ + 1 do
		6:
		vj ← �
		i γijk′(vj, zi)zi/ �
		i k′(vi, zi)
		7:
		foreach representation i and j in layer ℓ and ℓ + 1 do γij ← γij + α · k(vj, zi)
		8:
		loss ← log(�
		i,j γijk(vj, zi))
		9:
		if |loss − preloss| < ϵ then
		10:
		break
		11:
		else
		12:
		preloss ← loss
		13: return vj
		Best Layer and Layer-wise Consolidation
		Table 6 compares our word mover based metric com-
		bining BERT representations on different layers with stronger BERT representations consolidated from
		these layers (using p-means and routing)
	</Extractive Summary>
</Paper ID=ument53>


<Paper ID=ument53> <Table ID =7>
	<Abstractive Summary> =
		743
		Table 7: Absolute Pearson correlations with segment-level human judgments on WMT17 to-English translations
	</Abstractive Summary>
	<Extractive Summary> =
		Experiments
		Table 7, 8 and 9 show correlations between metrics (all baseline metrics and word
		mover based metrics) and human judgments on machine translation, text summarization and dialogue
		response generation, respectively
	</Extractive Summary>
</Paper ID=ument53>


<Paper ID=ument53> <Table ID =8>
	<Abstractive Summary> =
		694
		Table 8: Correlation of automatic metrics with summary-level human judgments for TAC-2008 and TAC-2009
	</Abstractive Summary>
</Paper ID=ument53>


<Paper ID=ument53> <Table ID =9>
	<Abstractive Summary> =
		182
		Table 9: Spearman correlation with utterance-level human judgments for BAGEL and SFHOTEL datasets
	</Abstractive Summary>
</Paper ID=ument53>


<Paper ID=ument530> <Table ID =1>
	<Abstractive Summary> =
		7218
		Table 1: Analyses on Spanish and French monolingual embeddings before and after bias mitigation
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 1 shows that Hybrid Ori signiﬁ-
		cantly decreases the difference of association be-
		tween two genders as indicated by MWEAT–Diff
		and p-value
	</Extractive Summary>
</Paper ID=ument530>


<Paper ID=ument530> <Table ID =2>
	<Abstractive Summary> =
		3492
		Table 2: Results on word translation and word pair translation based on Spanish-English and French-English
		bilingual embeddings
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 2 shows the results
	</Extractive Summary>
</Paper ID=ument530>


<Paper ID=ument531> <Table ID =1>
	<Abstractive Summary> =
		(2015)
		ﬁnd that the diversity of semantic relations discov-
		ered in word-aligned parallel corpora yields para-
		phrases that span the lexical relations deﬁned in
		Lexicon Entry
		Semantic Relation
		writer, �������
		writer is more speciﬁc than
		������� (creator)
		council, ��������
		council is more general than
		�������� (council of ministers)
		father, ���
		father is mutually exclusive to
		��� (father’s brother)
		Table 1: Semantic relations between word pairs in an
		English-Hindi lexicon (Lample et al
	</Abstractive Summary>
	<Extractive Summary> =
		However, translation lex-
		icons include semantic relations other than syn-
		onymy in practice, as can be seen (Table 1) in ex-
		amples drawn from the MUSE dictionary (Lam-
		ple et al
	</Extractive Summary>
</Paper ID=ument531>


<Paper ID=ument531> <Table ID =2>
	<Abstractive Summary> =
		com/
		Relation
		En-Hi En-Zh
		Equivalence
		158
		174
		Forward Entail
		220
		240
		Backward Entail
		215
		236
		Exclusion
		124
		154
		Other
		323
		94
		Total
		1040
		898
		Table 2: Distribution of the ﬁve semantic relations for
		the two crowdsourced test sets
	</Abstractive Summary>
</Paper ID=ument531>


<Paper ID=ument531> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: Precision (P), Recall (R) and F1-score (F) for BILEXNET and contrastive baselines on the two MULTI-
		LEXREL test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Attention Analysis
		We complement ablation
		experiments in Table 3 by examining a random
		sample of 25 monolingual training pairs (xe, ye)
		where ye has multiple translations in the bilin-
		gual dictionary
	</Extractive Summary>
</Paper ID=ument531>


<Paper ID=ument531> <Table ID =4>
	<Abstractive Summary> =
		This is similar to the the Co-
		5293
		Class
		En-Hi En-Zh En-En
		Equivalence
		33
		30
		31
		Exclusion
		33
		28
		23
		Forward Entail
		47
		48
		48
		Backward Entail
		45
		58
		48
		Other
		51
		29
		53
		Table 4: Per-class F1 scores for median En-Hi and En-
		Zh BILEXNET model and the ENLEXNET model
	</Abstractive Summary>
	<Extractive Summary> =
		Performance Per Class
		We break down the
		performance of the BILEXNET model per tar-
		get relation (Table 4)
	</Extractive Summary>
</Paper ID=ument531>


<Paper ID=ument532> <Table ID =1>
	<Abstractive Summary> =
		, 2017b), we choose SemEval 2007 Task
		17 (SE07) as our development data to pick the
		Task
		# Instances
		# Lexelts
		English all-words
		- SemCor (train)
		226,036
		22,436
		- SemEval-2007 (dev)
		455
		330
		- Senseval-2 (test)
		2,282
		1,093
		- Senseval-3 (test)
		1,850
		977
		- SemEval 2013 (test)
		1,664
		751
		- SemEval 2015 (test)
		1,022
		512
		English lexical sample
		- Senseval-2 (train)
		8,611
		177
		- Senseval-2 (test)
		4,328
		146
		- Senseval-3 (train)
		8,022
		57
		- Senseval-3 (test)
		3,944
		57
		Chinese OntoNotes
		- Train
		66,409
		431
		- Dev
		9,523
		341
		- BC (test)
		1,769
		160
		- BN (test)
		3,227
		253
		- MZ (test)
		1,876
		223
		- NW (test)
		1,483
		143
		- All (test)
		8,355
		324
		Table 1: Statistics of the datasets used for the English
		all-words task, English lexical sample task, and Chi-
		nese OntoNotes WSD task in terms of the number of
		instances and the number of distinct lexelts
	</Abstractive Summary>
</Paper ID=ument532>


<Paper ID=ument532> <Table ID =2>
	<Abstractive Summary> =
		0∗
		Table 2:
		English all-words task results in F1 measure (%), averaged over three runs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows our WSD results in F1 mea-
		sure
	</Extractive Summary>
</Paper ID=ument532>


<Paper ID=ument532> <Table ID =3>
	<Abstractive Summary> =
		8∗
		Table 3:
		English lexical sample task results in accu-
		racy (%), averaged over three runs
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, our BERT-based WSD
		approach with linear projection model outper-
		forms all prior approaches
	</Extractive Summary>
</Paper ID=ument532>


<Paper ID=ument532> <Table ID =4>
	<Abstractive Summary> =
		2∗
		Table 4:
		Chinese OntoNotes WSD results in accu-
		racy (%), averaged over three runs, for each genre
	</Abstractive Summary>
</Paper ID=ument532>


<Paper ID=ument533> <Table ID =1>
	<Abstractive Summary> =
		5309
		Question Type
		Example
		Reasoning Required
		Comparative (Binary)
		Which country is a bigger exporter, Brazil or Uruguay?
		Binary Comparison
		Comparative (Non-binary)
		Which player had a touchdown longer than 20 yards?
		Greater Than
		Superlative (Number)
		How many yards was the shortest ﬁeld goal?
		List Minimum
		Superlative (Span)
		Who kicked the longest ﬁeld goal?
		Argmax
		Table 1: We focus on DROP Comparative and Superlative questions which test NAQANet’s numeracy
	</Abstractive Summary>
	<Extractive Summary> =
		, second row of Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		, third row of Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		, the second
		question in Table 1 requires (1) extracting all the
		touchdown distances, (2) ﬁnding the distance that
		is greater than twenty, and (3) selecting the player
		associated with the touchdown of that distance
	</Extractive Summary>
	<Extractive Summary> =
		Table 11 shows that this
		data augmentation can improve both interpolation
		and extrapolation, e
	</Extractive Summary>
</Paper ID=ument533>


<Paper ID=ument533> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: NAQANet achieves higher accuracy on ques-
		tions that require numerical reasoning (Superlative and
		Comparative) than on standard validation questions
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Emergent Numeracy in NAQANet
		NAQANet’s accuracy on comparative and superla-
		tive questions is signiﬁcantly higher than its aver-
		age accuracy on the validation set (Table 2)
	</Extractive Summary>
</Paper ID=ument533>


<Paper ID=ument533> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: We stress test NAQANet’s numeracy by ma-
		nipulating the numbers in the validation paragraphs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results for different para-
		graph modiﬁcations
	</Extractive Summary>
</Paper ID=ument533>


<Paper ID=ument533> <Table ID =4>
	<Abstractive Summary> =
		70
		Table 4: Interpolation with integers (e
	</Abstractive Summary>
	<Extractive Summary> =
		We ﬁrst focus on integers (Table 4)
	</Extractive Summary>
</Paper ID=ument533>


<Paper ID=ument533> <Table ID =5>
	<Abstractive Summary> =
		02
		Table 5: Interpolation with ﬂoats (e
	</Abstractive Summary>
</Paper ID=ument533>


<Paper ID=ument533> <Table ID =6>
	<Abstractive Summary> =
		02
		Table 6: Interpolation with negatives (e
	</Abstractive Summary>
</Paper ID=ument533>


<Paper ID=ument533> <Table ID =7>
	<Abstractive Summary> =
		48
		Table 7: Extrapolation on list maximum
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the accuracy for models
		trained on the integer range [0,150] and tested on
		5313
		Interpolation
		List Maximum (5-classes)
		Decoding (RMSE)
		Addition (RMSE)
		Integer Range
		[0,99]
		[0,999]
		[0,9999]
		[0,99]
		[0,999]
		[0,9999]
		[0,99]
		[0,999]
		[0,9999]
		Random Vectors
		0
	</Extractive Summary>
</Paper ID=ument533>


<Paper ID=ument534> <Table ID =1>
	<Abstractive Summary> =
		05
		Table 1: SymAcc, BLEU and AnsAcc on the FollowUp dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Answer Level
		Table 1 shows AnsAcc results of
		competitive baselines on the test set
	</Extractive Summary>
	<Extractive Summary> =
		Query Level
		Table 1 also shows SymAcc and
		BLEU of different methods on the dev and test
		sets
	</Extractive Summary>
</Paper ID=ument534>


<Paper ID=ument534> <Table ID =2>
	<Abstractive Summary> =
		93
		Table 2: Variant results on FollowUp dev set
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2,
		there are three variants with ablation: “– Phase
		I” takes out SplitNet and performs Phase II on
		word level; “– Phase II” performs random guess in
		the recombination process for testing; and “– RL”
		only contains pre-training
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 2 and Figure 4, STAR learns better
		After
		theweek
		6
		, which
		opponents
		won
		theresult
		of
		thematch
		?
		Precedent Query
		how
		about
		before
		week
		10
		?
		Follow-up Query
		Figure 5: An example of similarity matrix in SplitNet
	</Extractive Summary>
</Paper ID=ument534>


<Paper ID=ument534> <Table ID =3>
	<Abstractive Summary> =
		For exam-
		ple, given the precedent query “what’s the biggest
		zone?”
		and the follow-up query “the smallest
		one”, STAR prefers to recognize “the biggest
		zone” and “the smallest one” as two spans, rather
		5323
		Case Analysis
		1
		Precedent
		Follow-up
		STAR
		: [ compared to glebe park ] [ , does ] [ hampden park ] [ holds more attendances at capacity ? ]
		: [ how about ] [ compared to balmoor
		]
		: compared to balmoor , does hampden park holds more attendances at capacity ?
		2
		Precedent
		Follow-up
		STAR
		: [ Is there any book which belongs to ] [ Nancy miller ]
		: [ I mean ] [ the writer Nancy miller 
		]
		: Is there any book which belongs to the writer Nancy miller
		3
		Precedent
		Follow-up
		STAR
		: [ show directors of ] [ greatest love and promised land ]
		: [ show air date of
		] [ those two films      ]
		: show air date of greatest love and promised land
		No
		Table 3: Case analysis of STAR on FollowUp dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Precedent Query
		Follow-up Query
		How much money has Smith earned
		How about Bill Collins
		How much money has  Smith  earned
		How about  Bill Collins
		Restated Query
		Phase I
		Phase II
		How much money has  Smith             earned
		Bill Collins
		Conflict
		Figure 1: The two-phase process of an example from
		the FollowUp dataset (More real cases of diverse
		follow-up scenarios can be found in Table 3)
	</Extractive Summary>
</Paper ID=ument534>


<Paper ID=ument534> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: Answer accuracy on SQA test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the answer accuracy of precedent
		and follow-up queries on test set
	</Extractive Summary>
</Paper ID=ument534>


<Paper ID=ument535> <Table ID =1>
	<Abstractive Summary> =
		For example, the op-
		erator node SUB with arity 2 is expecting two child
		nodes below it in the expression tree, while CON
		Category
		Node
		Interpretation
		Arity
		Operator
		EQU
		ni EQU nj ⇔ (ni = nj)
		2
		ADD
		ni ADD nj ⇔ (ni + nj)
		2
		SUB
		ni SUB nj ⇔ (ni − nj)
		2
		MUL
		ni MUL nj ⇔ (ni × nj)
		2
		DIV
		ni DIV nj ⇔ (ni ÷ nj)
		2
		Quantity
		CON
		A constant
		0
		VAR
		A variable
		0
		Table 1: Expression tree nodes with interpretations,
		where ni (nj) refers to the ﬁrst (second) operand
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists the above nodes
	</Extractive Summary>
</Paper ID=ument535>


<Paper ID=ument535> <Table ID =2>
	<Abstractive Summary> =
		75
		Table 2: Arithmetic Word Problem: Accuracy (%)
		on the two benchmark datasets
	</Abstractive Summary>
</Paper ID=ument535>


<Paper ID=ument535> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Equation Parsing: Accuracy (%) on equation
		parsing dataset
	</Abstractive Summary>
	<Extractive Summary> =
		To make a fair compari-
		son with previous works, we did not count such
		cases as correct during evaluation, which implies
		that accuracy reported in Table 3 is in fact higher
	</Extractive Summary>
</Paper ID=ument535>


<Paper ID=ument535> <Table ID =4>
	<Abstractive Summary> =
		25
		Table 4: Performance of different constructions for ex-
		pression of arithmetic word problems and the effects of
		incorporating inverse operators
	</Abstractive Summary>
	<Extractive Summary> =
		Empirical re-
		sults, reported in the second block in Table 4, show
		that Text2Math (without including inverse oper-
		ators) can obtain comparative results compared to
		the model variants with inverse operators
	</Extractive Summary>
</Paper ID=ument535>


<Paper ID=ument535> <Table ID =5>
	<Abstractive Summary> =
		Gold:
		X1 = 5 + (3 × X2)
		Pipeline:
		X1 × X2 = 5 − 3
		Text2Math:
		X1 = 5 + (3 × X2)
		Table 5: Comparison between predictions made by the
		previous state-of-the-art system (Roy et al
	</Abstractive Summary>
</Paper ID=ument535>


<Paper ID=ument535> <Table ID =6>
	<Abstractive Summary> =
		Gold:
		X1 + X2 = 150
		Text2Math:
		X1 = 150
		Table 6: Examples with wrong predictions
	</Abstractive Summary>
	<Extractive Summary> =
		Exempliﬁed by Example 3 in Table 6, consider-
		ing the sentence “Germany’s DAX opens 0
	</Extractive Summary>
	<Extractive Summary> =
		Example 6 and 7 from Table 6 illus-
		trate such cases
	</Extractive Summary>
</Paper ID=ument535>


<Paper ID=ument536> <Table ID =1>
	<Abstractive Summary> =
		1
		3,794
		Table 1: Dataset Statistics
		5341
		Table 1: dorm
		Table 2: has
		Table 3: amenity
		id
		id
		name
		name
		dorm_id amenity_id
		foreign	key
		foreign	key
		Column Headers
		Utterance: how many dorms have a TV louge
		(a) An example of user utterance and column headers
	</Abstractive Summary>
	<Extractive Summary> =
		We summarize and compare the data statistics
		in Table 1 and Table 2
	</Extractive Summary>
</Paper ID=ument536>


<Paper ID=ument536> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: % of SQL queries that contain a particular SQL component
		5341
		Table 1: dorm
		Table 2: has
		Table 3: amenity
		id
		id
		name
		name
		dorm_id amenity_id
		foreign	key
		foreign	key
		Column Headers
		Utterance: how many dorms have a TV louge
		(a) An example of user utterance and column headers
	</Abstractive Summary>
</Paper ID=ument536>


<Paper ID=ument536> <Table ID =3>
	<Abstractive Summary> =
		amenity name = ‘TV Lounge’)
		Table 3: SParC example
		5341
		Table 1: dorm
		Table 2: has
		Table 3: amenity
		id
		id
		name
		name
		dorm_id amenity_id
		foreign	key
		foreign	key
		Column Headers
		Utterance: how many dorms have a TV louge
		(a) An example of user utterance and column headers
	</Abstractive Summary>
</Paper ID=ument536>


<Paper ID=ument536> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4: Spider results on dev set and test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results on Spider
		dataset
	</Extractive Summary>
</Paper ID=ument536>


<Paper ID=ument536> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5: SParC results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results on SParC
		dataset
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 5, editing the gold query con-
		sistently improves both question match and inter-
		action match accuracy
	</Extractive Summary>
	<Extractive Summary> =
		Finally, as an ablation study, Table 5 also re-
		ports the result with only query attention (use pre-
		dicted query) on the dev set
	</Extractive Summary>
</Paper ID=ument536>


<Paper ID=ument536> <Table ID =6>
	<Abstractive Summary> =
		1
		Table 6: ATIS results on dev set and test set
	</Abstractive Summary>
</Paper ID=ument536>


<Paper ID=ument537> <Table ID =1>
	<Abstractive Summary> =
		Dataset
		#sent
		#token
		#pred
		#arg
		Catalan
		13,200 390,302
		37,431
		84,367
		Chinese
		22,277 609,060 102,813 231,869
		Czech
		38,727 652,544 414,237 365,255
		English
		39,279 958,167 179,014 393,699
		German
		36,020 648,677
		17,400
		34,276
		Japanese
		4,393 112,555
		25,712
		43,957
		Spanish
		14,329 427,442
		43,824
		99,054
		Table 1: Training data statistics of sentences, tokens,
		predicates and arguments
	</Abstractive Summary>
</Paper ID=ument537>


<Paper ID=ument537> <Table ID =2>
	<Abstractive Summary> =
		42
		Table 2: Precision, recall and semantic F1-score on CoNLL-2009 English in-domain data and Chinese test set
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results and Discussion
		In Table 2, we compare our single model (AP is an
		acronym for argument pruning) against previous
		work on English in-domain data and Chinese test
		set
	</Extractive Summary>
</Paper ID=ument537>


<Paper ID=ument537> <Table ID =3>
	<Abstractive Summary> =
		60
		Table 3: Semantic F1-score on CoNLL-2009 in-domain test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents all test results on seven lan-
		guages of CoNLL-2009 datasets
	</Extractive Summary>
	<Extractive Summary> =
		Besides, we report the scores of leveraging
		ELMo and BERT for multiple languages (the last
		three rows in Table 3)
	</Extractive Summary>
</Paper ID=ument537>


<Paper ID=ument537> <Table ID =4>
	<Abstractive Summary> =
		15
		Table 4: Results of full end-to-end model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results of end-to-end set-
		ting
	</Extractive Summary>
</Paper ID=ument537>


<Paper ID=ument537> <Table ID =5>
	<Abstractive Summary> =
		47)
		Table 5: Ablation of POS tag and lemma on test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 reports the F1
		scores of model which removes POS tag or lemma
		from the baseline
	</Extractive Summary>
</Paper ID=ument537>


<Paper ID=ument537> <Table ID =6>
	<Abstractive Summary> =
		17
		Table 6: Comparison of our model with syntactic rule
		and k-order argument pruning
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the
		performance gaps between two pruning methods
	</Extractive Summary>
</Paper ID=ument537>


<Paper ID=ument537> <Table ID =7>
	<Abstractive Summary> =
		82
		Table 7: Syntactic contribution to multilingual SRL
	</Abstractive Summary>
</Paper ID=ument537>


<Paper ID=ument538> <Table ID =1>
	<Abstractive Summary> =
		Our experiments consider three
		model sizes shown in Table 1: There are two CNN
		input models in a base and large conﬁguration as
		well as a Byte-Pair-Encoding based model (BPE;
		Sennrich et al5
		Table 1: Hyper-parameters for our models
	</Abstractive Summary>
	<Extractive Summary> =
		The BPE model trains much faster than
		the character CNN models (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Results are based on the CNN
		base model (Table 1)
		CRF with 1E-03 and pretrained language model
		with 1E-05 gave us the best result
	</Extractive Summary>
	<Extractive Summary> =
		Results are based on the CNN base model (Table 1)
	</Extractive Summary>
</Paper ID=ument538>


<Paper ID=ument538> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Test results as per the GLUE evaluation server
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows results for three conﬁgurations
		of our approach (cf
	</Extractive Summary>
	<Extractive Summary> =
		Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Table 2)
	</Extractive Summary>
</Paper ID=ument538>


<Paper ID=ument538> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: CoNLL-2003 Named Entity Recognition re-
		sults
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results, with comparison
		to previous published ELMoBASE results (Peters
		et al
	</Extractive Summary>
</Paper ID=ument538>


<Paper ID=ument538> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Penn Treebank Constituency Parsing results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results
	</Extractive Summary>
</Paper ID=ument538>


<Paper ID=ument538> <Table ID =5>
	<Abstractive Summary> =
		4
		Table 5: Different loss functions on the development sets of GLUE (cf
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows that the cloze loss performs sig-
		niﬁcantly better than the bilm loss and that com-
		bining the two loss types does not improve over
		the cloze loss by itself
	</Extractive Summary>
</Paper ID=ument538>


<Paper ID=ument538> <Table ID =6>
	<Abstractive Summary> =
		9
		Table 6: Effect of different domains and amount of data for pretraining on the on the development sets of GLUE
		(cf
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows a breakdown into individual
		5367
		train data
		(M tok)
		CoLA
		(mcc)
		SST-2
		(acc)
		MRPC
		(F1)
		STS-B
		(scc)
		QQP
		(F1)
		MNLI-m
		(acc)
		QNLI
		(acc)
		RTE
		(acc)
		Avg
		ccrawl
		562
		52
	</Extractive Summary>
</Paper ID=ument538>


<Paper ID=ument539> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Sample sentence pairs from TREC Microblog 2013, Quora, and TrecQA
	</Abstractive Summary>
</Paper ID=ument539>


<Paper ID=ument539> <Table ID =2>
	<Abstractive Summary> =
		853
		Table 2: Results on TrecQA, TwitterURL, and Quora
	</Abstractive Summary>
	<Extractive Summary> =
		3
		4
		Results
		Our main results on the TrecQA, TwitterURL, and
		Quora datasets are shown in Table 2 and results
		on TREC Microblog 2013–2014 are shown in Ta-
		ble 3
	</Extractive Summary>
</Paper ID=ument539>


<Paper ID=ument539> <Table ID =3>
	<Abstractive Summary> =
		6485
		Table 3: Results on TREC Microblog 2013–2014, or-
		ganized in the same manner as Table 2
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we observe that the query expan-
		sion method (RM3) outperforms most of the neu-
		ral ranking models except for BERT, which is con-
		sistent with Yang et al
	</Extractive Summary>
	<Extractive Summary> =
		Comparing our proposed approaches in Table 3,
		RM achieves fairly good scores while SM is not
		effective at all, afﬁrming our hypothesis that term
		matching signals are essential to IR tasks
	</Extractive Summary>
</Paper ID=ument539>


<Paper ID=ument539> <Table ID =4>
	<Abstractive Summary> =
		635
		Table 4: Evaluation of different encoders in Sec
	</Abstractive Summary>
</Paper ID=ument539>


<Paper ID=ument539> <Table ID =5>
	<Abstractive Summary> =
		7280
		- Does RBI send its employees for higher education
		such as MBA , like sponsoring the education or
		allowing paid / unpaid leaves ?
		- Does EY send its employees for higher education
		such as MBA , like sponsoring the education or
		allowing paid / unpaid leaves ?
		Table 5: Sample pairs from Quora
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Qualitative Sample Analysis
		We present sample outputs in Table 5 to gain more
		insight into model behavior
	</Extractive Summary>
</Paper ID=ument539>


<Paper ID=ument54> <Table ID =1>
	<Abstractive Summary> =
		Table 1:
		Headline generation examples from our
		model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows an example
	</Extractive Summary>
</Paper ID=ument54>


<Paper ID=ument54> <Table ID =2>
	<Abstractive Summary> =
		181
		Table 2: Diversity of content selection
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists the re-
		sults
	</Extractive Summary>
</Paper ID=ument54>


<Paper ID=ument54> <Table ID =3>
	<Abstractive Summary> =
		81
		Table 3: Self-Bleu score by ﬁxing selection mask
	</Abstractive Summary>
</Paper ID=ument54>


<Paper ID=ument54> <Table ID =4>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 4
		show that VRS signiﬁcantly outperforms the other
		two in both intra-consistency and inter-diversity
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the
		best ﬂuency is achieved for Enc-Dec
	</Extractive Summary>
</Paper ID=ument54>


<Paper ID=ument54> <Table ID =5>
	<Abstractive Summary> =
		53
		Table 5: Gigaword best-select results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5/6 further measure the metric scores on
		Gigaword/Wikibio by decoding text from the best
		selection mask based on the selector’s distribution
		(set βi = 1 if B(γi) > 0
	</Extractive Summary>
</Paper ID=ument54>


<Paper ID=ument54> <Table ID =6>
	<Abstractive Summary> =
		57
		Table 6: Wikibio best-select results
	</Abstractive Summary>
</Paper ID=ument54>


<Paper ID=ument540> <Table ID =1>
	<Abstractive Summary> =
		91
		Table 1: Experimental results of syntax-aware methods we compare on CPB1
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows the results of these syntax-aware methods
		on CPB1
	</Extractive Summary>
</Paper ID=ument540>


<Paper ID=ument540> <Table ID =2>
	<Abstractive Summary> =
		54
		Table 2: Results and comparison with previous works
		on CPB1
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results
		of our baseline model and proposed framework us-
		ing external dependency trees on CPB1
	</Extractive Summary>
</Paper ID=ument540>


<Paper ID=ument540> <Table ID =3>
	<Abstractive Summary> =
		57
		Table 3: F1 scores of end-to-end settings on CPB1
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results of our framework in
		the end-to-end setting
	</Extractive Summary>
</Paper ID=ument540>


<Paper ID=ument540> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Results and comparison with previous works
		on CoNLL-2009 Chinese test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the
		results of our framework and comparison with
		previous works on the CoNLL-2009 Chinese test
		data
	</Extractive Summary>
</Paper ID=ument540>


<Paper ID=ument541> <Table ID =1>
	<Abstractive Summary> =
		2M
		Table 1: Numbers of sentential and phrasal paraphrases
		after the phrase alignment process
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Training Setting
		We collected paraphrases from various sources as
		summarized in Table 1, which shows the num-
		bers of sentential and phrasal paraphrase pairs
		after phrase alignment
	</Extractive Summary>
</Paper ID=ument541>


<Paper ID=ument541> <Table ID =2>
	<Abstractive Summary> =
		Table 2: GLUE tasks and evaluation metrics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the tasks and
		evaluation metrics at GLUE
	</Extractive Summary>
</Paper ID=ument541>


<Paper ID=ument541> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: GLUE test results scored by the GLUE evaluation server
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Effect on Semantic Equivalence
		Assessment Tasks
		Table 3 shows ﬁne-tuning results on GLUE; our
		model, denoted as Transfer Fine-Tuning, is com-
		pared against BERT-base and BERT-large
	</Extractive Summary>
	<Extractive Summary> =
		3
		Effect on NLI Tasks
		The second set of columns in Table 3 shows the
		results on NLI tasks
	</Extractive Summary>
	<Extractive Summary> =
		4
		Effect on Single-Sentence Tasks
		The last two columns of Table 3 show results on
		single-sentence tasks; SST and CoLA, which are
		the most distant tasks from paraphrase classiﬁca-
		tion
	</Extractive Summary>
</Paper ID=ument541>


<Paper ID=ument541> <Table ID =4>
	<Abstractive Summary> =
		0)
		Table 4: Development set scores of the BERT-base
		model and our model (and their differences) that were
		ﬁne-tuned using subsamples and full-size training sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		shows scores on the development sets
	</Extractive Summary>
</Paper ID=ument541>


<Paper ID=ument541> <Table ID =5>
	<Abstractive Summary> =
		1
		Table 5: Results of the ablation study where the best scores are represented in bold and scores higher than those
		of BERT-base are underlined
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results; the last three
		rows show performances when conducting only
		sentential paraphrase classiﬁcation, phrasal para-
		phrase classiﬁcation, and binary classiﬁcation of
		paraphrase and in-paraphrase pairs, re-
		spectively
	</Extractive Summary>
</Paper ID=ument541>


<Paper ID=ument541> <Table ID =6>
	<Abstractive Summary> =
		2
		Table 6: Test results on semantic equivalence assessment and NLI tasks scored by the GLUE evaluation server
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 compares this new model (denoted as
		Simple Transfer Fine-Tuning) to BERT models as
		well as our model with the elaborate feature gen-
		eration described in Sec
	</Extractive Summary>
</Paper ID=ument541>


<Paper ID=ument541> <Table ID =7>
	<Abstractive Summary> =
		6
		Table 7: Results of the ablation study where the best scores are represented in bold and scores higher than those
		of BERT-base are underlined
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 re-
		ports an ablation study
	</Extractive Summary>
</Paper ID=ument541>


<Paper ID=ument542> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Performance improvement of the neural semantic parser on WikiSQL
	</Abstractive Summary>
	<Extractive Summary> =
		Moreover, we perform ablation
		studies by 1) removing the supervision for the
		anonymization model (denoted as ‘-Supervision’
		in Table 1), and 2) simply using the output of the
		trained anonymization model as the input for the
		parser without training them as a whole (denoted
		as ‘-Co-training’ in Table 1)
	</Extractive Summary>
</Paper ID=ument542>


<Paper ID=ument542> <Table ID =2>
	<Abstractive Summary> =
		5)
		Table 2: Performance improvement of the neural semantic parser on Spider with different hardness levels
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the re-
		sults
	</Extractive Summary>
</Paper ID=ument542>


<Paper ID=ument542> <Table ID =3>
	<Abstractive Summary> =
		#Total
		#Distinct x
		#Distinct ˜x
		Dev
		8421
		8387
		5488
		Test
		15878
		15828
		9680
		Table 3: The number of distinct input utterances and
		distinct anonymous utterances on WikiSQL
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows that by anonymizing table-
		related tokens, the number of distinct utterances
		is reduced from 8387 to 5488 on dev set and from
		15828 to 9680 on test set
	</Extractive Summary>
</Paper ID=ument542>


<Paper ID=ument542> <Table ID =4>
	<Abstractive Summary> =
		Furthermore, per-
		5412
		˜x
		which column have column of cell?
		x
		which place [column] has a rank [column] of 71 [cell]?
		which county [column] has a median household income [column] of $98,090 [cell] ?
		˜x
		what be column when column be cell?
		x
		what is the inclination [column] when the alt name [column] is ops-1584 [cell]?
		what was the district [column] when the reason for change [column] was died january 1, 1974 [cell]?
		˜x
		name column for cell
		x
		name the candidates [column] for georgia 8 [cell]
		name the party [column] for jack thomas brinkley [cell]
		Table 4: Top frequent anonymous utterances on dev set of WikiSQL
	</Abstractive Summary>
	<Extractive Summary> =
		2) although
		it is ignored when giving examples in Figure 1 and
		Table 4 for ease of read
	</Extractive Summary>
	<Extractive Summary> =
		Furthermore, Table 4
		shows three anonymous utterances that are most
		frequent on dev set and examples of correspond-
		ing input utterances
	</Extractive Summary>
</Paper ID=ument542>


<Paper ID=ument542> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Performances of different anonymization models on WikiSQL
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows that DAE signiﬁcantly outper-
		forms TypeSQL and AnnotatedSeq2Seq on all the
		evaluation metrics
	</Extractive Summary>
</Paper ID=ument542>


<Paper ID=ument543> <Table ID =1>
	<Abstractive Summary> =
		98
		Table 1: The performance of our model (w/o global
		node) with different discount coefﬁcients η of loss
		L∗(θ) on the English development set
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Model Selection
		Table 1 shows the performance of our model
		trained with loss L∗(θ) for different values of dis-
		count coefﬁcient η on the English development
		set
	</Extractive Summary>
</Paper ID=ument543>


<Paper ID=ument543> <Table ID =2>
	<Abstractive Summary> =
		72
		Table 2: The F1 scores of previous systems on the En-
		glish test set and out-of-domain (Ood) set
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Overall Results
		Table 2 compares our model with previous state-
		of-the-art SRL systems on English
	</Extractive Summary>
</Paper ID=ument543>


<Paper ID=ument543> <Table ID =3>
	<Abstractive Summary> =
		75
		Table 3: Ablation results in English development and test sets
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Ablation
		Table 3 gives the performance of models with ab-
		lation on some key components, which shows the
		contribution of each component in our model
	</Extractive Summary>
</Paper ID=ument543>


<Paper ID=ument543> <Table ID =4>
	<Abstractive Summary> =
		It takes the mean of capsules uj|i
		in the word capsule layer as the semantic role log-
		its bj|i (see Eq (19)), and hence could be viewed as
		an enhanced (‘ensembled’) version of the baseline
		Sent Len
		# of Props
		0 - 9
		181
		10 - 19
		1,806
		20 - 29
		3,514
		30 - 39
		3,102
		40 - 49
		1,383
		50 - 59
		391
		60 - 69
		121
		# of Args
		# of Props
		1
		2,591
		2
		4,497
		3
		2,189
		4
		889
		5
		259
		6
		39
		7
		7
		Table 4: Numbers of propositions with different sen-
		tence lengths and different numbers of arguments on
		the English test set
	</Abstractive Summary>
</Paper ID=ument543>


<Paper ID=ument543> <Table ID =5>
	<Abstractive Summary> =
		07
		Table 5: Labeled F1 score (including senses) for all languages on the CoNLL-2009 in-domain test sets
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Multilingual Results
		Table 5 gives the results of the proposed Capsu-
		leNet SRL (with global node) on the in-domain
		test sets of all languages from CoNLL-2009
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 5, the proposed model consistently
		outperforms the non-reﬁnement baseline model
		and achieves state-of-the-art performance on Cata-
		lan (Ca), Czech (Cz), English (En), Japanese (Jp)
		and Spanish (Es)
	</Extractive Summary>
</Paper ID=ument543>


<Paper ID=ument544> <Table ID =1>
	<Abstractive Summary> =
		0)
		Table 1: Exact Match and BLEU scores for our simpli-
		ﬁed model (Iyer-Simp) with and without idioms, com-
		pared with results from Iyer et al
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Results and Discussion
		Table 1 presents exact match and BLEU scores on
		the original CONCODE train/validation/test split
	</Extractive Summary>
</Paper ID=ument544>


<Paper ID=ument544> <Table ID =2>
	<Abstractive Summary> =
		7
		11
		Table 2: Variation in Exact Match, BLEU score, and
		training time on the validation set of CONCODE with
		number of idioms used
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we illustrate the variations in EM,
		BLEU and training time with the number of id-
		ioms
	</Extractive Summary>
</Paper ID=ument544>


<Paper ID=ument544> <Table ID =3>
	<Abstractive Summary> =
		6)
		Table 3: Exact Match and BLEU scores on the test
		(validation) set of CONCODE by training Iyer-Simp-
		400 on the extended training set released by Iyer et al
	</Abstractive Summary>
</Paper ID=ument544>


<Paper ID=ument544> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Denotational Accuracy for Seq2Prod with and
		without idioms, compared with results from Iyer et al
	</Abstractive Summary>
</Paper ID=ument544>


<Paper ID=ument545> <Table ID =1>
	<Abstractive Summary> =
		, 2016)
		Table 1: Comparison of JuICe with various recently released NL-code datasets along four dimensions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 compares JuICe with
		various recently released code generation datasets
		on four different dimensions: interactive history,
		domain speciﬁcity, scale, and human annotation
	</Extractive Summary>
</Paper ID=ument545>


<Paper ID=ument545> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Statistics for the JuICe dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2
		presents dataset statistics for JuICe
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the dataset statistics
	</Extractive Summary>
	<Extractive Summary> =
		Most ex-
		amples require using variables or methods deﬁned
		above (similar to statistics in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		use a token from
		the context (from Table 2), whereas 86% exam-
		ples required contextual reasoning that includes
		reasoning from data and idioms as well
	</Extractive Summary>
</Paper ID=ument545>


<Paper ID=ument545> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Qualitative analysis of NL from the dev set of JuICe
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents statistics of 50 target NL
		cells classiﬁed into overlapping categories
	</Extractive Summary>
	<Extractive Summary> =
		The last three rows of Table 3 present
		interesting NL phenomenon
	</Extractive Summary>
</Paper ID=ument545>


<Paper ID=ument545> <Table ID =4>
	<Abstractive Summary> =
		Code Type
		%
		Data Exploration
		25
		Data Wrangling
		23
		Machine Learning
		20
		Miscellaneous
		16
		Visualization
		13
		Systems
		3
		Table 4: JuICe includes code for various real world ap-
		plications, primarily for data science/machine learning
	</Abstractive Summary>
	<Extractive Summary> =
		Code Analysis
		Table 4 presents statistics of 50
		code cells sampled from the dev set
	</Extractive Summary>
</Paper ID=ument545>


<Paper ID=ument545> <Table ID =5>
	<Abstractive Summary> =
		fit(X, y)
		    return gs
		d = 4
		d = 1
		Target
		Table 5: Types of contextual reasoning required for target cell generation
	</Abstractive Summary>
	<Extractive Summary> =
		For example, in row 2 of
		Table 5, knowing that unemployment stores the
		rates for every quarter in the array, suggests the use
		of the diff function which ﬁnds the difference
		between every consecutive value
	</Extractive Summary>
	<Extractive Summary> =
		In row
		4 of Table 5, the GridSearch object created in
		one cell needs to be used to select the best hyper-
		parameter values, and retrain the model using the
		same procedure found in another code cell
	</Extractive Summary>
</Paper ID=ument545>


<Paper ID=ument545> <Table ID =6>
	<Abstractive Summary> =
		71
		Table 6: Exact Match and BLEU score for the full code
		generation task on both the dev and test sets of JuICe
		for all baselines
	</Abstractive Summary>
	<Extractive Summary> =
		8
		Results and Discussion
		We present EM and BLEU scores for the code
		generation task in Table 6, and precision/recall for
		the API sequence task in Table 7, on both the dev
		5444
		and test sets of JuICe for all our baselines
	</Extractive Summary>
	<Extractive Summary> =
		5 million examples
		with a context length of 3 (Table 6)
	</Extractive Summary>
</Paper ID=ument545>


<Paper ID=ument545> <Table ID =7>
	<Abstractive Summary> =
		46
		Table 7: Precision and Recall for the API sequence task
		on the dev/test sets of JuICe for all baselines
		context size K and dataset size N
	</Abstractive Summary>
	<Extractive Summary> =
		8
		Results and Discussion
		We present EM and BLEU scores for the code
		generation task in Table 6, and precision/recall for
		the API sequence task in Table 7, on both the dev
		5444
		and test sets of JuICe for all our baselines
	</Extractive Summary>
</Paper ID=ument545>


<Paper ID=ument545> <Table ID =8>
	<Abstractive Summary> =
		they
		contain some correct code lines but miss some de-
		Error Category
		%
		Challenging NL Reasoning
		39
		Arguments Missed
		17
		Contextual Reasoning
		26
		Needs Longer Context
		10
		Partially Correct
		26
		Semantically Equivalent
		15
		Table 8: Qualitative error analysis on 50 incorrectly
		generated code cells from our dev set for our best per-
		forming baseline
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Error Analysis
		We conduct a qualitative error analysis using
		50 erroneous predictions of our top performing
		model - LSTM K=3, N=1,518,049 on dev set
		examples (Table 8)
	</Extractive Summary>
</Paper ID=ument545>


<Paper ID=ument546> <Table ID =1>
	<Abstractive Summary> =
		Is this condition correct? → Q[val∥WHERE col op val]
		Table 1: Domain-general lexicon and grammar for NL generation in MISP-SQL (illustrated for WikiSQL; a more
		comprehensive grammar for Spider can be found in Appendix A)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows rules covering
		5451
		[Lexicon]
		is greater than|equals to|is less than → OP[>|=|<]
		sum of values in|average value in|number of|minimum value in|maximum value in → AGG[sum|avg|count|min|max]
		[Grammar]
		“col” → COL[col]
		Does the system need to return information about COL[col] ? → Q[col∥SELECT agg? col]
		Does the system need to return AGG[agg] COL[col] ? → Q[agg∥SELECT agg col]
		Does the system need to return a value after any mathematical calculations on COL[col] ? → Q[agg=None∥SELECT col]
		Does the system need to consider any conditions about COL[col] ? → Q[col∥WHERE col op val]
		The system considers the following condition: COL[col] OP[op] a value
	</Extractive Summary>
</Paper ID=ument546>


<Paper ID=ument546> <Table ID =2>
	<Abstractive Summary> =
		488
		Table 2: Simulation evaluation of MISP-SQL (based on SQLNet or SQLova) on WikiSQL Test set
	</Abstractive Summary>
</Paper ID=ument546>


<Paper ID=ument546> <Table ID =3>
	<Abstractive Summary> =
		5%
		Table 3: Portion of interaction questions on right pre-
		dictions (Qr%) for each agent setting on WikiSQL Dev
		set (smaller is better)
	</Abstractive Summary>
</Paper ID=ument546>


<Paper ID=ument546> <Table ID =4>
	<Abstractive Summary> =
		905
		Table 4: Simulation evaluation of MISP-SQL (built on
		SyntaxSQLNet) on Spider Dev set
	</Abstractive Summary>
	<Extractive Summary> =
		, 2018, Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the results
	</Extractive Summary>
</Paper ID=ument546>


<Paper ID=ument546> <Table ID =5>
	<Abstractive Summary> =
		647
		Table 5: Human evaluation on 100 random examples
		for MISP-SQL agents based on SQLNet, SQLova and
		SyntaxSQLNet, respectively
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		This discrepancy is clearly manifested in our
		human evaluation with SQLova (Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		To verify this, we conduct an additional
		experiment with SQLova where human evaluators
		can view the table content as well as the gold SQL
		query before starting the interaction to better un-
		derstand the true intent (denoted as “w/ full info”
		in Table 5)
	</Extractive Summary>
</Paper ID=ument546>


<Paper ID=ument547> <Table ID =1>
	<Abstractive Summary> =
		5462
		xi
		xj
		Structural label sequence
		he
		convict-01
		:ARG1↑
		he
		7
		:ARG1↑ :ARG2↓ :quant↓
		he
		he
		None
		Table 1: Examples of structural path between a few
		concept pairs in Figure 1
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 demonstrates structural label sequences
		between a few concept pairs in Figure 1
	</Extractive Summary>
	<Extractive Summary> =
		In this way, the structural label sequence for
		he and 7 in Table 1, for example, will be None
	</Extractive Summary>
</Paper ID=ument547>


<Paper ID=ument547> <Table ID =2>
	<Abstractive Summary> =
		88
		Table 2: Ablation results of our baseline system on the
		LDC2015E86 development set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents the re-
		sults of the ablation test on the development set of
		LDC2015E86 by either removing BPE, or vocabu-
		lary sharing, or both of them from the baseline sys-
		tem
	</Extractive Summary>
</Paper ID=ument547>


<Paper ID=ument547> <Table ID =3>
	<Abstractive Summary> =
		3
		-
		-
		-
		-
		-
		-
		Table 3:
		Comparison results of our approaches and related studies on the test sets of LDC2015E86 and
		LDC2017T10
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the comparison of our ap-
		proach and related works on the test sets of
		LDC2015E86 and LDC2017T10
	</Extractive Summary>
</Paper ID=ument547>


<Paper ID=ument547> <Table ID =4>
	<Abstractive Summary> =
		92
		Table 4: Performance on the test set of our approach
		with or without modeling structural information of in-
		directly connected concept pairs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 compares the performance of our ap-
		proach with or without modeling structural infor-
		mation of indirectly connected concept pairs
	</Extractive Summary>
</Paper ID=ument547>


<Paper ID=ument548> <Table ID =1>
	<Abstractive Summary> =
		Dataset
		Positive
		Neutral
		Negative
		Laptop-Train
		767
		373
		673
		Laptop-Dev
		220
		87
		193
		Laptop-Test
		341
		169
		128
		Restaurant-Train
		1886
		531
		685
		Restaurant-Dev
		278
		102
		120
		Restaurant-Test
		728
		196
		196
		Table 1: Statistics of the datasets
	</Abstractive Summary>
</Paper ID=ument548>


<Paper ID=ument548> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Comparison results of different methods on
		laptop and restaurant datasets
	</Abstractive Summary>
</Paper ID=ument548>


<Paper ID=ument548> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: An ablation study shows the effect of explicit
		target information
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, explicitly capturing as-
		pect target information consistently improves the
		performance of the TD-GAT-GloVe over the GAT-
		GloVe model
	</Extractive Summary>
</Paper ID=ument548>


<Paper ID=ument548> <Table ID =4>
	<Abstractive Summary> =
		49
		Table 4: The model size (number of parameters) of our
		model as well as baselines
	</Abstractive Summary>
</Paper ID=ument548>


<Paper ID=ument549> <Table ID =1>
	<Abstractive Summary> =
		They
		5482
		#Target
		#+
		#−
		#0
		English
		3,288
		707
		275
		2,306
		Spanish
		6,658
		1,555
		1,007
		4,096
		(a) Statistics on polarity of named entities
		Target length
		1
		2
		3
		>= 4
		English
		1,910
		1,032
		232
		114
		Spanish
		4,201
		1,794
		417
		246
		(b) Statistics on target length
		#Target
		1
		2
		3
		>= 4
		English
		1,692
		465
		135
		58
		Spanish
		3,855
		903
		221
		69
		(c) Statistics on number of targets per sentence
		Table 1: Corpus Statistics of Main Dataset
		contain 2,350 English tweets and 7,105 Spanish
		tweets, with target and targeted sentiment anno-
		tated
	</Abstractive Summary>
	<Extractive Summary> =
		See Table 1 for corpus statistics
	</Extractive Summary>
</Paper ID=ument549>


<Paper ID=ument549> <Table ID =2>
	<Abstractive Summary> =
		11
		Table 2: Main Results
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Main Results
		The main results are presented in Table 2, where
		explicit structures as well as implicit structures are
		indicated for each model for clear comparisons
	</Extractive Summary>
</Paper ID=ument549>


<Paper ID=ument549> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Results on subjectivity as well as non-neutral
		sentiment analysis on the Spanish dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Results are
		reported in Table 37
	</Extractive Summary>
</Paper ID=ument549>


<Paper ID=ument549> <Table ID =4>
	<Abstractive Summary> =
		14
		Table 4: Effect of Implicit Structures
		Figure 5: Results of different lengths on Spanish
		+ / 0
		+ / +
		+ / +
		Czech
		Republic
		,
		Greece
		and
		Russian
	</Abstractive Summary>
</Paper ID=ument549>


<Paper ID=ument549> <Table ID =5>
	<Abstractive Summary> =
		4
		Table 5: F1 scores of targets (target) and their asso-
		ciated sentiment (sent) on SemEval 2016 Restaurant
		Dataset
	</Abstractive Summary>
</Paper ID=ument549>


<Paper ID=ument55> <Table ID =1>
	<Abstractive Summary> =
		23
		Table 1: Statistics of the three datasets
	</Abstractive Summary>
	<Extractive Summary> =
		The keyphrase bank size is lim-
		ited to 70 for argument, and 30 for Wikipedia and
		AGENDA data (based on the average numbers in
		Table 1), with keyphrases truncated to 10 words
	</Extractive Summary>
</Paper ID=ument55>


<Paper ID=ument55> <Table ID =2>
	<Abstractive Summary> =
		4%
		Table 2: Sentence style distribution for argument and
		Wikipedia datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Statistics are displayed in Table 2, and sam-
		ple rules are shown below, with the complete list
		in the Supplementary:
		• CLAIM:
		must be shorter than 20 tokens and
		matches any of the following patterns:
		(a) i
		(don’t)?
		(believe|agree|
	</Extractive Summary>
</Paper ID=ument55>


<Paper ID=ument55> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Results on argument generation with BLEU
		(up to bigrams), ROUGE-L, and METEOR (MTR)
	</Abstractive Summary>
</Paper ID=ument55>


<Paper ID=ument55> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Results on Wikipedia generation
	</Abstractive Summary>
	<Extractive Summary> =
		Results on Wikipedia
		(Table 4) show similar trends, where our models
		almost always outperform all comparisons across
		metrics
	</Extractive Summary>
</Paper ID=ument55>


<Paper ID=ument55> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: Results on paper abstract generation
	</Abstractive Summary>
</Paper ID=ument55>


<Paper ID=ument55> <Table ID =6>
	<Abstractive Summary> =
		55
		Table 6: Human evaluation on argument generation
		(Upper) and Wikipedia generation (Bottom)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6, on both tasks, our models
		with style speciﬁcation produce more ﬂuent and
		correct generations, compared to the ones without
		such information
	</Extractive Summary>
</Paper ID=ument55>


<Paper ID=ument55> <Table ID =7>
	<Abstractive Summary> =
		We ﬁrst calculate the most
		599
		Human
		Our model
		C It doesn’t mean that; every-
		one should be able to
		I don’t believe that it is nec-
		essary; don’t need to be
		able to
		P have the freedom to;
		is
		leagal in the US; imagine
		for a moment if; Let’s say
		(you/your partner/a friend)
		have
		the
		right
		to
		(bear
		arms/cast a ballot vote);
		For example, (if you look
		at/let’s look at/I don’t think)
		F Why is that?;
		that’s ok;
		Would
		it
		change
		your
		mind?
		I’m not sure (why/if) this is;
		TLDR: I don’t care about
		this
		Table 7: Top frequent patterns captured in style CLAIM
		(C), PREMISE (P), and FUNCTIONAL (F) from argu-
		ments by human and our model
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Further Analysis and Discussions
		We further investigate the usage of different styles,
		and show the top frequent patterns for each argu-
		ment style from human arguments and our system
		generation (Table 7)
	</Extractive Summary>
</Paper ID=ument55>


<Paper ID=ument550> <Table ID =1>
	<Abstractive Summary> =
		81
		Table 1: Experimental results
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Main Results
		As shown in Table 1, IACapsNet achieves the best
		performance on all the datasets
	</Extractive Summary>
	<Extractive Summary> =
		From Table 1, we
		can have the following observations
	</Extractive Summary>
</Paper ID=ument550>


<Paper ID=ument550> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		Positive
		Neural
		Negative
		Train Test Train Test Train Test
		Restaurant 2164 728
		637
		196
		807
		196
		Laptop
		994
		341
		464
		169
		870
		128
		Twitter
		1561 173 3127 346 1560 173
		Table 2: Summary statistics of the datasets
	</Abstractive Summary>
</Paper ID=ument550>


<Paper ID=ument550> <Table ID =3>
	<Abstractive Summary> =
		84
		Table 3: Ablation study on Restaurant dataset
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 3 indi-
		cate: (1) EM routing based IACapsNet outper-
		forms IACapsNet-Cosine, which routes capsules
		by cosine similarity (Sabour et al
	</Extractive Summary>
</Paper ID=ument550>


<Paper ID=ument550> <Table ID =4>
	<Abstractive Summary> =
		024
		Table 4: Evaluation of efﬁciency
	</Abstractive Summary>
	<Extractive Summary> =
		Moreover, EM routing also brings a boost on ef-
		ﬁciency with fewer trainable parameters and faster
		speed which is intuitively shown in Table 4 (IAN
		is listed as baseline)
	</Extractive Summary>
</Paper ID=ument550>


<Paper ID=ument551> <Table ID =1>
	<Abstractive Summary> =
		378
		Table 1: The statistics of emotion distribution in the
		dataset
		Emtion
		Post Number
		Happiness
		2,915
		Sadness
		2,454
		Fear
		359
		Anger
		153
		Surprise
		601
		None
		4,675
		5
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 illustrates the statis-
		tics of each emotion
	</Extractive Summary>
</Paper ID=ument551>


<Paper ID=ument551> <Table ID =2>
	<Abstractive Summary> =
		0001, and λ1 : λ2 :
		λ3 = 1 : 1 : 1
		5504
		Table 2: Experimental results of different models
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results
		We compare the proposed Neural Personal Dis-
		crimination (NPD) model with several represen-
		tative baselines models in Table 2, where,1) SVM
		is a widely used baseline to predict the emotion of
		a post in social media (Yang et al
	</Extractive Summary>
	<Extractive Summary> =
		,
		2017)
		From Table 2, we ﬁnd that all of the neural mod-
		els outperform SVM signiﬁcantly
	</Extractive Summary>
</Paper ID=ument551>


<Paper ID=ument551> <Table ID =3>
	<Abstractive Summary> =
		5505
		Table 3: Comparison of various models with different
		personal attributes
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Inﬂuence of Personal Attributes
		We illustrate the inﬂuence of personal attributes
		in the proposed NPD model in Table 3, where,
		1)LSTM-attributes is a LSTM based multi-label
		classiﬁcation model, which predicts both the e-
		motion and the attribute labels of each post col-
		lectively
	</Extractive Summary>
</Paper ID=ument551>


<Paper ID=ument551> <Table ID =4>
	<Abstractive Summary> =
		378
		Table 4: Inﬂuence of network structures
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Inﬂuence of Network Structures
		After analyzing the inﬂuence of different at-
		tributes, we analyze the inﬂuence of network
		structures in Table 4, where LSTM-attention ab-
		lates the adversarial discriminators, and only uti-
		lizes attention mechanisms with a multi-label clas-
		siﬁcation setting, and LSTM-adversarial ablates
		the attention mechanisms and only utilizes adver-
		sarial discriminators for emotion detection
	</Extractive Summary>
	<Extractive Summary> =
		From Table 4, we can see that both the attention
		mechanisms (LSTM-attention) and the adversar-
		ial discriminators (LSTM-adversarial) are effec-
		tive in emotion detection
	</Extractive Summary>
</Paper ID=ument551>


<Paper ID=ument551> <Table ID =5>
	<Abstractive Summary> =
		Also, the experimen-
		5506
		Table 5: Examples predicted labels of LSTM and NPD
		Post
		LSTM
		NPD
		[E3]@$$Ê, Þ]ý‹�ý�Z, ô�è†, °°ÉQ94œ�Á
	</Abstractive Summary>
</Paper ID=ument551>


<Paper ID=ument552> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples generated by four different previous
		methods
	</Abstractive Summary>
	<Extractive Summary> =
		For instance, “Michael” in Table 1
		is a human name, which belongs to the speciﬁc in-
		formation
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 intuitively illustrates
		this problem
	</Extractive Summary>
</Paper ID=ument552>


<Paper ID=ument552> <Table ID =2>
	<Abstractive Summary> =
		The ﬁnal hidden representation of the i-th
		Module Source
		Target Role
		Input
		Output
		SCφ
		x+/x−
		+/− Separate content and sentiment
		S2Sθpe
		c+
		s+
		Generate positive sentiment words
		S2Sθne
		c−
		s−
		Generate negative sentiment words
		S2Sθpf (c+, s+)
		x+
		Generate positive sentence
		S2Sθnf (c−, s−)
		x−
		Generate negative sentence
		Table 2: The role of each component of our approach
		at the training stage
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the source
		input, target output, and function of each compo-
		nent at the training stage
	</Extractive Summary>
</Paper ID=ument552>


<Paper ID=ument552> <Table ID =3>
	<Abstractive Summary> =
		Finally, (c+, s−) is inputted into the model
		5512
		Dataset
		Training
		Validation
		Test
		Yelp
		580K
		15K
		4K
		Amazon
		230K
		10K
		2K
		Table 3: The statistics of two datasets
	</Abstractive Summary>
</Paper ID=ument552>


<Paper ID=ument552> <Table ID =4>
	<Abstractive Summary> =
		com/
		Yelp
		Amazon
		Module
		LSTM
		Hidden
		LSTM
		Hidden
		Layer
		Size
		Layer
		Size
		SCφ
		2
		512
		2
		256
		S2Sθpe
		(2, 3)
		(256, 512)
		(2, 2)
		(256, 512)
		S2Sθne
		(2, 2)
		(256, 512)
		(1, 2)
		(128, 256)
		S2Sθpf
		(2, 3)
		(256, 512)
		(2, 2)
		(256, 512)
		S2Sθnf
		(2, 2)
		(256, 512)
		(1, 2)
		(128, 256)
		Table 4: Main hyper-parameters
	</Abstractive Summary>
</Paper ID=ument552>


<Paper ID=ument552> <Table ID =5>
	<Abstractive Summary> =
		97
		Table 5: Automatic evaluations of our method and
		baselines, from which we can see that our approach
		achieves the best overall performance on both datasets
	</Abstractive Summary>
</Paper ID=ument552>


<Paper ID=ument552> <Table ID =6>
	<Abstractive Summary> =
		71
		Table 6:
		Human evaluations of different systems,
		showing that our approach outperforms the baselines
		by a large margin, especially in content preservation
	</Abstractive Summary>
</Paper ID=ument552>


<Paper ID=ument552> <Table ID =7>
	<Abstractive Summary> =
		10
		Table 7: Ablation study
	</Abstractive Summary>
	<Extractive Summary> =
		The
		results are shown in Table 7, illustrating that
		the removal of the emotionalization module leads
		to an obvious reduction in model performance
	</Extractive Summary>
</Paper ID=ument552>


<Paper ID=ument552> <Table ID =8>
	<Abstractive Summary> =
		Table 8: Outputs of different modules of our method
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Case Study
		Table 8 shows several examples of the testing pro-
		cess for our approach, which intuitively reﬂects
		Yelp
		ACC
		BLEU
		G-score
		Full model
		68
	</Extractive Summary>
</Paper ID=ument552>


<Paper ID=ument552> <Table ID =9>
	<Abstractive Summary> =
		Proposal: Annette in optical was rude!
		Table 9: Examples generated by different systems on
		the Yelp dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 presents several outputs of different sys-
		tems on the Yelp dataset
	</Extractive Summary>
</Paper ID=ument552>


<Paper ID=ument553> <Table ID =1>
	<Abstractive Summary> =
		Speciﬁcally, we deﬁne ﬁve actions repre-
		senting the replacement decision, whether a word
		Relation
		Syntactic Category
		Examples
		Synonymy
		N, V, Aj, Av
		(pipe, tube)
		(similar)
		(rise, ascend)
		(sad, unhappy)
		Hyponymy and
		N
		(sugar maple, maple)
		Hypernymy
		(maple, tree)
		(super-subordinate)
		(tree, plant)
		Note: N = Nouns, Aj = Adjectives, V = Verbs, Av = Adverbs
		Table 1: Basic relations in WordNet
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 1, WordNet has two basic relations:
		• Synonymy
	</Extractive Summary>
</Paper ID=ument553>


<Paper ID=ument553> <Table ID =2>
	<Abstractive Summary> =
		donkey
		Table 2: Actions deﬁned in this work
	</Abstractive Summary>
</Paper ID=ument553>


<Paper ID=ument553> <Table ID =3>
	<Abstractive Summary> =
		#w
		Train
		Dev
		Test
		SST2
		2
		19
		6,920
		872
		1,821
		SST5
		5
		18
		8,544
		1,101
		2,210
		RT
		2
		21
		8,608
		964
		1,089
		Yelp
		5
		89
		100,000
		10,000
		10,000
		Table 3: Dataset statistics
	</Abstractive Summary>
</Paper ID=ument553>


<Paper ID=ument553> <Table ID =4>
	<Abstractive Summary> =
		1
		Learning rate
		1e-4
		BERT Classiﬁer
		Transformer layer
		24
		Embedding size
		1,024
		Hidden size
		1,024
		Head
		16
		Learning rate
		2e-5
		Fine-turning epoch
		3
		Table 4: Settings of model-speciﬁc hyper-parameters
	</Abstractive Summary>
</Paper ID=ument553>


<Paper ID=ument553> <Table ID =5>
	<Abstractive Summary> =
		50
		Table 5: Comparisons between LexicalAT and base-
		lines on four datasets
	</Abstractive Summary>
</Paper ID=ument553>


<Paper ID=ument553> <Table ID =6>
	<Abstractive Summary> =
		17
		Table 6: Comparisons of different classiﬁers on defending attacks
	</Abstractive Summary>
</Paper ID=ument553>


<Paper ID=ument553> <Table ID =7>
	<Abstractive Summary> =
		49
		Table 7: Effects of different replacement actions
	</Abstractive Summary>
	<Extractive Summary> =
		As
		Table 7 shows, only some actions are useful for
		the robustness improvement
	</Extractive Summary>
</Paper ID=ument553>


<Paper ID=ument553> <Table ID =8>
	<Abstractive Summary> =
		27
		Table 8: Results of different classiﬁers trained with
		the examples generated by different attacking poli-
		cies on SST-2 dataset
	</Abstractive Summary>
</Paper ID=ument553>


<Paper ID=ument553> <Table ID =9>
	<Abstractive Summary> =
		Table 9: Adversarial examples generated by our ap-
		proach
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Case Study
		Table 9 presents the adversarial examples gener-
		ated by the generator on SST-5
	</Extractive Summary>
</Paper ID=ument553>


<Paper ID=ument554> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Summary of the evaluation datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes our datasets
	</Extractive Summary>
</Paper ID=ument554>


<Paper ID=ument554> <Table ID =2>
	<Abstractive Summary> =
		We split data into ﬁve folds
		5533
		Table 2: Lists of sentiment words
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows Trait’s sentiment
		word list as prior knowledge to set asymmetric pri-
		ors
	</Extractive Summary>
</Paper ID=ument554>


<Paper ID=ument554> <Table ID =3>
	<Abstractive Summary> =
		We conduct ﬁve-
		Table 3: Topic coherence: Hotel reviews
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows average NMPI and W2V scores
		for different numbers of aspects
	</Extractive Summary>
</Paper ID=ument554>


<Paper ID=ument554> <Table ID =4>
	<Abstractive Summary> =
		31‡
		Table 4: Topic coherence: Restaurant reviews
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		shows similar conclusions for restaurant reviews
	</Extractive Summary>
</Paper ID=ument554>


<Paper ID=ument554> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Accuracy and AUC of sentiment classiﬁcation
		on hotel reviews
	</Abstractive Summary>
	<Extractive Summary> =
		86†
		Table 5 reports the results of sentiment classi-
		ﬁcation on hotel reviews
	</Extractive Summary>
</Paper ID=ument554>


<Paper ID=ument554> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Accuracy and AUC of sentiment classiﬁcation
		on restaurant reviews
	</Abstractive Summary>
</Paper ID=ument554>


<Paper ID=ument554> <Table ID =7>
	<Abstractive Summary> =
		For Miami, Transportation is attractive,
		Table 7: Top ﬁve aspects discovered by Trait
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 (bottom rows) lists the top ﬁve aspects
		for four authors from HotelUser
	</Extractive Summary>
</Paper ID=ument554>


<Paper ID=ument555> <Table ID =1>
	<Abstractive Summary> =
		37
		X
		Table 1: Scores on sentiment label
		tively the token, sentence and text weights provides
		the best text-level results
	</Abstractive Summary>
</Paper ID=ument555>


<Paper ID=ument555> <Table ID =2>
	<Abstractive Summary> =
		14
		Table 2: Joint and independent prediction of entities
		and polarities
		detail the case of Entities in the Table 3 and present
		the results obtained for the most common entity
		categories (among 11)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2
		contains the results obtained with the architecture
		described in Figure 2 on the task of joint polarity
		1described in Section 4
		5546
		Figure 2: Best architecture selected during the Experiment 1
		and entity prediction as well as the results obtained
		when dealing with these tasks independently
	</Extractive Summary>
</Paper ID=ument555>


<Paper ID=ument555> <Table ID =3>
	<Abstractive Summary> =
		58
		154
		Table 3: F1 score per label for the top entity categories
		annotated at the sentence level (mean score averaged
		over 7 runs), value counts are provided on the test set
	</Abstractive Summary>
	<Extractive Summary> =
		This task is harder than the previ-
		ously studied polarity prediction task due to (1)
		the problem of label imbalance appearing in the
		label distribution reported in the Table 3 and (2) the
		diversity of the vocabulary incurred when dealing
		with many entities
	</Extractive Summary>
	<Extractive Summary> =
		97
		F1 entities
		Entities
		X
		Table 3
		Table 3
		MAE score
		review level
		0
	</Extractive Summary>
	<Extractive Summary> =
		14
		Table 2: Joint and independent prediction of entities
		and polarities
		detail the case of Entities in the Table 3 and present
		the results obtained for the most common entity
		categories (among 11)
	</Extractive Summary>
</Paper ID=ument555>


<Paper ID=ument556> <Table ID =1>
	<Abstractive Summary> =
		After obtaining the DA word embed-
		dings for words in Vcommon and calculating the
		shift ψ, we take the top 200 words that shifted
		5553
		action
		cost
		dream
		help
		media
		reform
		women
		amendment
		court
		education
		honor
		need
		republican
		work
		attack
		crisis
		fact
		hope
		order
		right
		world
		budget
		deal
		force
		income
		pledge
		risk
		burden
		debate
		freedom
		information
		police
		rule
		business
		debt
		fund
		insurance
		poll
		school
		candidate
		decision
		funding
		justice
		power
		spending
		care
		defense
		future
		labor
		president
		state
		class
		deﬁcit
		generation
		leader
		problem
		truth
		college
		democrat
		government
		leadership
		program
		value
		congress
		development
		governor
		legislature
		protection
		violence
		control
		divide
		health
		legislation
		race
		wealth
		Table 1: This table presents 74 words representing key
		political concepts common to Liberal and Conservative
		users on Twitter
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents words from the gold standard
		list with words in bold face indicating words that
		shift the most across Liberal and Conservative do-
		mains
	</Extractive Summary>
</Paper ID=ument556>


<Paper ID=ument556> <Table ID =2>
	<Abstractive Summary> =
		1
		-
		Table 2: This table reports performance (accuracy score) of the baseline algorithms on the LibCon and Beauty,
		Book and Music data sets in balanced and imbalanced setting
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 2 presents results on the LibCon and the bal-
		anced (B) and imbalanced (I) Beauty, Book and
		Music data sets and Table 3 presents results on the
		SST and MR data sets
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2, it is observed that on the Lib-
		Con data set, where we have a considerable differ-
		ence in language use between the two groups of
		users, the adapted BiLSTM and adapted CNN per-
		form much better than the vanilla baselines
	</Extractive Summary>
</Paper ID=ument556>


<Paper ID=ument556> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: This table reports performance (Accuracy) on
		the MR and SST data sets
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 2 presents results on the LibCon and the bal-
		anced (B) and imbalanced (I) Beauty, Book and
		Music data sets and Table 3 presents results on the
		SST and MR data sets
	</Extractive Summary>
</Paper ID=ument556>


<Paper ID=ument556> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: This table presents the accuracy obtained by
		Vanilla and adapted baselines on smaller subsamples of
		the training data for the MR and SST data sets
	</Abstractive Summary>
	<Extractive Summary> =
		Results Table 4 strengthens
		our hypothesis that the adaptation layer is partic-
		ularly well suited for small training and test data
		regimes
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 compares accuracy on the
		MR and SST data sets, of Vanilla and adapted en-
		coders trained with 1000 and 2500 points
	</Extractive Summary>
</Paper ID=ument556>


<Paper ID=ument557> <Table ID =1>
	<Abstractive Summary> =
		Comparing aspect terms
		5564
		Dataset
		#Pos #Neg #Neu Total
		Restaurants (R) Train 2164
		805
		633
		3502
		Test
		728
		196
		196
		1120
		Laptops (L)
		Train 987
		866
		460
		2313
		Test
		341
		128
		169
		638
		Twitters (T)
		Train 1561 1560 3127 6248
		Test
		173
		173
		346
		692
		Table 1:
		Datasets statistics
	</Abstractive Summary>
</Paper ID=ument557>


<Paper ID=ument557> <Table ID =2>
	<Abstractive Summary> =
		56
		Table 2: Evaluation results of baselines in terms of accuracy (%) and macro-f1 (%)
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Experimental Analysis
		We report the classiﬁcation accuracy and macro-f1
		of various methods in Table 2 and Table 3, and the
		best scores on each metric are marked in bold
	</Extractive Summary>
</Paper ID=ument557>


<Paper ID=ument557> <Table ID =3>
	<Abstractive Summary> =
		56
		Table 3: Evaluation results of variants of our model in terms of accuracy(%) and macro-f1(%)
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Experimental Analysis
		We report the classiﬁcation accuracy and macro-f1
		of various methods in Table 2 and Table 3, and the
		best scores on each metric are marked in bold
	</Extractive Summary>
</Paper ID=ument557>


<Paper ID=ument558> <Table ID =1>
	<Abstractive Summary> =
		Aspect-Category
		food
		service
		Aspect-Term
		The appetizers
		service
		Sentiment Polarity
		Neutral
		Negative
		Table 1: The instance contains different sentiment po-
		larities towards two aspects
	</Abstractive Summary>
	<Extractive Summary> =
		For instance, in Table 1, the aspect-category
		sentiment analysis is going to predict the senti-
		ment polarity towards the aspect “food”, which
		∗ Work was done when Yunlong Liang was an intern at
		Pattern Recognition Center, WeChat AI, Tencent Inc, China
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, sentiment polarity may
		be different when different aspects are consid-
		ered
	</Extractive Summary>
	<Extractive Summary> =
		In each “DS”, there are some sentences
		like the example in Table 1, containing different
		sentiment labels, each of which associates with
		an aspect (term)
	</Extractive Summary>
	<Extractive Summary> =
		For instance, Table 1 shows the
		customer’s different attitude towards two aspects:
		“food” (“The appetizers”) and “service”
	</Extractive Summary>
	<Extractive Summary> =
		, IARM,
		TNet, VAE, PBAN, AOA and MGAN, in Table 10
	</Extractive Summary>
	<Extractive Summary> =
		86
		Table 10: The three-class accuracy of the aspect-term
		sentiment analysis task on SemEval 2014
	</Extractive Summary>
	<Extractive Summary> =
		”
		shown in Table 1, it has different sentiment labels
		towards different aspects
	</Extractive Summary>
</Paper ID=ument558>


<Paper ID=ument558> <Table ID =2>
	<Abstractive Summary> =
		5573
		Positive
		Negative
		Neutral
		Conﬂict
		Total
		DS
		HDS
		DS
		HDS
		DS
		HDS
		DS
		HDS
		DS
		HDS
		Restaurant-14
		Train
		2,179 139
		839
		136
		500 50
		195 40
		3,713 365
		Test
		657
		32
		222
		26
		94
		12
		52
		19
		1,025 89
		Restaurant-Large
		Train
		2,710 182
		1,198 178
		757 107
		-
		-
		4,665 467
		Test
		1,505 92
		680
		81
		241 61
		-
		-
		2,426 234
		Table 2: Statistics of datasets for the aspect-category sentiment analysis task
	</Abstractive Summary>
	<Extractive Summary> =
		All instances are shown in Table 2 and Table 3
	</Extractive Summary>
</Paper ID=ument558>


<Paper ID=ument558> <Table ID =3>
	<Abstractive Summary> =
		Positive
		Negative
		Neutral
		Conﬂict
		Total
		NC
		DS
		HDS
		DS
		HDS
		DS
		HDS
		DS HDS
		DS
		HDS
		DS
		Restaurant
		Train
		2,164 379
		805 323
		633 293
		91
		43
		3,693 1,038
		3,602
		Test
		728
		92
		196 62
		196 83
		14
		8
		1,134 245
		1,120
		Laptop
		Train
		987
		159
		866 147
		460 173
		45
		17
		2,358 496
		2,313
		Test
		341
		31
		128 25
		169 49
		16
		3
		654
		108
		638
		Table 3: Statistics of datasets for the aspect-term sentiment analysis task
	</Abstractive Summary>
</Paper ID=ument558>


<Paper ID=ument558> <Table ID =4>
	<Abstractive Summary> =
		50
		Table 4: The accuracy of the aspect-category sentiment analysis task
	</Abstractive Summary>
</Paper ID=ument558>


<Paper ID=ument558> <Table ID =5>
	<Abstractive Summary> =
		26
		Table 5: The accuracy of the aspect-term sentiment analysis task
	</Abstractive Summary>
	<Extractive Summary> =
		Aspect-Term Sentiment Analysis Task
		As shown in Table 5, our AGDT consistently
		outperforms all compared methods on both do-
		mains
	</Extractive Summary>
</Paper ID=ument558>


<Paper ID=ument558> <Table ID =6>
	<Abstractive Summary> =
		73 6⃝
		Table 6: Ablation study of the AGDT on the aspect-
		category sentiment analysis task
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 6 and Table 7, we can conclude:
		 48
		 52
		 56
		 60
		 64
		 68
		 72
		 76
		 80
		 84
		 0
	</Extractive Summary>
</Paper ID=ument558>


<Paper ID=ument558> <Table ID =7>
	<Abstractive Summary> =
		30 6⃝
		Table 7: Ablation study of the AGDT on the aspect-
		term sentiment analysis task
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 6 and Table 7, we can conclude:
		 48
		 52
		 56
		 60
		 64
		 68
		 72
		 76
		 80
		 84
		 0
	</Extractive Summary>
</Paper ID=ument558>


<Paper ID=ument558> <Table ID =8>
	<Abstractive Summary> =
		63
		Table 8: The accuracy of model depth on the four
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows the change of accuracy on the test
		5576
		Depth
		1
		2
		3
		4
		5
		6
		D1
		DS
		81
	</Extractive Summary>
</Paper ID=ument558>


<Paper ID=ument558> <Table ID =9>
	<Abstractive Summary> =
		92
		Table 9: The accuracy of aspect reconstruction on the
		full test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 shows all results on four test
		datasets, which shows the effectiveness of aspect-
		reconstruction approach again
	</Extractive Summary>
</Paper ID=ument558>


<Paper ID=ument558> <Table ID =10>
	<Abstractive Summary> =
		86
		Table 10: The three-class accuracy of the aspect-term
		sentiment analysis task on SemEval 2014
	</Abstractive Summary>
</Paper ID=ument558>


<Paper ID=ument559> <Table ID =1>
	<Abstractive Summary> =
		5
		4
		Table 1: Statistics of three datasets
	</Abstractive Summary>
</Paper ID=ument559>


<Paper ID=ument559> <Table ID =2>
	<Abstractive Summary> =
		503
		Table 2: Comparison of our approaches and other baseline approaches to DASC
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results
		Table 2 shows the performance comparison of
		different approaches
	</Extractive Summary>
</Paper ID=ument559>


<Paper ID=ument559> <Table ID =3>
	<Abstractive Summary> =
		651
		Table 3: Ablation study of HRL on three different datasets
	</Abstractive Summary>
</Paper ID=ument559>


<Paper ID=ument56> <Table ID =1>
	<Abstractive Summary> =
		Plas
		FR [Dep]
		40,075
		73,094
		2,036
		Table 1: Train and Test Data for Monolingual Models
	</Abstractive Summary>
	<Extractive Summary> =
		39
		Table 6: We retrain the monolingual systems DE, FR
		using the original training sets (BL: Original) shown
		in Table 1 and inject our generated data in different
		sizes
	</Extractive Summary>
</Paper ID=ument56>


<Paper ID=ument56> <Table ID =2>
	<Abstractive Summary> =
		Cross-lingual Model
		# Sentences
		EN - DE-SRL (Akbik, 2015)
		63,397
		EN - FR-SRL (Akbik, 2015)
		40,827
		EN - FR (UN)
		100,000
		EN - DE (Europarl)
		100,000
		Table 2: Data used for Cross-lingual Models: From the
		SRL parallel data available we take 90% for training
		and use the rest as a Dev set for our experiments
	</Abstractive Summary>
	<Extractive Summary> =
		To
		do so, we train a single model using the concate-
		nation of the parallel datasets listed in Table 2 and
		described in Section 3
	</Extractive Summary>
	<Extractive Summary> =
		The upper part of Table 5 compares the scores
		of two versions of the Enc-Dec model trained on
		the cross-lingual data from Table 2 systems, one
		using GloVe embeddings and the second using
		BERT, respectively
	</Extractive Summary>
</Paper ID=ument56>


<Paper ID=ument56> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: CoNLL-09 and CoNLL-05 Test Sets for En-
		glish
	</Abstractive Summary>
</Paper ID=ument56>


<Paper ID=ument56> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4: F1 scores for role labeling on dependency-
		based SRL data
	</Abstractive Summary>
	<Extractive Summary> =
		The performance of DE and
		FR is shown in Table 4 where we compare all
		monolingual systems for the three languages (top
		half), against the one-to-one multilingual versions
		(bottom half)
	</Extractive Summary>
</Paper ID=ument56>


<Paper ID=ument56> <Table ID =5>
	<Abstractive Summary> =
		32
		Table 5:
		Cross-lingual (XL) system results using
		BLEU score on individual languages inside the Dev set
	</Abstractive Summary>
	<Extractive Summary> =
		The upper part of Table 5 compares the scores
		of two versions of the Enc-Dec model trained on
		the cross-lingual data from Table 2 systems, one
		using GloVe embeddings and the second using
		BERT, respectively
	</Extractive Summary>
	<Extractive Summary> =
		The bottom part of Table 5 shows the scores
		when restricting the evaluation to sentences with
		score ≥ 10
	</Extractive Summary>
</Paper ID=ument56>


<Paper ID=ument56> <Table ID =6>
	<Abstractive Summary> =
		39
		Table 6: We retrain the monolingual systems DE, FR
		using the original training sets (BL: Original) shown
		in Table 1 and inject our generated data in different
		sizes
	</Abstractive Summary>
	<Extractive Summary> =
		We see in Table 6 that
		adding our German data shows improvement in
		F1 score, despite the fact that the CoNLL-09 la-
		13The label distribution is given in the Supplement, A
	</Extractive Summary>
</Paper ID=ument56>


<Paper ID=ument560> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of GP-claims and matching rebuttals, created through the process described in §3
	</Abstractive Summary>
	<Extractive Summary> =
		Three examples from the GPR-
		KB are given in Table 1; henceforth we refer to
		the generated claims as GP-claims, or simply as
		claims when the context is clear
	</Extractive Summary>
</Paper ID=ument560>


<Paper ID=ument560> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: A comparison of GP-claims and topic-speciﬁc
		iDebate claims annotation
	</Abstractive Summary>
	<Extractive Summary> =
		Comparison to iDebate18
		Table 2 compares
		the results of this annotation to that of iDebate18,
		which contains topic-speciﬁc claims annotated for
		the same set of speeches7
	</Extractive Summary>
</Paper ID=ument560>


<Paper ID=ument560> <Table ID =3>
	<Abstractive Summary> =
		The resulting dataset is freely avail-
		Annotation
		# pairs
		# positives
		Motion – GP-claim
		2,750
		1,265
		Speech – GP-claim
		3,246
		1,491
		Sentence – GP-claim
		4,271
		854
		Sentence –
		iDebate claim
		2,164
		368
		Table 3: Summary of annotation experiment results
	</Abstractive Summary>
</Paper ID=ument560>


<Paper ID=ument561> <Table ID =1>
	<Abstractive Summary> =
		11
		Table 1: Statistics of the datasets used for the Sentiment Classiﬁcation task
	</Abstractive Summary>
</Paper ID=ument561>


<Paper ID=ument561> <Table ID =2>
	<Abstractive Summary> =
		622
		Table 2: Sentiment classiﬁcation results of competing models based on accuracy and RMSE metrics on the three
		datasets
	</Abstractive Summary>
</Paper ID=ument561>


<Paper ID=ument561> <Table ID =3>
	<Abstractive Summary> =
		69
		Table 3: Accuracy (higher is better) and perplexity
		(lower is better) of competing models on the Amazon
		dataset for the transfer tasks on product category clas-
		siﬁcation and review headline generation, respectively
	</Abstractive Summary>
</Paper ID=ument561>


<Paper ID=ument562> <Table ID =1>
	<Abstractive Summary> =
		)
		Documents
		2156
		Cause 1
		1949
		Clauses
		16259
		Cause 2
		164
		Causes
		2421
		Cause 3
		32
		Table 1: Details of the two datasets
	</Abstractive Summary>
</Paper ID=ument562>


<Paper ID=ument562> <Table ID =2>
	<Abstractive Summary> =
		7914
		Table 2: Experimental results on the Chinese dataset
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Main Results
		The experimental results on both datasets are
		shown in Table 2 and Table 3, respectively
	</Extractive Summary>
</Paper ID=ument562>


<Paper ID=ument562> <Table ID =3>
	<Abstractive Summary> =
		5975
		Table 3: Experimental results on the English dataset,
		we follow the results that are implemented in (Li
		et al
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Main Results
		The experimental results on both datasets are
		shown in Table 2 and Table 3, respectively
	</Extractive Summary>
</Paper ID=ument562>


<Paper ID=ument562> <Table ID =4>
	<Abstractive Summary> =
		5975
		Table 4:
		Effect of different components, i
	</Abstractive Summary>
	<Extractive Summary> =
		As illustrated in Table 4, all
		models with the proposed component consistently
		improve upon the Base model, verifying the ef-
		fectiveness of the proposed approach
	</Extractive Summary>
</Paper ID=ument562>


<Paper ID=ument562> <Table ID =5>
	<Abstractive Summary> =
		5975
		Table 5: The F-measure on sub-dataset (Sub
	</Abstractive Summary>
	<Extractive Summary> =
		The experiment results in Table 5 show that: 1)
		RHNN and HSR model achieve the best perfor-
		mance on the subset of two datasets respectively,
		similar observations can be found regarding on
		whole dataset; 2) With sentiment regularizer, the
		performance is boosted on both subsets compared
		to that on the whole dataset
	</Extractive Summary>
</Paper ID=ument562>


<Paper ID=ument562> <Table ID =6>
	<Abstractive Summary> =
		Null
		Table 6: The error instances of RHNN model for emo-
		tion cause analysis
	</Abstractive Summary>
	<Extractive Summary> =
		The results are listed in Table 6,
		where the ﬁrst column depicts the content of the
		emotion cause events and the second column de-
		picts the emotion causes identiﬁed by RHNN
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 6, the emotion causes appear in
		bold and emotion word is labeled between *
	</Extractive Summary>
	<Extractive Summary> =
		From Table 6, we can ﬁnd that there are two
		clauses contain the emotion cause in event 1
	</Extractive Summary>
</Paper ID=ument562>


<Paper ID=ument563> <Table ID =1>
	<Abstractive Summary> =
		Motion
		#Args
		Flu vaccination should be mandatory
		204
		Flu vaccination should not be mandatory
		174
		Gambling should be banned
		342
		Gambling should not be banned
		382
		Online shopping brings more harm than good
		198
		Online shopping brings more good than harm
		215
		Social media brings more harm than good
		879
		Social media brings more good than harm
		686
		We should adopt cryptocurrency
		172
		We should abandon cryptocurrency
		160
		We should adopt vegetarianism
		221
		We should abandon vegetarianism
		179
		We should ban the sale of vvg to minors
		275
		We should allow the sale of vvg to minors
		240
		We should ban fossil fuels
		146
		We should not ban fossil fuels
		116
		We should legalize doping in sport
		212
		We should ban doping in sport
		215
		We should limit autonomous cars
		313
		We should promote autonomous cars
		480
		We should support information privacy laws
		355
		We should discourage information privacy laws
		93
		Table 1: Motion list and statistics on data collection
	</Abstractive Summary>
	<Extractive Summary> =
		6 The full list of motions appears
		in Table 1 with the number of arguments collected
		for each
	</Extractive Summary>
	<Extractive Summary> =
		7In Table 1, vvg stands for violent video games
	</Extractive Summary>
</Paper ID=ument563>


<Paper ID=ument563> <Table ID =2>
	<Abstractive Summary> =
		Argument 1
		Argument 2
		Children emulate the
		media
		they
		consume
		and so will be more
		violent
		if
		you
		don’t
		ban them from violent
		video games
		These are less fun and
		more
		harmful
		games
		but speciﬁcally violent
		games are played in
		groups
		and
		exclude
		softer souls
		Table 2: An example of an argument pair for the mo-
		tion We should ban the sale of violent video games to
		minors
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Labeling Argument Pairs
		In this task, annotators were presented with a pair
		of arguments, having the same stance towards
		the concept (to reduce bias due to the annotator’s
		opinion), and were asked the following:
		Which of the two arguments would have been
		preferred by most people to support/contest the
		topic?
		Table 2 presents an example of such an ar-
		gument pair in which the annotators unanimously
		preferred the ﬁrst argument
	</Extractive Summary>
</Paper ID=ument563>


<Paper ID=ument563> <Table ID =3>
	<Abstractive Summary> =
		89
		Table 3: Accuracy and AUC on IBMPairs and UKPStrict
	</Abstractive Summary>
</Paper ID=ument563>


<Paper ID=ument563> <Table ID =4>
	<Abstractive Summary> =
		65
		Table 4: Pearson’s (r) and Spearman’s (ρ) correla-
		tion of Arg-Ranker-base, Arg-Ranker and GPPL on the
		IBMRank and UKPRank datasets
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 4, on the UKPRank
		dataset, Arg-Ranker is slightly better than GPPL
		for Pearson’s correlation, but slightly worse for
		Spearman’s correlation
	</Extractive Summary>
</Paper ID=ument563>


<Paper ID=ument563> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Examples of argument pairs for which there is a high difference between the argument selected by the
		annotators, marked in bold, and the argument predicted to be of higher quality by the model, marked in italics
	</Abstractive Summary>
</Paper ID=ument563>


<Paper ID=ument564> <Table ID =1>
	<Abstractive Summary> =
		”
		Table 1: Instances of the different propaganda techniques from our corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 for examples):
		1
	</Extractive Summary>
</Paper ID=ument564>


<Paper ID=ument564> <Table ID =2>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument564>


<Paper ID=ument564> <Table ID =3>
	<Abstractive Summary> =
		com
		1
		Personal Liberty
		18
		Table 3: Number of articles retrieved from news outlets
		deemed propagandistic by Media Bias/Fact Check
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the number of articles re-
		trieved from each propagandistic outlet
	</Extractive Summary>
</Paper ID=ument564>


<Paper ID=ument564> <Table ID =4>
	<Abstractive Summary> =
		39
		Table 4:
		γ inter-annotator agreement between an-
		notators spotting spans alone (spans) and spotting
		spans+labeling (+labels)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the γ agreements on the full cor-
		pus
	</Extractive Summary>
	<Extractive Summary> =
		Observ-
		ing such percentages together with the relatively
		low differences in Table 4 between γs and γsl for
		the same pairs (ai, aj) and (ai, cj), we can con-
		clude that disagreements are in general not due to
		the two annotators assigning different labels to the
		same or mostly overlapping spans, but rather be-
		cause one has missed an instance in the ﬁrst stage
	</Extractive Summary>
</Paper ID=ument564>


<Paper ID=ument564> <Table ID =5>
	<Abstractive Summary> =
		45
		Table 5: Corpus statistics including instances per tech-
		nique and their avg
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 reports some statistics
		about the annotations
	</Extractive Summary>
	<Extractive Summary> =
		Table 5), and instances span
		from single tokens to full sentences or even longer
		pieces of text
	</Extractive Summary>
</Paper ID=ument564>


<Paper ID=ument564> <Table ID =6>
	<Abstractive Summary> =
		58
		Table 6:
		Fragment-level experiments (FLC task)
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Fragment-Level Propaganda Detection
		Table 6 shows the performance for the three base-
		lines and for our multi-granularity network on the
		FLC task
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows that joint learning (BERT-Joint)
		hurts the performance compared to single-task
		BERT
	</Extractive Summary>
</Paper ID=ument564>


<Paper ID=ument564> <Table ID =7>
	<Abstractive Summary> =
		98
		Table 7: Sentence-level (SLC) results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Sentence-Level Propaganda Detection
		Table 7 shows the results for the SLC task
	</Extractive Summary>
</Paper ID=ument564>


<Paper ID=ument565> <Table ID =1>
	<Abstractive Summary> =
		41
		Table 1: Results of sentiment and emotion analysis for the proposed approach
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
</Paper ID=ument565>


<Paper ID=ument565> <Table ID =2>
	<Abstractive Summary> =
		Multi-label
		No
		One
		Two
		Three
		Four
		Five
		Six
		Count
		3372
		11050
		5526
		2084
		553
		84
		8
		Table 2: Statistics of multi-label emotions in CMU-MOSEI: Emotions-per-utterance
	</Abstractive Summary>
</Paper ID=ument565>


<Paper ID=ument565> <Table ID =3>
	<Abstractive Summary> =
		30
		Table 3: Ablation results for IIM module
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we depict the evaluation
		results for both- with and without IIM
	</Extractive Summary>
</Paper ID=ument565>


<Paper ID=ument565> <Table ID =4>
	<Abstractive Summary> =
		18 (W-Acc) for multi label in EC
		Batch
		16
		Epochs
		50
		Table 4: Hyper-parameters for our experiments where
		N, D, SC, SI and EC stands for #neurons, dropout,
		sentiment classiﬁcation, sentiment intensity and emo-
		tion classiﬁcation respectively
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4)
	</Extractive Summary>
</Paper ID=ument565>


<Paper ID=ument565> <Table ID =5>
	<Abstractive Summary> =
		040
		(b) Sentiment Analysis
		Table 5: Comparative results
	</Abstractive Summary>
	<Extractive Summary> =
		We show the comparative results in Table 5a
		and Table 5b for emotion and sentiment analy-
		sis, respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 5b), for all the
		ﬁve datasets and different experimental setups, the
		proposed CIA framework obtains the improved
		accuracies for the classiﬁcation tasks
	</Extractive Summary>
</Paper ID=ument565>


<Paper ID=ument565> <Table ID =6>
	<Abstractive Summary> =
		In Figure 3a, each cell(i,j) of
		5655
		Sentiment
		Emotion
		Utterances
		Actual
		CIA−IIM
		CIA
		Actual
		CIA−IIM
		CIA
		1
		these critics argue that the welfare state breeds dependence
		and incompetence among those who receive it they
		Pos
		Pos
		Pos
		Happy, Sad
		Happy
		Happy, Sad
		2
		argue that it creates social pathologies such as single parent
		families excess fertility and laziness
		Neg
		Pos
		Neg
		Sad
		Happy
		Sad
		3
		some argue that people who receive welfare beneﬁts cannot
		spend their beneﬁts rationally and
		Pos
		Neg
		Pos
		Fear, Sad
		Sad
		Fear, Sad
		4
		and then lastly some people on the moral side argue that noth-
		ing should be given to a person without requiring a reciprocal
		obligation from that person so
		Pos
		Pos
		Pos
		Sad
		Happy, Sad
		Sad
		Table 6: Comparison between proposed CIA and CIA−IIM frameworks in MOSEI dataset
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 6, we
		list the utterances of a CMU-MOSEI video along
		with their correct and predicted labels for both the
		proposed and baseline systems
	</Extractive Summary>
	<Extractive Summary> =
		The video in Table 6 has 4 utterances, out of
		which the correct sentiments of three utterances
		(i
	</Extractive Summary>
</Paper ID=ument565>


<Paper ID=ument566> <Table ID =1>
	<Abstractive Summary> =
		262
		Table 1: Accuracy scores in percentage of all models on every dataset
		Figure 6: Attention weights for some of the sentences from the SST2 dataset
		Model
		No
	</Abstractive Summary>
</Paper ID=ument566>


<Paper ID=ument566> <Table ID =2>
	<Abstractive Summary> =
		of parameters
		Small SCARN
		68,425
		Large SCARN
		166,639
		RCNN
		180,601
		C-LSTM
		676,651
		Table 2: Number of parameters for each model
		6
		Results and Discussion
		Results of the experiments are tabulated in Table
		1
	</Abstractive Summary>
</Paper ID=ument566>


<Paper ID=ument566> <Table ID =3>
	<Abstractive Summary> =
		In Figure 7, we show the
		5665
		Dataset
		Dataset size
		Train
		Dev
		Test
		Max Vocab size
		Classes
		IMDB
		50000
		20000
		5000
		25000
		30000
		2
		TREC
		5952
		4906
		546
		500
		5000
		6
		SO
		10000
		8100
		900
		1000
		30000
		2
		RT
		10662
		8100
		900
		1662
		30000
		2
		Pol
		2000
		1280
		320
		400
		30000
		2
		AR
		121565
		80000
		20000
		21565
		30000
		5
		SST-2
		9613
		6920
		872
		1821
		10000
		2
		Table 3: Summary Statistics of all datasets
		(a) Mean
		(b) Standard Deviation
		Figure 7: Statistics of each feature in concatenation
		layer outputs on the IMDB dataset’s training set
	</Abstractive Summary>
	<Extractive Summary> =
		The statistics for the datasets are shown in
		Table 3
		5
	</Extractive Summary>
</Paper ID=ument566>


<Paper ID=ument567> <Table ID =1>
	<Abstractive Summary> =
		The distribution
		of claims with the given range of number of im-
		5671
		# impact votes
		# claims
		[3, 5)
		4,495
		[5, 10)
		5,405
		[10, 15)
		5,338
		[15, 20)
		2,093
		[20, 25)
		934
		[25, 50)
		992
		[50, 333)
		255
		Table 1: Number of claims for the given range of num-
		ber of votes
	</Abstractive Summary>
</Paper ID=ument567>


<Paper ID=ument567> <Table ID =2>
	<Abstractive Summary> =
		Table 4 shows
		number of claims with the given range of con-
		5672
		3-class case
		5-class case
		Agreement score
		Number of claims
		Number of claims
		> 50%
		10,848
		7,304
		> 60%
		7,386
		4,329
		> 70%
		4,412
		2,195
		> 80%
		2,068
		840
		Table 2: Number of claims, with at least 5 votes, above the given threshold of agreement percentage for 3-class
		and 5-class cases
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the number of claims with the
		given agreement score thresholds when we include
		the claims with at least 5 votes
	</Extractive Summary>
</Paper ID=ument567>


<Paper ID=ument567> <Table ID =3>
	<Abstractive Summary> =
		Impact label
		# votes- all claims
		No impact
		32,681
		Low impact
		37,457
		Medium impact
		60,136
		High impact
		52,764
		Very high impact
		58,846
		Total # votes
		241,884
		Table 3: Number of votes for the given impact label
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the dis-
		tribution of the number of votes for each of the
		impact categories
	</Extractive Summary>
</Paper ID=ument567>


<Paper ID=ument567> <Table ID =4>
	<Abstractive Summary> =
		Context length
		# claims
		1
		1,524
		2
		1,977
		3
		1,181
		[4, 5]
		1,436
		(5, 10]
		1,115
		> 10
		153
		Table 4: Number of claims for the given range of con-
		text length, for claims with more than 5 votes and an
		agreement score greater than 60%
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows
		number of claims with the given range of con-
		5672
		3-class case
		5-class case
		Agreement score
		Number of claims
		Number of claims
		> 50%
		10,848
		7,304
		> 60%
		7,386
		4,329
		> 70%
		4,412
		2,195
		> 80%
		2,068
		840
		Table 2: Number of claims, with at least 5 votes, above the given threshold of agreement percentage for 3-class
		and 5-class cases
	</Extractive Summary>
</Paper ID=ument567>


<Paper ID=ument567> <Table ID =5>
	<Abstractive Summary> =
		33
		Table 5: Results for the baselines and the BERT models with and without the context
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results and Analysis
		Table 5 shows the macro precision, recall and F1
		scores for the baselines as well as the BERT mod-
		els with and without context representations9
	</Extractive Summary>
</Paper ID=ument567>


<Paper ID=ument567> <Table ID =6>
	<Abstractive Summary> =
		22
		Table 6: F1 scores of each model for the claims with various context length values
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the F1
		score of the BERT model without context and with
		ﬂat context representation with different lengths
		of context
	</Extractive Summary>
</Paper ID=ument567>


<Paper ID=ument568> <Table ID =1>
	<Abstractive Summary> =
		Positive
		Neutral
		Negative
		Dataset
		train
		test
		train
		test
		train
		test
		Rest14
		2164
		727
		637
		196
		807
		196
		Laptop14
		976
		337
		455
		167
		851
		128
		Rest16
		1657
		611
		101
		44
		748
		204
		Twitter
		1507
		172
		3016
		336
		1528
		169
		Table 1: Distribution of samples by class labels on
		benchmark datasets
		5
	</Abstractive Summary>
</Paper ID=ument568>


<Paper ID=ument568> <Table ID =2>
	<Abstractive Summary> =
		93
		Table 2: Performance comparison on different models on the benchmark datasets
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2 and Figure 4, it is clear that GCN
		complements the BiLSTM to improve model per-
		formance
	</Extractive Summary>
</Paper ID=ument568>


<Paper ID=ument569> <Table ID =1>
	<Abstractive Summary> =
		39 (3)
		Table 1: Main BLEU results (CTC=0
	</Abstractive Summary>
	<Extractive Summary> =
		CTC Table 1 shows the main BLEU results of dif-
		ferent methods on the test set
	</Extractive Summary>
	<Extractive Summary> =
		Similar
		to Table 1, the DA methods are shown in their �
		value respect to the baseline
	</Extractive Summary>
</Paper ID=ument569>


<Paper ID=ument569> <Table ID =2>
	<Abstractive Summary> =
		232 (6)
		Table 2: Sensitivity measure (CTC=0
	</Abstractive Summary>
</Paper ID=ument569>


<Paper ID=ument569> <Table ID =3>
	<Abstractive Summary> =
		016 (3)
		Table 3: Margin measure (CTC=0
	</Abstractive Summary>
</Paper ID=ument569>


<Paper ID=ument57> <Table ID =1>
	<Abstractive Summary> =
		23
		Table 1: Syntactic dependency performance for differ-
		ent parsers
	</Abstractive Summary>
	<Extractive Summary> =
		Syntactic Parsers In Table 1, both AUTO and
		GOLD syntactic dependencies are provided by the
		dataset
	</Extractive Summary>
</Paper ID=ument57>


<Paper ID=ument57> <Table ID =2>
	<Abstractive Summary> =
		(2018)
		RELAWE
		Relation-aware self-attention
		Table 2: A glossary of abbreviations for different sys-
		tem conﬁgurations in our experiments
	</Abstractive Summary>
</Paper ID=ument57>


<Paper ID=ument57> <Table ID =3>
	<Abstractive Summary> =
		22
		Table 3: SRL results with dependency trees of differ-
		ent quality on the Chinese dev set
	</Abstractive Summary>
	<Extractive Summary> =
		Experiment results in
		Table 3 demonstrate that, incorporating syntactic
		knowledge into the SRL model can achieve better
		performance and overall, the better the quality is,
		the better the SRL model performs
	</Extractive Summary>
	<Extractive Summary> =
		Comparing Table 3 and 4, when using gold
		dependencies, DEPPATH&RELPATH can achieve
		much better result than DEP&REL
	</Extractive Summary>
</Paper ID=ument57>


<Paper ID=ument57> <Table ID =4>
	<Abstractive Summary> =
		37
		Table 4: SRL results with different syntactic represen-
		tations on the Chinese dev set
	</Abstractive Summary>
	<Extractive Summary> =
		label information is more important than the head
		and the combination of the two achieves better
		performance as our experiment results in Table 4
		show
	</Extractive Summary>
</Paper ID=ument57>


<Paper ID=ument57> <Table ID =5>
	<Abstractive Summary> =
		96
		Table 5: SRL results with different incorporation meth-
		ods of the syntactic information on the Chinese dev set
	</Abstractive Summary>
	<Extractive Summary> =
		Firstly, results in Table 5 show that with lit-
		tle dependency information (DEP), LISA per-
		forms better, while incorporating richer syntactic
		knowledge (DEP&REL or DEP&RELPATH), three
		methods achieve similar performance
	</Extractive Summary>
</Paper ID=ument57>


<Paper ID=ument57> <Table ID =6>
	<Abstractive Summary> =
		84
		Table 6: SRL results with different external knowledge
		on the Chinese dev set
	</Abstractive Summary>
</Paper ID=ument57>


<Paper ID=ument57> <Table ID =7>
	<Abstractive Summary> =
		14
		Table 7: SRL results on the Chinese test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows that our OPEN model achieves
		more than 3 points of f1-score than the state-
		of-the-art
		result,
		and
		RELAWE
		with
		DEP-
		PATH&RELPATH
		achieves
		the
		best
		in
		both
		CLOSED and OPEN settings
	</Extractive Summary>
</Paper ID=ument57>


<Paper ID=ument57> <Table ID =8>
	<Abstractive Summary> =
		70
		Table 8: SRL results on the English test set
	</Abstractive Summary>
</Paper ID=ument57>


<Paper ID=ument570> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Online decoding accuracy for a direct model
		(DIR), ensembling two direct models (DIR ENS) and the
		channel approach (CH+DIR+LM)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that adding a lan-
		guage model to DIR (DIR+LM) gives a good im-
		provement (Gulcehre et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
</Paper ID=ument570>


<Paper ID=ument570> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Re-ranking BLEU with different n-best list
		sizes on news2016 of WMT De-En
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows that the noisy channel model out-
		performs the baseline (DIR) by up to 4
	</Extractive Summary>
</Paper ID=ument570>


<Paper ID=ument570> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Re-ranking accuracy with k1 = 50 on four
		language directions on the respective test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Test results on all language directions conﬁrm that
		CH+DIR+LM performs best (Table 3)
	</Extractive Summary>
</Paper ID=ument570>


<Paper ID=ument570> <Table ID =4>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 in the appendix shows standard deviations
	</Extractive Summary>
</Paper ID=ument570>


<Paper ID=ument570> <Table ID =5>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5
		in the appendix shows standard deviations
	</Extractive Summary>
</Paper ID=ument570>


<Paper ID=ument571> <Table ID =1>
	<Abstractive Summary> =
		html
		Domain
		Languages
		Train
		Dev
		Test
		MLDoc
		News
		EN, DE, ES, FR,
		1k / 2k /
		2k
		10k
		IT, JA, RU, ZH
		5k / 10k
		CLS
		Product
		EN, DE, FR, JA
		2k
		-
		2k
		reviews
		Table 1: The domain, languages, and number of train-
		ing, development, and test examples in each dataset
	</Abstractive Summary>
</Paper ID=ument571>


<Paper ID=ument571> <Table ID =2>
	<Abstractive Summary> =
		52
		Table 2: Comparison of zero-shot and supervised meth-
		ods on MLDoc
	</Abstractive Summary>
</Paper ID=ument571>


<Paper ID=ument571> <Table ID =3>
	<Abstractive Summary> =
		59
		Table 3: Comparison of zero-shot, translation-based and supervised methods (with 2k training examples) on all
		domains of CLS
	</Abstractive Summary>
</Paper ID=ument571>


<Paper ID=ument571> <Table ID =4>
	<Abstractive Summary> =
		LSTM
		QRNN
		Language model pretraining
		143
		71
		Classiﬁer ﬁne-tuning
		467
		156
		Table 4: Comparison of LSTM and QRNN per-batch
		training speed on a Tesla V100 (in ms) in MultiFiT
	</Abstractive Summary>
</Paper ID=ument571>


<Paper ID=ument571> <Table ID =5>
	<Abstractive Summary> =
		52
		Table 5: Comparison of MultiFiT results with different
		pretraining corpora and ULMFiT, ﬁne-tuned with 1k
		labels on MLDoc
	</Abstractive Summary>
</Paper ID=ument571>


<Paper ID=ument571> <Table ID =6>
	<Abstractive Summary> =
		45
		Table 6: Bootstrapping results on MLDoc with and
		without pretraining, trained on 1k/10k LASER labels
	</Abstractive Summary>
</Paper ID=ument571>


<Paper ID=ument571> <Table ID =7>
	<Abstractive Summary> =
		65
		Table 7: Comparison of different tokenization strate-
		gies for different languages on MLDoc
	</Abstractive Summary>
</Paper ID=ument571>


<Paper ID=ument572> <Table ID =1>
	<Abstractive Summary> =
		8×
		Table 1: Performance on WMT14 En-De, De-En and IWSLT14 De-En tasks
	</Abstractive Summary>
</Paper ID=ument572>


<Paper ID=ument572> <Table ID =2>
	<Abstractive Summary> =
		63
		Table 2: Ablation studies on IWSLT14 De-En
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, the
		Model
		Lnll
		Lnll + Lalign
		Lnll + Lalign + Lhid
		BLEU
		23
	</Extractive Summary>
</Paper ID=ument572>


<Paper ID=ument573> <Table ID =1>
	<Abstractive Summary> =
		28
		Table 1: A comparison of all methods on held-out test languages
	</Abstractive Summary>
	<Extractive Summary> =
		Results: Table 1 presents our cross-lingual trans-
		fer results
	</Extractive Summary>
	<Extractive Summary> =
		In-
		deed from Table 1 and Fig
	</Extractive Summary>
</Paper ID=ument573>


<Paper ID=ument573> <Table ID =2>
	<Abstractive Summary> =
		WALS ID
		82A
		83A
		85A
		86A
		87A
		88A
		Logreg
		87
		85
		97
		92
		94
		92
		Majority
		61
		56
		87
		75
		51
		82
		Table 2: Performance of typology prediction using hid-
		den states of the parser’s encoder, compared to a major-
		ity baseline which predicts the most frequent category
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents the
		results of probing the ﬁnal max-pooled output of
		the BiLSTM encoder for typological features on a
		sentence level
	</Extractive Summary>
</Paper ID=ument573>


<Paper ID=ument573> <Table ID =3>
	<Abstractive Summary> =
		12
		Table 3:
		Average UAS results when training with
		Galactic Dependencies
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the results of train-
		ing on this setting
	</Extractive Summary>
</Paper ID=ument573>


<Paper ID=ument573> <Table ID =4>
	<Abstractive Summary> =
		P@1
		P@3
		P@5
		P@10
		TL
		13
		33
		60
		80
		TD
		27
		67
		67
		93
		TS
		13
		27
		27
		73
		Table 4: Precision@k for identifying the best parsing
		transfer language, for the k typological neighbors
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, we observe
		that while there is some correlation between the
		two, they are far from perfectly aligned
	</Extractive Summary>
</Paper ID=ument573>


<Paper ID=ument574> <Table ID =1>
	<Abstractive Summary> =
		54
		Table 1: Results (LAS%) on test sets
	</Abstractive Summary>
</Paper ID=ument574>


<Paper ID=ument574> <Table ID =2>
	<Abstractive Summary> =
		1M
		10K
		Table 2: Results (UAS%/LAS%) on test sets
	</Abstractive Summary>
</Paper ID=ument574>


<Paper ID=ument575> <Table ID =1>
	<Abstractive Summary> =
		Language
		UD Treebank
		Language Family
		Corpus Size
		ET
		Estonian
		Finnic
		11404
		FI
		Finnish
		Finnic
		9648
		NL
		Dutch
		Germanic
		8783
		EN
		English
		Germanic
		7674
		DE
		German
		Germanic
		7447
		NO
		Norwegian
		Germanic
		10017
		GRC
		Ancient Greek
		Hellenic
		9387
		HI
		Hindi
		Indo-Irian
		4997
		JA
		Japanese
		Janponic
		7441
		FR
		French
		Romance
		4976
		IT
		Italian
		Romance
		6492
		LA
		Latin-ITTB
		Romance
		10136
		BG
		Bulgarian
		Slavonic
		6507
		SL
		Slovenian
		Slavonic
		3800
		EU
		Basque
		Vasconic
		4271
		Table 1: Languages and treebanks used in our experi-
		ments
	</Abstractive Summary>
</Paper ID=ument575>


<Paper ID=ument575> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: DDA of monolingual and multilingual ap-
		proaches
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the
		experimental results
	</Extractive Summary>
</Paper ID=ument575>


<Paper ID=ument575> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Comparison of the recent state-of-the-art ap-
		proaches and G/G+I
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3 we compare our method with recent
		state-of-the-art approaches on the UD Treebank
		dataset: Convex-MST (Grave and Elhadad, 2015),
		LC-DMV (Noji et al
	</Extractive Summary>
</Paper ID=ument575>


<Paper ID=ument576> <Table ID =1>
	<Abstractive Summary> =
		5736
		bg
		ca
		el
		es
		fr
		he
		hi
		hr
		it
		lt
		lv
		pl
		pt
		ro
		ru
		sk
		sl
		uk
		en
		2596
		2720
		2872
		4947
		6257
		1489
		828
		443
		4800
		923
		881
		1646
		3918
		397
		5779
		1056
		293
		975
		ja
		2586
		2596
		2886
		3383
		4654
		2223
		1421
		486
		3849
		1241
		1215
		1884
		3615
		497
		4532
		375
		419
		1380
		ko
		1856
		1843
		1982
		1840
		2812
		1774
		1269
		371
		2513
		1089
		997
		1357
		2442
		364
		2680
		247
		317
		1169
		tr
		1578
		1623
		1735
		1766
		2580
		1275
		817
		274
		2287
		903
		826
		1163
		2223
		303
		2470
		218
		281
		909
		zh
		2275
		2190
		2454
		2693
		3722
		1810
		1266
		373
		3170
		1111
		1110
		1643
		3084
		480
		3652
		286
		341
		1196
		Table 1: The number of inanimate nouns for each gendered–genderless language pair
	</Abstractive Summary>
	<Extractive Summary> =
		We suspect that these results are due the rela-
		tively small number of inanimate nouns considered
		for each of these language pairs (see Table 1 for the
		counts)
	</Extractive Summary>
</Paper ID=ument576>


<Paper ID=ument577> <Table ID =1>
	<Abstractive Summary> =
		60
		Table 1: Sensitivity of NLP models to named entities in text
	</Abstractive Summary>
	<Extractive Summary> =
		69) (see Table 1 for more examples)
	</Extractive Summary>
</Paper ID=ument577>


<Paper ID=ument577> <Table ID =2>
	<Abstractive Summary> =
		364
		Table 2: ScoreDev is the per-sentence standard deviation of
		scores upon name perturbation, averaged across all sentences
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 2 shows the results of perturbation sensi-
		tivity analysis on different corpora
	</Extractive Summary>
</Paper ID=ument577>


<Paper ID=ument578> <Table ID =1>
	<Abstractive Summary> =
		72
		Table 1: Summary statistics of our datasets
	</Abstractive Summary>
</Paper ID=ument578>


<Paper ID=ument578> <Table ID =2>
	<Abstractive Summary> =
		Assuming a
		non-informative prior distribution on fi, the pos-
		Female-Associated
		Male-Associated
		girl, cover, husband,
		wedding, gown, fash-
		ion, mom, pregnancy,
		photo, top, hair, look
		movie, president, wife,
		dad, death, ﬁlm, host,
		assault, claim, miscon-
		duct, action, director
		respond,
		email,
		rec-
		ommend, help, love,
		accept, need, send, re-
		ply, communicate
		learn,
		teach,
		know,
		write,
		lecture,
		chal-
		lenge, solve, ramble,
		push, joke, bore
		easy,
		rude,
		wonder-
		ful, kind, caring, hot,
		strict,
		timely,
		mean,
		disorganized, beautiful
		knowledgeable,
		real,
		challenging,
		bril-
		liant, arrogant, hard,
		passionate, practical
		Table 2: Top: Sample from the top-25 most gender-
		associated nouns in the celebrity domain
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2,
		we present a sample of the most gender-associated
		nouns from the celebrity domain
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 also includes some of the most gender-
		associated verbs and adjectives from the professor
		domain
	</Extractive Summary>
</Paper ID=ument578>


<Paper ID=ument578> <Table ID =3>
	<Abstractive Summary> =
		Cluster Labels
		Celeb
		gown, top, dress, pant, skirt, neckline
		25:0
		covering, cloth covering, clothing
		ﬁlm, release, role, character, project
		4:16
		movie, show, event
		boyfriend, beau, hubby, wife, girlfriend
		15:7
		lover, person, relative
		Professor
		response, email, contact, answer
		13:0
		statement, message, communication
		material, concept, topic, stuff, subject
		1:8
		content, idea, cognition
		teacher, woman, lady, prof, guy, dude
		5:7
		man, adult, woman
		Table 3: Sample of our clusters and predicted cluster labels
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Findings
		Table 3 displays a sample of our results – we ﬁnd
		that the clusters are coherent in context and the la-
		bels seem reasonable
	</Extractive Summary>
</Paper ID=ument578>


<Paper ID=ument579> <Table ID =1>
	<Abstractive Summary> =
		Event Type
		Positive
		Negative
		Hate Crime
		1979
		3192
		Homicide
		1664
		1327
		Kidnapping
		1864
		1104
		Table 1: Frequency of events from Patch annotations
	</Abstractive Summary>
</Paper ID=ument579>


<Paper ID=ument579> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Event detection F1 scores for the test set
		notated for each article
	</Abstractive Summary>
</Paper ID=ument579>


<Paper ID=ument579> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Event extraction scores of MIL
		After checking for pairs of articles from the
		same state and city, with the same reported target
		victim and crime action, reported at most one day
		apart from each other, we found 20 pairs of dupli-
		cated articles, indicating 658 unique incidents of
		hate in the cities with no representation in the FBI
		dataset
	</Abstractive Summary>
</Paper ID=ument579>


<Paper ID=ument579> <Table ID =4>
	<Abstractive Summary> =
		Table 4: First sentences of sample articles recognized
		as false positive results by our annotators
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 represents a few instances of
		false positives
	</Extractive Summary>
</Paper ID=ument579>


<Paper ID=ument58> <Table ID =1>
	<Abstractive Summary> =
		Furthermore, to
		628
		Cluster types
		#
		Argument roles
		#
		Meaning units
		#
		FrameNet
		Frames
		1,224
		Frame elements
		10,542
		Lexical units
		5,200
		VerbNet
		Levin’s classes
		329
		Thematic roles
		39
		Senses
		6,791
		PropBank
		Verbs
		5,649
		Proto-roles
		6
		Framesets
		10,687
		WordNet
		–
		–
		–
		–
		Synsets
		13,767
		VerbAtlas
		Frames
		466
		Semantic roles
		25
		Synsets
		13,767
		Table 1: Quantitative analysis of popular verbal resources
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1) which
		provide cross-frame argument structures
	</Extractive Summary>
</Paper ID=ument58>


<Paper ID=ument58> <Table ID =2>
	<Abstractive Summary> =
		EAT
		CLASSIFY
		SWITCH
		KILL
		CLEAN
		MOVE SOMETHING
		DRINK
		AMELIORATE
		JUDGE
		POUR
		TRANSPORT
		PERFORM
		OBTAIN
		BEHAVE
		STOP
		GIVE
		HIT
		COMBINE
		Table 2: Frames with the highest number of synsets
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2 we report the
		frames with the highest number of verb synsets
	</Extractive Summary>
</Paper ID=ument58>


<Paper ID=ument58> <Table ID =3>
	<Abstractive Summary> =
		, 2011; Allen and Teng, 2018),
		632
		Agent
		Material
		Attribute
		Patient
		Beneficiary
		Product
		Cause
		Purpose
		Co-Agent
		Recipient
		Co-Patient
		Result
		Co-Theme
		Source
		Destination
		Stimulus
		Experiencer
		Theme
		Extent
		Time
		Goal
		Topic
		Instrument
		Value
		Location
		Table 3: List of VerbNet roles which make up the
		VerbAtlas role inventory
	</Abstractive Summary>
	<Extractive Summary> =
		, Agent in place of Affector, see
		Table 3)
	</Extractive Summary>
</Paper ID=ument58>


<Paper ID=ument58> <Table ID =4>
	<Abstractive Summary> =
		Affector
		Manner
		Axis
		Path
		Context
		Pivot
		Duration
		Precondition
		Final_Time
		Predicate
		Initial_Location
		Reflexive
		Initial_State
		Trajectory
		Table 4: List of VerbNet roles unused in VerbAtlas
	</Abstractive Summary>
	<Extractive Summary> =
		,
		Affector), therefore 14 of the roles (Table 4)
		were subsumed by coarser and more common
		roles (e
	</Extractive Summary>
</Paper ID=ument58>


<Paper ID=ument58> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5: Results on the English in-domain test
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		In-domain SRL
		Table 5 reports the results
		of our syntax-agnostic baseline model on the
		CoNLL-2009 in-domain test set
	</Extractive Summary>
</Paper ID=ument58>


<Paper ID=ument58> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: Predicate disambiguation results on the En-
		glish in-domain test
	</Abstractive Summary>
	<Extractive Summary> =
		In-domain predicate disambiguation
		Remark-
		ably, as shown in Table 6, our model outperforms
		the previously reported best scores in the predicate
		disambiguation subtask, scoring 96
	</Extractive Summary>
</Paper ID=ument58>


<Paper ID=ument58> <Table ID =7>
	<Abstractive Summary> =
		7
		Table 7: Results on the English out-of-domain test
	</Abstractive Summary>
</Paper ID=ument58>


<Paper ID=ument58> <Table ID =8>
	<Abstractive Summary> =
		9
		Table 8: The model shows a signiﬁcant F1 decrease
		when it is trained without exploiting VerbAtlas frames
		and roles (second row) falling in line with Cai et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 (right
		column) shows that removing the PropBank bi-
		afﬁne role scorer leads to a negligible performance
		drop (0
	</Extractive Summary>
</Paper ID=ument58>


<Paper ID=ument580> <Table ID =1>
	<Abstractive Summary> =
		5761
		Type of pairs
		# of pairs
		AL (Automatically Labeled Pairs)
		1,000,000
		CA (CAUSE Pairs)
		5,000,000
		CO (CONCESSION Pairs)
		5,000,000
		Table 1: Statistics of the AL, CA, and CO datasets
	</Abstractive Summary>
</Paper ID=ument580>


<Paper ID=ument580> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		Event polarity
		# of events
		Train
		Positive
		299,834
		Negative
		300,164
		Dev
		Positive
		50,118
		Negative
		49,882
		Test
		Positive
		50,046
		Negative
		49,954
		Table 2: Details of the ACP dataset
	</Abstractive Summary>
</Paper ID=ument580>


<Paper ID=ument580> <Table ID =3>
	<Abstractive Summary> =
		503
		Table 3: Performance of various models on the ACP
		test set
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results and Discussion
		Table 3 shows accuracy
	</Extractive Summary>
</Paper ID=ument580>


<Paper ID=ument580> <Table ID =4>
	<Abstractive Summary> =
		879
		Table 4: Results for small labeled training data
	</Abstractive Summary>
	<Extractive Summary> =
		As the results shown in Table 4 demon-
		strate, our method is effective when labeled data
		are small
	</Extractive Summary>
</Paper ID=ument580>


<Paper ID=ument580> <Table ID =5>
	<Abstractive Summary> =
		653
		Table 5: Examples of polarity scores predicted by the
		BiGRU model trained with AL+CA+CO
	</Abstractive Summary>
</Paper ID=ument580>


<Paper ID=ument581> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Overall performance of different methods on
		the test set with gold-standard entities
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents the performance comparison
		between different methods
	</Extractive Summary>
</Paper ID=ument581>


<Paper ID=ument581> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Performance of modiﬁed architectures based
		on MOGANED
	</Abstractive Summary>
</Paper ID=ument581>


<Paper ID=ument582> <Table ID =1>
	<Abstractive Summary> =
		child
		spouse
		member
		employer
		educatedAt
		#subj
		40,145
		45,261
		8,901
		58,731
		273,128
		#subj w/ ≥2 obj
		15,022
		4,055
		1,022
		12,885
		72,847
		seeds
		1,000
		1,000
		1,000
		1,000
		8,000
		Sentences +/-
		135/2,050
		119/2,444
		672/10,358
		47/1,499
		447/
		2,603
		Paragraphs +/-
		217/1,595
		385/2,044
		930/ 5,362
		108/1,248
		339/
		2,384
		Table 1: Number of Wikidata subjects and derived la-
		belled text segments
	</Abstractive Summary>
</Paper ID=ument582>


<Paper ID=ument582> <Table ID =2>
	<Abstractive Summary> =
		58
		Table 2: Performance of coverage prediction
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results and Discussion
		Table 2 shows the precision, recall and F1-score in
		terms of identifying complete text segments
	</Extractive Summary>
</Paper ID=ument582>


<Paper ID=ument582> <Table ID =3>
	<Abstractive Summary> =
		”) among
		5775
		child
		spouse
		hasPart
		educatedAt
		⟨pname⟩ sons
		married twice
		featuring lineup
		educated ⟨pname⟩
		grandsons ⟨pname⟩
		children ⟨pname⟩
		lineup ⟨pname⟩
		brieﬂy attended
		⟨num⟩ grandsons
		second marriage
		consists ⟨pname⟩
		attended ⟨pname⟩
		daughters ⟨pname⟩
		⟨num⟩ son
		⟨num⟩ tour
		left graduating
		sons ⟨pname⟩
		later married
		vocals ⟨propname⟩
		⟨pname⟩ left
		Table 3: Selected important paragraph-level bigrams
		indicating completeness for SVMs
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3 we show the most informative bi-
		grams for the n-gram-based SVMs on the para-
		graph level, for predicates having reasonably good
		F1 scores
	</Extractive Summary>
</Paper ID=ument582>


<Paper ID=ument582> <Table ID =4>
	<Abstractive Summary> =
		17
		Table 4: Example LSTM predictions at sentence-level
	</Abstractive Summary>
</Paper ID=ument582>


<Paper ID=ument583> <Table ID =1>
	<Abstractive Summary> =
		5
		Hidden Layer Dimension
		300
		Kernel Size
		3
		Position Embedding Dimension
		5
		Event Type Embedding Dimension
		5
		uc dimension
		900
		Wb dimension
		900
		Table 1: Hyperparameter settings for CNN models
	</Abstractive Summary>
</Paper ID=ument583>


<Paper ID=ument583> <Table ID =2>
	<Abstractive Summary> =
		1
		uc dimension
		900
		Wb dimension
		900
		Table 2: Hyperparameter settings for BERT models
	</Abstractive Summary>
</Paper ID=ument583>


<Paper ID=ument583> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: The overall results (%) on ACE 2005
	</Abstractive Summary>
	<Extractive Summary> =
		The results are shown in Table 3 and Table 4
	</Extractive Summary>
</Paper ID=ument583>


<Paper ID=ument583> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: The overall results (%) on TAC KBP 2016
	</Abstractive Summary>
</Paper ID=ument583>


<Paper ID=ument584> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: DYGIE++ achieves state-of-the-art results
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results and Analyses
		State-of-the-art Results
		Table 1 shows test set
		F1 on the entity, relation and event extraction tasks
	</Extractive Summary>
</Paper ID=ument584>


<Paper ID=ument584> <Table ID =2>
	<Abstractive Summary> =
		5
		-
		Table 2: F1 scores on NER
	</Abstractive Summary>
	<Extractive Summary> =
		Beneﬁts of Graph Propagation
		Table 2 shows
		that Coreference propagation (CorefProp) im-
		proves named entity recognition performance
		across all three domains
	</Extractive Summary>
</Paper ID=ument584>


<Paper ID=ument584> <Table ID =3>
	<Abstractive Summary> =
		3
		-
		Table 3: F1 scores on Relation
	</Abstractive Summary>
</Paper ID=ument584>


<Paper ID=ument584> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: F1 scores on ACE05-E
	</Abstractive Summary>
	<Extractive Summary> =
		Our best event extraction results did not use any
		propagation techniques (Table 4)
	</Extractive Summary>
</Paper ID=ument584>


<Paper ID=ument584> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Comparison of contextualization methods
	</Abstractive Summary>
	<Extractive Summary> =
		Pre-training or Fine Tuning BERT Under Lim-
		ited Resources
		Table 5 shows that ﬁne-tuning
		BERT generally performs slightly better than using
		the pre-trained BERT embeddings combined with
		a ﬁnal LSTM layer
	</Extractive Summary>
</Paper ID=ument584>


<Paper ID=ument584> <Table ID =6>
	<Abstractive Summary> =
		8
		Table 6: Effect of BERT cross-sentence context
	</Abstractive Summary>
	<Extractive Summary> =
		Beneﬁts of Cross-Sentence Context with BERT
		Table 6 shows that both variations of our BERT
		model beneﬁt from wider context windows
	</Extractive Summary>
</Paper ID=ument584>


<Paper ID=ument584> <Table ID =7>
	<Abstractive Summary> =
		5
		Table 7: In-domain pre-training: SciBERT vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 compares the results of BERT and
		SciBERT with the best-performing model conﬁgu-
		rations
	</Extractive Summary>
</Paper ID=ument584>


<Paper ID=ument585> <Table ID =1>
	<Abstractive Summary> =
		83)
		Table 1: Accuracy (%) with standard deviation in brackets of implicit discourse relation classiﬁcation on different
		settings of PDTB level 2 relations
	</Abstractive Summary>
	<Extractive Summary> =
		The re-
		sults in Table 1 and Table 2 conﬁrm that removing
		the “Next Sentence Prediction” hurts the perfor-
		mance on both PDTB and BioDRB
	</Extractive Summary>
</Paper ID=ument585>


<Paper ID=ument585> <Table ID =2>
	<Abstractive Summary> =
		04∗
		Table 2: Accuracy (%) on BioDRB level 2 relations
		with different settings
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2, we
		can see that the BERT base model achieved almost
		12% points improvement over the Bi-LSTM base-
		line and 15% points over Bai and Zhao (2018)
	</Extractive Summary>
	<Extractive Summary> =
		The re-
		sults in Table 1 and Table 2 conﬁrm that removing
		the “Next Sentence Prediction” hurts the perfor-
		mance on both PDTB and BioDRB
	</Extractive Summary>
	<Extractive Summary> =
		, 2019) in Table 2
		show that having large in-domain data for pre-
		training also has limited ability in learning domain
		speciﬁc representations
	</Extractive Summary>
</Paper ID=ument585>


<Paper ID=ument585> <Table ID =3>
	<Abstractive Summary> =
		34)
		Table 3: F1-score (Accuracy) of binary classiﬁcation on level 1 implicit relation in BioDRB
	</Abstractive Summary>
</Paper ID=ument585>


<Paper ID=ument585> <Table ID =4>
	<Abstractive Summary> =
		61
		351
		Table 4: Precision, Recall and F1 score for each level-2 relation on PDTB-Lin setting and BioDRB with “BERT +
		WSJ/GENIA” systems w/ and w/o NSP
	</Abstractive Summary>
	<Extractive Summary> =
		As illus-
		trated in Table 4, we can see that performances
		on relations like Temporal
	</Extractive Summary>
</Paper ID=ument585>


<Paper ID=ument586> <Table ID =1>
	<Abstractive Summary> =
		66
		Table 1: Statistics of the datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the statistics of the
		datasets
	</Extractive Summary>
</Paper ID=ument586>


<Paper ID=ument586> <Table ID =2>
	<Abstractive Summary> =
		789
		Table 2: Micro Span F1 scores for RST-DT and PCC
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 2 shows the evaluation results
	</Extractive Summary>
</Paper ID=ument586>


<Paper ID=ument586> <Table ID =3>
	<Abstractive Summary> =
		856
		Table 3: Micro Span F1 scores on the test set of RST-
		DT
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results
	</Extractive Summary>
</Paper ID=ument586>


<Paper ID=ument587> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: OntoNotes: BERT improves the c2f-coref model on English by 0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that BERT-base offers an im-
		provement of 0
	</Extractive Summary>
	<Extractive Summary> =
		Strengths
		We did not ﬁnd salient qualitative dif-
		ferences between ELMo and BERT-base models,
		which is consistent with the quantitative results
		(Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		We fo-
		cus on three observations – (a) Table 4 shows how
		models perform distinctly worse on longer docu-
		ments, (b) both models are unable to use larger
		segments more effectively (Table 5) and perform
		worse when the max segment len of 450 and
		512 are used, and, (c) using overlapping segments
		to provide additional context does not improve re-
		sults (Table 1)
	</Extractive Summary>
</Paper ID=ument587>


<Paper ID=ument587> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: GAP: BERT improves the c2f-coref model by
		11
	</Abstractive Summary>
</Paper ID=ument587>


<Paper ID=ument587> <Table ID =3>
	<Abstractive Summary> =
		He is my, She is my Goddess , ah
		17
		17
		Total
		93
		74
		Table 3: Qualitative Analysis: #base and #large refers to the number of cluster-level errors on a subset of the
		OntoNotes English development set
	</Abstractive Summary>
	<Extractive Summary> =
		A qualitative analysis of BERT and ELMo-
		based models (Table 3) suggests that BERT-large
		(unlike BERT-base) is remarkably better at dis-
		tinguishing between related yet distinct entities or
		concepts (e
	</Extractive Summary>
	<Extractive Summary> =
		4
		Analysis
		We performed a qualitative comparison of ELMo
		and BERT models (Table 3) on the OntoNotes En-
		glish development set by manually assigning error
		categories (e
	</Extractive Summary>
	<Extractive Summary> =
		Modeling pronouns especially in the context of
		conversations (Table 3), continues to be difﬁcult
		for all models, perhaps partly because c2f-coref
		does very little to model dialog structure of the
		document
	</Extractive Summary>
</Paper ID=ument587>


<Paper ID=ument587> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Performance on the English OntoNotes dev
		set generally drops as the document length increases
	</Abstractive Summary>
	<Extractive Summary> =
		We fo-
		cus on three observations – (a) Table 4 shows how
		models perform distinctly worse on longer docu-
		ments, (b) both models are unable to use larger
		segments more effectively (Table 5) and perform
		worse when the max segment len of 450 and
		512 are used, and, (c) using overlapping segments
		to provide additional context does not improve re-
		sults (Table 1)
	</Extractive Summary>
</Paper ID=ument587>


<Paper ID=ument587> <Table ID =5>
	<Abstractive Summary> =
		6
		Table 5:
		Performance on the English OntoNotes
		dev set with varying values for max segment len
	</Abstractive Summary>
	<Extractive Summary> =
		We fo-
		cus on three observations – (a) Table 4 shows how
		models perform distinctly worse on longer docu-
		ments, (b) both models are unable to use larger
		segments more effectively (Table 5) and perform
		worse when the max segment len of 450 and
		512 are used, and, (c) using overlapping segments
		to provide additional context does not improve re-
		sults (Table 1)
	</Extractive Summary>
</Paper ID=ument587>


<Paper ID=ument588> <Table ID =1>
	<Abstractive Summary> =
		5812
		Train
		Valid
		Test
		Papers
		16,173
		899
		899
		SciFi
		157,031
		8,724
		8,724
		Fantasy
		317,654
		17,649
		17,649
		Table 1: Number of paragraphs in our dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the numbers
		of paragraphs for each domain
	</Extractive Summary>
</Paper ID=ument588>


<Paper ID=ument588> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Performance on bridging task
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, both discourse and delta driven
		FlowNet outperform the baseline models across
		most of the metrics except for VecterExtrema on
		SciFi
	</Extractive Summary>
</Paper ID=ument588>


<Paper ID=ument588> <Table ID =3>
	<Abstractive Summary> =
		97
		Table 3:
		Comparison
		of different delta func-
		tions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows performance comparison among
		different delta operations: SUBTRACT, ADD, and
		5813
		FIRST: Satyrs never wear armor, including helmets, Newel began, using his hands expressively
	</Extractive Summary>
</Paper ID=ument588>


<Paper ID=ument588> <Table ID =4>
	<Abstractive Summary> =
		): [M1] perhaps they were not quite good, but he was not a master, and they were the most powerful [M2] the
		only way to do not like a little, but i’ d been in the world [M3] ”you’re right,” he said ”i am not a fool you’re here [M4] you’re
		going to be a bit more than the other
		Table 4:
		An example paragraph and predicted texts in Fantasy dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows an example paragraph with text
		produced by the models as well as reference and
		human annotation
	</Extractive Summary>
</Paper ID=ument588>


<Paper ID=ument589> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Equations used in ProposedRU
		θ (θ = 0
	</Abstractive Summary>
</Paper ID=ument589>


<Paper ID=ument589> <Table ID =2>
	<Abstractive Summary> =
		L = αLMV + β
		∑
		ann
		Llbl
		ann + γLru
		(5)
		LMV = − log P(yMV|x)
		(6)
		Llbl
		ann = − log P lbl
		ann(yann|x)
		(7)
		Lru =
		{
		0
		if yA = yB = yC
		− log P ru(ann|x) otherwise
		(8)
		Table 2: Loss functions used in ProposedRU
		phrase (“smoke cigarettes”) and effect phrase
		(“died of lung cancer”)
	</Abstractive Summary>
	<Extractive Summary> =
		For train-
		ing ProposedRU, we designed a special loss func-
		tion L in Equation (5) in Table 2 as the weighted
		sum of LMV, Llbl
		ann, and Lru, each of which is a
		loss function for P(yMV|x), P lbl
		ann(yann|x), and
		P ru(ann|x), respectively
	</Extractive Summary>
</Paper ID=ument589>


<Paper ID=ument589> <Table ID =3>
	<Abstractive Summary> =
		Data
		#Instances
		#True causalities
		Training
		107,068
		8,986
		Development
		23,602
		3,759
		Test
		23,650
		3,647
		Table 3: Statistics of datasets
		(d) VanillaBERT
		(e) MajorityMulti
		𝑃"#"(𝑦&'|𝐱)
		BERT"#"
		𝑃(𝑦&'|𝐱)
		𝑦&'
		𝐱
		𝑃"#$#(𝑦'(|𝐱)𝑃,#$#(𝑦'(|𝐱)𝑃-#$#(𝑦'(|𝐱)
		BERT#$#
		,
		BERT#$#
		"
		Avg
		BERT#$#
		-
		𝑃(𝑦'(|𝐱)
		𝑦'(
		𝐱
		We trained a single
		BERT model and its
		softmax layer with
		majority vote labels
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows their statistics5
	</Extractive Summary>
</Paper ID=ument589>


<Paper ID=ument589> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Results of event-causality recognition
		Model
		R
		P
		F
		Avg
	</Abstractive Summary>
</Paper ID=ument589>


<Paper ID=ument589> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Results of ProposedRU and its variants
		trained the models for 0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results of our investiga-
		tion into the pre-training of some sort of back-
		ground knowledge
	</Extractive Summary>
	<Extractive Summary> =
		Among the methods to be
		compared in Table 5, the ﬁrst three methods
		(i
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows
		that ProposedRU+BK improved the average pre-
		cision over ProposedRU by about 2
	</Extractive Summary>
</Paper ID=ument589>


<Paper ID=ument59> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Pearson’s r ⇥ 100 on STSB
	</Abstractive Summary>
</Paper ID=ument59>


<Paper ID=ument59> <Table ID =2>
	<Abstractive Summary> =
		22
		Table 2: MAP on CQA subtask B
	</Abstractive Summary>
</Paper ID=ument59>


<Paper ID=ument59> <Table ID =3>
	<Abstractive Summary> =
		60
		-
		-
		-
		Table 3: Results on supervised tasks
	</Abstractive Summary>
</Paper ID=ument59>


<Paper ID=ument59> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4:
		Comparison of different conﬁgurations
		demonstrates the effectiveness of our model on STSB
		dev set and SUBJ
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in in Table 4, ev-
		ery GEM weight (↵n, ↵s, ↵u) and proposed prin-
		cipal components removal methods contribute to
		the performance
	</Extractive Summary>
	<Extractive Summary> =
		As shown on the
		right in Table 4, every weight contributes to the
		performance of our model
	</Extractive Summary>
</Paper ID=ument59>


<Paper ID=ument59> <Table ID =5>
	<Abstractive Summary> =
		10
		Table 5: Run time of GEM, InferSent and SkipThought
		on encoding sentences in STSB test set
	</Abstractive Summary>
</Paper ID=ument59>


<Paper ID=ument591> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Strength of evidence for improvements in
		agreement prediction accuracy as a result of increasing
		corpus size averaging across layer size (left) or layer
		size averaging across corpus size (right), as quantiﬁed
		by Bayes factors
	</Abstractive Summary>
	<Extractive Summary> =
		creases in model size had no effect (see Table 1
		for the statistical tests)
	</Extractive Summary>
</Paper ID=ument591>


<Paper ID=ument591> <Table ID =2>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		1a)
		beneﬁted from increasing the hidden layer size to
		400, but showed little improvement when hidden
		7See Table 2 in the Appendix for a Bayes factor analysis
		of the improvement in each construction for each amount of
		training data
	</Extractive Summary>
</Paper ID=ument591>


<Paper ID=ument592> <Table ID =1>
	<Abstractive Summary> =
		66
		Table 1: Spearman’s ρ between VCorrLi−Lj , VlogF req
	</Abstractive Summary>
</Paper ID=ument592>


<Paper ID=ument593> <Table ID =1>
	<Abstractive Summary> =
		, 2005) and lexically-similar literature (Gutenberg)
		Table 1: Training regimes
		different regimes
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes these train-
		ing regimes
	</Extractive Summary>
</Paper ID=ument593>


<Paper ID=ument593> <Table ID =2>
	<Abstractive Summary> =
		0774 In the Wilderness
		Robert Hichens
		Table 2: Alice-like books from Project Gutenberg
		tence length in this selection was 17 words
	</Abstractive Summary>
</Paper ID=ument593>


<Paper ID=ument594> <Table ID =1>
	<Abstractive Summary> =
		09
		Table 1: Accuracy scores (percent) for retrieving the correct concept from amongst the top N most similar nearest
		neighbours
	</Abstractive Summary>
	<Extractive Summary> =
		(2015)) and 120 dimen-
		sions for a range of values of N (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		We compare these predicted embeddings to the
		held-out Glove embeddings (Table 1)
	</Extractive Summary>
</Paper ID=ument594>


<Paper ID=ument594> <Table ID =2>
	<Abstractive Summary> =
		30
		Table 2: The average percentage of features that each
		method can predict for a given concept vector
	</Abstractive Summary>
	<Extractive Summary> =
		rank all feature embeddings by their distance to
		the embedding for the target word, using cosine
		similarity, and take the top K most similar fea-
		tures (Table 2)
	</Extractive Summary>
</Paper ID=ument594>


<Paper ID=ument594> <Table ID =3>
	<Abstractive Summary> =
		For Feature2Vec, we
		5857
		Concepts
		Properties
		Kingﬁsher
		has wings does ﬂy has a beak has feathers is a bird
		does eat has a tail does swim has legs does lay eggs
		Avocado
		is eaten edible is tasty does grow is green is healthy is used in cooking
		has skin peel
		is red
		is food
		is a vegetable
		Door
		made of metal has a door doors is useful has a handle handles made of wood
		made of plastic
		is heavy
		is furniture
		does contain hold
		is found in kitchens
		Dragon
		is big large is an animal has a tail does eat is dangerous
		has legs
		has claws
		is grey
		is small
		does ﬂy
		Table 3: Top 10 predictions from CSLB-trained Feature2Vec model
	</Abstractive Summary>
</Paper ID=ument594>


<Paper ID=ument595> <Table ID =1>
	<Abstractive Summary> =
		We compare to a number of baseline
		models to put our CTM results into context:
		5864
		Table 1: Performance comparison of baseline VQA trained on VQA2
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results and Analysis
		Table 1 shows quantitative results on our L-
		ConVQA and CS-ConVQA datasets
	</Extractive Summary>
</Paper ID=ument595>


<Paper ID=ument596> <Table ID =1>
	<Abstractive Summary> =
		01
		Table 1: Proportions of correctly answered questions
	</Abstractive Summary>
</Paper ID=ument596>


<Paper ID=ument597> <Table ID =1>
	<Abstractive Summary> =
		Table 1:
		Accuracy on benchmark datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the average accuracy over all
		questions for the original ToM-bAbI benchmarks
		(ToM-easy, ToM) and our improved dataset (ToMi)
	</Extractive Summary>
	<Extractive Summary> =
		This is especially clear when
		looking at the joint accuracy of Table 1, which is
		not inﬂated by easy-to-answer memory and reality
		questions
	</Extractive Summary>
</Paper ID=ument597>


<Paper ID=ument597> <Table ID =2>
	<Abstractive Summary> =
		5876
		Table 2: Average accuracy by question type (MemNN)
		ToM-easy
		ToM
		ToMi
		Memory
		100
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows an
		ablation for MemNNs that illustrates this effect
	</Extractive Summary>
</Paper ID=ument597>


<Paper ID=ument597> <Table ID =3>
	<Abstractive Summary> =
		66
		Table 3: Average accuracy per question type in ToMi
	</Abstractive Summary>
	<Extractive Summary> =
		Our ablation in Table 3 provides further
		insights into these results
	</Extractive Summary>
</Paper ID=ument597>


<Paper ID=ument598> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: Results on the validation set of OpenSQuAD
	</Abstractive Summary>
	<Extractive Summary> =
		We
		vary passage length among {50, 100, 200}, and
		list the results as models (2) (3) (4) in Table 1,
		respectively
	</Extractive Summary>
	<Extractive Summary> =
		Re-
		sults are given in Table 1 as models (8) and (9) re-
		spectively
	</Extractive Summary>
	<Extractive Summary> =
		Model (10) in Table 1 shows the
		result of jointly training the BERT encoder and
		the QANet model
	</Extractive Summary>
	<Extractive Summary> =
		Model (12) in Table 1
		shows the result
	</Extractive Summary>
</Paper ID=ument598>


<Paper ID=ument598> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Comparison with state-of-the-art models, where the ﬁrst group are models without using BERT, the second
		group are BERT-based models, and the last group are our multi-passage BERT models
	</Abstractive Summary>
	<Extractive Summary> =
		Experimental results from our models as
		well as other state-of-the-art models are shown in
		Table 2, where the ﬁrst group are open-domain QA
		models without using the BERT model, the second
		group are BERT-based models, and the last group
		are our multi-passage BERT models
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2, we can see that our multi-passage
		BERT model outperforms all state-of-the-art mod-
		els across all benchmarks, and it works consis-
		tently better than our BERT-RC model which has
		the same settings except the global normalization
	</Extractive Summary>
</Paper ID=ument598>


<Paper ID=ument599> <Table ID =1>
	<Abstractive Summary> =
		Train
		Dev
		Test
		Challenge
		Question #
		10,321
		3,351
		4,895
		504
		Answer per Q
		1
		3
		3
		3
		Max P tokens
		962
		961
		980
		916
		Max Q tokens
		89
		56
		50
		47
		Max A tokens
		100
		85
		92
		77
		Avg P tokens
		452
		469
		472
		464
		Avg Q tokens
		15
		15
		15
		18
		Avg A tokens
		17
		9
		9
		19
		Table 1: Statistics of the CMRC 2018 dataset
	</Abstractive Summary>
</Paper ID=ument599>


<Paper ID=ument599> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2:
		Baseline results and CMRC 2018 participants’ results
	</Abstractive Summary>
	<Extractive Summary> =
		Not surprisingly, as shown in the last column of
		Table 2, though the top-ranked systems obtain de-
		cent scores on the development and test set, they
		are failed to give satisfactory results on the chal-
		lenge set
	</Extractive Summary>
</Paper ID=ument599>


<Paper ID=ument6> <Table ID =1>
	<Abstractive Summary> =
		172
		Table 1: The performance of various static embeddings on word embedding benchmark tasks
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1, we plot the performance of
		these PC static embeddings on several benchmark
		tasks2
	</Extractive Summary>
	<Extractive Summary> =
		We leave out lay-
		ers 3 - 10 in Table 1 because their performance is
		2The Word Embeddings Benchmarks package was used
		for evaluation
	</Extractive Summary>
</Paper ID=ument6>


<Paper ID=ument60> <Table ID =1>
	<Abstractive Summary> =
		3613
		2972
		1834
		2045
		4000
		Test
		3758
		3024
		2418
		2308
		4000
		Table 1: Size of datasets in DiscoEval
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the number of instances in each
		DiscoEval task introduced above
	</Extractive Summary>
</Paper ID=ument60>


<Paper ID=ument60> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Results for SentEval and DiscoEval
	</Abstractive Summary>
	<Extractive Summary> =
		The “Baseline” row in Table 2 are embeddings
		trained with only the NSP loss
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 2 shows the experiment results over all Sent-
		Eval and DiscoEval tasks
	</Extractive Summary>
</Paper ID=ument60>


<Paper ID=ument60> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: Average of the layer number for the best layers
		in SentEval and DiscoEval
	</Abstractive Summary>
</Paper ID=ument60>


<Paper ID=ument60> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Accuracies with baseline encoder on Dis-
		course Coherence task, with or without a hidden layer
		in the classiﬁer
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, when adding a
		hidden layer of 2000 to this task, the performance
		on DC improves dramatically
	</Extractive Summary>
</Paper ID=ument60>


<Paper ID=ument60> <Table ID =5>
	<Abstractive Summary> =
		1
		Table 5: Accuracies (%) for a human annotator and BERT-Large on Sentence Position, Binary Sentence Ordering,
		and Discourse Coherence tasks
	</Abstractive Summary>
	<Extractive Summary> =
		While the re-
		sults in Table 5 show that the overall human ac-
		curacies exceed those of the classiﬁer based on
		BERT-Large by a large margin, we observe that
		within some speciﬁc domains, for example Wiki in
		BSO, BERT-Large demonstrates very strong per-
		formance
	</Extractive Summary>
</Paper ID=ument60>


<Paper ID=ument60> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: Accuracies (%) for baseline encoder on Sen-
		tence Position task when using downstream classiﬁer
		with or without context
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the comparison of the base-
		line model performance on Sentence Position with
		or without the surrounding sentences and a ran-
		dom baseline
	</Extractive Summary>
</Paper ID=ument60>


<Paper ID=ument600> <Table ID =1>
	<Abstractive Summary> =
		07
		Table 1: Results of the diﬀerent models on the InsuranceQA and WikiPassageQA
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 1 shows the results on WikiPassageQA
		and InsuranceQA datasets
	</Extractive Summary>
</Paper ID=ument600>


<Paper ID=ument600> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		# of Questions
		Answer
		Train
		Valid
		Test
		Length
		WikipassageQA
		3332
		417
		416
		153
		InsuranceQA
		10391
		1592
		1625
		112
		Table 2: Statistics of datasets
		builds N:N matching matrices respectively
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the statistics of two
		datasets
	</Extractive Summary>
</Paper ID=ument600>


<Paper ID=ument601> <Table ID =1>
	<Abstractive Summary> =
		78
		Table 1: The performance of different models on ReCoRD dataset
	</Abstractive Summary>
</Paper ID=ument601>


<Paper ID=ument601> <Table ID =2>
	<Abstractive Summary> =
		55
		Table 2:
		The effectiveness of introducing external
		knowledge
	</Abstractive Summary>
</Paper ID=ument601>


<Paper ID=ument601> <Table ID =3>
	<Abstractive Summary> =
		79
		Table 3: The results of different ways of introducing
		external knowledge
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 3, our SKG model is more proper
		for introducing external knowledgse
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 3, the
		performance drops 2
	</Extractive Summary>
</Paper ID=ument601>


<Paper ID=ument602> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: SQA test results
	</Abstractive Summary>
</Paper ID=ument602>


<Paper ID=ument602> <Table ID =2>
	<Abstractive Summary> =
		5906
		What are all the nations?
		Australia, Italy, Germany, Soviet Union,
		Switzerland, United States, Great Britain, France
		Which won gold medals?
		Australia, Italy, Germany, Soviet Union
		Which won more than one?
		Australia
		Rank
		Nation
		Gold
		Silver
		Bronze
		Total
		1
		Australia
		2
		1
		0
		3
		2
		Italy
		1
		1
		1
		3
		3
		Germany
		1
		0
		1
		2
		4
		Soviet Union
		1
		0
		0
		1
		5
		Switzerland
		0
		2
		1
		3
		6
		United States
		0
		1
		0
		1
		7
		Great Britain
		0
		0
		1
		1
		7
		France
		0
		0
		1
		1
		Table 2: A sequence of questions (left) and the corresponding table (right) selected from the SQA dataset that
		is answered correctly by our approach
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows an example
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows an example that
		is consistently handled correctly by the model
	</Extractive Summary>
</Paper ID=ument602>


<Paper ID=ument602> <Table ID =3>
	<Abstractive Summary> =
		Error type
		Counts
		TABLE UNDERSTANDING
		29
		COMPLEX MATCH
		12
		MATCH
		26
		GOLD
		15
		ANSWER SET
		15
		OTHER
		3
		(a)
		Error type
		Counts
		TABLE UNDERSTANDING
		11
		COMPLEX MATCH
		38
		MATCH
		17
		GOLD
		13
		ANSWER SET
		4
		OTHER
		9
		CONTEXT
		8
		(b)
		Table 3: Errors on 100 random initial (a) and follow-up
		(b) questions
		COMPLEX MATCH
		A question that would
		require a numerical value match, some sort of sort-
		ing or a negation to be answered
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 contains the error counts for initial ques-
		tions and follow-ups, respectively
	</Extractive Summary>
</Paper ID=ument602>


<Paper ID=ument602> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Some interesting error cases with comments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 con-
		tains interesting examples
	</Extractive Summary>
</Paper ID=ument602>


<Paper ID=ument603> <Table ID =1>
	<Abstractive Summary> =
		34
		Table 1:
		Comparison of answer selection datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents some statistics on the training
		data used in these datasets
	</Extractive Summary>
</Paper ID=ument603>


<Paper ID=ument603> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Answer selection on InsuranceQA
	</Abstractive Summary>
	<Extractive Summary> =
		008 on Test1 and Test2 respectively) on In-
		suranceQA (Table 2)
	</Extractive Summary>
</Paper ID=ument603>


<Paper ID=ument603> <Table ID =3>
	<Abstractive Summary> =
		015
		Table 3: Answer selection on LargeQA; reported num-
		bers are relative to the baseline
	</Abstractive Summary>
	<Extractive Summary> =
		LargeQA: The gains due to hard negatives are
		also observed on LargeQA (Table 3), with an im-
		provement of 2
	</Extractive Summary>
</Paper ID=ument603>


<Paper ID=ument603> <Table ID =4>
	<Abstractive Summary> =
		896
		Table 4: Answer selection on SelQA
	</Abstractive Summary>
	<Extractive Summary> =
		SelQA: For answer selection on SelQA (See
		Table 4), Max-Pooling models performed best
		among the models we investigated, perhaps due to
		the smaller size of the dataset (other results omit-
		ted)
	</Extractive Summary>
</Paper ID=ument603>


<Paper ID=ument603> <Table ID =5>
	<Abstractive Summary> =
		21
		Table 5:
		Answer triggering on SelQA
	</Abstractive Summary>
	<Extractive Summary> =
		32 points) on SelQA (See Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Quadruplet loss with the Max-Min-Pooling model
		without hard negatives is competitive with base-
		lines (Table 5), justifying its usage for answer trig-
		gering
	</Extractive Summary>
</Paper ID=ument603>


<Paper ID=ument604> <Table ID =1>
	<Abstractive Summary> =
		90
		Table 1: Manual inspection of 50 rewritten context-
		independent questions from CANARD suggests that the
		new questions have enough context to be independently
		understandable
	</Abstractive Summary>
</Paper ID=ument604>


<Paper ID=ument604> <Table ID =2>
	<Abstractive Summary> =
		A post-processing script flags suspicious
		5920
		ORIGINAL: Was this an honest mistake by
		the media?
		REWRITE: Was the claim of media regarding
		Leblanc’s room come to true?
		ORIGINAL: What was a single from their al-
		bum?
		REWRITE: What was a single from horslips’
		album?
		ORIGINAL: Did they marry?
		REWRITE: Did Hannah Arendt and Heideg-
		ger marry?
		Table 2: Not all rewrites correctly encode the context
		required to answer a question
	</Abstractive Summary>
</Paper ID=ument604>


<Paper ID=ument604> <Table ID =3>
	<Abstractive Summary> =
		92
		Table 3: BLEU scores of the baseline models on devel-
		opment and test data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the BLEU scores produced by the
		baselines and humans over both the validation and
		the test sets
	</Extractive Summary>
</Paper ID=ument604>


<Paper ID=ument604> <Table ID =4>
	<Abstractive Summary> =
		Table 4: An example that had over ten flagged proper
		nouns in the history
	</Abstractive Summary>
</Paper ID=ument604>


<Paper ID=ument604> <Table ID =5>
	<Abstractive Summary> =
		5922
		Seq2Seq output
		Reference
		1
		What did Chamberlain’s men do?
		What did Chamberlain’s men do during the Battle of
		Gettysburg?
		2
		How many games did Ozzie Smith win?
		How many games did the Cardinals win while Ozzie
		Smith played?
		3
		Did 108th get to the finals?
		Did the US Women’s Soccer Team get to the finals in
		the 1999 World Cup?
		4
		Did Gabriel Batistuta reside in any other
		countries, besides touring in the Copa
		America?
		Besides Argentina, did Gabriel Batistuta reside in any
		other countries?
		5
		Did La Comedia have any more works
		than La Comedia 3?
		Did Giannina Braschi have any more works than United
		States of Banana, La Comedia and Asalto al tiempo?
		Table 5: Example erroneous rewrites generated by the Seq2Seq models and their corresponding reference rewrites
	</Abstractive Summary>
	<Extractive Summary> =
		Another source of errors is having related entities
		mentioned in the context as Example 4 in Table 5,
		where the model confused “Copa America” with
		“Argentina”
	</Extractive Summary>
	<Extractive Summary> =
		Example 5 in Table 5 show the output
		and the reference rewrites of the question “Did she
		have any more works than those 3?”, where two of
		the three entities—“United States of Banana”, “La
		Comedia” and “Asalto al tiempo”—are lost in the
		rewrite
	</Extractive Summary>
</Paper ID=ument604>


<Paper ID=ument605> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Key statistics of QUOREF splits
	</Abstractive Summary>
</Paper ID=ument605>


<Paper ID=ument605> <Table ID =2>
	<Abstractive Summary> =
		How does Arieh’s wife
		die?
		kills herself
		by overdose
		Table 2: Phenomena in QUOREF
	</Abstractive Summary>
	<Extractive Summary> =
		The associ-
		ated question, Which city was bombed?, does not
		really require coreference resolution from a model
		4For example, the last question in Table 2 is about the
		coreference of {she, Fania, his mother}, but none of these
		mentions is the answer
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows percent-
		ages and examples of analyzed questions that fall
		into these two categories
	</Extractive Summary>
</Paper ID=ument605>


<Paper ID=ument605> <Table ID =3>
	<Abstractive Summary> =
		41
		Table 3: Performance of various baselines on QUOREF,
		measured by exact match (EM) and F1
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 3 presents the performance of all baseline
		models on QUOREF
	</Extractive Summary>
</Paper ID=ument605>


<Paper ID=ument606> <Table ID =1>
	<Abstractive Summary> =
		10
		+Chinese
		Table 1: EM/F1 scores over Chinese testing set
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results
		Table 1 shows the result of different models
		trained on either Chinese or English and tested
		on Chinese
	</Extractive Summary>
</Paper ID=ument606>


<Paper ID=ument606> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: EM/F1 score of multi-BERTs ﬁne-tuned on
		different training sets and tested on different languages
		(En: English, Fr: French, Zh: Chinese, Jp: Japanese,
		Kr: Korean, xx-yy: translated from xx to yy)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results of multi-BERT ﬁne-
		tuned on different languages and then tested on
		English , Chinese and Korean
	</Extractive Summary>
	<Extractive Summary> =
		In the lower half of Table 2, the results are ob-
		5935
		tained by the translated training data
	</Extractive Summary>
	<Extractive Summary> =
		1
		The Effect of Machine Translation
		Table 2 shows that ﬁne-tuning on un-translated
		target language data achieves much better per-
		formance than data translated into the target lan-
		guage
	</Extractive Summary>
</Paper ID=ument606>


<Paper ID=ument606> <Table ID =3>
	<Abstractive Summary> =
		68
		Table 3: EM/F1 scores over artiﬁcially created unseen
		languages (English-permuted and Chinese-permuted)
	</Abstractive Summary>
</Paper ID=ument606>


<Paper ID=ument606> <Table ID =4>
	<Abstractive Summary> =
		46
		32%
		Table 4:
		EM/F1 scores on artiﬁcial code-switching
		datasets generated by replacing some of the words in
		English dataset with synonyms in another languages
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		shows that the performance of multi-BERT drops
		drastically on the dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows that on all the code-switching
		datasets, the EM/F1 score drops, indicating that
		the semantics of representations are not totally dis-
		entangled from language
	</Extractive Summary>
</Paper ID=ument606>


<Paper ID=ument606> <Table ID =5>
	<Abstractive Summary> =
		is the substitution ratio of the dataset)
		Source
		Example
		pred:
		second 法 律 of 熱 力 學 (Zh)
		gt:
		second law of thermodynamics
		pred:
		エ レ ク ト リ ッ ク motors (Jp)
		gt:
		electric motors
		pred:
		fermionic nature des lectrons (Fr)
		gt:
		fermionic nature of electrons
		pred:
		the ᄎ
		ᅡᄋ
		ᅵᄌ
		ᅥ
		ᆷ in ᄌ
		ᅡ
		ᆷᄌ
		ᅢᄅ
		ᅧ
		ᆨ ᄋ
		ᅦᄂ
		ᅥᄌ
		ᅵ (Kr)
		gt:
		the difference in potential energy
		Table 5:
		Answers inferenced on code-switching
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		of the answers of the model (Table 5) show that
		multi-BERT could ﬁnd the correct answer spans
		although some keywords in the spans have been
		translated into another language
	</Extractive Summary>
</Paper ID=ument606>


<Paper ID=ument606> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6:
		EM/F1 scores over artiﬁcially created
		typology-manipulated dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows that when we change the English
		typology order to SOV or OSV order, the perfor-
		5937
		Train/Test
		English
		Chinese
		Korean
		En
		81
	</Extractive Summary>
</Paper ID=ument606>


<Paper ID=ument606> <Table ID =7>
	<Abstractive Summary> =
		14
		Table 7: EM/F1 scores on DRCD dev-set
	</Abstractive Summary>
</Paper ID=ument606>


<Paper ID=ument607> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of crowdsourced questions Q and corpus knowledge K in QUARTZ, illustrating phenomena
		5943
		being compared using the template below, illus-
		trated using K2 from Table 1:
		”The smaller its mass is, the greater its accel-
		eration for a given amount of force
	</Abstractive Summary>
	<Extractive Summary> =
		3
		The Task
		Examples of QuaRTz questions Qi are shown
		in Table 1, along with a sentence Ki express-
		ing the relevant qualitative relationship
	</Extractive Summary>
</Paper ID=ument607>


<Paper ID=ument607> <Table ID =2>
	<Abstractive Summary> =
		1/4
		Table 2: Statistics of QUARTZ
	</Abstractive Summary>
</Paper ID=ument607>


<Paper ID=ument607> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Performance of various models on QUARTZ
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results
		The results are shown in Table 3, and provide in-
		sights into both the data and the models:
		1
	</Extractive Summary>
</Paper ID=ument607>


<Paper ID=ument607> <Table ID =4>
	<Abstractive Summary> =
		“eighteen wheeler”
		Table 4: Examples of linguistic and semantic gaps be-
		tween knowledge Ki (left) and question Qi (right)
	</Abstractive Summary>
</Paper ID=ument607>


<Paper ID=ument608> <Table ID =1>
	<Abstractive Summary> =
		NAQANet: −55893
		Ours: Diff(55893, 48341) = 7552
		Table 1: Example from the DROP development set
	</Abstractive Summary>
	<Extractive Summary> =
		For example,
		in Table 1, the model predicts subtraction (Diff)
		over two numbers in the passage, and executes it
		to produce the ﬁnal answer
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 1, the system needs to perform
		fuzzy matching between “from Europe” and “Eu-
		ropean nationals” in order to identify the argu-
		ments
	</Extractive Summary>
	<Extractive Summary> =
		Moreover, each
		number is tagged independently, which can cause
		global inconsistencies; for instance, in Table 1 it
		assigns a single minus label and no plus labels,
		leading to a prediction of negative people
	</Extractive Summary>
</Paper ID=ument608>


<Paper ID=ument608> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Operations supported by the model
	</Abstractive Summary>
</Paper ID=ument608>


<Paper ID=ument608> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Accuracies on the DROP dev and test set in terms of exact match (EM) and token-level F1
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 3 show
		the dev set oracle performance using these shallow
		derivations, by answer type
	</Extractive Summary>
	<Extractive Summary> =
		1
		DROP Results
		As shown in Table 3, our model achieves over
		50% relative improvement (over 33% absolute)
		over the previous state-of-the-art NAQANet sys-
		tem
	</Extractive Summary>
</Paper ID=ument608>


<Paper ID=ument608> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4:
		Accuracy on the Illinois (IL) dataset8of
		562 single-step word problems, using the ﬁve cross-
		validation folds of Roy and Roth (2015)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, when we added Mul and
		Div to our basic DROP operations, the model
		was able to learn to use them
	</Extractive Summary>
</Paper ID=ument608>


<Paper ID=ument609> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of positive question-answer pairs from the StackExchangeQA dataset
		performance as the mean average precision (MAP)
		and mean reciprocal rank (MRR) 3
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows some examples
		of positive question-answer pairs from the dataset
	</Extractive Summary>
</Paper ID=ument609>


<Paper ID=ument609> <Table ID =2>
	<Abstractive Summary> =
		820
		—
		—
		Table 2: Results on the TrecQA and WikiQA datasets
		ture or simply from having more parameters due
		to added complexity
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Comparison with Previous Methods
		Table 2 summarizes the performances of our pro-
		posed models and compares them to the base-
		lines on the TrecQA and WikiQA datasets
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2, we can
		see that both the variants [BERT + GSAMN] and
		[BERT + Transfer Learning] have better perfor-
		mance than the original BERT baseline
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 clearly shows that GSAMN outperforms
		the Transformer based variants, with or without
		the transfer learning component
	</Extractive Summary>
	<Extractive Summary> =
		The results in Table 2 show that
		our model signiﬁcantly outperforms [ELMo +
		Compare-Aggregate] as well
	</Extractive Summary>
</Paper ID=ument609>


<Paper ID=ument61> <Table ID =1>
	<Abstractive Summary> =
		78
		Table 1: Inter-annotator agreements (observed and Cohen’s κ) having access to (a) the text and (b) the text and
		image
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		inter-annotator agreements (observed and Cohen’s
		κ) when annotators have access to (a) only the text
		and (b) the text and image
	</Extractive Summary>
</Paper ID=ument61>


<Paper ID=ument61> <Table ID =2>
	<Abstractive Summary> =
		8%
		Table 2: Label distributions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the label dis-
		tributions for the three annotation tasks: posses-
		sion existence, possession type (yes: alienable
		or control) and interest in the possessee
	</Extractive Summary>
</Paper ID=ument61>


<Paper ID=ument61> <Table ID =3>
	<Abstractive Summary> =
		5%
		Table 3: Distribution of temporal anchor labels depend-
		ing on the possession type (alienable or control)
	</Abstractive Summary>
</Paper ID=ument61>


<Paper ID=ument61> <Table ID =4>
	<Abstractive Summary> =
		, Type
		Temporal
		Interest
		never
		n/a
		no
		unk
		n/a
		yes
		never
		n/a
		no
		Table 4: Annotation examples when annotators have access to the text and image
	</Abstractive Summary>
</Paper ID=ument61>


<Paper ID=ument61> <Table ID =5>
	<Abstractive Summary> =
		0%
		Table 5: Changes in labels depending on whether annotators have access to the image
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 summarizes
		the changes in annotations
	</Extractive Summary>
</Paper ID=ument61>


<Paper ID=ument61> <Table ID =6>
	<Abstractive Summary> =
		, Type
		Temporal
		Interest
		T
		unk
		n/a
		no
		unk
		n/a
		no
		alienable
		B, D, A
		no
		T+I
		alienable
		B, D, A
		yes
		alienable
		B, D, A
		no
		never
		n/a
		no
		Table 6: Examples of tweets which are annotated different depending on whether annotators have access to only
		the text (ﬁrst row of labels, T) or the text and image (second row of labels, T+I)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows examples of changes in annota-
		tion
	</Extractive Summary>
</Paper ID=ument61>


<Paper ID=ument61> <Table ID =7>
	<Abstractive Summary> =
		58
		Table 7: Results for predicting possession existence, possession type and interest in possessee
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Neural Network
		Table 7 presents the results obtained with four ver-
		sions of the neural network: using (a) only the
		text component, (b) the text component and the
		weights from InceptionNet (text + IN), (c) the text
		component and the tags from the Vision API as an
		additional textual input (text + Itags), and (d) the
		full network (text + img)
	</Extractive Summary>
</Paper ID=ument61>


<Paper ID=ument61> <Table ID =8>
	<Abstractive Summary> =
		67
		Table 8: Results for predicting temporal anchors with the neural network (only text, and text and image)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 presents results ob-
		tained with the neural network when predicting
		temporal anchors
	</Extractive Summary>
</Paper ID=ument61>


<Paper ID=ument610> <Table ID =1>
	<Abstractive Summary> =
		47%
		Table 1: Performance of full parroting v
	</Abstractive Summary>
	<Extractive Summary> =
		It can be observed that the
		average metric scores for QUORA are similar to
		the scores in Table 1, whereas the average scores
		for TWITTER are noticeably better than those in
		Table 2
	</Extractive Summary>
</Paper ID=ument610>


<Paper ID=ument610> <Table ID =2>
	<Abstractive Summary> =
		87
		0
		-
		Table 2: Performance of full parroting v
	</Abstractive Summary>
</Paper ID=ument610>


<Paper ID=ument610> <Table ID =3>
	<Abstractive Summary> =
		7%
		Table 3: Performance of full parroting v
	</Abstractive Summary>
</Paper ID=ument610>


<Paper ID=ument610> <Table ID =4>
	<Abstractive Summary> =
		41
		Table 4: Performance of full parroting on randomly sampled test sets
	</Abstractive Summary>
</Paper ID=ument610>


<Paper ID=ument611> <Table ID =1>
	<Abstractive Summary> =
		2
		Related work
		Extractive compression shortens a sentence by
		removing tokens, typically for summarization
		5970
		Approach
		Complexity
		Constrained
		ILP
		exponential
		yes
		LSTM tagger
		linear
		no
		VERTEX ADDITION
		linear
		yes
		Table 1: Our VERTEX ADDITION technique (§3) con-
		structs constrained compressions in linear time
	</Abstractive Summary>
	<Extractive Summary> =
		We evaluate theoretical gains from VERTEX
		ADDITION (Table 1) by measuring empirical la-
		tency
	</Extractive Summary>
	<Extractive Summary> =
		Theoret-
		ical gains (Table 1) create real speedups
	</Extractive Summary>
</Paper ID=ument611>


<Paper ID=ument611> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Test results for constrained compression
	</Abstractive Summary>
	<Extractive Summary> =
		We test VERTEX AD-
		DITIONNN using a CPU: the method is too slow
		for use in search applications in areas without ac-
		cess to specialized hardware (Table 2)
	</Extractive Summary>
</Paper ID=ument611>


<Paper ID=ument612> <Table ID =1>
	<Abstractive Summary> =
		983%
		Dev
		7,023
		6,621
		7,023
		–
		Test
		12,455
		11,695
		12,455
		–
		Table 1: Statistics of Food
	</Abstractive Summary>
</Paper ID=ument612>


<Paper ID=ument612> <Table ID =2>
	<Abstractive Summary> =
		165
		Table 2: Metrics on generated recipes from test set
	</Abstractive Summary>
	<Extractive Summary> =
		All person-
		alized models outperform baseline in BPE per-
		plexity (Table 2) with Prior Name performing the
		best
	</Extractive Summary>
</Paper ID=ument612>


<Paper ID=ument612> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Sample generated recipe
	</Abstractive Summary>
	<Extractive Summary> =
		Qualitative Analysis: We present sample out-
		puts for a cocktail recipe in Table 3, and addi-
		tional recipes in the appendix
	</Extractive Summary>
</Paper ID=ument612>


<Paper ID=ument612> <Table ID =4>
	<Abstractive Summary> =
		78
		Table 4: Coherence metrics on generated recipes from
		test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows that our personalized models
		achieve average recipe-level coherence scores of
		1
	</Extractive Summary>
</Paper ID=ument612>


<Paper ID=ument613> <Table ID =1>
	<Abstractive Summary> =
		13
		Table 1: Experiment results on the test set
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Experiment results are shown in Table 1, from
		which we observe that our methods substantially
		improves the baseline in terms of all evaluation
		metrics
	</Extractive Summary>
</Paper ID=ument613>


<Paper ID=ument613> <Table ID =2>
	<Abstractive Summary> =
		?
		251/57
		111/32
		Table 2: Frequency of generic questions generated by the baseline and our methods
	</Abstractive Summary>
</Paper ID=ument613>


<Paper ID=ument613> <Table ID =3>
	<Abstractive Summary> =
		Answer: catholic services
		Question: what was forbidden in all provinces ?
		Baseline: what was the “ public ” church in the repub-
		lic ?
		QA-based reranking:
		what was forbidden in all
		provinces in the republic ?
		Table 3: QG examples
	</Abstractive Summary>
	<Extractive Summary> =
		In the ﬁrst example displayed in Table 3, the
		phrase “what school” is difﬁcult to be generated as
		it does not appear in the input passage
	</Extractive Summary>
	<Extractive Summary> =
		In the second example of Table 3, the ques-
		tion with the highest score generated by the base-
		line is “what church”, while the ground truth
		question is asking “what was forbidden”
	</Extractive Summary>
</Paper ID=ument613>


<Paper ID=ument614> <Table ID =1>
	<Abstractive Summary> =
		2 by reusing the same
		Dataset
		Size
		Role
		WritingPrompts
		272K Stories
		Story Generation
		BookCorpus
		10K Books
		Domain Adaptation
		SWAG
		73K Questions
		Common Sense
		Synthetic
		250K Pairs
		Common Sense
		Table 1: Datasets, training set sizes and their roles
		softmax layer used for the primary language mod-
		eling task
	</Abstractive Summary>
	<Extractive Summary> =
		Our
		pipeline uses four different datasets (Table 1), each
		of which plays a role in improving model perfor-
		mance
	</Extractive Summary>
</Paper ID=ument614>


<Paper ID=ument614> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Top two highest likelihood story completions
		from 10 random completion samples generated by our
		models when primed with a premise from the Story
		Cloze validation set
	</Abstractive Summary>
	<Extractive Summary> =
		We present example
		story completions in Table 2 and full sampled sto-
		ries in our appendix and Table 4
	</Extractive Summary>
</Paper ID=ument614>


<Paper ID=ument614> <Table ID =3>
	<Abstractive Summary> =
		1%
		Table 3: Performance of models on the test set of WritingPrompts and validation set of SWAG and Story Cloze
	</Abstractive Summary>
	<Extractive Summary> =
		We observe that a pre-trained
		GPT2 performing zero-shot inference on Writing-
		Prompts (GPT2 in Table 3) is a strong baseline
	</Extractive Summary>
</Paper ID=ument614>


<Paper ID=ument614> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Sample generated by GPT2 → BC → WP + SWAG + SYNTH primed with the same prompt as Fan et al
	</Abstractive Summary>
</Paper ID=ument614>


<Paper ID=ument615> <Table ID =1>
	<Abstractive Summary> =
		org/
		Dataset
		#Text
		#Summ
		Train Real(SwissText)
		90K
		90K
		Train RealSynth(Swiss+CC)
		190K
		190K
		Train RealSynthRegen(Swiss+CC)
		190K
		190K
		Dev(SwissText)
		5K
		5K
		Test(SwissText)
		5K
		5K
		Table 1: Statistics of the experimental data which in-
		clude the number of texts and their summaries
	</Abstractive Summary>
</Paper ID=ument615>


<Paper ID=ument615> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Evaluation results of our models on development (dev) and testing (test) sets
	</Abstractive Summary>
	<Extractive Summary> =
		The automatic evaluation
		results based on the dev and test set are shown in
		Table 2 with sample summaries in Table 3
	</Extractive Summary>
	<Extractive Summary> =
		In terms of Rouge
		score model S3 outperforms model S1 but perform
		worse than model S2 (see Table 2)
	</Extractive Summary>
</Paper ID=ument615>


<Paper ID=ument615> <Table ID =3>
	<Abstractive Summary> =
		In the Middle Ages, the ship was
		under the name “Vuolle” 1 “for the Finnish Navy 1
		Table 3: Sample summaries on test set
	</Abstractive Summary>
</Paper ID=ument615>


<Paper ID=ument616> <Table ID =1>
	<Abstractive Summary> =
		77
		Median
		3
		3
		3
		4
		3
		3
		Mode
		2
		2
		3
		4
		4
		2
		Table 1: The average, median and mode of the evalua-
		tion results
		Table 1 shows the median and mode of the re-
		sults in addition to the average values
	</Abstractive Summary>
</Paper ID=ument616>


<Paper ID=ument617> <Table ID =1>
	<Abstractive Summary> =
		843
		Table 1: Spearman’s ρ, Kendall’s τ and Pearson’s r correlations on DUC-05, DUC-06 and DUC-07 for Q1–Q5
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results
		Table 1 shows Spearman’s ρ, Kendall’s τ and
		Pearson’s r for all datasets and models
	</Extractive Summary>
</Paper ID=ument617>


<Paper ID=ument617> <Table ID =2>
	<Abstractive Summary> =
		59)
		Table 2: Mean manual scores (± standard deviation)
		for each Q across datasets
	</Abstractive Summary>
</Paper ID=ument617>


<Paper ID=ument618> <Table ID =1>
	<Abstractive Summary> =
		02
		Table 1: Performance on Quora dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the model per-
		formances on the Quora test set, and Table 2 shows
		the model performances on the Twitter test set
	</Extractive Summary>
	<Extractive Summary> =
		As shown in line 6 – 11 from Table 1, the addi-
		tional training with whichever variant algorithms
		can certainly enhance the generation performance
		over the pre-trained model (line 5)
	</Extractive Summary>
</Paper ID=ument618>


<Paper ID=ument618> <Table ID =2>
	<Abstractive Summary> =
		44
		Table 2: Performance on Twitter dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the model per-
		formances on the Quora test set, and Table 2 shows
		the model performances on the Twitter test set
	</Extractive Summary>
	<Extractive Summary> =
		As shown in line 2 – 7 from Table 2, the
		additional training with variant algorithms also
		shows improved performance over the pre-trained
		model (line 1)
	</Extractive Summary>
	<Extractive Summary> =
		Besides, from Table 2, we
		also ﬁnd that IL (line 6 – 7) outperforms RL (line
		2 – 3), which is consist with the experimental re-
		sults in Table 1
	</Extractive Summary>
</Paper ID=ument618>


<Paper ID=ument619> <Table ID =1>
	<Abstractive Summary> =
		Table 1: ‘Lead’ (ﬁrst 3 sentences of source) can produce
		extremely faithful (top) to disastrously inaccurate (bottom)
		summaries
	</Abstractive Summary>
	<Extractive Summary> =
		, 2018), and the lead baseline, as
		shown in Table 1, does not always produce con-
		vincing summaries
	</Extractive Summary>
</Paper ID=ument619>


<Paper ID=ument619> <Table ID =2>
	<Abstractive Summary> =
		98
		Table 2: BanditSum’s performance—calculated as the average between ROUGE-1,-2, and -L F1—on the validation set of the
		CNN/Daily Mail corpus
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we show BanditSum’s perfor-
		mance,1 when trained and tested on the various
		datasets
	</Extractive Summary>
</Paper ID=ument619>


<Paper ID=ument619> <Table ID =3>
	<Abstractive Summary> =
		13
		Table 3: ROUGE scores for systems
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Results and Discussion
		Table 3 reports the F1 scores for ROUGE-1,-2 and
		-L (Lin, 2004)
	</Extractive Summary>
</Paper ID=ument619>


<Paper ID=ument619> <Table ID =4>
	<Abstractive Summary> =
		96
		Table 4: Average ROUGE-1, -2 and -L F1 scores on Dearly,
		and Dmed, Dlate
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, we can see
		that the auxiliary loss model’s improvements are
		even more ampliﬁed on Dmed and Dlate
	</Extractive Summary>
	<Extractive Summary> =
		The second line in Table 4 reports the ora-
		cle ROUGE scores of the best possible extractive
		summary
	</Extractive Summary>
</Paper ID=ument619>


<Paper ID=ument62> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: LIGHT dataset statistics
	</Abstractive Summary>
</Paper ID=ument62>


<Paper ID=ument62> <Table ID =2>
	<Abstractive Summary> =
		none
		(c) Example objects annotated via object collection tasks
		Table 2: Example entities from the LIGHT environment
	</Abstractive Summary>
	<Extractive Summary> =
		See Table 2a for an example
	</Extractive Summary>
	<Extractive Summary> =
		See Table 2c for examples
	</Extractive Summary>
	<Extractive Summary> =
		See
		Table 2b for detailed examples
	</Extractive Summary>
</Paper ID=ument62>


<Paper ID=ument62> <Table ID =3>
	<Abstractive Summary> =
		chimney
		ship
		bones of the innocent
		Hand carved stone
		a large ornate table
		citrus fruit
		corn
		seagulls on the dock
		adventurer’s remains
		garden bench
		beer keg
		fruit trees
		characters
		chickens
		boat captain
		spirits of our ancestors
		gardener
		tavern owner
		a deer
		fox trying to steal chickens
		captain
		mourner
		stable hand
		bartender
		a songbird
		farmers
		merchant
		zombies
		Garden dog
		Goblin King’s bartender
		fruit bats
		The farmers
		boat workers
		families
		stable boy
		A serving wench
		parent
		farmer
		workers
		bandit
		A stable boy
		Serving wench
		butterﬂy
		locations
		Chicken Pen
		Pirate Ship
		Old Crypt
		Across the King’s Garden
		The werewolves tavern
		Lush meadow
		Corn ﬁeld
		Dock at the Port
		sacristy
		Hidden garden
		Tavern of Browntavia
		Flower Field
		Farmer’s house
		Loading Dock
		Disposal area
		The garden courtyard
		Port Tavern
		ﬂower garden
		Large Farm
		Fishing Dock
		inside temple crypt
		Church garden
		The bar
		Mushroom Hut
		Pig Pen
		crew berthing
		Sacriﬁce Chamber
		Tool Shed
		bazaar outside the royal city
		Archery zone
		actions
		get chicken
		hug pirate
		put torch in coﬃn
		get rake
		hug tavern owner
		get ﬂower from meadow
		hug chicken
		hit pirate
		get torch from coﬃn
		drop Rake
		give food item to tavern owner
		put ﬂower in Meadow
		hit chicken
		steal sword from pirate
		put bone in coﬃn
		steal Rake from gardener
		give telescope to tavern owner
		give Flower to a deer
		give cowbell to chicken
		steal cargo from pirate
		get bone from coﬃn
		give Rake to thing
		drink drink
		give Flower to deer
		steal sword from chicken
		give cargo to pirate
		hit archaeologist
		give Rake to person
		drop drink
		steal Flower from a deer
		vocabulary
		bock
		crew
		archaeologist
		vegetable
		drink
		ﬂower
		tasty
		ye
		robber
		carved
		drinks
		amulet
		bawk
		port
		crypt
		alice
		regular
		songbird
		moo
		sea
		loss
		hook
		item
		wasp
		egg
		seas
		adventures
		exorcisms
		tip
		an
		Table 3: Neighboring Starspace phrase embeddings (no pretraining from other data) for diﬀerent types of entities
		and actions
	</Abstractive Summary>
</Paper ID=ument62>


<Paper ID=ument62> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Ranking model test performance
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		The ranking models are compared in Table 4 on
		the seen and unseen test sets, and ablations are
		shown for both the BERT-based Bi-Ranker and
		679
		Test Seen
		Test Unseen
		Dialogue
		Action
		Emote
		Dialogue
		Action
		Emote
		Method
		R@1/20
		Acc
		Acc
		R@1/20
		Acc
		Acc
		Random baseline
		5
	</Extractive Summary>
	<Extractive Summary> =
		We conducted
		Wilcoxon signed-rank tests and found compar-
		isons between Starspace and the diﬀering trans-
		formers for dialogue on test seen in Table 4 are
		all signiﬁcantly diﬀerent at the p<0
	</Extractive Summary>
	<Extractive Summary> =
		2
		Generalization Capability on Unseen Test
		The six new unseen test settings are a slightly
		easier task in absolute numbers (Table 4, right),
		with improved scores for humans and some mod-
		els
	</Extractive Summary>
</Paper ID=ument62>


<Paper ID=ument62> <Table ID =5>
	<Abstractive Summary> =
		6
		-
		Table 5: BERT-based Bi-Ranker ablations (valid set)
	</Abstractive Summary>
</Paper ID=ument62>


<Paper ID=ument62> <Table ID =6>
	<Abstractive Summary> =
		2
		Table 6: Generative Transformer ablations (valid set)
	</Abstractive Summary>
</Paper ID=ument62>


<Paper ID=ument62> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Predicted dialogue by the BERT-based Bi-Ranker (as the traveler character) given diﬀerent settings
	</Abstractive Summary>
</Paper ID=ument62>


<Paper ID=ument62> <Table ID =8>
	<Abstractive Summary> =
		You
		Mermaid
		laugh
		know I don’t fraternize
		Thief
		frown
		with land dwellers, pout
		He is a terrible knight
		Mermaid
		scream
		and I hate him, cry
		Troll
		laugh
		I will battle him until the
		Mermaid
		stare
		end of my days, scream
		Orc
		nod
		Table 8: Predicted emotes by the Generative Trans-
		former given example inputs from dialogue partner
	</Abstractive Summary>
</Paper ID=ument62>


<Paper ID=ument62> <Table ID =9>
	<Abstractive Summary> =
		Context aﬀects predicted utterances
		We in-
		vestigate the eﬀect of the environmental context
		on the predictions by modifying the context and
		examining the changes in predicted dialogue, ac-
		681
		Input from Partner: Wizard
		Prediction (Self name: Servant)
		I’m feeling sad
		hug wizard
		You must die!
		hit master wizard
		Try putting on something else
		remove patterned outﬁt
		I’d like you to feed me
		give food to master wizard
		Can you grab me a paper
		give book to wizard’s assistant
		Can you grab me a beer
		get beer
		Clean up
		get duster
		Hide the gold
		put gold in satchel
		Input from diﬀerent agents
		Prediction
		Wizard: Can I have some drink?
		drop potion
		Servant: Can I have some drink?
		give wine to servant
		Bear: Can I have some drink?
		give water to bear
		Table 9:
		Predicted actions by the BERT-based Bi-
		Ranker given example inputs from the dialogue partner
	</Abstractive Summary>
	<Extractive Summary> =
		The input dialogue and speaker has a strong ef-
		fect on the predicted action, as shown in Table 9,
		ranking over all training set actions
	</Extractive Summary>
</Paper ID=ument62>


<Paper ID=ument620> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Sampling efﬁciency: We obtain 10K samples of
		poetry without additional intervention during decoding, and
		report the expected samples as inverse of the fraction of sam-
		ples satisfying valid rhyming patterns for the corresponding
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		As can be observed from Table 1, RHYME-
		GAN needs fewer samples than other methods
		to produce an acceptable quatrain or a limerick,
		indicating that it has learned natural rhyming
		structures more effectively from data
	</Extractive Summary>
</Paper ID=ument620>


<Paper ID=ument620> <Table ID =2>
	<Abstractive Summary> =
		49
		Table 2: Held out negative log likelihood per token for po-
		ems in test split
	</Abstractive Summary>
	<Extractive Summary> =
		Likelihood on held out data We report negative
		log likelihood (NLL) on test splits (Table 2)
	</Extractive Summary>
</Paper ID=ument620>


<Paper ID=ument620> <Table ID =3>
	<Abstractive Summary> =
		92
		Table 3:
		Rhyming probe: We use the cosine similarity
		score of the learned representations to predict a word pair
		as rhyming or not, and report F1 score for this classiﬁcation
		task
	</Abstractive Summary>
	<Extractive Summary> =
		90 (Table 3) on the
		test split (threshold chosen to maximize f1 on
		dev split)
	</Extractive Summary>
</Paper ID=ument620>


<Paper ID=ument621> <Table ID =1>
	<Abstractive Summary> =
		29%
		Table 1: Proportions of each type of questions on two datasets
	</Abstractive Summary>
	<Extractive Summary> =
		According to the statistics on SQuAD, 78% of
		questions in the training set begin with the 7 most
		common used question words as Table 1 shows
	</Extractive Summary>
</Paper ID=ument621>


<Paper ID=ument621> <Table ID =2>
	<Abstractive Summary> =
		52
		Table 2: Upper bound analysis by incorporating question words with different ways
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 2, comparing with the baseline model, the
		performance gets 3
	</Extractive Summary>
</Paper ID=ument621>


<Paper ID=ument621> <Table ID =3>
	<Abstractive Summary> =
		59
		Table 3: Experimental results of our model comparing with previous methods on two datasets
	</Abstractive Summary>
</Paper ID=ument621>


<Paper ID=ument621> <Table ID =4>
	<Abstractive Summary> =
		48%
		Table 4: Experiments of the beginning question word
		accuracy
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 displays the BQWA of two
		models on both datasets
	</Extractive Summary>
</Paper ID=ument621>


<Paper ID=ument621> <Table ID =5>
	<Abstractive Summary> =
		10
		Table 5:
		Experiments on different settings of our
		model
	</Abstractive Summary>
</Paper ID=ument621>


<Paper ID=ument621> <Table ID =6>
	<Abstractive Summary> =
		Reference: Which article made provisions for concentra-
		tions or mergers and the abuse of a dominant position by
		companies?
		Baseline: How many provisions made provisions for con-
		centrations?
		Uniﬁed-model: Which article made provisions for con-
		centrations, or mergers?
		Table 6: Examples of generated questions
	</Abstractive Summary>
	<Extractive Summary> =
		Case Study To show the effect of question words
		prediction on question generation, Table 6 lists
		some typical examples
	</Extractive Summary>
</Paper ID=ument621>


<Paper ID=ument622> <Table ID =1>
	<Abstractive Summary> =
		96
		Table 1: Results on Gigawords and CNN/Daily Mail for abstractive summarization
	</Abstractive Summary>
</Paper ID=ument622>


<Paper ID=ument622> <Table ID =2>
	<Abstractive Summary> =
		3%
		Table 2:
		Human evaluation results on Gigaword and CNN/Daily Mail for abstractive summarization
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, DSR and DSR+XENT mod-
		els improve the relevance and ﬂuency of generated
		summary signiﬁcantly
	</Extractive Summary>
</Paper ID=ument622>


<Paper ID=ument622> <Table ID =3>
	<Abstractive Summary> =
		35
		Table 3: Qualitative analysis on repetition(Rep) / di-
		versity(Div)
	</Abstractive Summary>
</Paper ID=ument622>


<Paper ID=ument622> <Table ID =4>
	<Abstractive Summary> =
		47}
		Table 4: Qualitative analysis of generated samples on Gigaword corpus
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, we list a few generated
		samples on Gigaword Corpus
	</Extractive Summary>
	<Extractive Summary> =
		In our ﬁrst exam-
		ple in Table 4, the word “sino-german” provides
		Model
		Gigawords
		CNN/Daily Mail
		Rep(%) Div(%)
		Rep(%) Div(%)
		ROUGE+XENT
		11
	</Extractive Summary>
	<Extractive Summary> =
		In addition, the
		second example in Table 4 shows that RL model
		with DSR corrects the sentence’ grammar and sig-
		niﬁcantly improves the FBERT score by switching
		“down” to an unseen word “drops”
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 gives an ex-
		ample that DSR produces a repeated word (from
		example 4), but it does not reﬂect the overall dis-
		tribution of repeated word generation for all eval-
		uated models
	</Extractive Summary>
</Paper ID=ument622>


<Paper ID=ument623> <Table ID =1>
	<Abstractive Summary> =
		7%
		-
		Table 1: Accuracy of exact SQL matching with different hardness levels
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Result and Analysis
		Table 1 shows the exact SQL matching accuracy
		of our model and previous models
	</Extractive Summary>
</Paper ID=ument623>


<Paper ID=ument623> <Table ID =2>
	<Abstractive Summary> =
		5%
		Table 2: F1 scores of SQL component matching on the dev set
	</Abstractive Summary>
</Paper ID=ument623>


<Paper ID=ument623> <Table ID =3>
	<Abstractive Summary> =
		student id ORDER BY *
		DESC LIMIT 1
		Table 3: Sample SQL predictions by our model and previous state-of-the-art models on the dev split
	</Abstractive Summary>
	<Extractive Summary> =
		A
		Sample SQL Predictions
		In Table 3, we show some examples of predicted
		SQL queries from different models
	</Extractive Summary>
</Paper ID=ument623>


<Paper ID=ument624> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example triples and retrieved premise sentences, with labels, used for training word embedding-based
		models and language model ﬁne-tuning
	</Abstractive Summary>
</Paper ID=ument624>


<Paper ID=ument624> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: Final label distribution
		rious examples, such as a black guitar’s cord being
		black, where the cord is likely black, but not as a
		result of the guitar being black
	</Abstractive Summary>
</Paper ID=ument624>


<Paper ID=ument624> <Table ID =3>
	<Abstractive Summary> =
		785
		Table 3: Test set results for multi-class prediction
		Test set results for these methods are in Table 3
	</Abstractive Summary>
</Paper ID=ument624>


<Paper ID=ument625> <Table ID =1>
	<Abstractive Summary> =
		44
		Table 1: Performance for answer sentence selection on
		WikiQA dataset
	</Abstractive Summary>
</Paper ID=ument625>


<Paper ID=ument625> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Accuracy on SNLI dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the comparisons of our method
		with the state-of-the-art methods on the SNLI
		dataset
	</Extractive Summary>
</Paper ID=ument625>


<Paper ID=ument626> <Table ID =1>
	<Abstractive Summary> =
		339
		Table 1: Precision@K (average within examples sharing the same target words / average within examples sharing
		the same (target word, deﬁnition)) and cosine distance for models using various input features
	</Abstractive Summary>
	<Extractive Summary> =
		For Seen experiments, Table 1 shows that
		the context-dependent component contains abun-
		dant sense-informative cues, where contextualized
		word embeddings, especially BERT, expresses the
		strong capability of producing corresponding def-
		initions with about 15% enhancement of P@1
		comparing to the second baseline
	</Extractive Summary>
</Paper ID=ument626>


<Paper ID=ument626> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: BLEU@4 / ROUGE-L:F scores of NLG-based
		models and various proposed architectures
	</Abstractive Summary>
	<Extractive Summary> =
		The results are in Table 2, where both No-
		raset et al
	</Extractive Summary>
</Paper ID=ument626>


<Paper ID=ument626> <Table ID =3>
	<Abstractive Summary> =
		3rd Deﬁnition: propel a ball with a bat racket stick etc to score runs or points in a game
		Ground Truth: hit the ball so that it deviates slightly usually as a result of spin
		Table 3: The analysis of the top 3 selected deﬁnitions on the Unseen task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3
		shows a randomly-sampled example from the out-
		put of the BERT-base model
	</Extractive Summary>
</Paper ID=ument626>


<Paper ID=ument626> <Table ID =4>
	<Abstractive Summary> =
		64
		Table 4: The results on Word-in-Context (WiC) data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows
		that the proposed model with contextualized word
		embeddings outperforms all previous models
	</Extractive Summary>
</Paper ID=ument626>


<Paper ID=ument627> <Table ID =1>
	<Abstractive Summary> =
		Textbook
		Train
		Test
		Correct
		Incorrect
		Partial
		Correct
		Incorrect
		Partial
		Phy
		9,676
		1,197
		2,524
		4,784
		593
		1,321
		Gov
		9,405
		3,483
		2,433
		4,784
		1,699
		1,177
		Psy-I
		8,151
		736
		1,700
		4,144
		354
		795
		Psy-II
		8,324
		755
		2,588
		4,146
		370
		1,317
		Table 1: Class-wise student answer distribution in train
		and test sets for the four domains used in experiments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the train-test splits for each of the domains
	</Extractive Summary>
</Paper ID=ument627>


<Paper ID=ument627> <Table ID =2>
	<Abstractive Summary> =
		3M
		Phy + Gov + Psy-I,II
		Table 2: Size and domain of pre-training text corpora
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the
		sizes of the textbook corpora (1
	</Extractive Summary>
</Paper ID=ument627>


<Paper ID=ument627> <Table ID =3>
	<Abstractive Summary> =
		74
		Table 3:
		Effect of domain data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, a combined QA dataset from Psy
		and Gov subjects is used for running additional
		steps of LM pre-training
	</Extractive Summary>
</Paper ID=ument627>


<Paper ID=ument627> <Table ID =4>
	<Abstractive Summary> =
		41
		Table 4:
		Effect of Question-Answer data in seen-
		domains scenario
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		reports the corresponding results showing that the
		QA data helps improve the model for all the do-
		mains, consistently
	</Extractive Summary>
</Paper ID=ument627>


<Paper ID=ument628> <Table ID =1>
	<Abstractive Summary> =
		6079
		Count of
		Train
		Dev
		Test
		Total
		Topics
		87
		23
		12
		122
		Paragraphs
		261
		77
		41
		379
		Inﬂuence graphs
		1453
		424
		230
		2107
		Questions
		29808
		6894
		3993
		40695
		# Questions
		Train
		Dev
		Test
		Total
		Question
		in-para
		7303
		1655
		935
		9893
		type
		out-of-para
		12567
		2941
		1598
		17108
		no-eﬀect
		9936
		2298
		1460
		13694
		Total
		29808
		6894
		3993
		40695
		Number
		#hops=1
		6754
		1510
		835
		9099
		of hops
		#hops=2
		8969
		2145
		1153
		12267
		(in- & out-
		#hops=3
		4149
		941
		545
		5635
		of-para qns)
		Total
		19872
		4596
		2533
		27001
		Table 1: Dataset statistics
		cannot be made
	</Abstractive Summary>
</Paper ID=ument628>


<Paper ID=ument628> <Table ID =2>
	<Abstractive Summary> =
		33
		Table 2: Comparing models on WIQA test partition
		Figure 3: Accuracy of the best baselines drops as num-
		ber of hops increase, quicker for ‘no para’ version
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Prediction Accuracy
		The results (Table 2) provide several insights:
		1
	</Extractive Summary>
</Paper ID=ument628>


<Paper ID=ument628> <Table ID =3>
	<Abstractive Summary> =
		67
		Table 3: As the Bert model (that has access to the
		paragraph in context) reads more paragraphs in con-
		text, its accuracy is better
	</Abstractive Summary>
</Paper ID=ument628>


<Paper ID=ument629> <Table ID =1>
	<Abstractive Summary> =
		2)
		Table 1: Examples from the CommitmentBank, with
		NLI class and original annotation mean
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 con-
		tains examples from CB with the original mean
		annotation and the gold NLI label
	</Extractive Summary>
</Paper ID=ument629>


<Paper ID=ument629> <Table ID =2>
	<Abstractive Summary> =
		com/mcdm/CommitmentBank
		Entailment
		Neutral
		Contradiction
		Total
		Train
		115
		16
		119
		250
		Dev
		23
		5
		28
		56
		Test
		113
		16
		121
		250
		Total
		251
		37
		268
		556
		Table 2: Number of items with each gold label in each
		split
	</Abstractive Summary>
</Paper ID=ument629>


<Paper ID=ument629> <Table ID =3>
	<Abstractive Summary> =
		An ##tar ’
		Base
		‘ ‘ I hope
		guy
		: I don ’
		notice
		An ##tar ’ s
		radio
		‘ I hope you
		jury
		, I mean ,
		Table 3: Unigrams and 4-grams with top 5 PMI with each class for the hypotheses and premises
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 gives the ﬁve uni-
		grams and 4-grams with the highest PMI values
	</Extractive Summary>
</Paper ID=ument629>


<Paper ID=ument629> <Table ID =4>
	<Abstractive Summary> =
		8
		-
		Table 4:
		Performance on the CB test set and the
		MultiNLI dev set
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 4 shows the results
	</Extractive Summary>
</Paper ID=ument629>


<Paper ID=ument629> <Table ID =5>
	<Abstractive Summary> =
		5
		Table 5: F1 scores of the three BERT models on the
		CB test set divided by whether Heuristics predicts the
		correct label (size of each subset in parentheses)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 reports the models’ F1
		scores on the two subsets
	</Extractive Summary>
</Paper ID=ument629>


<Paper ID=ument629> <Table ID =6>
	<Abstractive Summary> =
		Hypothesis: he was Franz Kafka
		H & B: entailment Gold: contradiction
		Table 6: Items in the test set with predictions the by
		Heuristics baseline (H) and MNLI+CBB (B)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows some items on which
		MNLI+CBB still fails
	</Extractive Summary>
</Paper ID=ument629>


<Paper ID=ument63> <Table ID =1>
	<Abstractive Summary> =
		” The agent starts at a ran-
		Request
		Multimodal Simulated
		Problem
		assistance instructions
		humans
		VLN
		
		
		
		VNLA
		
		
		
		CVDN
		
		
		
		HANNA (this work)
		
		
		
		Table 1: Comparing HANNA with other photo-realistic
		navigation problems
	</Abstractive Summary>
	<Extractive Summary> =
		Nevertheless, mod-
		eling human assistance in these problems remains
		simplistic (Table 1): they either do not incorporate
		the ability to request additional help while execut-
		ing tasks (Misra et al
	</Extractive Summary>
</Paper ID=ument63>


<Paper ID=ument63> <Table ID =2>
	<Abstractive Summary> =
		The
		Split
		Environments
		Tasks
		ANNA Instructions
		Train
		51
		82,484
		8,586
		Val SeenEnv
		51
		5,001
		4,287
		Val UnseenAll
		7
		5,017
		2,103
		Test SeenEnv
		51
		5,004
		4,287
		Test Unseen
		10
		5,012
		2,331
		Table 2: Data split
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the dataset
		split
	</Extractive Summary>
</Paper ID=ument63>


<Paper ID=ument63> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Results on test splits
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 3, we see that our
		problem is challenging:
		simple heuristic-based
		baselines such as RANDOMWALK and FOR-
		WARD10 attain success rates less than 7%
	</Extractive Summary>
</Paper ID=ument63>


<Paper ID=ument63> <Table ID =4>
	<Abstractive Summary> =
		45
		Table 4: Success rates (%) of agents on test splits with
		different types of assistance
	</Abstractive Summary>
	<Extractive Summary> =
		As
		seen in Table 4, the improvement from language
		on TEST UNSEENALL (+15
	</Extractive Summary>
</Paper ID=ument63>


<Paper ID=ument63> <Table ID =5>
	<Abstractive Summary> =
		8
		Table 5: Success rates (%) of different help-request
		policies on test splits
	</Abstractive Summary>
	<Extractive Summary> =
		Is learning to request help effective?
		Table 5
		compares our learned help-request policies with
		baselines
	</Extractive Summary>
</Paper ID=ument63>


<Paper ID=ument63> <Table ID =6>
	<Abstractive Summary> =
		10
		Table 6: Results on TEST UNSEENALL of our model,
		trained with and without curiosity-encouraging loss,
		and an LSTM-based encoder-decoder model (both
		models have about 15M parameters)
	</Abstractive Summary>
	<Extractive Summary> =
		As seen in Table 6, our hierarchical re-
		current model outperforms this model by a large
		margin on TEST UNSEENALL (+28
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows that training
		with the curiosity-encouraging objective reduces
		the chance of the agent looping and making the
		same decisions repeatedly
	</Extractive Summary>
</Paper ID=ument63>


<Paper ID=ument630> <Table ID =1>
	<Abstractive Summary> =
		04%
		Table 1: Performance of our models (bottom four)
		along with the state-of-the-art baseline models (top
		four)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 represents the performance compar-
		ison of our proposed models and the baselines,
		which shows that incorporation of knowledge
		graph embeddings helps to improve the model per-
		formance
	</Extractive Summary>
</Paper ID=ument630>


<Paper ID=ument631> <Table ID =1>
	<Abstractive Summary> =
		Sinhala–English and English–Sinhala), we built
		test sets with mixed original-translationese (Ba-
		orig
		lang
		dev
		devtest
		test
		uniq
		tot
		uniq
		tot
		uniq
		tot
		Nepali–English
		English
		693
		1,181
		800
		1,393
		850
		1,462
		Nepali
		825
		1,378
		800
		1,442
		850
		1,462
		1,518
		2,559
		1,600
		2,835
		1,700
		2,924
		Sinhala–English
		English
		1,123
		1,913
		800
		1,395
		850
		1,465
		Sinhala
		565
		985
		800
		1,371
		850
		1,440
		1,688
		2,898
		1600
		2,766
		1700
		2,905
		Table 1: Number of unique sentences (uniq) and to-
		tal number of sentence pairs (tot) per FLORES test set
		grouped by their original languages
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1 we present the statistics of the re-
		sulting sets
	</Extractive Summary>
</Paper ID=ument631>


<Paper ID=ument631> <Table ID =2>
	<Abstractive Summary> =
		7M
		Table 2: Parallel, comparable, and monolingual data
		used in experiments in §4
	</Abstractive Summary>
	<Extractive Summary> =
		We report BLEU on a held-out subset of
		1,000 sentences from the Open Subtitles training data
		(see Table 2) and on devtest (see §3)
	</Extractive Summary>
</Paper ID=ument631>


<Paper ID=ument631> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: BLEU scores of NMT using various learning settings on devtest (see §3)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 reports results using NMT in
		all the other learning conﬁgurations described in
		§4
	</Extractive Summary>
</Paper ID=ument631>


<Paper ID=ument631> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: Weakly supervised experiments: Adding noisy par-
		allel data from ﬁltered Paracrawl improves translation quality
		in some conditions
	</Abstractive Summary>
	<Extractive Summary> =
		This is studied in more depth in Table 4
		for Sinhala–English and Nepali–English
	</Extractive Summary>
</Paper ID=ument631>


<Paper ID=ument631> <Table ID =5>
	<Abstractive Summary> =
		5 (+542%)
		Table 5: In-domain vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows that
		translation quality on in-domain data is between
		10 and 16 BLEU points higher
	</Extractive Summary>
</Paper ID=ument631>


<Paper ID=ument631> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: Distribution of the topics of the sentences in the dev, devtest and test sets according to the Wikipedia
		document they were sampled from
	</Abstractive Summary>
	<Extractive Summary> =
		In Appendix Table 6, we present the aggre-
		gate distribution of topics per sentence for the
		datasets in Nepali–English and Sinhala–English,
		which shows a diverse representation of topics
		ranging from General (e
	</Extractive Summary>
</Paper ID=ument631>


<Paper ID=ument631> <Table ID =7>
	<Abstractive Summary> =
		Nepali–English
		English–Nepali
		Sinhala–English
		English–Sinhala
		Automatic ﬁltering
		14%
		18%
		24%
		7%
		Manual ﬁltering
		Translation quality
		10%
		19%
		13%
		16%
		Fluency
		10%
		-
		17%
		-
		Table 7: Percentage of translations that did not pass the automatic and manual ﬁltering checks
	</Abstractive Summary>
</Paper ID=ument631>


<Paper ID=ument631> <Table ID =8>
	<Abstractive Summary> =
		org
		Sedai+
		Religion
		Table 8: List of documents by Wikipedia domain, their document name or English translation, and corresponding
		topics
	</Abstractive Summary>
</Paper ID=ument631>


<Paper ID=ument631> <Table ID =9>
	<Abstractive Summary> =
		Table 9: Examples of sentences from the En-Ne, Ne-En, En-Si and Si-En devtest set
	</Abstractive Summary>
</Paper ID=ument631>


<Paper ID=ument632> <Table ID =1>
	<Abstractive Summary> =
		71
		— —
		— —
		Table 1: The performance (BLEU) of CMLMs with mask-predict, compared to other parallel decoding machine
		translation methods
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows that among the parallel decoding
		methods, our approach yields the highest BLEU
		scores by a considerable margin
	</Extractive Summary>
</Paper ID=ument632>


<Paper ID=ument632> <Table ID =2>
	<Abstractive Summary> =
		65
		Table 2: The performance (BLEU) of CMLMs with mask-predict, compared to the standard (sequential) trans-
		former on WMT’ 17 EN-ZH
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows that these trends also hold for
		English-Chinese translation, in both directions,
		despite major linguistic differences between the
		two languages
	</Extractive Summary>
</Paper ID=ument632>


<Paper ID=ument632> <Table ID =3>
	<Abstractive Summary> =
		61%
		Table 3: The performance (BLEU) and percentage of
		repeating tokens when decoding with a different num-
		ber of mask-predict iterations (T)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows that, indeed, the proportion of
		repetitive tokens drops drastically during the ﬁrst
		2-3 iterations
	</Extractive Summary>
</Paper ID=ument632>


<Paper ID=ument632> <Table ID =4>
	<Abstractive Summary> =
		1
		Table 4: The performance (BLEU) of base CMLM
		with different amounts of mask-predict iterations (T)
		on WMT’14 EN-DE, bucketed by target sequence
		length (N)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows that increasing the number of de-
		coding iterations (T) appears to mainly improve
		the performance on longer sequences
	</Extractive Summary>
</Paper ID=ument632>


<Paper ID=ument632> <Table ID =5>
	<Abstractive Summary> =
		20
		—
		Table 5: The performance (BLEU) of base CMLM
		with 10 mask-predict iterations (T = 10), varied by the
		number of length candidates (ℓ), compared to decoding
		with the reference target length (Gold)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows that having multiple candidates
		can increase performance almost as much as con-
		ditioning on the gold length
	</Extractive Summary>
</Paper ID=ument632>


<Paper ID=ument632> <Table ID =6>
	<Abstractive Summary> =
		08
		Table 6: The performance (BLEU) of base CMLM,
		trained with either raw data (Raw) or knowledge dis-
		tillation from an autoregressive model (Dist)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows that in every case, training with
		model distillation substantially outperforms train-
		ing on raw data
	</Extractive Summary>
</Paper ID=ument632>


<Paper ID=ument633> <Table ID =1>
	<Abstractive Summary> =
		com/THUNLP-MT/L2Copy4APE
		src
		I ate a cake yesterday
		mt
		Ich esse einen Hamburger
		pe
		Ich hatte gestern einen Kuchen gegessen
		Table 1:
		Example of automatic post-editing (APE)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 1, two German words “Ich” and “einen” oc-
		cur in both mt and pe
	</Extractive Summary>
	<Extractive Summary> =
		, “einen” in Table 1)
	</Extractive Summary>
</Paper ID=ument633>


<Paper ID=ument633> <Table ID =2>
	<Abstractive Summary> =
		PBSMT
		training set
		23,000
		dev2016
		1,000
		test2016
		2,000
		test2017
		2,000
		NMT
		training set
		13,442
		dev2018
		1,000
		Additional
		artiﬁcial-small
		526,368
		artiﬁcial-big
		4,391,180
		eSCAPE-PBSMT
		7,258,533
		eSCAPE-NMT
		7,258,533
		Table 2: Statistics of the English-German datasets in
		the WMT APE task
	</Abstractive Summary>
</Paper ID=ument633>


<Paper ID=ument633> <Table ID =3>
	<Abstractive Summary> =
		99
		Table 3: Effect of α and λ
	</Abstractive Summary>
</Paper ID=ument633>


<Paper ID=ument633> <Table ID =4>
	<Abstractive Summary> =
		62
		Table 4: Results on the English-German PBSMT sub-task
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Main Results
		Results on the PBSMT Sub-task
		Table 4 shows the results of the PBSMT sub-task
	</Extractive Summary>
</Paper ID=ument633>


<Paper ID=ument633> <Table ID =5>
	<Abstractive Summary> =
		40
		Table 5:
		Experiments on the WMT 2018 English-
		German NMT sub-task
	</Abstractive Summary>
	<Extractive Summary> =
		Results on the NMT Sub-task
		Table 5 shows the results on the NMT sub-task
	</Extractive Summary>
</Paper ID=ument633>


<Paper ID=ument633> <Table ID =6>
	<Abstractive Summary> =
		92
		Table 6: Ablation study
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Ablation Study
		Table 6 shows the results of ablation study
	</Extractive Summary>
</Paper ID=ument633>


<Paper ID=ument633> <Table ID =7>
	<Abstractive Summary> =
		61
		Table 7: Comparison of copying accuracies
	</Abstractive Summary>
	<Extractive Summary> =
		63% (see Table 7)
	</Extractive Summary>
	<Extractive Summary> =
		Table 7 shows the comparison of copying ac-
		curacies between MS UEDIN, COPYNET, and our
		approach
	</Extractive Summary>
</Paper ID=ument633>


<Paper ID=ument634> <Table ID =1>
	<Abstractive Summary> =
		org/wiki/Word_
		Association
		Cue
		R1
		R2
		R3
		way
		path
		via
		method
		extra
		plus
		special
		additional
		i
		you
		me
		eye
		come
		go
		closer
		on
		than
		then
		there
		though
		son
		daughter
		sun
		boy
		mind
		brain
		cognition
		thinking
		Table 1: Examples of word association test records in
		SWOWEN datasets (De Deyne et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists some
		examples of the test
	</Extractive Summary>
</Paper ID=ument634>


<Paper ID=ument634> <Table ID =2>
	<Abstractive Summary> =
		he / she
		brother / sister
		father / mother
		man / woman
		son / daughter
		boy / girl
		husband / wife
		uncle / aunt
		grandfather / grandmother
		gentleman / lady
		Table 2: Gender-speciﬁc word pairs in L
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 is the L we use by
		default
	</Extractive Summary>
</Paper ID=ument634>


<Paper ID=ument634> <Table ID =3>
	<Abstractive Summary> =
		98
		Table 3: Validation of stereotype scores
	</Abstractive Summary>
	<Extractive Summary> =
		census data
		(census data in Table 3) 4 (Ruggles et al
	</Extractive Summary>
	<Extractive Summary> =
		based crowd workers (human judgments
		in Table 3) (Bolukbasi et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 lists the correlation and implicit associ-
		ation test results
	</Extractive Summary>
</Paper ID=ument634>


<Paper ID=ument634> <Table ID =4>
	<Abstractive Summary> =
		concept
		words
		career
		executive, professional,
		corporation, salary, ofﬁce, business, career
		family
		home, parents, children, family,
		cousin, marriage, wedding, relatives
		maths
		math, algebra, geometry, calculus,
		equation, compute, numbers, addition
		arts
		poetry, art, dance, literature, novel,
		symphony, drama
		science
		science, technology, physics, chemistry,
		Einstein, NASA, experiment, astronomy
		Table 4: Words in each concept
	</Abstractive Summary>
</Paper ID=ument634>


<Paper ID=ument634> <Table ID =5>
	<Abstractive Summary> =
		24
		Table 5: Descriptive statistics of graphs
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Graph Properties
		We summarize some descriptive statistics of the
		three graphs in Table 5, where L is the average
		shortest path length between every possible word
		pair
	</Extractive Summary>
	<Extractive Summary> =
		From Table 5, we ﬁnd that all three graphs ex-
		hibit small world properties
	</Extractive Summary>
</Paper ID=ument634>


<Paper ID=ument634> <Table ID =6>
	<Abstractive Summary> =
		6140
		Type
		Word Association Graph
		Word Embedding Graph
		semantic priming
		boy - energy - carbohydrate
		boy - kid - really - actually
		girl - potential - maybe
		girl - person
		neighborhood effects
		son - some
		X
		she - pronoun - pro - projection
		frequency effects
		husband - married - a - few
		X
		girl - lady - the - article
		co-occurrence
		father - respect - others
		he - even - more
		mother - birth - day
		she - cook - pickle
		Table 6: Examples of different connection types between words
	</Abstractive Summary>
</Paper ID=ument634>


<Paper ID=ument634> <Table ID =7>
	<Abstractive Summary> =
		34
		< 10−10
		Table 7: Correlation analysis between word associa-
		tion bias scores and bias scores from de-biased word
		embeddings
	</Abstractive Summary>
</Paper ID=ument634>


<Paper ID=ument635> <Table ID =1>
	<Abstractive Summary> =
		Hence, a se-
		quence to sequence module analogous to Section
		6147
		5-grams: getting up for school facts, getting yelled at by people, trying to schedule my classes, feeling like every single
		person, walking to class in pouring, making people who already hate, working on my last day, spending countless hours
		at doctors, getting overdraft statements in mail
		4-grams: talking about world politics, stuck in a generation, sitting in class wondering, canceled at short notice,
		distancing myself from certain, wipe my own tears
		3-grams: born not breathing, paid to sleep, scared those faces, taking a shower, starting your monday, accused of
		everything, worrying about someone, ﬁght jealousy arguments, license to trill, awarded literature prize
		2-grams: scratching itchy, looking chair, getting hiv, shot ﬁrst, collecting death, lost respect
		1-gram: canceled, sleeping, trying, buying, stapling
		Table 1: Example negative situations extracted using bootstrapping technique (Riloff et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows some
		example negative situation phrases extracted from
		our dataset
	</Extractive Summary>
</Paper ID=ument635>


<Paper ID=ument635> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Evaluation results for our system and various baselines
	</Abstractive Summary>
</Paper ID=ument635>


<Paper ID=ument635> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Human judgment scores for various systems
		our system are more sarcastic than the comparison
		systems
	</Abstractive Summary>
</Paper ID=ument635>


<Paper ID=ument635> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Examples of generated outputs from different
		systems
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents example generations from dif-
		ferent systems
	</Extractive Summary>
</Paper ID=ument635>


<Paper ID=ument636> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Statistics of our dataset
		2017), we deﬁne the following loss function:
		Lrl = E
		S∈S[(RR(STS) − τ) log P(S|Ds(Et(TS))],
		(5)
		where τ is a manually set threshold
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the statistics of our training,
		validation and test set
	</Extractive Summary>
</Paper ID=ument636>


<Paper ID=ument636> <Table ID =2>
	<Abstractive Summary> =
		Table 2: A few poems generated by our model from their corresponding vernacular paragraphs
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Reborn Poems: Generating Poems from
		Vernacular Translations
		As illustrated in Table 2 (ID 1)
	</Extractive Summary>
	<Extractive Summary> =
		As illustrated in Table 2, both para-
		graph 2 and 3 are generated from pop song lyrics,
		paragraph 2 uses many poetic images from clas-
		sical literature (e
	</Extractive Summary>
	<Extractive Summary> =
		For ex-
		ample, in Table 2, both paragraph 4 (more descrip-
		tive) and paragraph 5 (more philosophical) were
		6162
		Literature form
		Fluency
		Semantic
		coherence
		Semantic
		preservability
		Poeticness
		Total
		Prose
		2
	</Extractive Summary>
</Paper ID=ument636>


<Paper ID=ument636> <Table ID =3>
	<Abstractive Summary> =
		46
		Table 3: Perplexity and BLEU scores of generating poems from vernacular translations
	</Abstractive Summary>
	<Extractive Summary> =
		We report mean
		perplexity and BLEU scores in Table 3 (Where
		+Anti OT refers to adding the reinforcement loss
		to mitigate over-ﬁtting and +Anti UT refers to
		adding phrase segmentation-based padding to mit-
		igate under-translation), human evaluation results
		in Table 4
	</Extractive Summary>
</Paper ID=ument636>


<Paper ID=ument636> <Table ID =4>
	<Abstractive Summary> =
		13
		Table 4: Human evaluation results of generating poems from vernacular translations
	</Abstractive Summary>
</Paper ID=ument636>


<Paper ID=ument636> <Table ID =5>
	<Abstractive Summary> =
		91
		Table 5: Human evaluation results for generating poems from various literature forms
	</Abstractive Summary>
</Paper ID=ument636>


<Paper ID=ument636> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Examples of generated poems and their cor-
		responding gold poems used in human discrimination
		test
	</Abstractive Summary>
	<Extractive Summary> =
		3
		As demonstrated in Table 6, although the gen-
		eral meanings in human poems and generated po-
		ems seem to be the same, the wordings they em-
		ploy are quite different
	</Extractive Summary>
	<Extractive Summary> =
		As demonstrated in Table 6, the last two sen-
		tences in both human poems (marked as red) echo
		each other well, while the sentences in machine-
		3We did not require the expert group’s participation as
		many of them have known the gold poems already
	</Extractive Summary>
</Paper ID=ument636>


<Paper ID=ument636> <Table ID =7>
	<Abstractive Summary> =
		8
		Table 7: The performance of human discrimination
		test
	</Abstractive Summary>
	<Extractive Summary> =
		According to the test
		results in Table 7, human evaluators only achieved
		65
	</Extractive Summary>
</Paper ID=ument636>


<Paper ID=ument636> <Table ID =8>
	<Abstractive Summary> =
		75
		Table 8:
		Perplexity and BLEU scores of different
		padding schemas
	</Abstractive Summary>
</Paper ID=ument636>


<Paper ID=ument637> <Table ID =1>
	<Abstractive Summary> =
		6168
		Model
		Accuracy
		SVM
		73 %
		Logistic Regression
		91 %
		Table 1: Instruction vs
	</Abstractive Summary>
</Paper ID=ument637>


<Paper ID=ument637> <Table ID =2>
	<Abstractive Summary> =
		30
		Table 2: Evaluation I: Content Generation
	</Abstractive Summary>
</Paper ID=ument637>


<Paper ID=ument637> <Table ID =3>
	<Abstractive Summary> =
		22
		Table 3: Evaluation II: Content Ordering: P is Preci-
		sion, R is Recall and F is F- Measure
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 3 show that there
		is a considerable improvement in the quality of
		ordering with after introducing content ordering
		mechanism in the neural architecture
	</Extractive Summary>
</Paper ID=ument637>


<Paper ID=ument637> <Table ID =4>
	<Abstractive Summary> =
		6170
		Method
		%
		Set2SingleSeq
		20
		Set2MultipleSeq
		77
		Ambigous
		3
		Table 4: Evaluation III: Gramaticality: The values rep-
		resent percentage of times instructions generated by the
		model is chosen by the human evaluator
	</Abstractive Summary>
</Paper ID=ument637>


<Paper ID=ument637> <Table ID =5>
	<Abstractive Summary> =
		They were then
		asked to choose the model that generated instruc-
		tions retaining the most information from the ref-
		Method
		%
		Set2SingleSeq
		30
		Set2MultipleSeq
		63
		Ambigous
		7
		Table 5: Evaluation III: Informativeness: The values
		represent percentage of times instructions generated by
		the model is chosen by a human evaluator
	</Abstractive Summary>
	<Extractive Summary> =
		Results shown
		in Table 5 explains that incorporating neural
		components for subset selection and content or-
		dering helps in improving informative instruction
		generation
	</Extractive Summary>
</Paper ID=ument637>


<Paper ID=ument637> <Table ID =6>
	<Abstractive Summary> =
		Method
		%
		Set2MultipleSeq
		38
		Set2MultipleSeq+opt
		61
		Ambigous
		1
		Table 6: Evaluation III: Informativeness: The values
		represent percentage of times instructions generated by
		the model is chosen
		erence instructions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows that penalizing redundancy during
		beam search decoding reduces noise and helps
		in generating instructions with rich information
		density
	</Extractive Summary>
</Paper ID=ument637>


<Paper ID=ument637> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Examples from Evaluation II: Content Ordering
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Qualitative Comparison Across Models
		Table 7 shows an example of instructions gener-
		ated by the different approaches we investigated
	</Extractive Summary>
</Paper ID=ument637>


<Paper ID=ument638> <Table ID =1>
	<Abstractive Summary> =
		Input
		Output
		[zIps]
		[zIpt]
		[zIpz]
		[zIpd]
		[zIps]
		[zIpt]
		[nidz]
		[nid@d]
		[nidz]
		[nid d]
		[nidz]
		[nid@d]
		Table 1: Underlying form inference on English verbs
	</Abstractive Summary>
</Paper ID=ument638>


<Paper ID=ument638> <Table ID =2>
	<Abstractive Summary> =
		Edit
		LHS
		RHS
		Change
		/z/ → [s]
		[+v −s +c]
		[−v −s +c]
		[−v]
		/d/ → [t]
		[+v −s −c]
		[−v −s −c]
		∅→ [@]
		∅
		[@]
		[@]
		Table 2: Change inference on English verbs
	</Abstractive Summary>
</Paper ID=ument638>


<Paper ID=ument638> <Table ID =3>
	<Abstractive Summary> =
		u
		ℓ
		Features
		/#zI/ ⊥ [+#]
		[+v −s][+v +s]
		/zIp/ ⊥ [+v −s][+v +s][−v −s]
		/Ipz/
		? [+v +s][−v −s][+v −s]
		/pz#/ ⊤ [−v −s][+v −s]
		[+#]
		Table 3: Input to condition inference for change [−v]
		on /zIpz/ → [zIps]
		Inference by program synthesis
	</Abstractive Summary>
</Paper ID=ument638>


<Paper ID=ument638> <Table ID =4>
	<Abstractive Summary> =
		For
		each dataset, we learn a rule set from 20, 50 and 100
		6183
		Accuracy
		Rule Match
		UF
		Precision
		Recall
		SP
		SP-
		SP
		SP-
		SP
		SP-
		Flap 20
		76
		52
		50
		66
		31
		25
		100
		Flap 50
		93
		79
		86
		71
		86
		71
		100
		Flap 100
		100
		79
		100
		71
		100
		71
		100
		Verb 20
		86
		73
		48
		42
		83
		61
		100
		Verb 50
		88
		78
		52
		50
		92
		80
		100
		Verb 100
		95
		81
		62
		58
		100
		82
		100
		Table 4:
		Accuracy results for the English ﬂapping
		and verbs corpora data sets on 20, 50 and 100 training
		examples
	</Abstractive Summary>
</Paper ID=ument638>


<Paper ID=ument639> <Table ID =1>
	<Abstractive Summary> =
		822
		Table 1: Chinese Text Spam Detection Performance Comparison of Different Models
		Aggregation Learning Function
	</Abstractive Summary>
</Paper ID=ument639>


<Paper ID=ument639> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		Part
		All
		Spam
		Normal
		SMS
		Train
		48,884
		23,891
		24,993
		Test
		48,896
		23,891
		25,005
		Review
		Train
		37,299
		17,299
		20,000
		Test
		37,299
		17,299
		20,000
		Table 2: Statistics of Two Chinese Spam Text Datasets
		In the constructed variation graph, there are
		totally 25,949 Chinese characters (vertexes) and
		7,705,051 variation relations
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we summarize the statis-
		tics of the two real-world spam datasets (in Chi-
		nese)
	</Extractive Summary>
</Paper ID=ument639>


<Paper ID=ument639> <Table ID =3>
	<Abstractive Summary> =
		com/Giruvegan/stoneskipping
		6193
		Character
		Text
		Chinese
		Graph
		Proposed model
		Skipgram
		Cw2vec
		VFGE
		SS
		运(move)
		C
		捷(prompt)
		C
		捷(prompt)
		G P
		云(cloud)
		S C
		转(transmit)
		C
		站(stop)
		C
		站(stop)
		G P
		纭(numerous)
		G P
		芸(weed)
		C
		客(guest)
		S C
		输(transport)
		G
		坛(altar)
		G P
		云(cloud)
		惊(shock)
		S C
		讶(surprised)
		S C
		讶(surprised)
		G P
		景(view)
		S C
		慌(ﬂurried)
		S C
		愕(startled)
		S C
		撼(shake)
		G
		晾(dry)
		G
		琼(jade)
		S C
		吓(scare)
		S C
		愕(startled)
		G
		谅(forgive)
		G S C
		悚(afraid)
		G : Glyph; P : Phonetic; S : Semantic; C :Context
		Table 3: Case Study: given the target character, we list the top 3 similar characters from each algorithm
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 3, for exemplary characters, the most
		6195
		similar characters, based on skipgram embedding
		(general textual based baseline), are all semanti-
		cally similar or/and context-related
	</Extractive Summary>
</Paper ID=ument639>


<Paper ID=ument64> <Table ID =1>
	<Abstractive Summary> =
		5
		63
		84
		48
		33
		60
		Table 1: Intrinsic evaluations carried out on the grounded space for models with g = MLP; the textual space for
		T, CM (text) and models with g = id; and the visual space for CM (vis)
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1, we report the
		structural measures and the semantic relatedness
		scores of the baselines, namely T and CM, and on
		the various scenarios of our model
	</Extractive Summary>
	<Extractive Summary> =
		Results highlight that: (1) Using a grounded
		space is beneﬁcial; indeed, semantic relatedness
		and mNNO scores are higher in the lower half of
		Table 1, e
	</Extractive Summary>
</Paper ID=ument64>


<Paper ID=ument64> <Table ID =2>
	<Abstractive Summary> =
		Query
		Textual model
		Grounded model
		Two people are in love
		Two people are fencing indoors
		A couple just got married and are taking a picture with family
		A man is horriﬁed
		A man and a woman are smiling
		A teenage boy wearing a cap looks irritated
		This is a tragedy
		A group of people are at a party
		Men doing a war reenactment
		Table 2: Qualitative analysis: nearest neighbor of a given query (containing an abstract word) among Flickr30K
		sentences
	</Abstractive Summary>
	<Extractive Summary> =
		Nearest neighbors search
		Furthermore, we
		show in Table 2 that concrete knowledge acquired
		via our grounded model can also be transferred to
		abstract sentences
	</Extractive Summary>
</Paper ID=ument64>


<Paper ID=ument64> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Extrinsic evaluations with SentEval
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 reports evaluations of our base-
		lines and scenarios on SentEval (Conneau and
		Kiela, 2018), a classical benchmark used for eval-
		uating sentence embeddings
	</Extractive Summary>
</Paper ID=ument64>


<Paper ID=ument640> <Table ID =1>
	<Abstractive Summary> =
		1M
		74
		2
		KNET (Wiki-Man)
		-
		-
		100
		74
		2
		BBN
		32,739
		-
		6,430
		56
		2
		Table 1: Data set statistics: Numbers of train/dev/test
		instances, label set size, max hierarchy depth
	</Abstractive Summary>
</Paper ID=ument640>


<Paper ID=ument640> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Results on the OntoNotes test set
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Evaluation Results
		We compare the performance of our model with
		state-of-the-art methods on OntoNotes, FIGER,
		and KNET in Table 2, 3, and 4
	</Extractive Summary>
</Paper ID=ument640>


<Paper ID=ument640> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Results on the FIGER (Gold) test set
	</Abstractive Summary>
</Paper ID=ument640>


<Paper ID=ument640> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Results on KNET test sets
	</Abstractive Summary>
</Paper ID=ument640>


<Paper ID=ument640> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: Results on the BBN test set
	</Abstractive Summary>
</Paper ID=ument640>


<Paper ID=ument640> <Table ID =6>
	<Abstractive Summary> =
		8
		Table 6: Ablation study on the FIGER (Gold) test set
	</Abstractive Summary>
</Paper ID=ument640>


<Paper ID=ument641> <Table ID =1>
	<Abstractive Summary> =
		6K
		Table 1: TimeBank (TB), AQUAINT (AQ), and Plat-
		inum (PT) are from MATRES (Ning et al
	</Abstractive Summary>
</Paper ID=ument641>


<Paper ID=ument641> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Performances on the MATRES test set (i
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Note that in Table 2, CogCompTime performed
		slightly different to Ning et al
	</Extractive Summary>
	<Extractive Summary> =
		9 (Table 2
		Line 3 therein) and here we obtained F1=66
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 compares the two different ways to han-
		dle event positions discussed in Sec
	</Extractive Summary>
	<Extractive Summary> =
		, 2017), we ﬁnd that, with only two excep-
		tions (underlined in Table 2), the Concat system
		saw consistent gains under various embeddings
		6http://cogcomp
	</Extractive Summary>
</Paper ID=ument641>


<Paper ID=ument641> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: Further evaluation of the proposed system, i
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 furthermore applies CogCompTime and
		the proposed Concat+CSE system on a different
		test set called TCR (Ning et al
	</Extractive Summary>
</Paper ID=ument641>


<Paper ID=ument642> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Fine-grained entity typing performance
	</Abstractive Summary>
</Paper ID=ument642>


<Paper ID=ument643> <Table ID =1>
	<Abstractive Summary> =
		(%)
		1 segment 2 segments 3 segments
		Train
		534
		544 (46)
		607 (51)
		44 (4)
		205 (17)
		Dev
		303
		357 (45)
		421 (53)
		18 (2)
		240 (30)
		Test
		430
		584 (48)
		610 (50)
		16 (1)
		327 (27)
		Table 1: Statistics of the dataset
	</Abstractive Summary>
</Paper ID=ument643>


<Paper ID=ument643> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Main results
	</Abstractive Summary>
</Paper ID=ument643>


<Paper ID=ument643> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Results on handling multiple entity types
	</Abstractive Summary>
</Paper ID=ument643>


<Paper ID=ument643> <Table ID =4>
	<Abstractive Summary> =
		0001
		Table 4: Hyperparameters of our joint model
	</Abstractive Summary>
</Paper ID=ument643>


<Paper ID=ument644> <Table ID =1>
	<Abstractive Summary> =
		01
		Table 1: Mean average precisions (MAPs) on test data
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 1 illustrates the performance of each
		method
	</Extractive Summary>
</Paper ID=ument644>


<Paper ID=ument644> <Table ID =2>
	<Abstractive Summary> =
		679
		Default hyperparameter: K = 20
		Table 2:
		Ablation study:
		mean average precisions
		(MAPs) on dev data (Wiki-90k)
	</Abstractive Summary>
	<Extractive Summary> =
		Ablation Study: Table 2 illustrates the perfor-
		mance of various settings of our proposed method
	</Extractive Summary>
</Paper ID=ument644>


<Paper ID=ument645> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Experiment results on ACE2005 named entity
		mention recognition
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Overall Results
		Table 1 shows the overall results of our methods
		compared with baselines
	</Extractive Summary>
</Paper ID=ument645>


<Paper ID=ument645> <Table ID =2>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Effects of Gazetteer Network
		To further investigate the effect of introducing
		gazetteers, Table 2 shows the results when training
		data size varies
	</Extractive Summary>
</Paper ID=ument645>


<Paper ID=ument646> <Table ID =1>
	<Abstractive Summary> =
		6240
		pattern
		regex Wikidata blacklist true VA
		the-of
		12,748,735
		90,712
		3,591
		2,779
		a-of
		5,900,839
		11,860
		705
		118
		an-of
		956,247
		4,539
		88
		14
		the-for
		2,960,459
		8,070
		817
		24
		a-for
		1,869,946
		4,812
		536
		59
		an-for
		304,529
		1,424
		296
		13
		the-among
		122,345
		139
		13
		3
		a-among
		67,019
		82
		25
		13
		an-among
		11,158
		12
		1
		0
		sum
		24,941,277 121,650
		6,072
		3,023
		Table 1: Number of VA candidates after each step and
		manually conﬁrmed VAs for all used patterns
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1, we show the
		number of VA candidate phrases after each step
	</Extractive Summary>
	<Extractive Summary> =
		8% (see Table 1)
	</Extractive Summary>
</Paper ID=ument646>


<Paper ID=ument646> <Table ID =2>
	<Abstractive Summary> =
		1%
		Table 2: Performance of the three proposed approaches
		in comparison with the baseline
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Baseline VA Candidates
		We cannot determine recall for our approaches,
		instead we compute precision, recall and f-score
		based on the baseline data set (see Section 3), which
		is shown in Table 2 as well as precision of the base-
		line data set
	</Extractive Summary>
</Paper ID=ument646>


<Paper ID=ument647> <Table ID =1>
	<Abstractive Summary> =
		57
		Table 1: Experimental results
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results
		Table 1 shows the experimental results
	</Extractive Summary>
</Paper ID=ument647>


<Paper ID=ument647> <Table ID =2>
	<Abstractive Summary> =
		9680
		Table 2: Accuracy of NEs covered (Acc
	</Abstractive Summary>
</Paper ID=ument647>


<Paper ID=ument647> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Comparison with best results on BioCreative
		IV’s CHEMDNER task
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Related Works
		BioCreative IV’s CHEMDNER task
		Table 3 shows a comparison with the previous
		best results
	</Extractive Summary>
</Paper ID=ument647>


<Paper ID=ument648> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example for a 2-way 1-shot scenario, in-
		cluding both few-shot DA and few-shot NOTA
	</Abstractive Summary>
</Paper ID=ument648>


<Paper ID=ument648> <Table ID =2>
	<Abstractive Summary> =
		09
		Table 2: Accuracies (%) on few-shot DA
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Evaluation Results on Few-Shot DA
		Table 2 demonstrates the evaluation results of
		few-shot DA on the existing FewRel test set and
		the new test set
	</Extractive Summary>
</Paper ID=ument648>


<Paper ID=ument648> <Table ID =3>
	<Abstractive Summary> =
		43
		Table 3: Accuracies (%) on few-shot NOTA
	</Abstractive Summary>
</Paper ID=ument648>


<Paper ID=ument649> <Table ID =1>
	<Abstractive Summary> =
		66
		Table 1: Modern tools trained on cased data perform
		well on cased test data, but poorly on uncased (low-
		ercased) test data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 demonstrates how popular modern sys-
		tems trained on cased data perform well on cased
		data, but suffer dramatic performance drops when
		evaluated on lowercased text
	</Extractive Summary>
</Paper ID=ument649>


<Paper ID=ument649> <Table ID =2>
	<Abstractive Summary> =
		22
		Table 2: Truecaser word-level performance on English
		data
	</Abstractive Summary>
</Paper ID=ument649>


<Paper ID=ument649> <Table ID =3>
	<Abstractive Summary> =
		25
		Table 3: Results from NER+ELMo experiments, tested
		on CoNLL 2003 English test set
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results
		Results for NER are shown in Table 3, and results
		for POS are shown in Table 4
	</Extractive Summary>
</Paper ID=ument649>


<Paper ID=ument649> <Table ID =4>
	<Abstractive Summary> =
		38
		Table 4: Results from POS+ELMo experiments, tested
		on WSJ 22-24, from PTB
	</Abstractive Summary>
</Paper ID=ument649>


<Paper ID=ument649> <Table ID =5>
	<Abstractive Summary> =
		66
		Table 5: Results on NER+ELMo on the Broad Twitter
		Corpus, set F, measured as mention detection F1
	</Abstractive Summary>
	<Extractive Summary> =
		Results are shown in Table 5, and a familiar pat-
		tern emerges
	</Extractive Summary>
</Paper ID=ument649>


<Paper ID=ument65> <Table ID =1>
	<Abstractive Summary> =
		”
		Table 1: Comparison with recent ﬁne-grained language-and-vision datasets
	</Abstractive Summary>
</Paper ID=ument65>


<Paper ID=ument65> <Table ID =2>
	<Abstractive Summary> =
		03
		Table 2: Experimental results for comparative paragraph generation on the proposed dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Results using these metrics are given in Table 2
		for the main baselines and model variants
	</Extractive Summary>
</Paper ID=ument65>


<Paper ID=ument65> <Table ID =3>
	<Abstractive Summary> =
		28
		Table 3: Variants and ablations for the Neural Naturalist model
	</Abstractive Summary>
</Paper ID=ument65>


<Paper ID=ument65> <Table ID =4>
	<Abstractive Summary> =
		00
		Table 4: Human evaluation results on 120 test set sam-
		ples, twenty per column
	</Abstractive Summary>
</Paper ID=ument65>


<Paper ID=ument650> <Table ID =1>
	<Abstractive Summary> =
		OIE2016
		( The number of ones ; equals ; the number of zeros plus one ; since the state containing only zeros
		can not occur ), ( the state ; containing ; only zeros ), ( the state containing only zeros ; occur )
		CaRB
		( The number of ones ; equals ; the number of zeros plus one ),
		( the state containing only zeros ; can not occur )
		Table 1: Sample gold annotations for OIE2016 vs
	</Abstractive Summary>
</Paper ID=ument650>


<Paper ID=ument650> <Table ID =2>
	<Abstractive Summary> =
		87)
		Table 2: One-to-One Match vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 displays an exam-
		ple wherein system 1 combines information from
		two gold tuples in a single extraction, and system
		2 only extracts one of the gold tuples
	</Extractive Summary>
</Paper ID=ument650>


<Paper ID=ument650> <Table ID =3>
	<Abstractive Summary> =
		(prec,rec)
		Gold
		(I; ate; an apple)
		OIE2016
		CaRB
		System 1
		(I; ate; an apple)
		(1,1)
		(1,1)
		System 2
		(ate; an apple; I)
		(1,1)
		(0,0)
		Table 3: Tuple Match vs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 illustrates an ex-
		ample when the arguments are shufﬂed, lexical
		match (OIE2016) shows no effect but tuple match
		(CaRB) rightfully decreases the scores
	</Extractive Summary>
</Paper ID=ument650>


<Paper ID=ument650> <Table ID =4>
	<Abstractive Summary> =
		78
		Table 4: Data quality using token-level match
		Precision
		Recall
		F1
		OIE2016
		0
	</Abstractive Summary>
</Paper ID=ument650>


<Paper ID=ument650> <Table ID =5>
	<Abstractive Summary> =
		73
		Table 5: Data quality using lexical match
		System
		Precision
		Recall
		F1
		AUC
		Ollie
		0
	</Abstractive Summary>
</Paper ID=ument650>


<Paper ID=ument650> <Table ID =6>
	<Abstractive Summary> =
		224
		Table 6: Performance of Open IE systems on CaRB
		create an expert dataset
	</Abstractive Summary>
</Paper ID=ument650>


<Paper ID=ument651> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Different model performances on the 2003
		CoNLL test set
	</Abstractive Summary>
	<Extractive Summary> =
		S is supervised, US is unsupervised
		and WS is weakly supervised
		Table 1 shows the performance of our model
	</Extractive Summary>
	<Extractive Summary> =
		To further investigate the impact of the differ-
		ent components of the model, we ablate our model
		components (Table 1)
	</Extractive Summary>
</Paper ID=ument651>


<Paper ID=ument651> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: MUC f-scores for the model
		Since our model does not have access to an-
		notated training data, it has no direct supervision
		for learning entity boundaries
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the performance of the model
		along the two axes, as well as the MUC score
	</Extractive Summary>
</Paper ID=ument651>


<Paper ID=ument652> <Table ID =1>
	<Abstractive Summary> =
		648
		Table 1: Corpus comparison related to our study
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the comparison in terms
		of size, annotation granularity level, and inter-
		annotator agreement (IAA) of the related corpora
	</Extractive Summary>
</Paper ID=ument652>


<Paper ID=ument652> <Table ID =2>
	<Abstractive Summary> =
		negative
		Table 2: Signiﬁcance of the proportion of EU fre-
		quency of OP vs
	</Abstractive Summary>
	<Extractive Summary> =
		Negative
		Table 2 shows the signiﬁcance of the proportion
		of each EU type for the post types, namely, OP vs
	</Extractive Summary>
</Paper ID=ument652>


<Paper ID=ument652> <Table ID =3>
	<Abstractive Summary> =
		00
		Table 3: Signiﬁcance of the EU position between a pos-
		itive and negative post
	</Abstractive Summary>
</Paper ID=ument652>


<Paper ID=ument652> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Performance of the average F1
	</Abstractive Summary>
	<Extractive Summary> =
		Results: Overall Performance
		Table 4 shows that our proposed sentence-level
		BLC with structural information performs best re-
		garding macro F1 in either boundary identiﬁca-
		tion or unit classiﬁcation tasks
	</Extractive Summary>
</Paper ID=ument652>


<Paper ID=ument653> <Table ID =1>
	<Abstractive Summary> =
		Size
		MM size
		Restaurant
		ATSA
		4
		4827
		1283
		Restaurant
		ACSA
		4
		4738
		454
		Laptop
		ATSA
		4
		3012
		604
		Twitter
		ATSA
		3
		6940
		6
		Table 1:
		Statistics of existing datasets for ABSA
	</Abstractive Summary>
	<Extractive Summary> =
		Although these three datasets have since
		become the benchmark datasets for the ABSA
		task, most sentences in these datasets consist
		of only one aspect or multiple aspects with the
		same sentiment polarity (see Table 1)2, which
		makes aspect-based sentiment analysis degener-
		ate to sentence-level sentiment analysis
	</Extractive Summary>
</Paper ID=ument653>


<Paper ID=ument653> <Table ID =2>
	<Abstractive Summary> =
		1929
		2084
		3077
		7090
		Train (small)
		1000
		1100
		1613
		3713
		Validation
		241
		259
		388
		888
		Test
		245
		263
		393
		901
		Table 2: Statistics of MAMS dataset
	</Abstractive Summary>
</Paper ID=ument653>


<Paper ID=ument653> <Table ID =3>
	<Abstractive Summary> =
		736
		Table 3:
		Experimental results on two MAMS datasets and SemEval-2014 Restaurant Review dataset for both
		ATSA and ACSA subtasks
	</Abstractive Summary>
	<Extractive Summary> =
		From
		Table 3 we draw the following conclusions
	</Extractive Summary>
	<Extractive Summary> =
		From Table 3 (last two rows) we can
		see that capsule-guided routing boosts the perfor-
		mance of CapsNet and CapsNet-BERT on all the
		datasets
	</Extractive Summary>
</Paper ID=ument653>


<Paper ID=ument654> <Table ID =1>
	<Abstractive Summary> =
		As the
		noise transition matrix Q indicates the transition
		values from clean labels to noisy labels, we com-
		6289
		#Noisy Training Data
		#Clean Training Data
		#Validation Data
		#Test Data
		Movie
		13539P, 13350N
		4265P, 4265N
		105P, 106N
		960P, 957N
		Laptop
		9702P, 7876N
		1064P, 490N
		33P, 20N
		298P, 175N
		Restaurant
		8094P, 10299N
		1087P, 823N
		39P, 14N
		339P, 116N
		Table 1: Summary statistics of the datasets
	</Abstractive Summary>
	<Extractive Summary> =
		, the last three columns in Table 1)
	</Extractive Summary>
</Paper ID=ument654>


<Paper ID=ument654> <Table ID =2>
	<Abstractive Summary> =
		7241
		Table 2: Accuracy (ACC) of both classes, F1 (F1 pos) of positive class and F1 (F1 neg) of negative class on clean
		test data/sentences
	</Abstractive Summary>
</Paper ID=ument654>


<Paper ID=ument655> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Genre distribution of the modern narratives
		which are common to narratives
	</Abstractive Summary>
</Paper ID=ument655>


<Paper ID=ument655> <Table ID =2>
	<Abstractive Summary> =
		io/
		6296
		Label
		Gutenberg
		Total
		Top Lexicons
		Neutral
		318
		1711
		take, love, long, really, want, always, though, away, look
		Fear
		159
		1412
		left, behind, right, want, let, death, go, say, think
		Sadness
		195
		1402
		father, always, little, look, something, us, really, mother, think
		Anger
		192
		1306
		feel, much, well, man, look, us, say, something, love
		Joy
		241
		1266
		see, always, let, long, make, hand, away, get, really
		Love
		162
		1157
		hand, know, right, let, happy, get, ever, us, look
		Anticipation
		147
		1020
		know, long, life, make, get, think, blood, want, feel
		Surprise
		102
		362
		love, ﬁnd, looking, know, well, much, something, door, really
		Disgust
		4
		74
		get, hand, inside, let, hate, table, men, always, make
		Table 2: Dataset label distribution
		Text
		Label
		I found this was a little too close upon him, but I made it up in what follows
	</Abstractive Summary>
</Paper ID=ument655>


<Paper ID=ument655> <Table ID =3>
	<Abstractive Summary> =
		Joy
		Table 3: Sample data from classic titles
		Model
		micro-F1
		TF-IDF + SVM
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows a few examples labelled data
		from classic titles
	</Extractive Summary>
</Paper ID=ument655>


<Paper ID=ument655> <Table ID =4>
	<Abstractive Summary> =
		604
		Table 4: Benchmark results (averaged 5-fold cross val-
		idation)
		each to model sentences and documents
	</Abstractive Summary>
</Paper ID=ument655>


<Paper ID=ument656> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Data distribution of SemEval-2016 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the distribution of
		these targets in the dataset
	</Extractive Summary>
</Paper ID=ument656>


<Paper ID=ument656> <Table ID =2>
	<Abstractive Summary> =
		04
		Table 2: Performance comparison of stance detection on the SemEval-2016 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 (top) shows the results of this ablation
		study
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 (bottom) shows the results of this com-
		parison as well
	</Extractive Summary>
</Paper ID=ument656>


<Paper ID=ument657> <Table ID =1>
	<Abstractive Summary> =
		6308
		Table 1: Accuracy (in %) on MLDoc experiments
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, our frame-
		work signiﬁcantly outperforms all baselines in 7
		languages on cross-lingual document classiﬁca-
		tion, including for phylogenetically unrelated lan-
		guages
	</Extractive Summary>
</Paper ID=ument657>


<Paper ID=ument657> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Accuracy (in %) on Chinese sentiment classi-
		ﬁcation without using labeled Chinese data
	</Abstractive Summary>
</Paper ID=ument657>


<Paper ID=ument657> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Percentages of instances added into the train-
		ing set that are correct for the MLDoc data using our
		method
	</Abstractive Summary>
</Paper ID=ument657>


<Paper ID=ument658> <Table ID =1>
	<Abstractive Summary> =
		76
		Table 1: Results on the Yelp and Amazon test sets
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 1 shows the results of various models
	</Extractive Summary>
</Paper ID=ument658>


<Paper ID=ument658> <Table ID =2>
	<Abstractive Summary> =
		(b) From positive to negative
		Table 2: Example outputs on the Yelp and Amazon test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the ex-
		ample outputs of our model and others
	</Extractive Summary>
</Paper ID=ument658>


<Paper ID=ument658> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Outputs of the proposed model given the same
		source sentences with different sentiment embeddings
		(sx = positive; sy = negative)
	</Abstractive Summary>
</Paper ID=ument658>


<Paper ID=ument659> <Table ID =1>
	<Abstractive Summary> =
		6318
		Table 1: Ontology information type
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Our Method
		As shown in Table 1, each information type has
		a rather rich description, so we think that us-
		ing the description can improve the performance
	</Extractive Summary>
	<Extractive Summary> =
		We use “description” and “intent type” in Table 1
		for the label text of the lower and upper classes, re-
		spectively
	</Extractive Summary>
</Paper ID=ument659>


<Paper ID=ument659> <Table ID =2>
	<Abstractive Summary> =
		6320
		Table 2: Experimental results
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 2 presents the results for our methods and
		part of the TREC 2018 participants’ results
	</Extractive Summary>
	<Extractive Summary> =
		Interestingly, our method
		works best for the upper class OTHER, which con-
		tains the fewest meanings in the upper class label
		4 Each sample has just one information type for develop-
		ment data and more than one for testing data, so the Macro
		F1 score between Table 2 and 3 cannot be compared directly
	</Extractive Summary>
</Paper ID=ument659>


<Paper ID=ument659> <Table ID =3>
	<Abstractive Summary> =
		Compar-
		ing our proposed method with the baseline meth-
		ods, differences in the Multi-type Macro F1 score
		Table 3: Comparing normalization methods
	</Abstractive Summary>
</Paper ID=ument659>


<Paper ID=ument659> <Table ID =4>
	<Abstractive Summary> =
		6321
		Table 4: Comparing macro F1 score for each upper
		class between proposed and Non-hier
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the macro F1 scores summarized
		for each upper class of the proposed method and
		Non-hier
	</Extractive Summary>
</Paper ID=ument659>


<Paper ID=ument66> <Table ID =1>
	<Abstractive Summary> =
		721
		Table 1: Links for mentions with multiple choices in
		Figure 2 and the ratio of respondents selecting that link
		Link
		Ratio
		[Russian] daily Kommersant
	</Abstractive Summary>
	<Extractive Summary> =
		Multiple links were proposed for the mentions underlined (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Link types: for this we introduce Table 1,
		where we see how respondents preferred dif-
		ferent types of reference (respondents could
		select multiple options); from this we con-
		clude that although respondents preferred to
		use a country directly to represent national-
		ity, they also preferred to resolve complex
		types of reference, such as the meronymic
		use of Moscow to refer to the government
		rather than the city, and the use of Putin’s title
		to refer to him rather than the title itself
	</Extractive Summary>
	<Extractive Summary> =
		Additionally, we label some mentions with
		multiple alternatives (per Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		, Table 1); on the other hand, evaluated systems
		predict a single link for each mention
	</Extractive Summary>
	<Extractive Summary> =
		In-
		terestingly, no system captures metonymic refer-
		ences, though as previously seen the results of our
		questionnaire (see Table 1) indicate that respon-
		dents prefer such types of links over their literal
		counterparts (e
	</Extractive Summary>
	<Extractive Summary> =
		8
		1
		Ranked combined categories
		F1
		Br
		Bs
		T
		D
		A
		F
		Figure 4: Cumulative results for Babelfy (relaxed/strict), TagME, DBpedia Spotlight, AIDA and FREME for the
		uniﬁed dataset over ranked combinations of categories
		6
		Fuzzy Recall and F1 Measures
		While the previous results provide insights into
		different design choices taken by different systems
		with respect to different types of EL annotations,
		still, per Table 1, the results are perhaps too ﬁne-
		grained
	</Extractive Summary>
</Paper ID=ument66>


<Paper ID=ument66> <Table ID =2>
	<Abstractive Summary> =
		com/henryrosalesmendez/
		categorized EMNLP datasets
		Table 2: Content of relabeled datasets
		KORE50
		ACE2004
		VoxEL
		Documents
		1
		20
		15
		Sentences
		50
		214
		94
		Annotations
		372
		3,351
		1,107
		Full Name
		41
		588
		227
		Short Name
		114
		307
		97
		Extended Name
		1
		8
		–
		Alias
		5
		94
		15
		Numeric/Temporal
		17
		276
		111
		Common Form
		157
		1,974
		615
		Pro-form
		37
		107
		42
		Singular Noun
		248
		1,943
		683
		Plural Noun
		39
		670
		182
		Adjective
		45
		501
		149
		Verb
		40
		232
		85
		Adverb
		–
		5
		8
		No Overlap
		307
		2,161
		792
		Maximal Overlap
		23
		392
		95
		Intermediate Overlap
		4
		62
		14
		Minimal Overlap
		38
		736
		206
		Direct
		262
		2,280
		750
		Anaphoric
		37
		107
		42
		Metaphoric
		8
		27
		38
		Metonymic
		3
		60
		21
		Related
		54
		698
		224
		Descriptive
		8
		179
		32
		Person
		117
		278
		66
		Organisation
		40
		199
		120
		Place
		19
		519
		168
		Miscellany
		196
		2,352
		753
		garding different types of EL annotations
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2 we summarize details of the recon-
		structed version of these three datasets, includ-
		ing the number of documents, sentences and an-
		notations
	</Extractive Summary>
</Paper ID=ument66>


<Paper ID=ument66> <Table ID =3>
	<Abstractive Summary> =
		724
		Table 3: Results for Babelfy (strict/relaxed), TagME, DBpedia Spotlight, AIDA and FREME on the uniﬁed dataset
	</Abstractive Summary>
</Paper ID=ument66>


<Paper ID=ument661> <Table ID =1>
	<Abstractive Summary> =
		4
		PAN14 SOME
		–
		–
		10
		–
		–
		10
		PAN16 RAND
		–
		–
		10
		–
		–
		10
		Table 1: Dataset sizes (in 1000 sentences)
	</Abstractive Summary>
	<Extractive Summary> =
		Preprocessing
		Table 1 shows sizes for all used
		datasets
	</Extractive Summary>
</Paper ID=ument661>


<Paper ID=ument661> <Table ID =2>
	<Abstractive Summary> =
		22
		Table 2:
		Mean and relative standard deviation of
		the leakage of 10 attackers trained on different sub-
		samples of the training data (PAN16 TWIT)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the mean and relative standard
		deviation (RStD) of the leakage of the 10 diagnos-
		tic classiﬁers in each setting
	</Extractive Summary>
</Paper ID=ument661>


<Paper ID=ument661> <Table ID =3>
	<Abstractive Summary> =
		1
		145
		Table 3: Class distribution for the conﬁdently predicted
		tweets and number of tweets per model condition for
		M(ale), F(emale) and Y(oung), O(ld)
		We use the intersection of leaked samples
		across the attackers, under the assumption that
		samples that were correctly labelled by all ten
		models (and with high probability) are most likely
		to exhibit protected attribute leakage
	</Abstractive Summary>
</Paper ID=ument661>


<Paper ID=ument661> <Table ID =4>
	<Abstractive Summary> =
		34
		Table 4: Cross-sample diagnostic classiﬁer accuracy of
		classiﬁcation of demographic attribute when trained on
		PAN16 TWIT data and evaluated on different test sets
	</Abstractive Summary>
	<Extractive Summary> =
		sarial training reduces the leakage of demographic
		attributes, but the diagnostic classiﬁer is still able
		to predict the sensitive demographic attribute from
		the data representations signiﬁcantly better than
		chance (line 1 of Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		The results in Table 4 show, however, that the
		performance and leakage of the adversarial models
		do not generalize well across domains, but for the
		most part the performance of the non-adversarial
		models doesn’t either
	</Extractive Summary>
</Paper ID=ument661>


<Paper ID=ument661> <Table ID =5>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		3Table 5 in the Appendix provides an overview of the
		class distribution
	</Extractive Summary>
</Paper ID=ument661>


<Paper ID=ument661> <Table ID =6>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Our
		development results (main and diagnostic classi-
		ﬁer) which are comparable to Elazar and Goldberg
		(2018) are reported in Table 6 in the Appendix;
		test set results are also in Table 4
	</Extractive Summary>
</Paper ID=ument661>


<Paper ID=ument661> <Table ID =7>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Exploring leaked samples
		Out of the LEAKED
		gender sentences, the top 10 sentences which re-
		ceived the highest conﬁdent scores are presented
		in Table 7 in the Appendix
	</Extractive Summary>
</Paper ID=ument661>


<Paper ID=ument662> <Table ID =1>
	<Abstractive Summary> =
		Models were trained with Adam
		Optimizer having a learning rate 1e−5 and a batch
		6339
		Catrgory
		Features for snippets
		Features for tweets
		NER
		LOC(***), ORG(***)
		LOC(***), ORG(***)
		POS
		CC(***),
		IN(***),
		MD(***),
		NNP(***),
		PRP(***),
		PRP$(***),
		RB(***),
		VB(***),
		VBD(***),
		VBP(***), VBZ(***), JJ(**), NN(**),
		NNS(**), TO(**), UH(**), WP(*)
		IN(***),
		MD(***),
		NN(***),
		NNP(***),
		NNS(***), PRP$(***), RB(***), UH(***),
		VB(***), VBG(***), VBP(***), VBZ(***),
		CC(**), TO(**), VBD(**), JJ(*), JJR(*),
		JJS(*), RP(*)
		Empath
		Appearance(***),
		Feminine(***),
		White Collar Job(***),
		Beauty(*),
		Cleaning(*),
		Exotic(*),
		Farming(*),
		Occupation(*), Violence(*)
		Affection(***), Ancient(***), Beach(***),
		Car(***),
		Cleaning(***),
		College(***),
		Domestic Work(***),
		Driving(***),
		Eat-
		ing(***),
		Economics(***),
		Family(***),
		Government(***), Health(***), Home(***),
		Love(***), Medical Emergency(***), Meet-
		ing(***), Morning(***), Occupation(***),
		Ocean(***), Ofﬁce(***), Optimism (***),
		Poor(***),
		Positive Emotion(***),
		Read-
		ing(***),
		Sailing(***),
		School(***),
		Science(***),
		Sports(***),
		Swim-
		ming(***),
		Traveling(***),
		Water(***),
		White Collor Job(***),
		Work(***),
		Ve-
		hicle(***),
		Art(**),
		Blue Collar Job(**),
		Business(**),
		Clothing(**),
		Cooking(**),
		Dance(**),
		Exotic(**),
		Fabric(**),
		Fem-
		inine(**),
		Friends(**),
		Hygiene(**),
		Liquid(**),
		Musical(**),
		Law(**),
		Plant(**),
		Restaurant(**),
		Ship(**),
		Toy(**),Achievement(*),
		Air Travel(*),
		Animal(*),
		Attractive(*),
		Childish(*),
		Children(*),
		Contentment(*),
		Exercise(*),
		Fashion(*), Furniture(*), Help(*), Leader(*),
		Leisure(*), Messaging(*), Music(*), Ner-
		vousness(*),
		Politics(*),
		Shopping(*),
		Sleep(*),
		Technology(*),
		Violence(*),
		Warmth(*), Weather(*), Wedding(*)
		LIWC
		Adverbs(***),
		Affect(***),
		AuxVb(***),
		CogMech(***),
		Conj(***), Excl(***), Humans(***),
		Negate(***), Past(***), Posemo(***),
		Ppron(***), Prep(***), Present(***),
		Pronoun(***),
		Relativ(***),
		SheHe(***), Social(***), Space(***),
		Tentat(***),
		They(***),
		Time(***),
		Verbs(***), Cause(**), Discrep(**),
		Funct(**),
		Future(**),
		Hear(**),
		Incl(**), Motion(**), Achiev(*), Arti-
		cle(*), Body(*), Death(*), Family(*),
		Insight(*), Percept(*), Quant(*)
		Adverbs(***),
		Affect(***),
		AuxVb(***),
		Body(***),
		CogMech(***),
		Conj(***),
		Discrep(***),
		Excl(***),
		Family(***),
		Funct(***), Health(***), Home(***), Hu-
		mans(***),
		Insight(***),
		Leisure(***),
		Past(***),
		Posemo(***),
		Ppron(***),
		Prep(***),
		Present(***),
		Pronoun(***),
		Sad(***),
		Social(***),
		Verbs(***),
		Work(***), Assent(**), Bio(**), Cause(**),
		Future(**),
		Ingest(**),
		Negate(**),
		Quant(**),
		Sexual(**),
		Certain(*),
		Friends(*), I(*), Incl(*), SheHe(*)
		Table 1: Features that are signiﬁcantly different for the target words compared to the other words
	</Abstractive Summary>
</Paper ID=ument662>


<Paper ID=ument662> <Table ID =2>
	<Abstractive Summary> =
		16
		Table 2: Comparison our models with the baseline
	</Abstractive Summary>
</Paper ID=ument662>


<Paper ID=ument662> <Table ID =3>
	<Abstractive Summary> =
		54
		Table 3: Macro (M) and micro (µ) F1 scores for the
		different models
	</Abstractive Summary>
</Paper ID=ument662>


<Paper ID=ument663> <Table ID =1>
	<Abstractive Summary> =
		8%)
		Table 1: Descriptive statistics of the BASIL dataset
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 1, the large majority of annota-
		tions in BASIL are classiﬁed as informational bias
	</Extractive Summary>
	<Extractive Summary> =
		Note that though NYT
		appears to have signiﬁcantly more informational
		bias spans against conservatives than HPO, this is
		because NYT tends to have longer articles than the
		other two sources (see Table 1), and thus naturally
		results in more annotation spans by raw count
	</Extractive Summary>
</Paper ID=ument663>


<Paper ID=ument663> <Table ID =2>
	<Abstractive Summary> =
		63
		Table 2: Sentence classiﬁcation (top) and sequence tag-
		ging (bottom) results on lexical and informational bias
		prediction
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2, we see that
		the BERT lexical sequence tagger produces bet-
		ter recall and F1 than the informational tagger,
		highlighting the additional difﬁculty of accurately
		identifying spans of informational bias
	</Extractive Summary>
</Paper ID=ument663>


<Paper ID=ument664> <Table ID =1>
	<Abstractive Summary> =
		81
		NA
		Table 1: Overall results for each dataset and model
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1 we report the test set results for all
		models
	</Extractive Summary>
</Paper ID=ument664>


<Paper ID=ument664> <Table ID =2>
	<Abstractive Summary> =
		28
		Table 2: Summary statistics for bootstrap results on
		BBC dataset
	</Abstractive Summary>
</Paper ID=ument664>


<Paper ID=ument664> <Table ID =3>
	<Abstractive Summary> =
		02
		Table 3: Summary statistics for bootstrap results on
		MFTC dataset domains
	</Abstractive Summary>
</Paper ID=ument664>


<Paper ID=ument665> <Table ID =1>
	<Abstractive Summary> =
		Daniels
		nonwhite
		“that otherworldly athleticism
		he has saw it with Michael
		Vick”
		Table 1: Example mentions from FOOTBALL that high-
		light racial bias in commentator sentiment patterns
	</Abstractive Summary>
	<Extractive Summary> =
		Our re-
		sulting FOOTBALL dataset contains over 1,400
		games spanning six decades, automatically an-
		notated with ∼250K player mentions (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		(Table 4) conﬁrms that nonwhite players are much
		more frequently praised for physical ability than
		white players, who are praised for personality and
		intelligence (see Table 1 for more examples)
	</Extractive Summary>
</Paper ID=ument665>


<Paper ID=ument665> <Table ID =2>
	<Abstractive Summary> =
		The adjective “quick”
		6358
		White
		long way, long time, valuable
		DB
		strong safety, free safety, state university
		RB
		second effort,single setback,ground game
		QB
		freshman quarterback, arm strength, easier
		WR
		quick slant, end zone touchdown, punt returner
		Table 2: Top terms for the white, defensive back (DB),
		running back (RB), quarterback (QB), and wide re-
		ceiver (WR) covariates for the log linear model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows several highest-valued γj,w for a
		subset of the J covariates
	</Extractive Summary>
</Paper ID=ument665>


<Paper ID=ument665> <Table ID =3>
	<Abstractive Summary> =
		7%
		Table 3: White players at the four major offensive posi-
		tions are referred to by last name more often than non-
		white players at the same positions, a discrepancy that
		may reﬂect unconscious racial boundary-marking
	</Abstractive Summary>
	<Extractive Summary> =
		A per-position analy-
		sis of player mentions in FOOTBALL corroborates
		these ﬁndings for all offensive positions (Table 3)
	</Extractive Summary>
</Paper ID=ument665>


<Paper ID=ument665> <Table ID =4>
	<Abstractive Summary> =
		6359
		Race
		Most positive words
		white (all)
		enjoying, favorite, calm, appreciate,
		loving, miracle, spectacular, perfect,
		cool, smart
		nonwhite (all)
		speed, gift, versatile, gifted, play-
		maker, natural, monster, wow, beast,
		athletic
		white (QBs)
		cool, smart, favorite, safe, spectacu-
		lar, excellent, class, fantastic, good,
		interesting
		nonwhite (QBs)
		ability, athletic, brilliant, aware-
		ness, quiet, highest, speed, wow, ex-
		cited, wonderful
		Table 4: Positive comments for nonwhite players (top
		two rows: all player mentions; bottom two rows: only
		quarterback mentions) focus on their athleticism, while
		white players are praised for personality and intelli-
		gence
	</Abstractive Summary>
	<Extractive Summary> =
		(Table 4) conﬁrms that nonwhite players are much
		more frequently praised for physical ability than
		white players, who are praised for personality and
		intelligence (see Table 1 for more examples)
	</Extractive Summary>
	<Extractive Summary> =
		The top two rows of Table 4,
		which were derived from all mentions regardless
		of position, are thus tainted by the positional
		confound discussed in Section 3
	</Extractive Summary>
	<Extractive Summary> =
		The bottom
		two rows of Table 4 are derived from the same
		analysis applied to just quarterback windows;
		qualitatively, the results appear similar to those
		in the top two rows
	</Extractive Summary>
</Paper ID=ument665>


<Paper ID=ument666> <Table ID =1>
	<Abstractive Summary> =
		cn/
		#single
		#multiple
		total
		Train
		147, 580
		42, 420
		190, 000
		Valid
		19, 350
		5, 602
		24, 952
		Test
		18, 539
		5, 258
		23, 797
		Table 1: The statistics of the proposed dataset
	</Abstractive Summary>
</Paper ID=ument666>


<Paper ID=ument666> <Table ID =2>
	<Abstractive Summary> =
		61
		Table 2: Results on charge-based prison term predic-
		tion(%)
	</Abstractive Summary>
</Paper ID=ument666>


<Paper ID=ument666> <Table ID =3>
	<Abstractive Summary> =
		43
		Table 3: Results on total term prediction(%)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the results of the total term
		prediction
	</Extractive Summary>
</Paper ID=ument666>


<Paper ID=ument667> <Table ID =1>
	<Abstractive Summary> =
		Split
		Inscriptions
		Words
		Chars
		Train
		34, 952
		2, 792k
		16, 300k
		Valid
		2, 826
		211k
		1, 230k
		Test
		2, 949
		223k
		1, 298k
		Table 1: Statistics for the PHI-ML corpus
	</Abstractive Summary>
	<Extractive Summary> =
		2 million words (Table 1)
	</Extractive Summary>
</Paper ID=ument667>


<Paper ID=ument667> <Table ID =2>
	<Abstractive Summary> =
		5%
		Table 2: Predictive performance on PHI-ML
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 2, the ancient historians’ restora-
		tions had a CER of 57
	</Extractive Summary>
</Paper ID=ument667>


<Paper ID=ument668> <Table ID =1>
	<Abstractive Summary> =
		089)
		Table 1: The results of our lexical centrality system using tensor embeddings along with the top four SemEval
		2017 Task 6B systems reproduced from Potash et al
	</Abstractive Summary>
	<Extractive Summary> =
		The results of our lexical centrality system us-
		ing tensor embeddings is shown in Table 1, where
		the ofﬁcial results of other state-of-the-art systems
		are taken from Potash et al
	</Extractive Summary>
</Paper ID=ument668>


<Paper ID=ument668> <Table ID =2>
	<Abstractive Summary> =
		797
		Table 2: The results of our label propagation system
	</Abstractive Summary>
</Paper ID=ument668>


<Paper ID=ument669> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Sentences generated using EDA
	</Abstractive Summary>
</Paper ID=ument669>


<Paper ID=ument669> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: Average performances (%) across ﬁve text
		classiﬁcation tasks for models with and without EDA
		on different training set sizes
	</Abstractive Summary>
</Paper ID=ument669>


<Paper ID=ument669> <Table ID =3>
	<Abstractive Summary> =
		1
		4
		Table 3: Recommended usage parameters
	</Abstractive Summary>
</Paper ID=ument669>


<Paper ID=ument669> <Table ID =4>
	<Abstractive Summary> =
		7 (5)
		yes
		no
		SR - kNN8 (1)
		no
		no
		EDA (5)
		no
		no
		Table 4: Related work in data augmentation
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, we show EDA’s ease of use compared
		with other techniques
	</Extractive Summary>
</Paper ID=ument669>


<Paper ID=ument669> <Table ID =5>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Summary statis-
		tics are shown in Table 5 in Supplemental Mate-
		rials
	</Extractive Summary>
</Paper ID=ument669>


<Paper ID=ument67> <Table ID =1>
	<Abstractive Summary> =
		(new improved variant of the Dridex trojan; was spread through; Andromeda botnet)
		Table 1: Extracted tuples by different OpenIE systems for an input sentence
		2
	</Abstractive Summary>
</Paper ID=ument67>


<Paper ID=ument67> <Table ID =2>
	<Abstractive Summary> =
		732
		Data Set
		# of Sentences
		# of Tuples
		AW-OIE
		3,300
		17,165
		AW-OIE-C
		3,300
		13,056
		WEB
		500
		461
		NYT
		222
		222
		PENN
		100
		51
		Table 2: Data sets used in this work
		AW-OIE corpus was created by extending the
		OIE2016 corpus released by (Stanovsky and Da-
		gan, 2016)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents more details on these
		datasets
	</Extractive Summary>
</Paper ID=ument67>


<Paper ID=ument67> <Table ID =3>
	<Abstractive Summary> =
		27
		-
		Table 3: Performance (F1-score) comparison of SenseOIE and the baseline systems
		datasets
	</Abstractive Summary>
</Paper ID=ument67>


<Paper ID=ument67> <Table ID =4>
	<Abstractive Summary> =
		52
		Table 4: Performance (F1-score) comparison of different feature sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows
		the F1-scores of several variations built with a dif-
		ferent subset of our features
	</Extractive Summary>
</Paper ID=ument67>


<Paper ID=ument67> <Table ID =5>
	<Abstractive Summary> =
		23
		Table 5: Performance (F1-score) of SupervisedOIE
		trained with the human-labeled data vs
	</Abstractive Summary>
</Paper ID=ument67>


<Paper ID=ument670> <Table ID =1>
	<Abstractive Summary> =
		29
		# news
		42,255
		# positive samples
		489,644
		# impressions
		445,230
		# negative samples
		6,651,940
		Table 1: Statistics of our dataset
	</Abstractive Summary>
</Paper ID=ument670>


<Paper ID=ument670> <Table ID =2>
	<Abstractive Summary> =
		4139
		Table 2: The results of different methods
	</Abstractive Summary>
</Paper ID=ument670>


<Paper ID=ument670> <Table ID =3>
	<Abstractive Summary> =
		7 min
		Table 3: The number of parameters in NRMS and base-
		line methods, and their time in encoding 1 million news
		and 1 million users
	</Abstractive Summary>
</Paper ID=ument670>


<Paper ID=ument671> <Table ID =1>
	<Abstractive Summary> =
		Target word
		Source word
		Uroman
		Type
		gr¨unen
		green
		gruenen
		ORG
		Europe
		iuropera
		LOC
		Table 1: Examples of Uroman that maps different lan-
		guages into the same character space
	</Abstractive Summary>
</Paper ID=ument671>


<Paper ID=ument671> <Table ID =2>
	<Abstractive Summary> =
		26
		Table 2: F1 score comparisons of cross-lingual models
		on Spanish, Dutch and German
	</Abstractive Summary>
	<Extractive Summary> =
		As seen in Table 2, our proposed method out-
		performs the previous works in nearly all cross-
		lingual tasks
	</Extractive Summary>
	<Extractive Summary> =
		With comparing to the overall performance
		across the three languages in Table 2, we can ob-
		serve that the single token shows relatively closer
		6399
		Language
		1
		2
		≥3
		Spanish
		68
	</Extractive Summary>
</Paper ID=ument671>


<Paper ID=ument671> <Table ID =3>
	<Abstractive Summary> =
		46
		Table 3: F1 scores of different ablation analyses, com-
		pared to our full model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows that bilingual word embed-
		dings have better performance than character em-
		beddings across the three languages
	</Extractive Summary>
</Paper ID=ument671>


<Paper ID=ument671> <Table ID =4>
	<Abstractive Summary> =
		22
		Table 4: F1 scores of different lengths of entities across
		the three languages: Spanish, Dutch and German
	</Abstractive Summary>
</Paper ID=ument671>


<Paper ID=ument671> <Table ID =5>
	<Abstractive Summary> =
		29
		Table 5: F1 score comparisons of translation-based
		models on Bengali
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 5 show that our
		model outperforms the previous methods without
		Wikipedia
	</Extractive Summary>
</Paper ID=ument671>


<Paper ID=ument671> <Table ID =6>
	<Abstractive Summary> =
		2
		Table 6: OOV rate (percentage) in our bilingual word
		embeddings across the four languages
	</Abstractive Summary>
</Paper ID=ument671>


<Paper ID=ument672> <Table ID =1>
	<Abstractive Summary> =
		Sources
		Words
		Sentences Jokes
		Websites
		2,397,816 23,508
		5,463
		Micro-blogs
		1,207,856 11,614
		2,581
		Books,Journals 504,920
		4,855
		1,079
		Total
		4,110,592 39,977
		9,123
		Table 1: Information on data sources
	</Abstractive Summary>
	<Extractive Summary> =
		(Gan, 2015; Stein,
		1998; Tsakona, 2009)
		• DataSource: Table 1 presents the source of
		jokes
	</Extractive Summary>
</Paper ID=ument672>


<Paper ID=ument673> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Human evaluation criteria
	</Abstractive Summary>
</Paper ID=ument673>


<Paper ID=ument673> <Table ID =2>
	<Abstractive Summary> =
		17)
		Table 2: Human evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the human evaluation results,
		where we compute the average score by ﬁrst com-
		4https://github
	</Extractive Summary>
</Paper ID=ument673>


<Paper ID=ument673> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Example anagrams generated by the proposed method and baseline methods
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows example anagrams
		generated by the proposed method and the base-
		line method
	</Extractive Summary>
</Paper ID=ument673>


<Paper ID=ument673> <Table ID =4>
	<Abstractive Summary> =
		Anagrams
		Proposed outputs
		William Shakespeare
		↓
		I am a weakish speller
		Phillies was a remake
		We all make his praise
		Speaker William Shea
		Hawaii keeps smaller
		William Ashe speaker
		Tom Marvolo Riddle
		↓
		I am Lord Voldemort
		I told Mr Lord a move
		Roll met David Romo
		Mr Ali drove to Mold
		Dr Millar moved too
		Mr Orlov told media
		Table 4: Results of generating famous anagrams
	</Abstractive Summary>
</Paper ID=ument673>


<Paper ID=ument674> <Table ID =1>
	<Abstractive Summary> =
		6415
		Split
		Supporting
		Pairs
		Opposing
		Pairs
		Total
		Pairs
		train
		3603
		3404
		7007
		dev
		1051
		1045
		2096
		test
		1471
		1302
		2773
		Total
		6125
		5751
		11876
		Table 1: Perspectrum data statistics
	</Abstractive Summary>
</Paper ID=ument674>


<Paper ID=ument674> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Comparison of our approach BERTCONS with different baseline models for stance classiﬁcation
	</Abstractive Summary>
</Paper ID=ument674>


<Paper ID=ument674> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Top phrases for determining stance
	</Abstractive Summary>
</Paper ID=ument674>


<Paper ID=ument675> <Table ID =1>
	<Abstractive Summary> =
		6421
		Language
		Train
		Dev
		Test
		EN
		39,094
		4,887
		4,894
		MT
		39,094
		4,887
		-
		FR
		-
		-
		4,336
		Table 1: Data set sizes for each approach
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the
		volume of each of the data sets
	</Extractive Summary>
</Paper ID=ument675>


<Paper ID=ument675> <Table ID =2>
	<Abstractive Summary> =
		73
		Table 2: Test set results for the experiments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the main
		results
	</Extractive Summary>
</Paper ID=ument675>


<Paper ID=ument676> <Table ID =1>
	<Abstractive Summary> =
		8%
		Table 1: The accuracy of NL-RX models
	</Abstractive Summary>
	<Extractive Summary> =
		SoftRegex: Table 1 shows the experimental re-
		sults of Deep-Regex, SemRegex, and our Soft-
		Regex model
	</Extractive Summary>
</Paper ID=ument676>


<Paper ID=ument676> <Table ID =2>
	<Abstractive Summary> =
		*\b
		\b([a-z]){4,}\b
		True error (type-4)
		Table 2: Example of errors caused by SoftRegex for NL-RX-Turk
	</Abstractive Summary>
	<Extractive Summary> =
		We investi-
		gate all 921 incorrect predictions of SoftRegex in
		NL-RX-Turk and categorize the resulting errors
		into 4 types (Table 2)
	</Extractive Summary>
</Paper ID=ument676>


<Paper ID=ument676> <Table ID =3>
	<Abstractive Summary> =
		→
		a character
		Table 3: The non-terminal and terminal operations
		with their corresponding natural language description
		as constructed by Locascio et al
	</Abstractive Summary>
</Paper ID=ument676>


<Paper ID=ument677> <Table ID =1>
	<Abstractive Summary> =
		453
		Table 1:
		Mean metrics for all three tasks
	</Abstractive Summary>
</Paper ID=ument677>


<Paper ID=ument678> <Table ID =1>
	<Abstractive Summary> =
		Our machine teacher tries to construct macaronic
		sentences that the human student ought to under-
		stand, given all the learning that our generic model
		predicts would have happened from the previous
		6439
		Sentence
		The
		Nile
		is
		a
		river
		in
		Africa
		Gloss
		Der
		Nil
		ist
		ein
		Fluss
		in
		Afrika
		Macaronic
		Der
		Nile
		ist
		a
		river
		in
		Africa
		Conﬁgurations
		The
		Nile
		is
		a
		Fluss
		in
		Africa
		Der
		Nil
		ist
		ein
		river
		in
		Africa
		Table 1:
		An example English (L1) sentence with
		German (L2) glosses
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows an
		example English sentence with German glosses and
		three possible macaronic conﬁgurations (there are
		exponentially many conﬁgurations)
	</Extractive Summary>
	<Extractive Summary> =
		A search state is a pair (i,x) where x is a macaronic
		conﬁguration (Table 1) whose ﬁrst i tokens may
		be either L1 or L2, but whose remaining tokens are
		still L1
	</Extractive Summary>
</Paper ID=ument678>


<Paper ID=ument678> <Table ID =2>
	<Abstractive Summary> =
		0062(14)
		Table 2: Average token guess quality (⌧ = 0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 reports the mean comprehension score
		over all subjects, broken down into comprehen-
		sion of function words (closed-class POS) and
		content words (open-class POS)
	</Extractive Summary>
</Paper ID=ument678>


<Paper ID=ument678> <Table ID =3>
	<Abstractive Summary> =
		0037(24)
		Table 3: Average type guess quality (⌧ = 0
	</Abstractive Summary>
	<Extractive Summary> =
		As Table 3
		shows, sGSM’s advantage over GSM on comprehen-
		sion holds up on retention
	</Extractive Summary>
	<Extractive Summary> =
		On the vocabulary quiz,
		students correctly translated > 30 of the 71 word
		types they had seen (Table 8), and more than half
		when near-synonyms earned partial credit (Table 3)
	</Extractive Summary>
</Paper ID=ument678>


<Paper ID=ument679> <Table ID =1>
	<Abstractive Summary> =
		html
		6446
		Intervention Type
		Example Snippet
		anti-retroviral treatment
		postpartum ARV drugs may also be given to infants
		capacity building human rights
		mission personnel are also engaged in building the capacity of national authorities
		to promote and respect human rights
		child friendly learning spaces
		promotes quality education for indigenous girls and boys through child-friendly
		learning environments
		provision of goods and services
		• provide cash
		cash distributions during emergencies
		• provide delivery kit
		distributing a home delivery kit to every pregnant woman
		• provide education kit
		developing and freely distributing education materials
		• provide farming tool
		the scope of the program encompasses provision of fertilizer
		• provide ﬁshing tool
		restoration of livelihoods through provision of ﬁshing boats and ﬁshing equipment
		• provide food
		food aid is often supplied in emergency situations together with seed aid
		• provide hygiene tool
		respond to humanitarian emergencies always aim to distribute soap routinely
		• provide livestock feed
		where they were provided with fodder
		• provide seed
		food aid is often supplied in emergency situations together with seed aid
		• provide veterinary service
		providing free or subsidized animal health services
		sexual violence management
		health professionals expected to provide post-rape care
		therapeutic feeding or treating
		therapeutic food provided in supplementary feeding centers
		vector control
		Malathion is commonly used to control mosquitoes
		Table 1: Types of interventions with example text snippets, where event triggers are italicized
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, a large number of the in-
		tervention types (e
	</Extractive Summary>
</Paper ID=ument679>


<Paper ID=ument679> <Table ID =2>
	<Abstractive Summary> =
		68
		Table 2: Intervention types with number of trigger ex-
		amples and F1-scores based on 5-fold cross validation
	</Abstractive Summary>
	<Extractive Summary> =
		The
		“Count” column of Table 2 shows numbers of ex-
		amples for each intervention type
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2,
		the coarse-grained F1-score of provision of goods
		and services is 0
	</Extractive Summary>
</Paper ID=ument679>


<Paper ID=ument68> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: The comparison to cross-lingual ED models
		for Chinese ED in ACE 2005
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 gives the results
	</Extractive Summary>
</Paper ID=ument68>


<Paper ID=ument68> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Experimental results in exploring different
		lexical mapping methods
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 sum-
		marizes the results
	</Extractive Summary>
</Paper ID=ument68>


<Paper ID=ument68> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Experimental results in exploring the shared
		syntactic order event detector
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		the results
	</Extractive Summary>
</Paper ID=ument68>


<Paper ID=ument68> <Table ID =4>
	<Abstractive Summary> =
		53
		people
		人
		人
		人(people), 人们(folk), 青年人(youngs)
		died
		死
		死
		死(die), 死亡(death), 死去(dying)
		every
		每
		每
		每个
		个
		个(every), 所有(all), 都(all)
		year
		年
		年
		年(year), 年代(years), 年龄(age)
		from
		以外(beyond), 从
		从
		从(from), 外(except)
		the
		这(this), 整个(total), 完全(completely)
		ﬂu
		感染(inﬂection), 流
		流
		流感
		感
		感(ﬂu), 疾病(disease)
		Table 4: Translation candidates of each English word
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 and Figure
		6 gives the Chinese translation candidates and the
		learned attention weights respectively
	</Extractive Summary>
</Paper ID=ument68>


<Paper ID=ument680> <Table ID =1>
	<Abstractive Summary> =
		57
		Table 1: Data Statistics of RUN: statistics over different maps and the full corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows statis-
		tics over the dataset
	</Extractive Summary>
</Paper ID=ument680>


<Paper ID=ument680> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Linguistic Analysis of RUN: we analyze 30 randomly sampled instructions in RUN
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 characterize linguis-
		tic phenomena in RUN, categorized according to
		the catalogue of Chen et al
	</Extractive Summary>
</Paper ID=ument680>


<Paper ID=ument680> <Table ID =3>
	<Abstractive Summary> =
		89
		Table 3: Quantitative Comparison of the HCRC (An-
		derson et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		a quantitative comparison of the RUN dataset to
		previous datasets of map-based navigation
	</Extractive Summary>
</Paper ID=ument680>


<Paper ID=ument680> <Table ID =4>
	<Abstractive Summary> =
		12
		Table 4: Bounds on Accuracy for Sentences\Paragraphs, weighted averages over folds (std)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results for the baseline mod-
		els as well as the HUMAN measured performance
		on the task
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the results of our model as an
		ablation study, and Table 5 shows typical errors
		of each variant
	</Extractive Summary>
</Paper ID=ument680>


<Paper ID=ument680> <Table ID =5>
	<Abstractive Summary> =
		
		
		
		
		Table 5: Error analysis of all models, for different instructions, showing succeeded / failure on predicting the path
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results of our model as an
		ablation study, and Table 5 shows typical errors
		of each variant
	</Extractive Summary>
</Paper ID=ument680>


<Paper ID=ument681> <Table ID =1>
	<Abstractive Summary> =
		T2 ?
		Chuck: How about the Thai place?
		Table 1: Messages and threads in a chat channel
	</Abstractive Summary>
	<Extractive Summary> =
		For example, in Table 1, Alice and Bob talk about
		work, while Bob and Chuck chat about lunch, and
		this results in intermingled messages
	</Extractive Summary>
</Paper ID=ument681>


<Paper ID=ument681> <Table ID =2>
	<Abstractive Summary> =
		464
		Table 2: CATD models are compared with baselines wrt
	</Abstractive Summary>
	<Extractive Summary> =
		Main Results:
		Table 2 compares the CATD
		models and baselines on NMI, ARI and F1
	</Extractive Summary>
</Paper ID=ument681>


<Paper ID=ument681> <Table ID =3>
	<Abstractive Summary> =
		433
		Table 3: Analysis on Politics dataset
		Evaluation Metrics:
		Normalized mutual infor-
		mation (NMI), Adjusted rand index (ARI) and F1
		score, following (Jiang et al
	</Abstractive Summary>
	<Extractive Summary> =
		Analysis
		: In Table 3, we analyze our models on
		Politics, the largest dataset
	</Extractive Summary>
</Paper ID=ument681>


<Paper ID=ument681> <Table ID =4>
	<Abstractive Summary> =
		414
		Table 4: Analysis on Gadgets dataset
		Comparison with Model Variations:
		In Ta-
		ble 3, we also shared the LSTM parameters for
		MATCH and FLOW models (A), with 4% drop
		on ARI
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4 and 5, we show the analysis for
		for Gadgets and Iphones datasets similar to Poli-
		tics dataset in Table 3
	</Extractive Summary>
</Paper ID=ument681>


<Paper ID=ument681> <Table ID =5>
	<Abstractive Summary> =
		434
		445
		Table 5: Analysis on Iphones dataset
		4
		Conclusion
		We propose context-aware thread detection mod-
		els to perform thread detection for multi-party chat
		conversations which take into account threads’
		contextual information
	</Abstractive Summary>
</Paper ID=ument681>


<Paper ID=ument682> <Table ID =1>
	<Abstractive Summary> =
		RuleID
		Rule
		Example
		R1
		O
		amod
		−−−→ A(NN)
		nice decor
		(nice
		amod
		−−−→ decor)
		R2
		A
		nsubj
		−−−−→ O(ADJ)
		the food was super tasty
		(food
		nsubj
		−−−−→ tasty)
		Table 1: Examples of the opinion and aspect terms ex-
		traction rules (I)
		(b) Revision history 2 (RH2)
		Table 1: Two revision histories, RH1 and RH2, from
		‘My husband and I enjoy LA Hilton Hotel7006
		Table 1: Results on test collections from the TREC Microblog Tracks, comparing BERT with selected neural
		ranking models
		Table 1: Examples of ranked predictions (from left-to-right) made by our system on a set of eight randomly selected
		test queries from SemEval 2018 English dataset5
		Table 1: Dev Set Performance on NQ with different
		pre-training & data augmentation techniques34
		Table 1: The accuracy of each condition tested on 30,000 lines and the perplexity of the original text and the text
		generated by our model
		Test Dataset
		Instances
		Alignment Type
		References
		PWKP
		93
		1-to-1
		1
		7
		1-to-N
		1
		TurkCorpus
		359
		1-to-1
		8
		HSplit
		359
		1-to-N
		4
		Table 1: Test datasets available in EASSE
		Table 1: Summary of dataset used for evaluation
		# Samples
		# First Names
		# Full Names
		2915
		1234
		1681
		For evaluating the personalization features, we
		augment the data with four different settings5
		Table 1: Test accuracies on GEO and ATIS datasets,
		where * indicates systems with extra resources are used
		Figure 7: The illustrate of semantic parser for search
		engine11
		Table 1: Individual macro-F1 scores following Schulz
		et al
		Table 1: An example conversation between Gunrock
		and a human user (User)2
		Table 1: Statistics for dataset of mobility information,
		using SpaCy and WordPiece tokenization0
		Table 1: Latency (lat22
		Table 1: The ﬁnal test results of article inference task
		Similarly ,
		↓
		↓
		Enter ←�
		In the
		The knowledge
		Thus ,
		So the
		उसी
		पर्कार
		मान�सक
		स्वास्थ्य
		के
		�लए
		ज्ञान
		क�
		पर्ाि�
		आवश्यक
		है
		In the same way , knowledge of knowledge is essential for mental health
		same way
		Tab Tab Tab Tab
		उसी
		पर्कार
		मान�सक
		स्वास्थ्य
		के
		�लए
		ज्ञान
		क�
		पर्ाि�
		आवश्यक
		है
		In the same way , knowledge of knowledge is essential for mental health
		of knowledge
		i
		is essential
		is necessary
		for mental
		उसी
		पर्कार
		मान�सक
		स्वास्थ्य
		के
		�लए
		ज्ञान
		क�
		पर्ाि�
		आवश्यक
		है
		In the same way , knowledge is essential for mental health
		is essential for
		Enter ←�
		is necessary for
		is required to
		उसी
		पर्कार
		मान�सक
		स्वास्थ्य
		के
		�लए
		ज्ञान
		क�
		पर्ाि�
		आवश्यक
		है
		In the same way , knowledge is essential for mental health
		Page ↓
		Table 1: Translation workflow when translating from “उसी पर्कार मान�सक स्वास्थ्य के �लए ज्ञान क� पर्ाि� आवश्यक है”
		to “In the same way , knowledge is essential for mental health”62
		Table 1: Python code statistics for OpenNMT-py (com-
		mit hash 624a0b3a), XNMT (a87e7b94) and Joey
		NMT (e55b615), 2002)
		YES
		YES
		YES
		NO
		NO
		TcK/TK
		Table 1: Annotator Tool Comparison Table
		Turn/Dialogue Segmentation: segment raw text into turns and dialogues, 2016)
		1
		7-DT
		×
		×
		×
		(K¨ohn, 2015)
		7
		7-PT
		×
		×
		Ours
		28
		16-PT
		×
		×
		×
		×
		×
		×
		Table 1: Features of previous evaluation applications compared to Ours (LINSPECTOR WEB) To automatically create the RelatedTo re-
		lation between concepts, we compute relatedness
		Figure 5: Study timeline in MAssistant
		Table 1: Features for discovering prerequisite relations
		between concepts A and B
		Features
		Explanation
		RefD
		Reference distance of A and B in
		Wikipedia (Liang et alco/
		142
		R1*
		R2*
		R1
		R2
		# Documents
		107
		117
		100
		100
		# Concepts
		351
		344
		317
		317
		# Historical
		67
		80
		79
		65
		# Not Historical
		276
		264
		238
		252
		Table 1: Total labelled ‘seizure’ symptom concepts and
		for each human annotator (R1, R2) for the ‘temporal-
		ity’ task of labelling concepts that have occurred the
		past relative to the hospital episode}
		Table 1: Example output: Model predictions the top-k answers and the attended memory nodes are partially
		shown for each question and ground-truth answer pair5%
		Table 1: Task analysis of NLP DNN jobs submitted to a
		commercial centralized GPU cluster21
		Table 1: Accuracies of various models on Wiki80 and
		SemEval 2010 Task-8 under the single sentence setting9
		Table 1: Experimental results for the negotiation ex-
		ampleorg
		196
		System
		Web-based
		project creation
		Project
		monitoring
		Curation
		feature
		Document
		propagation
		Class labels
		Dynamic
		Hierarchical
		Multi-label
		BRAT
		
		
		
		
		
		
		
		GATE
		
		
		
		
		
		
		
		SANTO
		
		
		
		
		
		
		
		SAWT
		
		
		
		
		
		
		
		YEDDA
		
		
		
		
		
		
		
		WebAnno
		
		
		
		
		
		
		
		Redcoat
		
		
		
		
		
		
		
		Table 1: A comparison of existing annotation tools with Redcoat5
		–
		–
		–
		–
		–
		–
		–
		–
		–
		–
		–
		–
		Table 1: Results of the comparative evaluation of semantic search models on: (1) monolingual document retrieval
		(LATimes); metrics: NDCG@100, MAP@1000 and Precision@10; and (2) cross-lingual sentence retrieval (Eu-
		roparl, 5K sentence pairs, EN-DE, EN-IT, and EN-FI); metrics: MRR and Hits@{1, 5, 10}
		207
		Feature
		Pronouns and non-content adjectives
		1
		Alius
		2
		Idem
		3
		Ipse
		4
		Iste
		5
		Quidam
		6
		Demonstrative Pronouns
		7
		Personal Pronouns
		8
		Third-Person Pronouns
		Conjunctions
		9
		Atque + Consonant
		10
		Conjunctions
		Subordinate clauses
		11
		Antequam
		12
		Cum
		13
		Dum
		14
		Priusquam
		15
		Quin
		16
		Quominus
		17
		Conditional Markers
		18
		Fraction of Sentences with Relative Clauses
		19
		Mean Length of Relative Clauses
		Miscellaneous
		20
		Ut
		21
		Interrogative Sentences
		22
		Mean Length of Sentences
		23
		Prepositions
		24
		Regular Superlatives
		25
		Selected Gerunds & Gerundives
		26
		Selected Vocatives
		Table 1: Full set of Latin stylometric features61) ‡
		Table 1: Tasks results for section-agnostic, and section-
		based
		Source
		Target
		<s> <s> k t b
		<s> <s> ka ta ba
		<s> k t b
		A l w l d
		<s> ka ta ba
		A lo wa la du
		k t b
		A l w l d
		A l d r s
		ka ta ba
		A lo wa la du
		A l d∼a ro sa
		A l w l d
		A l d r s
		<e>
		A lo wa la du
		A l d∼a ro sa
		<e>
		A l d r s
		<e> <e>
		A l d∼a ro sa
		<e> <e>
		Table 1: Example sentence: “ktb Alwld Aldrs” with
		context window size of 3, deﬁnition)
		Search
		Searching external resources such as corpora and dictionaries
		Table 1: WATs supported by TEASPN Then
		we produce an error type including an edit type
		(insert, delete, and replace) and PoS of the edit
		236
		Table 1: top 10 error codes
		Code
		Gloss
		Samples Sentence
		1com/huggingface
		241
		Model
		Corpus
		Encoder
		Target
		Skip-thoughts
		Bookcorpus
		GRU
		Conditioned LM
		Quick-thoughts
		Bookcorpus+UMBCcorpus
		GRU
		Sentence prediction
		CoVe
		English-German
		Bi-LSTM
		Machine translation
		Infersent
		Natural language inference
		LSTM;GRU;CNN;LSTM+Attention
		Classiﬁcation
		ELMO
		1billion benchmark
		Bi-LSTM
		Language model
		ULMFiT
		Wikipedia
		LSTM
		Language model
		GPT
		Bookcorpus; 1billion benchmark
		Transformer
		Language model
		BERT
		Wikipedia+bookcorpus
		Transformer
		Cloze+sentence prediction
		Table 1: 8 pre-training models and their differences We search Lakers as
		subject in every month and sum up all the label
		rank
		verbs for Le-
		Bron James
		ﬁxed main objects
		1
		miss
		games
		2
		suffer
		a groin strain injury
		3
		make
		no ﬁxed main objects
		4
		leave
		Cleveland Cavaliers
		5
		lead
		the team
		Table 1: Verb Rankings for LeBron James in January
		Figure 5: Breaking News Tracking on Trade Rumorscom/odashi/mteval
		253
		Source Type
		Example Tasks
		Text
		machine translation, text summarization,
		dialog generation, grammatical error cor-
		rection, open-domain question answering
		Image
		image captioning, visual question answer-
		ing, optical character recognition
		Audio
		speech recognition, speech translation
		Video
		video description
		Multimodal
		multimodal machine translation
		Table 1: Example text generation tasks supported by
		VizSeq
		262
		Tok
		MA
		MD
		POS
		Lem
		Feats
		Deps
		Joint
		Tasks
		MILA
		✓
		✓
		NITE
		✓
		✓
		✓
		Hebrew-NLP
		✓
		Adler
		✓
		✓
		✓
		Goldberg
		✓
		Pipelines
		UDPipe
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		CoreNLP
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		ONLP
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Table 1: Existing Coverage for Hebrew NLP Tasks
		sented in separate accordion tabs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows two of the 8 rules that are
		used along with example sentences
	</Extractive Summary>
	<Extractive Summary> =
		For example, in Table 1,
		human writers could prefer replacing the subject
		(Rs) and the object (Ro) as RH1 than replacing
		the verb (Rv) as RH2
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, P3, P4 in RH1 and
		P1, P3, P4 and P6 in RH2 are all valid revisions of
		the original sentence
	</Extractive Summary>
	<Extractive Summary> =
		Experimental results are shown in Table 1,
		where we present average precision (AP) and pre-
		cision at rank 30 (P@30), the two ofﬁcial met-
		rics of the evaluation (Ounis et al
	</Extractive Summary>
	<Extractive Summary> =
		5
		Conclusion and Future Work
		Table 1 shows the result of applying our KGIS sys-
		tem on the English Domain corpus of the SemEval
		2018 Hypernym Detection shared task
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		shows that this strategy can provide an absolute
		improvement of 2
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		labels these results as “BERT w/ U-MRC” and
		shows that this additional training on a MRC spe-
		ciﬁc unsupervised task improves the model’s ﬁnal
		ﬁne-tuned performance on the NQ task by 1
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1,
		the model can almost perfectly satisfy the request
		from users
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		gives the overall stats and name type distribution
		of the evaluation dataset
	</Extractive Summary>
	<Extractive Summary> =
		From Table 1 we can see that:
		1) Our
		method achieved comparative performances on
		both datasets
	</Extractive Summary>
	<Extractive Summary> =
		In Table 1, we report the per-
		formance of the BiLSTM implementation for pre-
		dicting epistemic activities in the Med and TEd
		data
	</Extractive Summary>
	<Extractive Summary> =
		, 2018), Gunrock
		generates more balanced conversations between
		human and machine by encouraging and under-
		standing more human inputs (see Table 1 for an
		example)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, our mobility dataset is
		considerably imbalanced between relevant and ir-
		relevant tokens
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes 90th percentile latency and
		recognition accuracy results for both res8 and
		res8-narrow on various devices
	</Extractive Summary>
	<Extractive Summary> =
		2% for res8-narrow (see
		the ﬁrst few rows in Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 compares LIDA
		with other recent annotation tools
	</Extractive Summary>
	<Extractive Summary> =
		4
		Evaluation
		Table 1 shows a comparison of LIDA to other an-
		notation tools
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 out-
		lines all the features we use
	</Extractive Summary>
	<Extractive Summary> =
		For each candidate concept pair,
		we ﬁrst compute the features in Table 1, and then
		feed the features to a logistic regression model
		to determine whether they have the prerequisite
		relation
	</Extractive Summary>
	<Extractive Summary> =
		3
		Demonstration
		Table 1 shows some of the example output from
		the demonstrated system given the input query
		and memory graph nodes
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 showed that about 87
	</Extractive Summary>
	<Extractive Summary> =
		From Table 1
		we can see that BERT-based models perform bet-
		172
		Model
		5-Way 1-Shot
		5-Way 5-Shot
		5-Way 1-Shot (*)
		5-Way 5-Shot (*)
		Prototype-CNN
		74
	</Extractive Summary>
	<Extractive Summary> =
		As reported in Table 1, planning (using either
		Forward or MCTS) improves the negotiation out-
		come over the baseline in terms of both reward and
		3(Py)OpenDial includes None action with utility 0 by de-
		fault, thus we assigned small positive utility to the generated
		utterances to be distinguished from the None action
	</Extractive Summary>
	<Extractive Summary> =
		5
		Comparison with existing tools
		Table 1 provides a qualitative comparison between
		Redcoat and other existing annotation tools
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 1 summarizes the monolingual document re-
		trieval and cross-lingual sentence retrieval results
	</Extractive Summary>
	<Extractive Summary> =
		3
		Features
		Our feature set comprises twenty-six stylometric
		features across four broad syntactic and grammat-
		ical categories (pronouns and non-content adjec-
		tives, subordinate clauses, conjunctions, and mis-
		cellaneous, as listed in Table 1) and is described in
		detail in a previous publication (Chaudhuri et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the results across
		the 3 tasks
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 provides an
		example for a three words sentence “ktb Alwld
		Aldrs” (the boy wrote the lesson) with a 3 word
		sliding window
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the list
		of WATs that are supported by TEASPN
	</Extractive Summary>
	<Extractive Summary> =
		For simplicity, we
		limit ourselves to Top 10 most common error types
		(in Table 1) in CLE-FCE (Yannakoudakis et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 lists 8 popular pre-training models and
		their main differences (Kiros et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 provides a list of
		example tasks supported by Vizseq
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the task-coverage of existing
		tools and toolkits for NLP in Hebrew, academic
		as well as private initiatives (NITE,Hebrew-NLP)
	</Extractive Summary>
</Paper ID=ument682>


<Paper ID=ument682> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Aspect term extraction evaluation (F1 score)5325
		Table 2: Results on Robust04, where nS denotes com-
		bining scores from the top n sentences in a document4
		Table 2: Dev Set Performance on SQuAD 2 with dif-
		ferent pre-training strategies45
		Table 2: Comparison of systems’ performance based
		on automatic metrics
		Table 2: Evaluation of the entity resolver with addition
		of different features
		Features
		R@1
		R@5
		+ Textual Features
		067
		Table 2: The overall results of the search engine, where
		+SP indicates systems with leveraging results from se-
		mantic parser
		Table 2: Example interaction between Gunrock and a
		human user (User) querying Gunrock’s backstory844
		Table 2: Annotation and ranking evaluation results on
		mobility documents, using three embedding sources2 min
		Table 2: Average in-browser ﬁne-tuning efﬁciency for res8-narrow under different conﬁgurations14
		Table 2: The results of article inference (Test set posi-
		tive:negative=1:4)98
		Table 2: Multi-BLEU Score with x% of partial input
		The difference in performance for different lan-
		guages is presumably because of the amount of re-
		sources available for each of them0
		Table 2: Results on WMT17 newstest201736
		Table 2: NeuronBlocks results on CoNLL-2003 English NER testb dataset892
		Table 2: Micro F1 scores of various models on Se-
		mEval 2010 Task-8 under the sentence-level RE set-
		ting1
		Table 2: Arabic variety identiﬁcation per input length
		When building the diacritization models, we
		used the OpenNMT-tf implementation for training
		with the hyperparameters suggested in the Open-
		NMT website24
		335 ± 91
		Table 2: Statistics of the written texts An-
		notated the input sentence with PoS, we could ex-
		237
		Table 2: Sample correction-feedback pairs from the training collection
		Problem Word
		Sentence and Feedback
		Analysis
		discuss
		 They would like to discuss about what to do next3
		Table 2: The performance of HuggingFace’s implementation and UER’s implementation on GLUE benchmark First, given
		250
		Rank
		Dec 2018
		Jan 2019
		Feb 2019
		Mar 2019
		1
		los angeles lakers
		los angeles lakers
		los angeles lakers
		los angeles lakers
		2
		lebron james
		pelicans
		lebron james
		lebron james
		3
		lonzo ball
		lebron james
		clippers
		clippers
		4
		clippers
		lonzo ball
		pelicans
		kevin durant
		5
		brandon ingram
		anthony davis
		boston celtics
		lonzo ball
		6
		kevin durant
		cavs
		kyle kuzma
		lebron
		7
		anthony davis
		boston celtics
		tobias harris
		giannis antetokounmpo
		8
		raptors
		rockets
		anthony davis
		magic johnson
		Table 2: Top 5 Words closest to the Word ‘lakers’ in Each Month
		Metrics
		VizSeq
		compare-
		mt
		nlg-
		eval
		MT-
		Compar-
		Eval
		BLEU
		chrF
		METEOR
		TER
		RIBES
		GLEU
		NIST
		ROUGE
		CIDEr
		WER
		LASER
		BERTScore
		Table 2: Comparison of VizSeq and its counterparts on
		n-gram-based and embedding-based metric coverage
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows an F1 score evaluation
		of the aspect term extraction task
	</Extractive Summary>
	<Extractive Summary> =
		It is noticeable from Table 2 that the results of
		the unsupervised output of the system (‘ABSApp-
		unsup’) are noisy, but that the weakly-supervised
		output results (‘ABSApp-wksup’) are quite com-
		parable to the cited transfer-learning based meth-
		ods, however, the latter require annotated data
		from a source domain (the results shown are av-
		eraged across tests using data from 2 different an-
		notated source domains), whereas ABSApp relies
		on a short weak supervision process but does not
		require any labeled data, which is often unavail-
		able in applied industrial settings
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 presents an extract of results
		from Yilmaz et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 similarly shows the beneﬁts of these pre-
		training strategies on the SQuAD 2
	</Extractive Summary>
	<Extractive Summary> =
		For cal-
		culating Reference values in Table 2, we sample
		one of the 8 human references for each instance as
		others have done (Zhang and Lapata, 2017)
	</Extractive Summary>
	<Extractive Summary> =
		8% improve-
		ment of R@1 over the baseline (Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 presents our results
	</Extractive Summary>
	<Extractive Summary> =
		One contribution of Gunrock is the extensive Gun-
		rock Persona Backstory database, consisting of
		over 1,000 responses to possible questions for
		Gunrock as well as reasoning for her responses for
		roughly 250 questions (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results on mobility
		Table 2 shows the token-level annotation and doc-
		ument ranking results for our experiments on mo-
		bility information
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the in-browser ﬁne-tuning
		efﬁciency for res8-narrow on the 2017 Mac-
		Book Pro and our desktop
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the
		average BLEU score for each language pair at dif-
		ferent values of x
	</Extractive Summary>
	<Extractive Summary> =
		7
		Table 2 shows that Joey NMT performs very well
		compared against other shallow, deep and Trans-
		former models, despite its simple code base
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows
		the results on CoNLL-2003 Englist testb dataset,
		with 12 different combinations of network lay-
		ers/blocks, such as word/character embedding,
		CNN/LSTM and CRF
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows one sample correction-
		explanation pairs with a problem word and an ex-
		planation template
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 and 3 compare UER’s perfor-
		mance to other publicly available systems
	</Extractive Summary>
	<Extractive Summary> =
		For ex-
		ample, Table 2 conﬁrms with Figure 5 that break-
		ing news of Anthony Davis and Lakers happened
		because of the trade rumors
	</Extractive Summary>
	<Extractive Summary> =
		2
		Metric Coverage and Scalability
		Table 2 shows the comparison of VizSeq and its
		counterparts on metric coverage
	</Extractive Summary>
</Paper ID=ument682>


<Paper ID=ument682> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Weakly-supervised ABSApp aspect term po-
		larity evaluation00
		Table 3: F1 and latencies for BERT base and large
		models running on GPU and CPU for a subset of the
		NQ dev set70
		Table 3: Transformation-based performance of the sen-
		tence simpliﬁcation systems in the TurkCorpus test set627
		Table 3: Performance of the ﬁnal system based on the
		distance between the caller and the callee
		Caller Distance
		R@1
		R@5
		Same team
		03
		Table 3: The results of analysis modules0
		Table 3: IWSLT14 test results8
		Table 3: NeuronBlocks results on GLUE benchmark development sets2
		-
		-
		Table 3: Accuracies of various models on FewRel under the different few-shot settings7
		Table 3: Number of words in training and test data for
		MSA, CA, and DA
		identical test sets
		Table 3: Template for Replace a Preposition
		Problem Word
		Sentence and Feedback
		Analysis
		arm
		 She would not stop crying until I held her on my arms6
		Table 3: The performance of ERNIE’s implementation and UER’s implementation on ERNIE benchmark
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows an evaluation of the weakly-
		supervised ABSApp lenient performance of the
		aspect term polarity task
	</Extractive Summary>
	<Extractive Summary> =
		It is seen from Table 3 that although the recall in
		both tests is not high (because it reﬂects correct de-
		tection of an aspect term, an opinion term and a re-
		lation between them), the precision is above 70%
	</Extractive Summary>
	<Extractive Summary> =
		Word-level Transformations
		In order to better
		understand the previous results, we use the word-
		level annotations of text transformations (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		Table 3
		breaks down the recall of the system by the inter-
		action distances
	</Extractive Summary>
	<Extractive Summary> =
		9 Table 3
		shows that Joey NMT performs well here, with
		both its recurrent and its Transformer model
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 3, the models built by Neuron-
		Blocks can achieve competitive or even better re-
		sults on GLUE tasks with minimal coding efforts
	</Extractive Summary>
	<Extractive Summary> =
		From Table 3
		we can see that for both few-shot settings, our ver-
		sion achieves better results than the original results
		from Han et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 lists the details of the training and test
		sets including the unique diacritized and undia-
		critized tokens and the percentage of OOVs in the
		test set that don’t appear in the training set
	</Extractive Summary>
	<Extractive Summary> =
		First, we use a template (in Table 3) that shows the
		grammar patterns of the problem-causing word
	</Extractive Summary>
</Paper ID=ument682>


<Paper ID=ument682> <Table ID =4>
	<Abstractive Summary> =
		21
		Table 4: Quality estimation features, which give addi-
		tional information on the output of different systems
		Table 4: WER comparison before and after ASR cor-
		rection
		Model
		Name WER
		Transcript WER
		ASR transcripts
		868941
		Table 4: NeuronBlocks results on Knowledge Distillation task42
		Table 4: AUC and F1 scores of various models on
		NYT10 under the bag-level RE setting8
		Table 4: Results and comparison of full diacritization
		systems, verb, noun, and adjective), we han-
		dled the errors by cases: (1) spelling, (2) tense,
		238
		Table 4: Template for Replacing a Word
		Problem Word
		Sentence and Feedback
		Analysis
		abandon
		 Since capital punishment was abandoned, the crime rate has increased8
		Table 4: Performance of pre-training models with dif-
		ferent targets
	</Abstractive Summary>
	<Extractive Summary> =
		Quality Estimation Features
		Table 4 displays
		a subset of QE features that reveal other aspects
		of the simpliﬁcation systems
	</Extractive Summary>
	<Extractive Summary> =
		Table 4
		shows the results, where AUC is used as the eval-
		uation criteria and Queries per Second (QPS) is
		used to measure inference speed
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows that our ver-
		sion of bag-level RE models achieves comparable
		or even better results than the original papers
	</Extractive Summary>
	<Extractive Summary> =
		1
		System Results
		Table 4 summarizes the results per dataset and
		compares our system to other SOTA systems on
		2https://github
	</Extractive Summary>
</Paper ID=ument682>


<Paper ID=ument682> <Table ID =5>
	<Abstractive Summary> =
		54
		Table 5: NeuronBlocks results on WikiQA
		2 $col denotes a collocation of $pw
		Table 5:
		Scores (0-2) for explanations on top 10 er-
		ror codes, generated by four systems:
		TellMeWhy-GEC
		(TMW-GEC),TellMeWhy-FindExample (TMW-FE) Gram-
		marly (GL) and Write & Improve (W&I)
		Code
		Gloss
		TMW-GEC
		TMW-FE
		GL
		W&I
		15
		Table 5: Performance of pre-training models with dif-
		ferent encoders
	</Abstractive Summary>
	<Extractive Summary> =
		One evaluation of comparison between TellMe-
		Why and other commercial systems is shown in
		Table 5 was evaluated by ten sentences for each
		error type and carried out by a linguist
	</Extractive Summary>
	<Extractive Summary> =
		Table 5
		lists the results of different encoders
	</Extractive Summary>
</Paper ID=ument682>


<Paper ID=ument682> <Table ID =6>
	<Abstractive Summary> =
		2
		Evaluation
		Once we have trained TellMeWhy as described,
		we evaluated the performance using ten randomly-
		Table 6: The number of testcases can be corrected by three
		systems: TellMeWhy-GEC (TMW-GEC), Grammarly (GL)
		and Write & Improve (W&I)
		Code
		Gloss
		TMW-GEC
		GL
		W&I
		1
	</Abstractive Summary>
	<Extractive Summary> =
		12484: Table 6
		in Appendix A
	</Extractive Summary>
</Paper ID=ument682>


<Paper ID=ument684> <Table ID =1>
	<Abstractive Summary> =
		RuleID
		Rule
		Example
		R1
		O
		amod
		−−−→ A(NN)
		nice decor
		(nice
		amod
		−−−→ decor)
		R2
		A
		nsubj
		−−−−→ O(ADJ)
		the food was super tasty
		(food
		nsubj
		−−−−→ tasty)
		Table 1: Examples of the opinion and aspect terms ex-
		traction rules
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows two of the 8 rules that are
		used along with example sentences
	</Extractive Summary>
</Paper ID=ument684>


<Paper ID=ument684> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Aspect term extraction evaluation (F1 score)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows an F1 score evaluation
		of the aspect term extraction task
	</Extractive Summary>
	<Extractive Summary> =
		It is noticeable from Table 2 that the results of
		the unsupervised output of the system (‘ABSApp-
		unsup’) are noisy, but that the weakly-supervised
		output results (‘ABSApp-wksup’) are quite com-
		parable to the cited transfer-learning based meth-
		ods, however, the latter require annotated data
		from a source domain (the results shown are av-
		eraged across tests using data from 2 different an-
		notated source domains), whereas ABSApp relies
		on a short weak supervision process but does not
		require any labeled data, which is often unavail-
		able in applied industrial settings
	</Extractive Summary>
</Paper ID=ument684>


<Paper ID=ument684> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Weakly-supervised ABSApp aspect term po-
		larity evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows an evaluation of the weakly-
		supervised ABSApp lenient performance of the
		aspect term polarity task
	</Extractive Summary>
	<Extractive Summary> =
		It is seen from Table 3 that although the recall in
		both tests is not high (because it reﬂects correct de-
		tection of an aspect term, an opinion term and a re-
		lation between them), the precision is above 70%
	</Extractive Summary>
</Paper ID=ument684>


<Paper ID=ument686> <Table ID =1>
	<Abstractive Summary> =
		(I)
		(b) Revision history 2 (RH2)
		Table 1: Two revision histories, RH1 and RH2, from
		‘My husband and I enjoy LA Hilton Hotel
	</Abstractive Summary>
	<Extractive Summary> =
		For example, in Table 1,
		human writers could prefer replacing the subject
		(Rs) and the object (Ro) as RH1 than replacing
		the verb (Rv) as RH2
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, P3, P4 in RH1 and
		P1, P3, P4 and P6 in RH2 are all valid revisions of
		the original sentence
	</Extractive Summary>
</Paper ID=ument686>


<Paper ID=ument687> <Table ID =1>
	<Abstractive Summary> =
		7006
		Table 1: Results on test collections from the TREC Microblog Tracks, comparing BERT with selected neural
		ranking models
	</Abstractive Summary>
	<Extractive Summary> =
		Experimental results are shown in Table 1,
		where we present average precision (AP) and pre-
		cision at rank 30 (P@30), the two ofﬁcial met-
		rics of the evaluation (Ounis et al
	</Extractive Summary>
</Paper ID=ument687>


<Paper ID=ument687> <Table ID =2>
	<Abstractive Summary> =
		5325
		Table 2: Results on Robust04, where nS denotes com-
		bining scores from the top n sentences in a document
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents an extract of results
		from Yilmaz et al
	</Extractive Summary>
</Paper ID=ument687>


<Paper ID=ument688> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Examples of ranked predictions (from left-to-right) made by our system on a set of eight randomly selected
		test queries from SemEval 2018 English dataset
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Conclusion and Future Work
		Table 1 shows the result of applying our KGIS sys-
		tem on the English Domain corpus of the SemEval
		2018 Hypernym Detection shared task
	</Extractive Summary>
</Paper ID=ument688>


<Paper ID=ument689> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Dev Set Performance on NQ with different
		pre-training & data augmentation techniques
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows that this strategy can provide an absolute
		improvement of 2
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		labels these results as “BERT w/ U-MRC” and
		shows that this additional training on a MRC spe-
		ciﬁc unsupervised task improves the model’s ﬁnal
		ﬁne-tuned performance on the NQ task by 1
	</Extractive Summary>
</Paper ID=ument689>


<Paper ID=ument689> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: Dev Set Performance on SQuAD 2 with dif-
		ferent pre-training strategies
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 similarly shows the beneﬁts of these pre-
		training strategies on the SQuAD 2
	</Extractive Summary>
</Paper ID=ument689>


<Paper ID=ument689> <Table ID =3>
	<Abstractive Summary> =
		00
		Table 3: F1 and latencies for BERT base and large
		models running on GPU and CPU for a subset of the
		NQ dev set
	</Abstractive Summary>
</Paper ID=ument689>


<Paper ID=ument69> <Table ID =1>
	<Abstractive Summary> =
		sentences
		9,000
		11,000
		N/A
		N/A
		N/A
		N/A
		N/A
		N/A
		Text span annotation
		yes
		yes
		no
		yes
		yes
		yes
		no
		yes
		Links to reference KB
		yes
		yes
		yes
		no
		yes
		no
		yes
		yes
		Cross-sentence facts
		yes
		yes
		yes
		no
		no
		yes
		yes
		yes
		Annotated facts
		13,000
		8,000
		84,000
		22,000
		56,000
		56,000
		40,000
		11M
		Properties
		15
		18
		41
		41
		100
		96
		5
		353
		New KB facts annotated
		77%
		100%
		100%
		100%
		0%
		100%
		0%
		0%
		Table 1: A dataset comparison according to our criteria for a desirable KBP benchmark dataset
	</Abstractive Summary>
</Paper ID=ument69>


<Paper ID=ument69> <Table ID =2>
	<Abstractive Summary> =
		(PER–ORG)
		635
		537
		318
		CHILD OF (PER–PER)
		888
		471
		296
		SPOUSE (PER–PER)
		1,338
		504
		298
		DATE FOUNDED (ORG–DATE)
		500
		543
		315
		HEADQUARTERS (ORG–LOC)
		880
		564
		296
		SUBSIDIARY OF (ORG–ORG)
		544
		481
		299
		FOUNDED BY (ORG–PER)
		764
		558
		346
		CEO (ORG–PER)
		643
		526
		350
		Total
		13,425
		9,073
		5,423
		Table 2: KnowledgeNet properties and their number of
		annotated facts and sentences
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents the number of annotated facts
		for each property
	</Extractive Summary>
	<Extractive Summary> =
		In total, excluding the proper-
		ties having literal objects (Table 2) we can assign
		a link to both subject and object for 52% of the
		facts
	</Extractive Summary>
</Paper ID=ument69>


<Paper ID=ument69> <Table ID =3>
	<Abstractive Summary> =
		82
		Table 3: The performance of our baseline approaches
		is well below human performance
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Experiments
		Table 3 presents the performance of our baseline
		systems compared to the human performance
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents the
		performance of our baselines
	</Extractive Summary>
</Paper ID=ument69>


<Paper ID=ument69> <Table ID =4>
	<Abstractive Summary> =
		59
		Table 4: The relation extraction component’s recall is
		limited by error propagation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the “maximum recall” for each
		baseline (i
	</Extractive Summary>
</Paper ID=ument69>


<Paper ID=ument691> <Table ID =1>
	<Abstractive Summary> =
		34
		Table 1: The accuracy of each condition tested on 30,000 lines and the perplexity of the original text and the text
		generated by our model
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1,
		the model can almost perfectly satisfy the request
		from users
	</Extractive Summary>
</Paper ID=ument691>


<Paper ID=ument692> <Table ID =1>
	<Abstractive Summary> =
		Test Dataset
		Instances
		Alignment Type
		References
		PWKP
		93
		1-to-1
		1
		7
		1-to-N
		1
		TurkCorpus
		359
		1-to-1
		8
		HSplit
		359
		1-to-N
		4
		Table 1: Test datasets available in EASSE
	</Abstractive Summary>
</Paper ID=ument692>


<Paper ID=ument692> <Table ID =2>
	<Abstractive Summary> =
		45
		Table 2: Comparison of systems’ performance based
		on automatic metrics
	</Abstractive Summary>
	<Extractive Summary> =
		For cal-
		culating Reference values in Table 2, we sample
		one of the 8 human references for each instance as
		others have done (Zhang and Lapata, 2017)
	</Extractive Summary>
</Paper ID=ument692>


<Paper ID=ument692> <Table ID =3>
	<Abstractive Summary> =
		70
		Table 3: Transformation-based performance of the sen-
		tence simpliﬁcation systems in the TurkCorpus test set
	</Abstractive Summary>
	<Extractive Summary> =
		Word-level Transformations
		In order to better
		understand the previous results, we use the word-
		level annotations of text transformations (Table 3)
	</Extractive Summary>
</Paper ID=ument692>


<Paper ID=ument692> <Table ID =4>
	<Abstractive Summary> =
		21
		Table 4: Quality estimation features, which give addi-
		tional information on the output of different systems
	</Abstractive Summary>
	<Extractive Summary> =
		Quality Estimation Features
		Table 4 displays
		a subset of QE features that reveal other aspects
		of the simpliﬁcation systems
	</Extractive Summary>
</Paper ID=ument692>


<Paper ID=ument694> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Summary of dataset used for evaluation
		# Samples
		# First Names
		# Full Names
		2915
		1234
		1681
		For evaluating the personalization features, we
		augment the data with four different settings
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		gives the overall stats and name type distribution
		of the evaluation dataset
	</Extractive Summary>
</Paper ID=ument694>


<Paper ID=ument694> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Evaluation of the entity resolver with addition
		of different features
		Features
		R@1
		R@5
		+ Textual Features
		0
	</Abstractive Summary>
	<Extractive Summary> =
		8% improve-
		ment of R@1 over the baseline (Table 2)
	</Extractive Summary>
</Paper ID=ument694>


<Paper ID=ument694> <Table ID =3>
	<Abstractive Summary> =
		627
		Table 3: Performance of the ﬁnal system based on the
		distance between the caller and the callee
		Caller Distance
		R@1
		R@5
		Same team
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3
		breaks down the recall of the system by the inter-
		action distances
	</Extractive Summary>
</Paper ID=ument694>


<Paper ID=ument694> <Table ID =4>
	<Abstractive Summary> =
		Table 4: WER comparison before and after ASR cor-
		rection
		Model
		Name WER
		Transcript WER
		ASR transcripts
		86
	</Abstractive Summary>
</Paper ID=ument694>


<Paper ID=ument695> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Test accuracies on GEO and ATIS datasets,
		where * indicates systems with extra resources are used
		Figure 7: The illustrate of semantic parser for search
		engine
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 1 we can see that:
		1) Our
		method achieved comparative performances on
		both datasets
	</Extractive Summary>
</Paper ID=ument695>


<Paper ID=ument695> <Table ID =2>
	<Abstractive Summary> =
		67
		Table 2: The overall results of the search engine, where
		+SP indicates systems with leveraging results from se-
		mantic parser
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents our results
	</Extractive Summary>
</Paper ID=ument695>


<Paper ID=ument696> <Table ID =1>
	<Abstractive Summary> =
		11
		Table 1: Individual macro-F1 scores following Schulz
		et al
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1, we report the per-
		formance of the BiLSTM implementation for pre-
		dicting epistemic activities in the Med and TEd
		data
	</Extractive Summary>
</Paper ID=ument696>


<Paper ID=ument697> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example conversation between Gunrock
		and a human user (User)
	</Abstractive Summary>
	<Extractive Summary> =
		, 2018), Gunrock
		generates more balanced conversations between
		human and machine by encouraging and under-
		standing more human inputs (see Table 1 for an
		example)
	</Extractive Summary>
</Paper ID=ument697>


<Paper ID=ument697> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Example interaction between Gunrock and a
		human user (User) querying Gunrock’s backstory
	</Abstractive Summary>
	<Extractive Summary> =
		One contribution of Gunrock is the extensive Gun-
		rock Persona Backstory database, consisting of
		over 1,000 responses to possible questions for
		Gunrock as well as reasoning for her responses for
		roughly 250 questions (see Table 2)
	</Extractive Summary>
</Paper ID=ument697>


<Paper ID=ument698> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Statistics for dataset of mobility information,
		using SpaCy and WordPiece tokenization
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, our mobility dataset is
		considerably imbalanced between relevant and ir-
		relevant tokens
	</Extractive Summary>
</Paper ID=ument698>


<Paper ID=ument698> <Table ID =2>
	<Abstractive Summary> =
		844
		Table 2: Annotation and ranking evaluation results on
		mobility documents, using three embedding sources
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results on mobility
		Table 2 shows the token-level annotation and doc-
		ument ranking results for our experiments on mo-
		bility information
	</Extractive Summary>
</Paper ID=ument698>


<Paper ID=ument699> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Latency (lat
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes 90th percentile latency and
		recognition accuracy results for both res8 and
		res8-narrow on various devices
	</Extractive Summary>
	<Extractive Summary> =
		2% for res8-narrow (see
		the ﬁrst few rows in Table 1)
	</Extractive Summary>
</Paper ID=ument699>


<Paper ID=ument699> <Table ID =2>
	<Abstractive Summary> =
		2 min
		Table 2: Average in-browser ﬁne-tuning efﬁciency for res8-narrow under different conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes the in-browser ﬁne-tuning
		efﬁciency for res8-narrow on the 2017 Mac-
		Book Pro and our desktop
	</Extractive Summary>
</Paper ID=ument699>


<Paper ID=ument7> <Table ID =1>
	<Abstractive Summary> =
		30
		Table 1: Average Precision @ 50 on the synthetic
		dataset of the two-step approach with CBOW
		note this would introduce another level of com-
		plexity in tuning model parameters
	</Abstractive Summary>
	<Extractive Summary> =
		The results are shown
		in Table 1 (two-step approach, comparing the ﬁrst
		and last time steps) and Table 2 (whole time se-
		ries)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows, how-
		ever, that alignment is necessary for continuously
		trained embeddings using the whole series as well
		as for independent ones when using the cosine dis-
		tance measure
	</Extractive Summary>
</Paper ID=ument7>


<Paper ID=ument7> <Table ID =2>
	<Abstractive Summary> =
		00
		Table 2: Average Precision @ 50 on the synthetic dataset using time series approaches with CBOW
	</Abstractive Summary>
	<Extractive Summary> =
		The results are shown
		in Table 1 (two-step approach, comparing the ﬁrst
		and last time steps) and Table 2 (whole time se-
		ries)
	</Extractive Summary>
	<Extractive Summary> =
		Overall, the approaches using the whole time se-
		ries (Table 2) are more effective than the two-step
		approaches; particularly with regard to ﬁnding C1
		pseudowords and avoiding D4 pseudowords
	</Extractive Summary>
	<Extractive Summary> =
		For the time series ap-
		proaches, independent training tends to perform
		better than continuous training (Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		We ﬁnd that the neighbourhood-based measure8
		tends to outperform the cosine measure for the
		change point detection approaches; however, co-
		sine tends to outperform the neighbourhood mea-
		sure for correlation approaches (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		For almost all conﬁgurations in Table 2, the AP
		@ 50 is better when the reference point is the last
		time-step
	</Extractive Summary>
	<Extractive Summary> =
		In general, however, the β
		values produced for D4 pseudowords appear to
		be smaller in magnitude than genuine semantic
		changes (C1–C3) resulting in average precision
		measures that generally match or exceed change
		point approaches (Table 2)
	</Extractive Summary>
</Paper ID=ument7>


<Paper ID=ument7> <Table ID =3>
	<Abstractive Summary> =
		Likewise the embedding for temple ini-
		tially reﬂected the popularity of the video game
		Temple Run but gradually shifted to the word’s
		canonical meaning, and the embedding for bcs ini-
		tially reﬂected its usage as an acronym for Bowl
		Championship Series (a selection system in Amer-
		ican college football), but then shifted towards
		74
		Comparing to ﬁrst
		time-step
		vine, temple, unfollowers, fa-
		vorited, mcm, glo, #ipadgames,
		shawn, retweeted, vow
		Comparing to last
		time-step
		isis, yasss, bcs, temple,
		,
		mcm,
		, ig, mila, glo
		Table 3: Top 10 semantic change candidates of the
		change-point detection approach without standardiza-
		tion, using independently trained and aligned CBOW
		embeddings and the cosine distance measure
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the top 10 seman-
		tic change candidates using independent, aligned
		CBOW embeddings
	</Extractive Summary>
</Paper ID=ument7>


<Paper ID=ument70> <Table ID =1>
	<Abstractive Summary> =
		Step T [SEP] Target Entity [CLS]
		Table 1: Templates for different proposed entity-centric modes of structuring input to the transformer networks
	</Abstractive Summary>
</Paper ID=ument70>


<Paper ID=ument70> <Table ID =2>
	<Abstractive Summary> =
		71
		Table 2: Performance of the rule-based baselines and
		the post conditioned models on the ingredient detection
		task of the RECIPES dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 compares the performance of the dis-
		cussed models against the baselines, evaluating
		per-step entity prediction performance
	</Extractive Summary>
</Paper ID=ument70>


<Paper ID=ument70> <Table ID =3>
	<Abstractive Summary> =
		05
		Table 3: Performances of different baseline models dis-
		cussed in Section 3, the ELMo baselines, and the pro-
		posed entity-centric approaches with the (D)ocument
		v (S)entence level variants formulated with both entity
		(F)irst v
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 3 compares the overall performances of
		our proposed models
	</Extractive Summary>
</Paper ID=ument70>


<Paper ID=ument70> <Table ID =4>
	<Abstractive Summary> =
		69
		Table 4: Top: we compare how much the model de-
		grades when it conditions on no ingredient at all (w/o
		ing
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results for these ablations
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows
		that while not being a strong baseline, the model
		achieves decent overall accuracy with the drop in
		UR being higher compared to CR
	</Extractive Summary>
</Paper ID=ument70>


<Paper ID=ument70> <Table ID =5>
	<Abstractive Summary> =
		22
		Table 5: Performance of the proposed models on the
		PROPARA dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 5 compares the performance of the pro-
		posed entity tracking models on the sentence level
		task
	</Extractive Summary>
	<Extractive Summary> =
		2
		State Change Detection
		For PROPARA, Table 5 shows that the model does
		not signiﬁcantly outperform the SOTA models in
		state change detection (Cat-1)
	</Extractive Summary>
</Paper ID=ument70>


<Paper ID=ument70> <Table ID =6>
	<Abstractive Summary> =
		0 → 0
		0 → 1
		1 → 0
		1 → 1
		#preds
		179
		526
		43
		301
		Table 6: Model predictions from the document level
		entity ﬁrst GPT model in 1049 cases of intermediate
		compositions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the
		model’s performance on this subset of cases, of
		which there are 1049 in the test set
	</Extractive Summary>
</Paper ID=ument70>


<Paper ID=ument70> <Table ID =7>
	<Abstractive Summary> =
		84
		Table 7: Performance for using unsupervised data for
		LM training
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 7, the incorpora-
		tion of external data leads to major improvements
		in the overall performance
	</Extractive Summary>
</Paper ID=ument70>


<Paper ID=ument70> <Table ID =8>
	<Abstractive Summary> =
		27
		Table 8: Results for each state change type
	</Abstractive Summary>
	<Extractive Summary> =
		We do a ﬁner-grained study in
		Table 8 by breaking down the performance for the
		three state changes: creation (C), movement (M),
		and destruction (D), separately
	</Extractive Summary>
</Paper ID=ument70>


<Paper ID=ument70> <Table ID =9>
	<Abstractive Summary> =
		79
		Table 9: Model’s performance degradation with input
		ablations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 presents these ablation studies
	</Extractive Summary>
</Paper ID=ument70>


<Paper ID=ument700> <Table ID =1>
	<Abstractive Summary> =
		22
		Table 1: The ﬁnal test results of article inference task
	</Abstractive Summary>
</Paper ID=ument700>


<Paper ID=ument700> <Table ID =2>
	<Abstractive Summary> =
		14
		Table 2: The results of article inference (Test set posi-
		tive:negative=1:4)
	</Abstractive Summary>
</Paper ID=ument700>


<Paper ID=ument700> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: The results of analysis modules
	</Abstractive Summary>
</Paper ID=ument700>


<Paper ID=ument701> <Table ID =1>
	<Abstractive Summary> =
		Similarly ,
		↓
		↓
		Enter ←�
		In the
		The knowledge
		Thus ,
		So the
		उसी
		पर्कार
		मान�सक
		स्वास्थ्य
		के
		�लए
		ज्ञान
		क�
		पर्ाि�
		आवश्यक
		है
		In the same way , knowledge of knowledge is essential for mental health
		same way
		Tab Tab Tab Tab
		उसी
		पर्कार
		मान�सक
		स्वास्थ्य
		के
		�लए
		ज्ञान
		क�
		पर्ाि�
		आवश्यक
		है
		In the same way , knowledge of knowledge is essential for mental health
		of knowledge
		i
		is essential
		is necessary
		for mental
		उसी
		पर्कार
		मान�सक
		स्वास्थ्य
		के
		�लए
		ज्ञान
		क�
		पर्ाि�
		आवश्यक
		है
		In the same way , knowledge is essential for mental health
		is essential for
		Enter ←�
		is necessary for
		is required to
		उसी
		पर्कार
		मान�सक
		स्वास्थ्य
		के
		�लए
		ज्ञान
		क�
		पर्ाि�
		आवश्यक
		है
		In the same way , knowledge is essential for mental health
		Page ↓
		Table 1: Translation workflow when translating from “उसी पर्कार मान�सक स्वास्थ्य के �लए ज्ञान क� पर्ाि� आवश्यक है”
		to “In the same way , knowledge is essential for mental health”
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
</Paper ID=ument701>


<Paper ID=ument701> <Table ID =2>
	<Abstractive Summary> =
		98
		Table 2: Multi-BLEU Score with x% of partial input
		The difference in performance for different lan-
		guages is presumably because of the amount of re-
		sources available for each of them
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the
		average BLEU score for each language pair at dif-
		ferent values of x
	</Extractive Summary>
</Paper ID=ument701>


<Paper ID=ument702> <Table ID =1>
	<Abstractive Summary> =
		62
		Table 1: Python code statistics for OpenNMT-py (com-
		mit hash 624a0b3a), XNMT (a87e7b94) and Joey
		NMT (e55b615)
	</Abstractive Summary>
</Paper ID=ument702>


<Paper ID=ument702> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Results on WMT17 newstest2017
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Table 2 shows that Joey NMT performs very well
		compared against other shallow, deep and Trans-
		former models, despite its simple code base
	</Extractive Summary>
</Paper ID=ument702>


<Paper ID=ument702> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: IWSLT14 test results
	</Abstractive Summary>
	<Extractive Summary> =
		9 Table 3
		shows that Joey NMT performs well here, with
		both its recurrent and its Transformer model
	</Extractive Summary>
</Paper ID=ument702>


<Paper ID=ument704> <Table ID =1>
	<Abstractive Summary> =
		, 2002)
		YES
		YES
		YES
		NO
		NO
		TcK/TK
		Table 1: Annotator Tool Comparison Table
		Turn/Dialogue Segmentation: segment raw text into turns and dialogues
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 compares LIDA
		with other recent annotation tools
	</Extractive Summary>
	<Extractive Summary> =
		4
		Evaluation
		Table 1 shows a comparison of LIDA to other an-
		notation tools
	</Extractive Summary>
</Paper ID=ument704>


<Paper ID=ument705> <Table ID =1>
	<Abstractive Summary> =
		, 2016)
		1
		7-DT
		×
		×
		×
		(K¨ohn, 2015)
		7
		7-PT
		×
		×
		Ours
		28
		16-PT
		×
		×
		×
		×
		×
		×
		Table 1: Features of previous evaluation applications compared to Ours (LINSPECTOR WEB)
	</Abstractive Summary>
</Paper ID=ument705>


<Paper ID=ument706> <Table ID =1>
	<Abstractive Summary> =
		To automatically create the RelatedTo re-
		lation between concepts, we compute relatedness
		Figure 5: Study timeline in MAssistant
		Table 1: Features for discovering prerequisite relations
		between concepts A and B
		Features
		Explanation
		RefD
		Reference distance of A and B in
		Wikipedia (Liang et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 out-
		lines all the features we use
	</Extractive Summary>
	<Extractive Summary> =
		For each candidate concept pair,
		we ﬁrst compute the features in Table 1, and then
		feed the features to a logistic regression model
		to determine whether they have the prerequisite
		relation
	</Extractive Summary>
</Paper ID=ument706>


<Paper ID=ument707> <Table ID =1>
	<Abstractive Summary> =
		co/
		143
		R1*
		R2*
		R1
		R2
		# Documents
		107
		117
		100
		100
		# Concepts
		351
		344
		317
		317
		# Historical
		67
		80
		79
		65
		# Not Historical
		276
		264
		238
		252
		Table 1: Total labelled ‘seizure’ symptom concepts and
		for each human annotator (R1, R2) for the ‘temporal-
		ity’ task of labelling concepts that have occurred the
		past relative to the hospital episode
	</Abstractive Summary>
</Paper ID=ument707>


<Paper ID=ument708> <Table ID =1>
	<Abstractive Summary> =
		}
		Table 1: Example output: Model predictions the top-k answers and the attended memory nodes are partially
		shown for each question and ground-truth answer pair
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Demonstration
		Table 1 shows some of the example output from
		the demonstrated system given the input query
		and memory graph nodes
	</Extractive Summary>
</Paper ID=ument708>


<Paper ID=ument71> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1: Comparison of the ﬁnal unsupervised MT performance (BLEU)
	</Abstractive Summary>
</Paper ID=ument71>


<Paper ID=ument71> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: Test BLEU scores with different usage of the pre-trained decoder
	</Abstractive Summary>
</Paper ID=ument71>


<Paper ID=ument71> <Table ID =3>
	<Abstractive Summary> =
		4016
		Table 3: Results of word alignment tasks using differ-
		ent cross-lingual word embeddings
	</Abstractive Summary>
</Paper ID=ument71>


<Paper ID=ument71> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Results of zero-shot cross-lingual classiﬁca-
		tion (on XNLI test sets)
	</Abstractive Summary>
</Paper ID=ument71>


<Paper ID=ument71> <Table ID =5>
	<Abstractive Summary> =
		4
		Table 5: Ablation study
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 5, we can ﬁnd that the combination
		of CMLM and MLM can improve the translation
		performance by about 0
	</Extractive Summary>
</Paper ID=ument71>


<Paper ID=ument711> <Table ID =1>
	<Abstractive Summary> =
		5%
		Table 1: Task analysis of NLP DNN jobs submitted to a
		commercial centralized GPU cluster
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 showed that about 87
	</Extractive Summary>
</Paper ID=ument711>


<Paper ID=ument711> <Table ID =2>
	<Abstractive Summary> =
		36
		Table 2: NeuronBlocks results on CoNLL-2003 English NER testb dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows
		the results on CoNLL-2003 Englist testb dataset,
		with 12 different combinations of network lay-
		ers/blocks, such as word/character embedding,
		CNN/LSTM and CRF
	</Extractive Summary>
</Paper ID=ument711>


<Paper ID=ument711> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: NeuronBlocks results on GLUE benchmark development sets
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 3, the models built by Neuron-
		Blocks can achieve competitive or even better re-
		sults on GLUE tasks with minimal coding efforts
	</Extractive Summary>
</Paper ID=ument711>


<Paper ID=ument711> <Table ID =4>
	<Abstractive Summary> =
		8941
		Table 4: NeuronBlocks results on Knowledge Distillation task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		shows the results, where AUC is used as the eval-
		uation criteria and Queries per Second (QPS) is
		used to measure inference speed
	</Extractive Summary>
</Paper ID=ument711>


<Paper ID=ument711> <Table ID =5>
	<Abstractive Summary> =
		54
		Table 5: NeuronBlocks results on WikiQA
	</Abstractive Summary>
</Paper ID=ument711>


<Paper ID=ument712> <Table ID =1>
	<Abstractive Summary> =
		21
		Table 1: Accuracies of various models on Wiki80 and
		SemEval 2010 Task-8 under the single sentence setting
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 1
		we can see that BERT-based models perform bet-
		173
		Model
		5-Way 1-Shot
		5-Way 5-Shot
		5-Way 1-Shot (*)
		5-Way 5-Shot (*)
		Prototype-CNN
		74
	</Extractive Summary>
</Paper ID=ument712>


<Paper ID=ument712> <Table ID =2>
	<Abstractive Summary> =
		892
		Table 2: Micro F1 scores of various models on Se-
		mEval 2010 Task-8 under the sentence-level RE set-
		ting
	</Abstractive Summary>
</Paper ID=ument712>


<Paper ID=ument712> <Table ID =3>
	<Abstractive Summary> =
		2
		-
		-
		Table 3: Accuracies of various models on FewRel under the different few-shot settings
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 3
		we can see that for both few-shot settings, our ver-
		sion achieves better results than the original results
		from Han et al
	</Extractive Summary>
</Paper ID=ument712>


<Paper ID=ument712> <Table ID =4>
	<Abstractive Summary> =
		42
		Table 4: AUC and F1 scores of various models on
		NYT10 under the bag-level RE setting
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows that our ver-
		sion of bag-level RE models achieves comparable
		or even better results than the original papers
	</Extractive Summary>
</Paper ID=ument712>


<Paper ID=ument715> <Table ID =1>
	<Abstractive Summary> =
		9
		Table 1: Experimental results for the negotiation ex-
		ample
	</Abstractive Summary>
	<Extractive Summary> =
		As reported in Table 1, planning (using either
		Forward or MCTS) improves the negotiation out-
		come over the baseline in terms of both reward and
		3(Py)OpenDial includes None action with utility 0 by de-
		fault, thus we assigned small positive utility to the generated
		utterances to be distinguished from the None action
	</Extractive Summary>
</Paper ID=ument715>


<Paper ID=ument716> <Table ID =1>
	<Abstractive Summary> =
		org
		197
		System
		Web-based
		project creation
		Project
		monitoring
		Curation
		feature
		Document
		propagation
		Class labels
		Dynamic
		Hierarchical
		Multi-label
		BRAT
		
		
		
		
		
		
		
		GATE
		
		
		
		
		
		
		
		SANTO
		
		
		
		
		
		
		
		SAWT
		
		
		
		
		
		
		
		YEDDA
		
		
		
		
		
		
		
		WebAnno
		
		
		
		
		
		
		
		Redcoat
		
		
		
		
		
		
		
		Table 1: A comparison of existing annotation tools with Redcoat
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Comparison with existing tools
		Table 1 provides a qualitative comparison between
		Redcoat and other existing annotation tools
	</Extractive Summary>
</Paper ID=ument716>


<Paper ID=ument717> <Table ID =1>
	<Abstractive Summary> =
		5
		–
		–
		–
		–
		–
		–
		–
		–
		–
		–
		–
		–
		Table 1: Results of the comparative evaluation of semantic search models on: (1) monolingual document retrieval
		(LATimes); metrics: NDCG@100, MAP@1000 and Precision@10; and (2) cross-lingual sentence retrieval (Eu-
		roparl, 5K sentence pairs, EN-DE, EN-IT, and EN-FI); metrics: MRR and Hits@{1, 5, 10}
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 1 summarizes the monolingual document re-
		trieval and cross-lingual sentence retrieval results
	</Extractive Summary>
</Paper ID=ument717>


<Paper ID=ument718> <Table ID =1>
	<Abstractive Summary> =
		208
		Feature
		Pronouns and non-content adjectives
		1
		Alius
		2
		Idem
		3
		Ipse
		4
		Iste
		5
		Quidam
		6
		Demonstrative Pronouns
		7
		Personal Pronouns
		8
		Third-Person Pronouns
		Conjunctions
		9
		Atque + Consonant
		10
		Conjunctions
		Subordinate clauses
		11
		Antequam
		12
		Cum
		13
		Dum
		14
		Priusquam
		15
		Quin
		16
		Quominus
		17
		Conditional Markers
		18
		Fraction of Sentences with Relative Clauses
		19
		Mean Length of Relative Clauses
		Miscellaneous
		20
		Ut
		21
		Interrogative Sentences
		22
		Mean Length of Sentences
		23
		Prepositions
		24
		Regular Superlatives
		25
		Selected Gerunds & Gerundives
		26
		Selected Vocatives
		Table 1: Full set of Latin stylometric features
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Features
		Our feature set comprises twenty-six stylometric
		features across four broad syntactic and grammat-
		ical categories (pronouns and non-content adjec-
		tives, subordinate clauses, conjunctions, and mis-
		cellaneous, as listed in Table 1) and is described in
		detail in a previous publication (Chaudhuri et al
	</Extractive Summary>
</Paper ID=ument718>


<Paper ID=ument719> <Table ID =1>
	<Abstractive Summary> =
		61) ‡
		Table 1: Tasks results for section-agnostic, and section-
		based
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the results across
		the 3 tasks
	</Extractive Summary>
</Paper ID=ument719>


<Paper ID=ument72> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: IWSLT’14 English-German and English-French results - shown are the BLEU scores of various
		models on TED talks translation tasks
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results on IWSLT’14 Tasks
		Table 1 compares LaSyn versions against some
		of the state-of-the-art models on the IWSLT’14
		dataset
	</Extractive Summary>
</Paper ID=ument72>


<Paper ID=ument72> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Translation examples on IWSLT’14 De-En dataset from our model and the Transformer baseline
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows one example where LaSyn pro-
		duces correct translations for a long input sen-
		tence
	</Extractive Summary>
</Paper ID=ument72>


<Paper ID=ument72> <Table ID =3>
	<Abstractive Summary> =
		6 s
		Table 3: IWSLT’14 De-En training and inference
		speed evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the
		training time per epoch, and the inference time for
		the whole test set
	</Extractive Summary>
</Paper ID=ument72>


<Paper ID=ument72> <Table ID =4>
	<Abstractive Summary> =
		354
		Table 4: IWSLT’14 En-De/De-En/En-Fr diversity
		translation evaluation
	</Abstractive Summary>
</Paper ID=ument72>


<Paper ID=ument72> <Table ID =5>
	<Abstractive Summary> =
		Table 5:
		Examples of translations decoded from
		speciﬁed POS sequences with different edit distances
		(shown as values in ﬁrst column)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows some example sentences
	</Extractive Summary>
</Paper ID=ument72>


<Paper ID=ument72> <Table ID =6>
	<Abstractive Summary> =
		9
		Table 6: WMT’14 English-German results - shown
		are the BLEU scores of various models on TED talks
		translation tasks
	</Abstractive Summary>
</Paper ID=ument72>


<Paper ID=ument720> <Table ID =1>
	<Abstractive Summary> =
		Source
		Target
		<s> <s> k t b
		<s> <s> ka ta ba
		<s> k t b
		A l w l d
		<s> ka ta ba
		A lo wa la du
		k t b
		A l w l d
		A l d r s
		ka ta ba
		A lo wa la du
		A l d∼a ro sa
		A l w l d
		A l d r s
		<e>
		A lo wa la du
		A l d∼a ro sa
		<e>
		A l d r s
		<e> <e>
		A l d∼a ro sa
		<e> <e>
		Table 1: Example sentence: “ktb Alwld Aldrs” with
		context window size of 3
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 provides an
		example for a three words sentence “ktb Alwld
		Aldrs” (the boy wrote the lesson) with a 3 word
		sliding window
	</Extractive Summary>
</Paper ID=ument720>


<Paper ID=ument720> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Arabic variety identiﬁcation per input length
		When building the diacritization models, we
		used the OpenNMT-tf implementation for training
		with the hyperparameters suggested in the Open-
		NMT website2
	</Abstractive Summary>
</Paper ID=ument720>


<Paper ID=ument720> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: Number of words in training and test data for
		MSA, CA, and DA
		identical test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 lists the details of the training and test
		sets including the unique diacritized and undia-
		critized tokens and the percentage of OOVs in the
		test set that don’t appear in the training set
	</Extractive Summary>
</Paper ID=ument720>


<Paper ID=ument720> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Results and comparison of full diacritization
		systems
	</Abstractive Summary>
	<Extractive Summary> =
		1
		System Results
		Table 4 summarizes the results per dataset and
		compares our system to other SOTA systems on
		2https://github
	</Extractive Summary>
</Paper ID=ument720>


<Paper ID=ument722> <Table ID =1>
	<Abstractive Summary> =
		, deﬁnition)
		Search
		Searching external resources such as corpora and dictionaries
		Table 1: WATs supported by TEASPN
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the list
		of WATs that are supported by TEASPN
	</Extractive Summary>
</Paper ID=ument722>


<Paper ID=ument722> <Table ID =2>
	<Abstractive Summary> =
		4
		335 ± 91
		Table 2: Statistics of the written texts
	</Abstractive Summary>
</Paper ID=ument722>


<Paper ID=ument723> <Table ID =1>
	<Abstractive Summary> =
		Then
		we produce an error type including an edit type
		(insert, delete, and replace) and PoS of the edit
		237
		Table 1: top 10 error codes
		Code
		Gloss
		Samples Sentence
		1
	</Abstractive Summary>
	<Extractive Summary> =
		For simplicity, we
		limit ourselves to Top 10 most common error types
		(in Table 1) in CLE-FCE (Yannakoudakis et al
	</Extractive Summary>
</Paper ID=ument723>


<Paper ID=ument723> <Table ID =2>
	<Abstractive Summary> =
		An-
		notated the input sentence with PoS, we could ex-
		238
		Table 2: Sample correction-feedback pairs from the training collection
		Problem Word
		Sentence and Feedback
		Analysis
		discuss
		 They would like to discuss about what to do next
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows one sample correction-
		explanation pairs with a problem word and an ex-
		planation template
	</Extractive Summary>
</Paper ID=ument723>


<Paper ID=ument723> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Template for Replace a Preposition
		Problem Word
		Sentence and Feedback
		Analysis
		arm
		 She would not stop crying until I held her on my arms
	</Abstractive Summary>
	<Extractive Summary> =
		First, we use a template (in Table 3) that shows the
		grammar patterns of the problem-causing word
	</Extractive Summary>
</Paper ID=ument723>


<Paper ID=ument723> <Table ID =4>
	<Abstractive Summary> =
		, verb, noun, and adjective), we han-
		dled the errors by cases: (1) spelling, (2) tense,
		239
		Table 4: Template for Replacing a Word
		Problem Word
		Sentence and Feedback
		Analysis
		abandon
		 Since capital punishment was abandoned, the crime rate has increased
	</Abstractive Summary>
</Paper ID=ument723>


<Paper ID=ument723> <Table ID =5>
	<Abstractive Summary> =
		2 $col denotes a collocation of $pw
		Table 5:
		Scores (0-2) for explanations on top 10 er-
		ror codes, generated by four systems:
		TellMeWhy-GEC
		(TMW-GEC),TellMeWhy-FindExample (TMW-FE) Gram-
		marly (GL) and Write & Improve (W&I)
		Code
		Gloss
		TMW-GEC
		TMW-FE
		GL
		W&I
		1
	</Abstractive Summary>
	<Extractive Summary> =
		One evaluation of comparison between TellMe-
		Why and other commercial systems is shown in
		Table 5 was evaluated by ten sentences for each
		error type and carried out by a linguist
	</Extractive Summary>
</Paper ID=ument723>


<Paper ID=ument723> <Table ID =6>
	<Abstractive Summary> =
		2
		Evaluation
		Once we have trained TellMeWhy as described,
		we evaluated the performance using ten randomly-
		Table 6: The number of testcases can be corrected by three
		systems: TellMeWhy-GEC (TMW-GEC), Grammarly (GL)
		and Write & Improve (W&I)
		Code
		Gloss
		TMW-GEC
		GL
		W&I
		1
	</Abstractive Summary>
</Paper ID=ument723>


<Paper ID=ument724> <Table ID =1>
	<Abstractive Summary> =
		com/huggingface
		242
		Model
		Corpus
		Encoder
		Target
		Skip-thoughts
		Bookcorpus
		GRU
		Conditioned LM
		Quick-thoughts
		Bookcorpus+UMBCcorpus
		GRU
		Sentence prediction
		CoVe
		English-German
		Bi-LSTM
		Machine translation
		Infersent
		Natural language inference
		LSTM;GRU;CNN;LSTM+Attention
		Classiﬁcation
		ELMO
		1billion benchmark
		Bi-LSTM
		Language model
		ULMFiT
		Wikipedia
		LSTM
		Language model
		GPT
		Bookcorpus; 1billion benchmark
		Transformer
		Language model
		BERT
		Wikipedia+bookcorpus
		Transformer
		Cloze+sentence prediction
		Table 1: 8 pre-training models and their differences
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists 8 popular pre-training models and
		their main differences (Kiros et al
	</Extractive Summary>
</Paper ID=ument724>


<Paper ID=ument724> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: The performance of HuggingFace’s implementation and UER’s implementation on GLUE benchmark
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 and 3 compare UER’s perfor-
		mance to other publicly available systems
	</Extractive Summary>
</Paper ID=ument724>


<Paper ID=ument724> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: The performance of ERNIE’s implementation and UER’s implementation on ERNIE benchmark
	</Abstractive Summary>
</Paper ID=ument724>


<Paper ID=ument724> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: Performance of pre-training models with dif-
		ferent targets
	</Abstractive Summary>
</Paper ID=ument724>


<Paper ID=ument724> <Table ID =5>
	<Abstractive Summary> =
		5
		Table 5: Performance of pre-training models with dif-
		ferent encoders
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5
		lists the results of different encoders
	</Extractive Summary>
</Paper ID=ument724>


<Paper ID=ument725> <Table ID =1>
	<Abstractive Summary> =
		We search Lakers as
		subject in every month and sum up all the label
		rank
		verbs for Le-
		Bron James
		ﬁxed main objects
		1
		miss
		games
		2
		suffer
		a groin strain injury
		3
		make
		no ﬁxed main objects
		4
		leave
		Cleveland Cavaliers
		5
		lead
		the team
		Table 1: Verb Rankings for LeBron James in January
		Figure 5: Breaking News Tracking on Trade Rumors
	</Abstractive Summary>
</Paper ID=ument725>


<Paper ID=ument725> <Table ID =2>
	<Abstractive Summary> =
		First, given
		251
		Rank
		Dec 2018
		Jan 2019
		Feb 2019
		Mar 2019
		1
		los angeles lakers
		los angeles lakers
		los angeles lakers
		los angeles lakers
		2
		lebron james
		pelicans
		lebron james
		lebron james
		3
		lonzo ball
		lebron james
		clippers
		clippers
		4
		clippers
		lonzo ball
		pelicans
		kevin durant
		5
		brandon ingram
		anthony davis
		boston celtics
		lonzo ball
		6
		kevin durant
		cavs
		kyle kuzma
		lebron
		7
		anthony davis
		boston celtics
		tobias harris
		giannis antetokounmpo
		8
		raptors
		rockets
		anthony davis
		magic johnson
		Table 2: Top 5 Words closest to the Word ‘lakers’ in Each Month
	</Abstractive Summary>
	<Extractive Summary> =
		For ex-
		ample, Table 2 conﬁrms with Figure 5 that break-
		ing news of Anthony Davis and Lakers happened
		because of the trade rumors
	</Extractive Summary>
</Paper ID=ument725>


<Paper ID=ument726> <Table ID =1>
	<Abstractive Summary> =
		com/odashi/mteval
		254
		Source Type
		Example Tasks
		Text
		machine translation, text summarization,
		dialog generation, grammatical error cor-
		rection, open-domain question answering
		Image
		image captioning, visual question answer-
		ing, optical character recognition
		Audio
		speech recognition, speech translation
		Video
		video description
		Multimodal
		multimodal machine translation
		Table 1: Example text generation tasks supported by
		VizSeq
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 provides a list of
		example tasks supported by Vizseq
	</Extractive Summary>
</Paper ID=ument726>


<Paper ID=ument726> <Table ID =2>
	<Abstractive Summary> =
		Metrics
		VizSeq
		compare-
		mt
		nlg-
		eval
		MT-
		Compar-
		Eval
		BLEU
		chrF
		METEOR
		TER
		RIBES
		GLEU
		NIST
		ROUGE
		CIDEr
		WER
		LASER
		BERTScore
		Table 2: Comparison of VizSeq and its counterparts on
		n-gram-based and embedding-based metric coverage
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Metric Coverage and Scalability
		Table 2 shows the comparison of VizSeq and its
		counterparts on metric coverage
	</Extractive Summary>
</Paper ID=ument726>


<Paper ID=ument727> <Table ID =1>
	<Abstractive Summary> =
		263
		Tok
		MA
		MD
		POS
		Lem
		Feats
		Deps
		Joint
		Tasks
		MILA
		✓
		✓
		NITE
		✓
		✓
		✓
		Hebrew-NLP
		✓
		Adler
		✓
		✓
		✓
		Goldberg
		✓
		Pipelines
		UDPipe
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		CoreNLP
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		ONLP
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Table 1: Existing Coverage for Hebrew NLP Tasks
		sented in separate accordion tabs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the task-coverage of existing
		tools and toolkits for NLP in Hebrew, academic
		as well as private initiatives (NITE,Hebrew-NLP)
	</Extractive Summary>
</Paper ID=ument727>


<Paper ID=ument728> <Table ID =1>
	<Abstractive Summary> =
		Rate
		Bo Xilai
		665
		336
		64%
		Deng Xiaoping
		281
		125
		70%
		Fire
		431
		530
		45%
		Injury/Dead Body
		1799
		1029
		51%
		Liu Xiaobo
		184
		123
		60%
		Mao Zedong
		1093
		486
		70%
		People’s Congress
		145
		113
		56%
		Policeman
		1311
		927
		59%
		Protest
		536
		220
		71%
		Prurient/Nudity
		2664
		2551
		51%
		Rainstorm
		153
		207
		43%
		Winnie the Pooh
		160
		177
		48%
		Xi Jinping
		1745
		1029
		63%
		Zhou Kehua
		102
		134
		43%
		Table 1: Percentage of censored posts per category
		4
		Datasets
		class
		# of occurrences
		Clean (Train)
		80977 (96%)
		Implicit Toxic (Train)
		2948 (4%)
		Clean (dev)
		9019 (96%)
		Implicit Toxic (dev)
		307 (4%)
		Clean (Test)
		33541 (83%)
		Explicit Toxic (Test)
		5085 (13%)
		Implicit Toxic (Test)
		1158 (4%)
		Table 1: Class distribution of Wikipedia dataset
		Total
		Troll
		924
		206
		229
		925
		2,284
		Control
		284,153
		63,146
		70,162
		284,153
		701,614
		Table 1: Dataset statistics91
		Table 1: Signiﬁcant components of our logistic regression model using the Coh-Metrix features2014
		Users liking the posts by the protest
		groups
		57754
		Table 1: Basic statistics on the Bolotnaya dataset
		Tahrir Square protests (Tufekci and Wilson, 2012)
		47
		Label
		Original
		All
		Labeled
		Pro-Russian
		512
		4,829
		Pro-Ukrainian
		910
		12,343
		Neutral
		6,923
		118,196
		Unlabeled
		-
		192,003
		377,679
		Total
		-
		200,348
		513,047
		Table 1: Label distribution and dataset sizes
		Table 1: Example of a claim and its diverse sentential
		arguments, 2019], enhanced 
		from PolitiFact and GossipCop 
		Table 1: Datasets for rumor detection and their properties 
		 
		supporting, denying, querying and commenting
		The label of each comment was aggregated based
		Train
		Validation
		Test
		Attack
		8,079
		2,755
		2,880
		Not-attack
		61,447
		20,405
		20,298
		All
		69,526
		23,160
		23,178
		Table 1: Statistics of the personal attacks corpus25
		Table 1: Experiment results of different models on the SLC task, and the model with the highest F1 score which
		has been underlined is chosen to be evaluated on the test set208
		Table 1: Evaluation results on ofﬁcial development set
		Technique
		Dev F1
		Test F1
		Appeal-Authority
		0
		0
		Appeal-Fear
		0
		Table 1: Features used in SLC and FLC tasks
		ing , stereotyping, etccom/
		98
		Propaganda Technique
		Frequency
		Loaded Language
		2,115
		Name Calling,Labeling
		1,085
		Repetition
		571
		Doubt
		490
		Exaggeration,Minimisation
		479
		Flag-Waving
		240
		Appeal to Fear/Prejudice
		239
		Causal Oversimpliﬁcation
		201
		Slogans
		136
		Appeal to Authority
		116
		Black-and-White Fallacy
		109
		Thought-terminating Cliches
		79
		Whataboutism
		57
		Reductio ad hitlerum
		54
		Red Herring
		33
		Bandwagon
		13
		Straw Men
		13
		Obfuscation,Intentional Vagueness,Confusion
		11
		Total
		6,041
		Table 1: Frequency of all eighteen propaganda tech-
		niques in the training data
		We also conduct our error analysis on the local dev
		set because we do not have access to the gold la-
		bels of the ofﬁcial dev and test sets of the shared
		task samples to split
		2
		Table 1: Hyperparameter values for GBT
		Features
		dimension
		Glove
		300
		TweetToEmbeddings
		100
		TweetToInputLeixicon
		4
		TweetToLexicon
		43
		TweetToSentiStrength
		2
		Table 1: Features used in our approach
		4
		Our Approach
		The architecture of our system consists of four
		sub-models: BiLSTM sub-model, XGBoost sub-
		model, BERT Cased and UnCased model (Figure
		1)16
		Table 1: Sentence-level (SLC) results11E-02
		0
		Table 1: p-values representing the similarity between
		(parts of) the train, test and development sets69
		Table 1: Performance of baselines on development set5 for the uncased
		Table 1: Models results on development and testing
		datasets
		Model
		Dataset
		F1-score
		Uncased BERT
		Dev
		66
		Label
		Train
		Propaganda
		4720
		Non-Propaganda
		12245
		Table 1: Data Distribution
		27656
		Table 1:
		Ablation study of our BERT-BiLSTM-
		Capsule model on the validation set085
		Table 1: Effect of using SMOTE and BLSTM ﬁne-
		tuning on the pre-trained language model using macro-
		averaged F1-score18)
		Table 1: Statistics about the gold annotations for the
		training and the development sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the number of posts found in each category as well
		as the percentage of posts in each category that
		was censored
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the label distribution of the anno-
		tated portion of the data as well as the total amount
		of original tweets, and original tweets plus their
		retweets/duplicates in the network
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		shows an example1 of the input and outputs of
		our system
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 lists the datasets for 
		rumor detection
	</Extractive Summary>
	<Extractive Summary> =
		The 
		data of datasets in Table 1 are collected from four 
		social media platforms: Twitter, Facebook, 
		Reddit and Weibo
	</Extractive Summary>
	<Extractive Summary> =
		The corpus comes
		with a 60-20-20 split into training, validation, and
		test set (see Table 1 for corpus statistics)
	</Extractive Summary>
	<Extractive Summary> =
		This inspires1 us in extract-
		ing different features (Table 1) including the com-
		plexity of text, sentiment, emotion, lexical (POS,
		NER, etc
	</Extractive Summary>
	<Extractive Summary> =
		Additionally, we apply a de-
		cision function (Table 1) such that a sentence is
		tagged as propaganda if prediction probability of
		the classiﬁer is greater than a threshold (τ)
	</Extractive Summary>
	<Extractive Summary> =
		We explore two ensemble strategies (Table 1):
		majority-voting and relax-voting to boost preci-
		sion and recall, respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 lists all eighteen propaganda
		techniques and their frequencies in the training
		data
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		shows all hyperparameter values set for the GBT
		model
	</Extractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 1 shows the ablation study results for the
		SLC task
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 1, BERT performed better than
		LR bert but worse than LR†‡, which indicates that
		the transfer learning when considering single se-
		mantic variable is not as effective as the combi-
		nation with other linguistic features
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 provides the
		minimum and maximum p-values and their inter-
		pretations for ten such runs of each pair reported
	</Extractive Summary>
	<Extractive Summary> =
		As shown
		in Table 1, the Perspective-API baseline achieves
		0
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the models results on development and testing
		datasets
	</Extractive Summary>
	<Extractive Summary> =
		GROVER
		performs better than BERT and ELMo without
		any ﬁne-tuning, and our ﬁne-tuning method on
		GROVER improved the precision but resulted in
		a lower overall recall (See Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		As some of
		our results are inconsistent (Table 1) ad-
		ditional evaluation using conventional ﬁne-
		tuning methods would aid us in understand-
		ing what is learned by ﬁne-tuning
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 reports the total number of instances per
		technique and the percentage with respect to the
		total number of annotations, for the training and
		for the development sets
	</Extractive Summary>
</Paper ID=ument728>


<Paper ID=ument728> <Table ID =2>
	<Abstractive Summary> =
		005
		Table 2: Survival regression per category
		This dataset consists of comments from
		class
		# of occurrences
		NAG (Train)
		4159 (46%)
		Implicit CAG (Train)
		3223 (36%)
		Implicit OAG (Train)
		1651 (18%)
		NAG (Dev)
		1029 (46%)
		Implicit CAG (Dev)
		806 (36%)
		Implicit OAG (Dev)
		420 (18%)
		NAG (F)
		491 (65%)
		Explicit CAG (F)
		35 (5%)
		Explicit OAG (F)
		56 (7%)
		Implicit CAG (F)
		95 (13%)
		Implicit OAG (F)
		73 (10%)
		NAG (T)
		431 (38%)
		Explicit CAG (T)
		85 (7%)
		Explicit OAG (T)
		103 (9%)
		Implicit CAG (T)
		328 (29%)
		Implicit OAG (T)
		188 (17%)
		Table 2: Class distribution of Facebook (F) and Twitter
		(T) datasets770
		Table 2: Overall results on the test dataset78
		Table 2: Results of classiﬁcation between fake news
		and satire articles using BERT pre-trained models,
		based on the headline, body and full text75
		Table 2: CTA classiﬁcation results
		7
		CTAs for Predicting social unrest
		To estimate the potential usefulness of CTAs
		as indicators of ofﬂine protest events, we ran
		the trained RuBERT CTA classiﬁer over 91K
		posts falling in the date range between Dec 2011
		through Jul 2013 from the Bolotnaya dataset
		Neutral
		#PrayForMH17 :(
		RT @deserto_fox: Russian terrorist stole wedding ring from dead passenger #MH17
		Table 2: Example tweets for each of the three classes4146
		Table 2: Automatic evaluation results on generation quality, 2018] 
		y 
		  
		y 
		  
		y 
		  
		RNN 
		Table 2: Previous studies, used information, and methods17
		Table 2:
		Effectiveness on the test set of the per-
		sonal attacks corpus (AUC and Spearman coefﬁcients):
		our proposed approach, the previous state of the art
		(Pavlopoulos et al10
		Table 2: Experiment results of the chosen model and
		the random baseline for the SLC task16
		0
		Table 2: Classwise F1 scores for ﬁnal submission
		the models we tried including the baseline BERT
		model0822
		Table 2: Comparison of our system (MIC-CIS) with
		top-5 participants: Scores on Test set for SLC and FLC
		use BIO tagging scheme of NER in FLC task80
		**handcrafted features include LIWC and presence of
		questions or quotes
		Table 2: SLC experiments on different feature sets
		Dataset
		P
		R
		F
		Local Dev
		01
		Table 2: Propaganda detection performance over 5-fold
		cross-validation477673
		Table 2: BiLSTM result on development data set
		100, batch size= 512, validation split= 33% (See
		Table 2)6276
		Table 2: F1 scores on an unseen (not used for train-
		ing) part of the training set and the development set on
		BERT using different augmentation techniques
		unigrams
		devastating, cruel, vile, irrational, absurd, brutal, vicious, stupid,
		coward, awful, ignorant, unbelievable, doomed, idiot, terrifying,
		disgusting, horrible, hideous, horriﬁc, pathetic
		bigrams
		shame less, totally insane, a horrible, utterly unacceptable, hys-
		terical nonsense, the horrible, this horriﬁc, absolutely disgusting,
		monumental stupidity, a pathetic, a disgusting, absolutely worth-
		less, truly disgusting, utterly insane, this murderous, incredibly
		stupid, monstrous fraud, this lunatic, a disgrace, a hideous
		Table 2: Top 20 unigrams and bigrams with highest likelihood of being propaganda3870
		Table 2: Uncased BERT model experiments results on
		development datasets
		Model
		F1-score
		More Training
		60
		Figure 1: Distribution of Classes in Training Set
		144
		Table 2: Model Architectures used for training and their optimal hyperparameters
		Model
		Hyperparameters
		BERT-1
		BERT-Base-Uncased
		batch-size=32, learning-rate=2e-5,epochs=3
		BERT-2
		DocumentEmbeddings {Stacked Embeddings BERT + GRU + Dropout (p=07717
		Table 2: Comparative results against our base models
		on the validation set03
		Train
		-
		160 106 123 107 127 125
		45
		62
		24
		27
		120
		78
		97
		17
		25
		80
		32
		115
		ELMo
		-
		307
		22
		-
		10
		25
		29
		15
		31
		13
		16
		-
		6
		86
		10
		21
		-
		9
		35
		BERT
		-
		93
		22
		-
		10
		23
		31
		15
		21
		12
		13
		-
		6
		65
		10
		13
		20
		25
		40
		GROVER
		-
		74
		22
		-
		69
		26
		40
		15
		20
		11
		13
		-
		5
		28
		10
		11
		20
		18
		44
		Table 2: (1) F1-score for classes in the FLC task Tuning
		CUNLP
		✓
		✓
		✓
		Stalin
		✓
		✓
		MIC-CIS
		✓
		✓
		✓
		ltuorp
		✓
		ProperGander
		✓
		✓
		newspeak
		✓
		✓
		Table 2: Overview of the approaches for the fragment-level classiﬁcation task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results of survival analy-
		sis per category
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, sentiment always has a
		negative sign and it is always statistically signif-
		icant at 5%
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2, we compare
		our model (CBS+Self) trained using the entire troll
		dataset
	</Extractive Summary>
	<Extractive Summary> =
		1
		Overall Performance
		Table 2 shows the evaluation results of each model
		in terms of generation quality using BLEU score
		and word embedding based metrics
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 presents the 
		studies and their related information
	</Extractive Summary>
	<Extractive Summary> =
		As can be seen in Table 2, our approach is
		slightly better than the re-implementation in terms
		of AUC and Spearman in both splits and the whole
		test set
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 (SLC) shows
		that our submission is ranked at 4th position
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 (FLC) shows
		that our submission is ranked at 3rd position
	</Extractive Summary>
	<Extractive Summary> =
		80 in
		the experiments shown in Table 2 to avoid overﬁt-
		ting on a one dataset
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results and Discussion
		Table 2 shows the results of all models over 5-
		fold cross-validation on the provided SLC training
		data
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the results of our experi-
		ments for different data augmentation techniques
		when, after shufﬂing the training data, we train the
		model on 75% of the training data and test it on the
		remaining 25% of the training data and the devel-
		opment data
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the uncased
		BERT model experiments results on developments
		dataset
	</Extractive Summary>
	<Extractive Summary> =
		Observing the
		results from Table 2, not all models are predict-
		ing meaningful span length on each class compar-
		ing to the average length in training data (i
	</Extractive Summary>
</Paper ID=ument728>


<Paper ID=ument728> <Table ID =3>
	<Abstractive Summary> =
		)
		Table 3: Data preprocessing example586
		Table 3:
		Ablation results using the validation
		dataset for the three major feature groups: Stylistic
		(S), Behavioral (B), and Content (C)78*
		Table 3: Summary of results of classiﬁcation between
		fake news and satire articles using the baseline Multi-
		nomial Naive Bayes method, the linguistic cues of
		text coherence and semantic representation with a pre-
		trained BERT model94
		Table 3: Classiﬁcation results on the English MH17 dataset measured as F1 and area under the precision-recall
		curve (AUC)6134
		Table 3:
		Automatic evaluation results on diversity of generation14
		Table 3: Effectiveness (AUC values and Spearman co-
		efﬁcients) of our approach’s ﬁrst ﬁve iterations751
		Table 3: SLC: Scores on Dev (internal) of Fold1 and
		Dev (external) using different classiﬁers and features70
		Table 3: SLC best model results on all three datasets
		Repetition and Doubt are the two most chal-
		lenging types for the classiﬁer even though they
		are in the four most frequent techniques6
		Table 3: Ofﬁcial results for propaganda detection task
		(on withheld test data)8
		Table 3: XGBoost Hyperparameter
		45533
		Table 3: Class-wise precision and recall with and with-
		out oversampling (OS) achieved on unseen part of the
		training set
		Table 3: Performance of different models on develop-
		ment data for SLC Task
		Model
		F1
		Pr
		Rc
		Naive Bayes
		(count vectorizer)
		04347
		Table 3: Comparative analysis against the ofﬁcial base-
		line result as well as the best performer of the SLC task000
		Table 3: Pearson correlation (r) and p-value for the pre-
		dicted span lengths of the models
		Team
		BERT
		LSTM
		logreg
		USE
		CNN
		Embeddings
		Features
		Context
		NSIT
		✓
		✓
		CUNLP
		✓
		✓
		✓
		JUSTDeep
		✓
		✓
		✓
		✓
		Tha3aroon
		✓
		✓
		LIACC
		✓
		✓
		✓
		MIC-CIS
		✓
		✓
		✓
		✓
		✓
		CAUnLP
		✓
		✓
		YMJA
		✓
		jinfen
		✓
		✓
		✓
		ProperGander
		✓
		Table 3: Overview of the approaches used for the sentence-level classiﬁcation task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 is an ex-
		ample of this data preprocessing
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 provides a summary of the results
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the AUC values and Spearman co-
		efﬁcients for the ﬁrst ﬁve iterations of our approach
		on the unraveled validation and test set
	</Extractive Summary>
	<Extractive Summary> =
		1
		Results: Sentence-Level Propaganda
		Table 3 shows the scores on dev (internal and ex-
		ternal) for SLC task
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents ours results on the ofﬁcial test
		data
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 sum-
		marizes XGBoost hyperparameters
	</Extractive Summary>
	<Extractive Summary> =
		The results of these
		experiments (Table 3) show that oversampling in-
		creases the overall recall while maintaining preci-
		sion
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 represents the performance of all the
		models trained on the training dataset and evalu-
		ated on the development data for the SLC Task
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, we present our
		results on the test set in comparison to the SLC
		task baseline and the highest-ranking team
	</Extractive Summary>
</Paper ID=ument728>


<Paper ID=ument728> <Table ID =4>
	<Abstractive Summary> =
		09
		Table 4: Results of different models on Wikipedia, Facebook and Twitter datasets, HAN: Hierarchical Attention
		Neural Net, HCL: Hierarchical C-LSTM
		Table 4: Examples for the different error categories4146
		Table 4: Automatic evaluation results on generation quality with different objective functions
		1
		2
		3
		4
		5
		Size
		1650 1725 1780 1829 1875
		Increment
		+75
		+55
		+49
		+46
		Partially abusive
		+20
		+30
		+24
		+18
		Abusive
		+14
		+13
		+18
		+21
		Non-abusive
		+41
		+12
		+ 7
		+ 7
		Table 4: Increment and of the abusive lexicon in the
		ﬁrst ﬁve iterations of our approach150
		Table 4: FLC: Scores on Dev (internal) of Fold1 and
		Dev (external) with different models, features and en-
		sembles The best model is a
		BiLSTM-CRF with ﬂair and urban glove embed-
		100
		Technique
		Count
		Accuracy
		Loaded Language
		299
		71%
		Name Calling,Labeling
		163
		69%
		Repetition
		124
		44%
		Doubt
		71
		40%
		Exaggeration,Minimisation
		63
		67%
		Flag-Waving
		35
		74%
		Appeal to Fear/Prejudice
		42
		52%
		Causal Oversimpliﬁcation
		24
		58%
		Slogans
		24
		54%
		Table 4: SLC accuracy on frequent propaganda tech-
		niques in the local development set
		dings with one hot encoded features as mentioned
		in Section 3387009
		Table 4: XGBoost results on development dataset
		Type
		seq length
		batch size
		lr
		epochs
		StopWord
		F1
		Precision
		Recall
		Cased
		400
		4
		1e-5
		3
		With
		0494168
		Table 4: Our results on the SLC task (2nd, in bold)
		alongside comparable results from the competition
		leaderboard0646
		Table 4: Ofﬁcial test results for the SLC task
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		Deep learning performance: Table 4 compares
		our hierarchical model against the baselines as
		well as state-of-the-arts
	</Extractive Summary>
	<Extractive Summary> =
		Table 4
		shows our ensemble with only one C-LSTM out-
		performs
	</Extractive Summary>
	<Extractive Summary> =
		1
		False Positives
		The false semantic groups and counts of false pos-
		itive errors are displayed in Table 4a
	</Extractive Summary>
	<Extractive Summary> =
		2
		False Negatives
		In Table 4b we display the counts of false nega-
		tives that fall into one of ﬁve groups: Support, Dis-
		cord, Political Concealment, Unknown Character,
		and Misc
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 presents examples for each
		of the error categories in both sets which we will
		discuss in the following
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 and Table 5 show the evalua-
		tion results of our models in terms of quality and
		diversity of generated text, respectively
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results: Fragment-Level Propaganda
		Table 4 shows the scores on dev (internal and ex-
		ternal) for FLC task
	</Extractive Summary>
	<Extractive Summary> =
		10
		Evaluation Results
		The results on the test set for the SLC task are
		shown in Table 4, while Table 5 presents the re-
		sults on the development set at the end of phase
		1 (cf
	</Extractive Summary>
</Paper ID=ument728>


<Paper ID=ument728> <Table ID =5>
	<Abstractive Summary> =
		55
		Table 5: Comparisons of F1 measure and AUC using Wikipedia dataset which has under 100 words, over 100 and
		under 200 words and over 200 words
		41
		23
		42
		20
		Total Error
		100
		62
		100
		47
		False Negatives
		Table 5:
		Manual analysis of false positives and false negatives for the Content (C) and Con-
		tent+Behavioral+Style (CBS) models
		Pro-R
		Pro-U
		Neutral
		Total
		# labeled edges in k10
		270
		678
		2193
		3141
		# candidate edges
		349
		488
		-
		873
		# added after ﬁltering
		predictions
		77
		110
		-
		187
		Table 5: Number of labeled edges in the k10 network
		before and after augmentation with predicted labels6134
		Table 5: Automatic evaluation results on diversity of generation with different objective functions
		Figure 2 illustrates some texts with the abusive-
		80
		Iteration
		Abusive Partially abusive Non-abusive
		2
		jerk
		masturbating
		headline
		fuckheads
		freak
		heck
		douchebag
		clowns
		nightmare
		3
		fucking
		rudely
		hometown
		fvck
		dunce
		lifetime
		bastard
		pederast
		imature
		4
		bithces
		ﬁlthy
		policemans
		sissy
		lame
		foot
		fuk
		harrassing
		die
		5
		niggers
		nazi
		pint
		faggots
		hypocritical
		boss
		fuckers
		imposter
		pay
		Table 5: The newly added abusive words in the ﬁrst it-
		erations1∗
		Table 5: Hyper-parameter settings for FLC task
		Table 5: Precision, recall and F1 scores of the FLC task
		on the development and test sets of the shared task492558
		Table 5: BERT result on development dataset
		F1
		Precision
		Recall
		BERT (Cased) + BERT (Uncased)
		0000008
		Table 5: Our results on the FLC task (7th, in bold)
		alongside those of better performing teams from the
		competition leaderboard2286
		Table 5: Results for the SLC task on the development
		set at the end of phase 1 (see Section 6)
	</Abstractive Summary>
	<Extractive Summary> =
		While the network
		shows a clear polarization, only a small subset of
		the edges present in the network are labeled (see
		Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the amount of polar-
		ized edges we can predict at this precision level
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 and Table 5 show the evalua-
		tion results of our models in terms of quality and
		diversity of generated text, respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the newly found words in each
		of the ﬁrst iterations
	</Extractive Summary>
	<Extractive Summary> =
		See
		Table 5 for hyper-parameter settings for FLC task
		using LSTM-CRF
	</Extractive Summary>
	<Extractive Summary> =
		2
		FLC Results
		In FLC, we only show the results of our best
		model in Table 5 to focus more on the differ-
		ences between propaganda techniques
	</Extractive Summary>
	<Extractive Summary> =
		Discussion of Propaganda Types:
		As we can
		see in Table 5,
		we can divide the propa-
		ganda techniques into three groups according
		to the model’s performance on the development
		and test sets
	</Extractive Summary>
	<Extractive Summary> =
		We have noticed that using Uncased and
		Cased models with ensembling between them
		gives the best results (Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		10
		Evaluation Results
		The results on the test set for the SLC task are
		shown in Table 4, while Table 5 presents the re-
		sults on the development set at the end of phase
		1 (cf
	</Extractive Summary>
</Paper ID=ument728>


<Paper ID=ument728> <Table ID =6>
	<Abstractive Summary> =
		do n t let
		him become another afzal guru in real ! shutdownjnu
		madarsajnu
		NAG
		OAG
		T
		59%
		kinner to vo h jo bar bar pakistan dwara hamare uper
		hamle hone par bhi apna much chhipa k baitha h kinner
		vo h jo bar bar bina bulaye pakistan ja raha h
		CAG
		NAG
		Table 6: Example of error caused by the proposed model for the Wikipedia (W), Facebook (F) and Twitter (T)
		dataset
		Table 6:
		Sample arguments of a claim generated by
		human and models756428
		Table 6: Ensembling result on development dataset
		result, we have combined both of them together by
		checking if the 4 models predict that the sentence
		is non-propaganda then it will be labeled as non-
		propaganda, otherwise it will be labeled as Propa-
		ganda0000
		Table 6: Ofﬁcial test results for the FLC task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 illustrates the best F1 score on the
		prediction
	</Extractive Summary>
</Paper ID=ument728>


<Paper ID=ument728> <Table ID =7>
	<Abstractive Summary> =
		0005
		Table 7: Results for FLC tasl on the development set
	</Abstractive Summary>
</Paper ID=ument728>


<Paper ID=ument73> <Table ID =1>
	<Abstractive Summary> =
		82
		Table 1: Comparison of conﬁdence measures
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Comparison of Conﬁdence Measures
		Table 1 shows the comparison of conﬁdence mea-
		sures on the Chinese-English development set
	</Extractive Summary>
</Paper ID=ument73>


<Paper ID=ument73> <Table ID =2>
	<Abstractive Summary> =
		82
		Table 2: Comparison between word- and sentence-
		level CEV conﬁdence measures
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Comparison between Word- and
		Sentence-level Conﬁdence Measures
		Table 2 shows the comparison between word-
		and sentence-level CEV (i
	</Extractive Summary>
</Paper ID=ument73>


<Paper ID=ument73> <Table ID =3>
	<Abstractive Summary> =
		37‡‡
		Table 3: BLEU scores on the NIST Chinese-English translation task
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Main Results
		The Chinese-English Task
		Table 3 shows the results of the Chinese-English
		task
	</Extractive Summary>
</Paper ID=ument73>


<Paper ID=ument73> <Table ID =4>
	<Abstractive Summary> =
		15‡‡††
		Table 4: BLEU scores on the WMT14 English-German translation task
	</Abstractive Summary>
	<Extractive Summary> =
		The English-German Task
		Table 4 shows the results of the English-German
		task
	</Extractive Summary>
</Paper ID=ument73>


<Paper ID=ument730> <Table ID =1>
	<Abstractive Summary> =
		Rate
		Bo Xilai
		665
		336
		64%
		Deng Xiaoping
		281
		125
		70%
		Fire
		431
		530
		45%
		Injury/Dead Body
		1799
		1029
		51%
		Liu Xiaobo
		184
		123
		60%
		Mao Zedong
		1093
		486
		70%
		People’s Congress
		145
		113
		56%
		Policeman
		1311
		927
		59%
		Protest
		536
		220
		71%
		Prurient/Nudity
		2664
		2551
		51%
		Rainstorm
		153
		207
		43%
		Winnie the Pooh
		160
		177
		48%
		Xi Jinping
		1745
		1029
		63%
		Zhou Kehua
		102
		134
		43%
		Table 1: Percentage of censored posts per category
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the number of posts found in each category as well
		as the percentage of posts in each category that
		was censored
	</Extractive Summary>
</Paper ID=ument730>


<Paper ID=ument730> <Table ID =2>
	<Abstractive Summary> =
		005
		Table 2: Survival regression per category
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results of survival analy-
		sis per category
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, sentiment always has a
		negative sign and it is always statistically signif-
		icant at 5%
	</Extractive Summary>
</Paper ID=ument730>


<Paper ID=ument731> <Table ID =1>
	<Abstractive Summary> =
		4
		Datasets
		class
		# of occurrences
		Clean (Train)
		80977 (96%)
		Implicit Toxic (Train)
		2948 (4%)
		Clean (dev)
		9019 (96%)
		Implicit Toxic (dev)
		307 (4%)
		Clean (Test)
		33541 (83%)
		Explicit Toxic (Test)
		5085 (13%)
		Implicit Toxic (Test)
		1158 (4%)
		Table 1: Class distribution of Wikipedia dataset
	</Abstractive Summary>
</Paper ID=ument731>


<Paper ID=ument731> <Table ID =2>
	<Abstractive Summary> =
		This dataset consists of comments from
		class
		# of occurrences
		NAG (Train)
		4159 (46%)
		Implicit CAG (Train)
		3223 (36%)
		Implicit OAG (Train)
		1651 (18%)
		NAG (Dev)
		1029 (46%)
		Implicit CAG (Dev)
		806 (36%)
		Implicit OAG (Dev)
		420 (18%)
		NAG (F)
		491 (65%)
		Explicit CAG (F)
		35 (5%)
		Explicit OAG (F)
		56 (7%)
		Implicit CAG (F)
		95 (13%)
		Implicit OAG (F)
		73 (10%)
		NAG (T)
		431 (38%)
		Explicit CAG (T)
		85 (7%)
		Explicit OAG (T)
		103 (9%)
		Implicit CAG (T)
		328 (29%)
		Implicit OAG (T)
		188 (17%)
		Table 2: Class distribution of Facebook (F) and Twitter
		(T) datasets
	</Abstractive Summary>
</Paper ID=ument731>


<Paper ID=ument731> <Table ID =3>
	<Abstractive Summary> =
		)
		Table 3: Data preprocessing example
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 is an ex-
		ample of this data preprocessing
	</Extractive Summary>
</Paper ID=ument731>


<Paper ID=ument731> <Table ID =4>
	<Abstractive Summary> =
		09
		Table 4: Results of different models on Wikipedia, Facebook and Twitter datasets, HAN: Hierarchical Attention
		Neural Net, HCL: Hierarchical C-LSTM
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		Deep learning performance: Table 4 compares
		our hierarchical model against the baselines as
		well as state-of-the-arts
	</Extractive Summary>
	<Extractive Summary> =
		Table 4
		shows our ensemble with only one C-LSTM out-
		performs
	</Extractive Summary>
</Paper ID=ument731>


<Paper ID=ument731> <Table ID =5>
	<Abstractive Summary> =
		55
		Table 5: Comparisons of F1 measure and AUC using Wikipedia dataset which has under 100 words, over 100 and
		under 200 words and over 200 words
	</Abstractive Summary>
</Paper ID=ument731>


<Paper ID=ument731> <Table ID =6>
	<Abstractive Summary> =
		do n t let
		him become another afzal guru in real ! shutdownjnu
		madarsajnu
		NAG
		OAG
		T
		59%
		kinner to vo h jo bar bar pakistan dwara hamare uper
		hamle hone par bhi apna much chhipa k baitha h kinner
		vo h jo bar bar bina bulaye pakistan ja raha h
		CAG
		NAG
		Table 6: Example of error caused by the proposed model for the Wikipedia (W), Facebook (F) and Twitter (T)
		dataset
	</Abstractive Summary>
</Paper ID=ument731>


<Paper ID=ument732> <Table ID =1>
	<Abstractive Summary> =
		Total
		Troll
		924
		206
		229
		925
		2,284
		Control
		284,153
		63,146
		70,162
		284,153
		701,614
		Table 1: Dataset statistics
	</Abstractive Summary>
</Paper ID=ument732>


<Paper ID=ument732> <Table ID =2>
	<Abstractive Summary> =
		770
		Table 2: Overall results on the test dataset
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we compare
		our model (CBS+Self) trained using the entire troll
		dataset
	</Extractive Summary>
</Paper ID=ument732>


<Paper ID=ument732> <Table ID =3>
	<Abstractive Summary> =
		586
		Table 3:
		Ablation results using the validation
		dataset for the three major feature groups: Stylistic
		(S), Behavioral (B), and Content (C)
	</Abstractive Summary>
</Paper ID=ument732>


<Paper ID=ument732> <Table ID =4>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		1
		False Positives
		The false semantic groups and counts of false pos-
		itive errors are displayed in Table 4a
	</Extractive Summary>
	<Extractive Summary> =
		2
		False Negatives
		In Table 4b we display the counts of false nega-
		tives that fall into one of ﬁve groups: Support, Dis-
		cord, Political Concealment, Unknown Character,
		and Misc
	</Extractive Summary>
</Paper ID=ument732>


<Paper ID=ument732> <Table ID =5>
	<Abstractive Summary> =
		41
		23
		42
		20
		Total Error
		100
		62
		100
		47
		False Negatives
		Table 5:
		Manual analysis of false positives and false negatives for the Content (C) and Con-
		tent+Behavioral+Style (CBS) models
	</Abstractive Summary>
</Paper ID=ument732>


<Paper ID=ument733> <Table ID =1>
	<Abstractive Summary> =
		91
		Table 1: Signiﬁcant components of our logistic regression model using the Coh-Metrix features
	</Abstractive Summary>
</Paper ID=ument733>


<Paper ID=ument733> <Table ID =2>
	<Abstractive Summary> =
		78
		Table 2: Results of classiﬁcation between fake news
		and satire articles using BERT pre-trained models,
		based on the headline, body and full text
	</Abstractive Summary>
</Paper ID=ument733>


<Paper ID=ument733> <Table ID =3>
	<Abstractive Summary> =
		78*
		Table 3: Summary of results of classiﬁcation between
		fake news and satire articles using the baseline Multi-
		nomial Naive Bayes method, the linguistic cues of
		text coherence and semantic representation with a pre-
		trained BERT model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 provides a summary of the results
	</Extractive Summary>
</Paper ID=ument733>


<Paper ID=ument734> <Table ID =1>
	<Abstractive Summary> =
		2014
		Users liking the posts by the protest
		groups
		57754
		Table 1: Basic statistics on the Bolotnaya dataset
		Tahrir Square protests (Tufekci and Wilson, 2012)
	</Abstractive Summary>
</Paper ID=ument734>


<Paper ID=ument734> <Table ID =2>
	<Abstractive Summary> =
		75
		Table 2: CTA classiﬁcation results
		7
		CTAs for Predicting social unrest
		To estimate the potential usefulness of CTAs
		as indicators of ofﬂine protest events, we ran
		the trained RuBERT CTA classiﬁer over 91K
		posts falling in the date range between Dec 2011
		through Jul 2013 from the Bolotnaya dataset
	</Abstractive Summary>
</Paper ID=ument734>


<Paper ID=ument735> <Table ID =1>
	<Abstractive Summary> =
		48
		Label
		Original
		All
		Labeled
		Pro-Russian
		512
		4,829
		Pro-Ukrainian
		910
		12,343
		Neutral
		6,923
		118,196
		Unlabeled
		-
		192,003
		377,679
		Total
		-
		200,348
		513,047
		Table 1: Label distribution and dataset sizes
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the label distribution of the anno-
		tated portion of the data as well as the total amount
		of original tweets, and original tweets plus their
		retweets/duplicates in the network
	</Extractive Summary>
</Paper ID=ument735>


<Paper ID=ument735> <Table ID =2>
	<Abstractive Summary> =
		Neutral
		#PrayForMH17 :(
		RT @deserto_fox: Russian terrorist stole wedding ring from dead passenger #MH17
		Table 2: Example tweets for each of the three classes
	</Abstractive Summary>
</Paper ID=ument735>


<Paper ID=ument735> <Table ID =3>
	<Abstractive Summary> =
		94
		Table 3: Classiﬁcation results on the English MH17 dataset measured as F1 and area under the precision-recall
		curve (AUC)
	</Abstractive Summary>
</Paper ID=ument735>


<Paper ID=ument735> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Examples for the different error categories
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents examples for each
		of the error categories in both sets which we will
		discuss in the following
	</Extractive Summary>
</Paper ID=ument735>


<Paper ID=ument735> <Table ID =5>
	<Abstractive Summary> =
		Pro-R
		Pro-U
		Neutral
		Total
		# labeled edges in k10
		270
		678
		2193
		3141
		# candidate edges
		349
		488
		-
		873
		# added after ﬁltering
		predictions
		77
		110
		-
		187
		Table 5: Number of labeled edges in the k10 network
		before and after augmentation with predicted labels
	</Abstractive Summary>
	<Extractive Summary> =
		While the network
		shows a clear polarization, only a small subset of
		the edges present in the network are labeled (see
		Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the amount of polar-
		ized edges we can predict at this precision level
	</Extractive Summary>
</Paper ID=ument735>


<Paper ID=ument736> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example of a claim and its diverse sentential
		arguments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows an example1 of the input and outputs of
		our system
	</Extractive Summary>
</Paper ID=ument736>


<Paper ID=ument736> <Table ID =2>
	<Abstractive Summary> =
		4146
		Table 2: Automatic evaluation results on generation quality
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Overall Performance
		Table 2 shows the evaluation results of each model
		in terms of generation quality using BLEU score
		and word embedding based metrics
	</Extractive Summary>
</Paper ID=ument736>


<Paper ID=ument736> <Table ID =3>
	<Abstractive Summary> =
		6134
		Table 3:
		Automatic evaluation results on diversity of generation
	</Abstractive Summary>
</Paper ID=ument736>


<Paper ID=ument736> <Table ID =4>
	<Abstractive Summary> =
		4146
		Table 4: Automatic evaluation results on generation quality with different objective functions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 and Table 5 show the evalua-
		tion results of our models in terms of quality and
		diversity of generated text, respectively
	</Extractive Summary>
</Paper ID=ument736>


<Paper ID=ument736> <Table ID =5>
	<Abstractive Summary> =
		6134
		Table 5: Automatic evaluation results on diversity of generation with different objective functions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 and Table 5 show the evalua-
		tion results of our models in terms of quality and
		diversity of generated text, respectively
	</Extractive Summary>
</Paper ID=ument736>


<Paper ID=ument736> <Table ID =6>
	<Abstractive Summary> =
		Table 6:
		Sample arguments of a claim generated by
		human and models
	</Abstractive Summary>
</Paper ID=ument736>


<Paper ID=ument737> <Table ID =1>
	<Abstractive Summary> =
		, 2019], enhanced 
		from PolitiFact and GossipCop 
		Table 1: Datasets for rumor detection and their properties 
		 
		supporting, denying, querying and commenting
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists the datasets for 
		rumor detection
	</Extractive Summary>
	<Extractive Summary> =
		The 
		data of datasets in Table 1 are collected from four 
		social media platforms: Twitter, Facebook, 
		Reddit and Weibo
	</Extractive Summary>
</Paper ID=ument737>


<Paper ID=ument737> <Table ID =2>
	<Abstractive Summary> =
		, 2018] 
		y 
		  
		y 
		  
		y 
		  
		RNN 
		Table 2: Previous studies, used information, and methods
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents the 
		studies and their related information
	</Extractive Summary>
</Paper ID=ument737>


<Paper ID=ument738> <Table ID =1>
	<Abstractive Summary> =
		The label of each comment was aggregated based
		Train
		Validation
		Test
		Attack
		8,079
		2,755
		2,880
		Not-attack
		61,447
		20,405
		20,298
		All
		69,526
		23,160
		23,178
		Table 1: Statistics of the personal attacks corpus
	</Abstractive Summary>
	<Extractive Summary> =
		The corpus comes
		with a 60-20-20 split into training, validation, and
		test set (see Table 1 for corpus statistics)
	</Extractive Summary>
</Paper ID=ument738>


<Paper ID=ument738> <Table ID =2>
	<Abstractive Summary> =
		17
		Table 2:
		Effectiveness on the test set of the per-
		sonal attacks corpus (AUC and Spearman coefﬁcients):
		our proposed approach, the previous state of the art
		(Pavlopoulos et al
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen in Table 2, our approach is
		slightly better than the re-implementation in terms
		of AUC and Spearman in both splits and the whole
		test set
	</Extractive Summary>
</Paper ID=ument738>


<Paper ID=ument738> <Table ID =3>
	<Abstractive Summary> =
		14
		Table 3: Effectiveness (AUC values and Spearman co-
		efﬁcients) of our approach’s ﬁrst ﬁve iterations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the AUC values and Spearman co-
		efﬁcients for the ﬁrst ﬁve iterations of our approach
		on the unraveled validation and test set
	</Extractive Summary>
</Paper ID=ument738>


<Paper ID=ument738> <Table ID =4>
	<Abstractive Summary> =
		1
		2
		3
		4
		5
		Size
		1650 1725 1780 1829 1875
		Increment
		+75
		+55
		+49
		+46
		Partially abusive
		+20
		+30
		+24
		+18
		Abusive
		+14
		+13
		+18
		+21
		Non-abusive
		+41
		+12
		+ 7
		+ 7
		Table 4: Increment and of the abusive lexicon in the
		ﬁrst ﬁve iterations of our approach
	</Abstractive Summary>
</Paper ID=ument738>


<Paper ID=ument738> <Table ID =5>
	<Abstractive Summary> =
		Figure 2 illustrates some texts with the abusive-
		81
		Iteration
		Abusive Partially abusive Non-abusive
		2
		jerk
		masturbating
		headline
		fuckheads
		freak
		heck
		douchebag
		clowns
		nightmare
		3
		fucking
		rudely
		hometown
		fvck
		dunce
		lifetime
		bastard
		pederast
		imature
		4
		bithces
		ﬁlthy
		policemans
		sissy
		lame
		foot
		fuk
		harrassing
		die
		5
		niggers
		nazi
		pint
		faggots
		hypocritical
		boss
		fuckers
		imposter
		pay
		Table 5: The newly added abusive words in the ﬁrst it-
		erations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the newly found words in each
		of the ﬁrst iterations
	</Extractive Summary>
</Paper ID=ument738>


<Paper ID=ument739> <Table ID =1>
	<Abstractive Summary> =
		25
		Table 1: Experiment results of different models on the SLC task, and the model with the highest F1 score which
		has been underlined is chosen to be evaluated on the test set
	</Abstractive Summary>
</Paper ID=ument739>


<Paper ID=ument739> <Table ID =2>
	<Abstractive Summary> =
		10
		Table 2: Experiment results of the chosen model and
		the random baseline for the SLC task
	</Abstractive Summary>
</Paper ID=ument739>


<Paper ID=ument74> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Case-sensitive BLEU scores on English-German and English-French translation
	</Abstractive Summary>
</Paper ID=ument74>


<Paper ID=ument74> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: English-German task: Ablation experiments
		of different technologies
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2 we draw the following con-
		Model
		BLEU
		Base CAPSNMT
		24
	</Extractive Summary>
</Paper ID=ument74>


<Paper ID=ument74> <Table ID =3>
	<Abstractive Summary> =
		5
		26
		Iteration
		BLEU
		2 caps
		4 caps
		6 caps
		8 caps
		Figure 3: Effects of Iterative Routing with different
		capsule numbers
		Model
		Num
		Latency(ms)
		Transformer
		-
		225
		CAPSNMT
		4
		146
		CAPSNMT
		6
		153
		CAPSNMT
		8
		168
		Table 3:
		Time required for decoding with the base
		model
	</Abstractive Summary>
</Paper ID=ument74>


<Paper ID=ument74> <Table ID =4>
	<Abstractive Summary> =
		Orlando Bloom and Miranda Kerr still love each other
		Orlando Bloom and Miranda Kerr still love each other
		Orlando Bloom and Miranda Kerr still love each other
		Orlando Bloom and Miranda Kerr still love each other
		Table 4: A visualization to show the perspective of a
		sentence from 4 different capsules at the third iteration
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, the color density of
		each word denotes the coefﬁcient αij at iteration
		3 in Eq
	</Extractive Summary>
</Paper ID=ument74>


<Paper ID=ument740> <Table ID =1>
	<Abstractive Summary> =
		208
		Table 1: Evaluation results on ofﬁcial development set
		Technique
		Dev F1
		Test F1
		Appeal-Authority
		0
		0
		Appeal-Fear
		0
	</Abstractive Summary>
</Paper ID=ument740>


<Paper ID=ument740> <Table ID =2>
	<Abstractive Summary> =
		16
		0
		Table 2: Classwise F1 scores for ﬁnal submission
		the models we tried including the baseline BERT
		model
	</Abstractive Summary>
</Paper ID=ument740>


<Paper ID=ument741> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Features used in SLC and FLC tasks
		ing , stereotyping, etc
	</Abstractive Summary>
	<Extractive Summary> =
		This inspires1 us in extract-
		ing different features (Table 1) including the com-
		plexity of text, sentiment, emotion, lexical (POS,
		NER, etc
	</Extractive Summary>
	<Extractive Summary> =
		Additionally, we apply a de-
		cision function (Table 1) such that a sentence is
		tagged as propaganda if prediction probability of
		the classiﬁer is greater than a threshold (τ)
	</Extractive Summary>
	<Extractive Summary> =
		We explore two ensemble strategies (Table 1):
		majority-voting and relax-voting to boost preci-
		sion and recall, respectively
	</Extractive Summary>
</Paper ID=ument741>


<Paper ID=ument741> <Table ID =2>
	<Abstractive Summary> =
		0822
		Table 2: Comparison of our system (MIC-CIS) with
		top-5 participants: Scores on Test set for SLC and FLC
		use BIO tagging scheme of NER in FLC task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 (SLC) shows
		that our submission is ranked at 4th position
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 (FLC) shows
		that our submission is ranked at 3rd position
	</Extractive Summary>
</Paper ID=ument741>


<Paper ID=ument741> <Table ID =3>
	<Abstractive Summary> =
		751
		Table 3: SLC: Scores on Dev (internal) of Fold1 and
		Dev (external) using different classiﬁers and features
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results: Sentence-Level Propaganda
		Table 3 shows the scores on dev (internal and ex-
		ternal) for SLC task
	</Extractive Summary>
</Paper ID=ument741>


<Paper ID=ument741> <Table ID =4>
	<Abstractive Summary> =
		150
		Table 4: FLC: Scores on Dev (internal) of Fold1 and
		Dev (external) with different models, features and en-
		sembles
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results: Fragment-Level Propaganda
		Table 4 shows the scores on dev (internal and ex-
		ternal) for FLC task
	</Extractive Summary>
</Paper ID=ument741>


<Paper ID=ument741> <Table ID =5>
	<Abstractive Summary> =
		1∗
		Table 5: Hyper-parameter settings for FLC task
	</Abstractive Summary>
	<Extractive Summary> =
		See
		Table 5 for hyper-parameter settings for FLC task
		using LSTM-CRF
	</Extractive Summary>
</Paper ID=ument741>


<Paper ID=ument742> <Table ID =1>
	<Abstractive Summary> =
		com/
		99
		Propaganda Technique
		Frequency
		Loaded Language
		2,115
		Name Calling,Labeling
		1,085
		Repetition
		571
		Doubt
		490
		Exaggeration,Minimisation
		479
		Flag-Waving
		240
		Appeal to Fear/Prejudice
		239
		Causal Oversimpliﬁcation
		201
		Slogans
		136
		Appeal to Authority
		116
		Black-and-White Fallacy
		109
		Thought-terminating Cliches
		79
		Whataboutism
		57
		Reductio ad hitlerum
		54
		Red Herring
		33
		Bandwagon
		13
		Straw Men
		13
		Obfuscation,Intentional Vagueness,Confusion
		11
		Total
		6,041
		Table 1: Frequency of all eighteen propaganda tech-
		niques in the training data
		We also conduct our error analysis on the local dev
		set because we do not have access to the gold la-
		bels of the ofﬁcial dev and test sets of the shared
		task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists all eighteen propaganda
		techniques and their frequencies in the training
		data
	</Extractive Summary>
</Paper ID=ument742>


<Paper ID=ument742> <Table ID =2>
	<Abstractive Summary> =
		80
		**handcrafted features include LIWC and presence of
		questions or quotes
		Table 2: SLC experiments on different feature sets
		Dataset
		P
		R
		F
		Local Dev
		0
	</Abstractive Summary>
	<Extractive Summary> =
		80 in
		the experiments shown in Table 2 to avoid overﬁt-
		ting on a one dataset
	</Extractive Summary>
</Paper ID=ument742>


<Paper ID=ument742> <Table ID =3>
	<Abstractive Summary> =
		70
		Table 3: SLC best model results on all three datasets
		Repetition and Doubt are the two most chal-
		lenging types for the classiﬁer even though they
		are in the four most frequent techniques
	</Abstractive Summary>
</Paper ID=ument742>


<Paper ID=ument742> <Table ID =4>
	<Abstractive Summary> =
		The best model is a
		BiLSTM-CRF with ﬂair and urban glove embed-
		101
		Technique
		Count
		Accuracy
		Loaded Language
		299
		71%
		Name Calling,Labeling
		163
		69%
		Repetition
		124
		44%
		Doubt
		71
		40%
		Exaggeration,Minimisation
		63
		67%
		Flag-Waving
		35
		74%
		Appeal to Fear/Prejudice
		42
		52%
		Causal Oversimpliﬁcation
		24
		58%
		Slogans
		24
		54%
		Table 4: SLC accuracy on frequent propaganda tech-
		niques in the local development set
		dings with one hot encoded features as mentioned
		in Section 3
	</Abstractive Summary>
</Paper ID=ument742>


<Paper ID=ument742> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Precision, recall and F1 scores of the FLC task
		on the development and test sets of the shared task
	</Abstractive Summary>
	<Extractive Summary> =
		2
		FLC Results
		In FLC, we only show the results of our best
		model in Table 5 to focus more on the differ-
		ences between propaganda techniques
	</Extractive Summary>
	<Extractive Summary> =
		Discussion of Propaganda Types:
		As we can
		see in Table 5,
		we can divide the propa-
		ganda techniques into three groups according
		to the model’s performance on the development
		and test sets
	</Extractive Summary>
</Paper ID=ument742>


<Paper ID=ument744> <Table ID =1>
	<Abstractive Summary> =
		samples to split
		2
		Table 1: Hyperparameter values for GBT
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows all hyperparameter values set for the GBT
		model
	</Extractive Summary>
</Paper ID=ument744>


<Paper ID=ument744> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Propaganda detection performance over 5-fold
		cross-validation
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results and Discussion
		Table 2 shows the results of all models over 5-
		fold cross-validation on the provided SLC training
		data
	</Extractive Summary>
</Paper ID=ument744>


<Paper ID=ument744> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Ofﬁcial results for propaganda detection task
		(on withheld test data)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents ours results on the ofﬁcial test
		data
	</Extractive Summary>
</Paper ID=ument744>


<Paper ID=ument745> <Table ID =1>
	<Abstractive Summary> =
		Features
		dimension
		Glove
		300
		TweetToEmbeddings
		100
		TweetToInputLeixicon
		4
		TweetToLexicon
		43
		TweetToSentiStrength
		2
		Table 1: Features used in our approach
		4
		Our Approach
		The architecture of our system consists of four
		sub-models: BiLSTM sub-model, XGBoost sub-
		model, BERT Cased and UnCased model (Figure
		1)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
</Paper ID=ument745>


<Paper ID=ument745> <Table ID =2>
	<Abstractive Summary> =
		477673
		Table 2: BiLSTM result on development data set
		100, batch size= 512, validation split= 33% (See
		Table 2)
	</Abstractive Summary>
</Paper ID=ument745>


<Paper ID=ument745> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: XGBoost Hyperparameter
		4
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 sum-
		marizes XGBoost hyperparameters
	</Extractive Summary>
</Paper ID=ument745>


<Paper ID=ument745> <Table ID =4>
	<Abstractive Summary> =
		387009
		Table 4: XGBoost results on development dataset
		Type
		seq length
		batch size
		lr
		epochs
		StopWord
		F1
		Precision
		Recall
		Cased
		400
		4
		1e-5
		3
		With
		0
	</Abstractive Summary>
</Paper ID=ument745>


<Paper ID=ument745> <Table ID =5>
	<Abstractive Summary> =
		492558
		Table 5: BERT result on development dataset
		F1
		Precision
		Recall
		BERT (Cased) + BERT (Uncased)
		0
	</Abstractive Summary>
	<Extractive Summary> =
		We have noticed that using Uncased and
		Cased models with ensembling between them
		gives the best results (Table 5)
	</Extractive Summary>
</Paper ID=ument745>


<Paper ID=ument745> <Table ID =6>
	<Abstractive Summary> =
		756428
		Table 6: Ensembling result on development dataset
		result, we have combined both of them together by
		checking if the 4 models predict that the sentence
		is non-propaganda then it will be labeled as non-
		propaganda, otherwise it will be labeled as Propa-
		ganda
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 illustrates the best F1 score on the
		prediction
	</Extractive Summary>
</Paper ID=ument745>


<Paper ID=ument746> <Table ID =1>
	<Abstractive Summary> =
		16
		Table 1: Sentence-level (SLC) results
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 1 shows the ablation study results for the
		SLC task
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 1, BERT performed better than
		LR bert but worse than LR†‡, which indicates that
		the transfer learning when considering single se-
		mantic variable is not as effective as the combi-
		nation with other linguistic features
	</Extractive Summary>
</Paper ID=ument746>


<Paper ID=ument747> <Table ID =1>
	<Abstractive Summary> =
		11E-02
		0
		Table 1: p-values representing the similarity between
		(parts of) the train, test and development sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 provides the
		minimum and maximum p-values and their inter-
		pretations for ten such runs of each pair reported
	</Extractive Summary>
</Paper ID=ument747>


<Paper ID=ument747> <Table ID =2>
	<Abstractive Summary> =
		6276
		Table 2: F1 scores on an unseen (not used for train-
		ing) part of the training set and the development set on
		BERT using different augmentation techniques
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results of our experi-
		ments for different data augmentation techniques
		when, after shufﬂing the training data, we train the
		model on 75% of the training data and test it on the
		remaining 25% of the training data and the devel-
		opment data
	</Extractive Summary>
</Paper ID=ument747>


<Paper ID=ument747> <Table ID =3>
	<Abstractive Summary> =
		6033
		Table 3: Class-wise precision and recall with and with-
		out oversampling (OS) achieved on unseen part of the
		training set
	</Abstractive Summary>
	<Extractive Summary> =
		The results of these
		experiments (Table 3) show that oversampling in-
		creases the overall recall while maintaining preci-
		sion
	</Extractive Summary>
</Paper ID=ument747>


<Paper ID=ument747> <Table ID =4>
	<Abstractive Summary> =
		494168
		Table 4: Our results on the SLC task (2nd, in bold)
		alongside comparable results from the competition
		leaderboard
	</Abstractive Summary>
</Paper ID=ument747>


<Paper ID=ument747> <Table ID =5>
	<Abstractive Summary> =
		000008
		Table 5: Our results on the FLC task (7th, in bold)
		alongside those of better performing teams from the
		competition leaderboard
	</Abstractive Summary>
</Paper ID=ument747>


<Paper ID=ument748> <Table ID =1>
	<Abstractive Summary> =
		69
		Table 1: Performance of baselines on development set
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 1, the Perspective-API baseline achieves
		0
	</Extractive Summary>
</Paper ID=ument748>


<Paper ID=ument748> <Table ID =2>
	<Abstractive Summary> =
		unigrams
		devastating, cruel, vile, irrational, absurd, brutal, vicious, stupid,
		coward, awful, ignorant, unbelievable, doomed, idiot, terrifying,
		disgusting, horrible, hideous, horriﬁc, pathetic
		bigrams
		shame less, totally insane, a horrible, utterly unacceptable, hys-
		terical nonsense, the horrible, this horriﬁc, absolutely disgusting,
		monumental stupidity, a pathetic, a disgusting, absolutely worth-
		less, truly disgusting, utterly insane, this murderous, incredibly
		stupid, monstrous fraud, this lunatic, a disgrace, a hideous
		Table 2: Top 20 unigrams and bigrams with highest likelihood of being propaganda
	</Abstractive Summary>
</Paper ID=ument748>


<Paper ID=ument749> <Table ID =1>
	<Abstractive Summary> =
		5 for the uncased
		Table 1: Models results on development and testing
		datasets
		Model
		Dataset
		F1-score
		Uncased BERT
		Dev
		66
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the models results on development and testing
		datasets
	</Extractive Summary>
</Paper ID=ument749>


<Paper ID=ument749> <Table ID =2>
	<Abstractive Summary> =
		3870
		Table 2: Uncased BERT model experiments results on
		development datasets
		Model
		F1-score
		More Training
		60
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the uncased
		BERT model experiments results on developments
		dataset
	</Extractive Summary>
</Paper ID=ument749>


<Paper ID=ument75> <Table ID =1>
	<Abstractive Summary> =
		21
		Table 1: Results comparison
	</Abstractive Summary>
</Paper ID=ument75>


<Paper ID=ument75> <Table ID =2>
	<Abstractive Summary> =
		752
		Table 2: Evaluation of weighted negative sampling
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the testing results with met-
		rics Hits@1 and MRR
	</Extractive Summary>
	<Extractive Summary> =
		We summarize Table 2 in three aspects
	</Extractive Summary>
</Paper ID=ument75>


<Paper ID=ument75> <Table ID =3>
	<Abstractive Summary> =
		755
		Table 3: Ablation study
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results of the
		two variants and the overall method
	</Extractive Summary>
	<Extractive Summary> =
		As expected, Table 3 shows that each variant
		gets better results compared with four baselines
	</Extractive Summary>
</Paper ID=ument75>


<Paper ID=ument75> <Table ID =4>
	<Abstractive Summary> =
		6%
		Table 4: Comparison of JAPE and our model in Hits@1 results by mapping properties of relations
	</Abstractive Summary>
</Paper ID=ument75>


<Paper ID=ument750> <Table ID =1>
	<Abstractive Summary> =
		Label
		Train
		Propaganda
		4720
		Non-Propaganda
		12245
		Table 1: Data Distribution
		2
	</Abstractive Summary>
</Paper ID=ument750>


<Paper ID=ument750> <Table ID =2>
	<Abstractive Summary> =
		Figure 1: Distribution of Classes in Training Set
		145
		Table 2: Model Architectures used for training and their optimal hyperparameters
		Model
		Hyperparameters
		BERT-1
		BERT-Base-Uncased
		batch-size=32, learning-rate=2e-5,epochs=3
		BERT-2
		DocumentEmbeddings {Stacked Embeddings BERT + GRU + Dropout (p=0
	</Abstractive Summary>
</Paper ID=ument750>


<Paper ID=ument750> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Performance of different models on develop-
		ment data for SLC Task
		Model
		F1
		Pr
		Rc
		Naive Bayes
		(count vectorizer)
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 represents the performance of all the
		models trained on the training dataset and evalu-
		ated on the development data for the SLC Task
	</Extractive Summary>
</Paper ID=ument750>


<Paper ID=ument751> <Table ID =1>
	<Abstractive Summary> =
		7656
		Table 1:
		Ablation study of our BERT-BiLSTM-
		Capsule model on the validation set
	</Abstractive Summary>
</Paper ID=ument751>


<Paper ID=ument751> <Table ID =2>
	<Abstractive Summary> =
		7717
		Table 2: Comparative results against our base models
		on the validation set
	</Abstractive Summary>
</Paper ID=ument751>


<Paper ID=ument751> <Table ID =3>
	<Abstractive Summary> =
		4347
		Table 3: Comparative analysis against the ofﬁcial base-
		line result as well as the best performer of the SLC task
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we present our
		results on the test set in comparison to the SLC
		task baseline and the highest-ranking team
	</Extractive Summary>
</Paper ID=ument751>


<Paper ID=ument752> <Table ID =1>
	<Abstractive Summary> =
		085
		Table 1: Effect of using SMOTE and BLSTM ﬁne-
		tuning on the pre-trained language model using macro-
		averaged F1-score
	</Abstractive Summary>
	<Extractive Summary> =
		GROVER
		performs better than BERT and ELMo without
		any ﬁne-tuning, and our ﬁne-tuning method on
		GROVER improved the precision but resulted in
		a lower overall recall (See Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		As some of
		our results are inconsistent (Table 1) ad-
		ditional evaluation using conventional ﬁne-
		tuning methods would aid us in understand-
		ing what is learned by ﬁne-tuning
	</Extractive Summary>
</Paper ID=ument752>


<Paper ID=ument752> <Table ID =2>
	<Abstractive Summary> =
		03
		Train
		-
		160 106 123 107 127 125
		45
		62
		24
		27
		120
		78
		97
		17
		25
		80
		32
		115
		ELMo
		-
		307
		22
		-
		10
		25
		29
		15
		31
		13
		16
		-
		6
		86
		10
		21
		-
		9
		35
		BERT
		-
		93
		22
		-
		10
		23
		31
		15
		21
		12
		13
		-
		6
		65
		10
		13
		20
		25
		40
		GROVER
		-
		74
		22
		-
		69
		26
		40
		15
		20
		11
		13
		-
		5
		28
		10
		11
		20
		18
		44
		Table 2: (1) F1-score for classes in the FLC task
	</Abstractive Summary>
	<Extractive Summary> =
		Observing the
		results from Table 2, not all models are predict-
		ing meaningful span length on each class compar-
		ing to the average length in training data (i
	</Extractive Summary>
</Paper ID=ument752>


<Paper ID=ument752> <Table ID =3>
	<Abstractive Summary> =
		000
		Table 3: Pearson correlation (r) and p-value for the pre-
		dicted span lengths of the models
	</Abstractive Summary>
</Paper ID=ument752>


<Paper ID=ument753> <Table ID =1>
	<Abstractive Summary> =
		18)
		Table 1: Statistics about the gold annotations for the
		training and the development sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 reports the total number of instances per
		technique and the percentage with respect to the
		total number of annotations, for the training and
		for the development sets
	</Extractive Summary>
</Paper ID=ument753>


<Paper ID=ument753> <Table ID =2>
	<Abstractive Summary> =
		Tuning
		CUNLP
		✓
		✓
		✓
		Stalin
		✓
		✓
		MIC-CIS
		✓
		✓
		✓
		ltuorp
		✓
		ProperGander
		✓
		✓
		newspeak
		✓
		✓
		Table 2: Overview of the approaches for the fragment-level classiﬁcation task
	</Abstractive Summary>
</Paper ID=ument753>


<Paper ID=ument753> <Table ID =3>
	<Abstractive Summary> =
		Team
		BERT
		LSTM
		logreg
		USE
		CNN
		Embeddings
		Features
		Context
		NSIT
		✓
		✓
		CUNLP
		✓
		✓
		✓
		JUSTDeep
		✓
		✓
		✓
		✓
		Tha3aroon
		✓
		✓
		LIACC
		✓
		✓
		✓
		MIC-CIS
		✓
		✓
		✓
		✓
		✓
		CAUnLP
		✓
		✓
		YMJA
		✓
		jinfen
		✓
		✓
		✓
		ProperGander
		✓
		Table 3: Overview of the approaches used for the sentence-level classiﬁcation task
	</Abstractive Summary>
</Paper ID=ument753>


<Paper ID=ument753> <Table ID =4>
	<Abstractive Summary> =
		0646
		Table 4: Ofﬁcial test results for the SLC task
	</Abstractive Summary>
	<Extractive Summary> =
		10
		Evaluation Results
		The results on the test set for the SLC task are
		shown in Table 4, while Table 5 presents the re-
		sults on the development set at the end of phase
		1 (cf
	</Extractive Summary>
</Paper ID=ument753>


<Paper ID=ument753> <Table ID =5>
	<Abstractive Summary> =
		2286
		Table 5: Results for the SLC task on the development
		set at the end of phase 1 (see Section 6)
	</Abstractive Summary>
	<Extractive Summary> =
		10
		Evaluation Results
		The results on the test set for the SLC task are
		shown in Table 4, while Table 5 presents the re-
		sults on the development set at the end of phase
		1 (cf
	</Extractive Summary>
</Paper ID=ument753>


<Paper ID=ument753> <Table ID =6>
	<Abstractive Summary> =
		0000
		Table 6: Ofﬁcial test results for the FLC task
	</Abstractive Summary>
</Paper ID=ument753>


<Paper ID=ument753> <Table ID =7>
	<Abstractive Summary> =
		0005
		Table 7: Results for FLC tasl on the development set
	</Abstractive Summary>
</Paper ID=ument753>


<Paper ID=ument754> <Table ID =1>
	<Abstractive Summary> =
		Word
		Token
		22/05/2019
		<L0D8O2>
		TR04
		<L2D2O0>
		10,000
		<L0D5O1>
		Table 1: Examples of word transformation algorithm
		We implemented our models using Tensorﬂow
		framework5)
		Table 1: Data size (number of sentences) for the three
		models03
		Table 1: VAD scores and their correlation with a broad
		set of economic indicators (excerpt)01
		bankruptcy
		Table 1: Event Types (target variables)
		Figure 1: Time structure of the model
		Figure 2: Corporate event sequence forecasting example: AT&T
		25
		• M denotes the size of memory, and H de-
		notes the size of forecasting horizon, both are
		measured in terms of the number of time win-
		dows348
		Table 1:
		Results on test set compared with base-
		lines; the results in this table and following tables have
		proven signiﬁcant with p < 0441
		Table 1: Results of baselines and proposed methods on the test set (input time window is 40 minutes, and prediction
		delay is 5 minutes, we observe similar result in other time settings)00
		Table 1: Economic activity class distribution
		Class
		# compl
		%
		Crime
		8,086
		5
		In the second iteration,
		the non-occurring
		62
		Table 1: Tags used to annotate LazadaQA-Taglish-7k
	</Abstractive Summary>
	<Extractive Summary> =
		It replaces
		the word with a token specifying the count of let-
		ters (L), digits (D) and punctuation characters (O)
		it contains (see Table 1 for examples)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 indi-
		cates the statistics of the resulting datasets, which
		will be released as part of this work
	</Extractive Summary>
	<Extractive Summary> =
		We kept the original train/test split (see
		Table 1) and denote this data set as SentiFM-
		binary
	</Extractive Summary>
	<Extractive Summary> =
		The linearly in-
		terpolated series of the emotion scores are highly
		correlated with a broad set of economic data in
		their respective geographic area (see Table 1): we
		compiled data sets for the Euro area and the US
		covering a measure of the change in the real econ-
		omy (approximated by industrial production), in-
		ﬂation, unemployment and interest rates
	</Extractive Summary>
	<Extractive Summary> =
		3
		Formal Deﬁnition
		Let’s formally deﬁne the problem as,
		E(i)j = g|K|
		k=0(S(i)jk)
		(1)
		y(i)q = fM
		j=0(E(i)j)
		(2)
		y(i)q ∈ Y H
		(i)
		(3)
		where,
		• y denotes the event types in Table 1, Y H de-
		notes the event sequence during H, and se-
		quence Y H = [y1,
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 is divided into three parts
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, all the three proposed
		05min
		10min
		15min
		20min
		25min
		30min
		Pred Delay
		10min
		20min
		30min
		40min
		50min
		60min
		Input Time
		68
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the distribution for economic ac-
		tivities
	</Extractive Summary>
	<Extractive Summary> =
		As mentioned previously, and plainly observ-
		able in Table 1, this classiﬁcation problem is very
		imbalanced
	</Extractive Summary>
	<Extractive Summary> =
		Table 10
		presents the confusion matrix for this task and
		shows there is a considerable amount of cases
		where the prediction is incorrect
	</Extractive Summary>
	<Extractive Summary> =
		Predicted
		ASAE
		Other
		Actual
		ASAE
		12,408
		2,243
		Other
		2,662
		6,644
		Table 10: Competence prediction confusion matrix us-
		ing SVM
		It should be noted that our results show that it is
		possible, to a large extent, to derive ASAE’s com-
		petence directly from the complaint text (with a
		recall of 85%)
	</Extractive Summary>
	<Extractive Summary> =
		Tags used for the study are listed on Table 1 ac-
		companied by examples where the tags apply
	</Extractive Summary>
</Paper ID=ument754>


<Paper ID=ument754> <Table ID =2>
	<Abstractive Summary> =
		As explained in the previous sec-
		tions, named entities occurring within the docu-
		Relation Type
		Description
		NONE
		Entities are not in the same
		transaction
		SAME DIVISION
		Entities are in the same division
		OTHER DIVISION
		One entity is in Sender while
		other is in the Receiver division
		RECEIVER OF
		One entity is in Receiver while
		other is in the Process Details
		division
		SENDER OF
		One entity is in Sender while
		other is in the Process Details
		division
		Table 2: Possible relation types and their description
		ments might have been stated multiple times by
		the author in different places of the document
		438
		10627
		Facebook
		302
		6827
		Qualcomm
		120
		3332
		FedEx
		67
		1808
		Anadarko Petroleum
		91
		1478
		Xilinx
		53
		569
		MGM Resorts International
		32
		463
		Accenture
		24
		442
		Allergan
		35
		421
		Campbell Soup Company
		27
		307
		Table 2:
		Number of articles and sentences in the
		News-2019 evaluation data14%
		Table 2: Model performance (precise evaluation, F1%)
		Event Type
		MCMC
		Baseline
		GRU
		GRU attention
		business combination and
		restructuring
		22330
		Table 2: Different Text Input Forms in SSPM6
		Table 2: Impact of selection number in each group in
		BHAM-Category00
		Table 2: Infraction severity class distribution
		The complaints are evenly distributed across
		time, roughly 14,000 per year, with a slight in-
		crease towards the last 5 years First
		solve for pj
		pj =
		1
		Nn
		N
		�
		i=1
		nij, 1 =
		k
		�
		j=1
		pj
		(1)
		Table 2: LazadaQA-Taglish-7k
		Tag Distribution
		Tag
		Occurrence
		Speciﬁcation Inquiry
		4143
		Opening
		971
		Inquiry (others)
		684
		Thanking
		679
		Other
		396
		Product Complaint
		389
		Delivery Complaint
		386
		Delivery Inquiry
		362
		Availability Inquiry
		351
		Process Inquiry
		347
		Price Inquiry
		265
		Expression
		175
		Request
		168
		Service Complaint
		131
		Payment Method
		Inquiry
		107
		Warranty Inquiry
		101
		Return / Exchange /
		Refund
		82
		Price Inquiry
		65
		Contact Details Inquiry
		65
		Backchannel
		61
		Deﬁnition Inquiry
		60
		Follow-up
		57
		Price Complaint
		51
		Clariﬁcation
		51
		Product Recommendation
		Request
		41
		Purchase
		33
		Order Cancellation
		30
		Agreement / Accept /
		Yes-answer
		16
		where pj is the proportion of all assignments to the
		j-th tag
	</Abstractive Summary>
	<Extractive Summary> =
		different relation labels (detailed in Table 2) in or-
		der to distinguish the divisions to which the enti-
		ties belong within a transaction
	</Extractive Summary>
	<Extractive Summary> =
		Speciﬁcally, we selected the 10 S&P companies
		with the largest number of events from 2019 men-
		tioned in their Wikipedia page (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		To this end, the top 20 model
		predictions of SentiFM-binary and Extended-
		wiki for the companies in Table 2 were annotated
		by three co-authors of this work
	</Extractive Summary>
	<Extractive Summary> =
		In both Table 2 and Table 3, we can see the
		sequence-to-sequence models perform better than
		the baseline simulation model
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, the
		BHAM-Category performs best when the selec-
		tion number is 3 in all currency pairs
	</Extractive Summary>
	<Extractive Summary> =
		Instead of predicting infractions, how-
		ever, we focus on their severity, in a three-layered
		framework (as shown in Table 2)
	</Extractive Summary>
</Paper ID=ument754>


<Paper ID=ument754> <Table ID =3>
	<Abstractive Summary> =
		35
		Table 3: F1 scores of binary relation extraction step
		In order to measure the performance of our
		transaction extraction stage, we used a slot level
		entity matching evaluation94
		Table 3: Model performance on its test set (upper) and
		on the Extended-wiki test set (lower)
		Model
		Recall
		Avg34%
		Table 3: Model performance (fuzzy evaluation, F1%)
		Event Type
		MCMC
		Baseline
		GRU
		GRU attention
		Perplexity
		175305
		Table 3: Result on different sets of test data00
		Table 3: Competence class distribution (binary setting)
		analysis reveals that more densely populated areas
		generate more complaints, as expected Lastly, plug the values of P and Pe
		into the following equation to get the value of κ:
		κ = P − Pe
		1 − Pe
		(5)
		64
		Table 3: Kappa Scores from Highest to Lowest Relia-
		bility
		Label
		Kappa
		1
		Opening
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 gives relation extraction
		scores of our model both with FastText and ELMo
		embeddings
	</Extractive Summary>
	<Extractive Summary> =
		As shown
		in Table 3, all models reach high performance
		when tested on the same type of data used in
		training
	</Extractive Summary>
	<Extractive Summary> =
		Next, we evaluate these models on
		the Extended-wiki test set (see Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		7
		Discussion
		We show our precise model performance in Table
		2, fuzzy model performance in Table 3, and model
		perplexity in Table 4
	</Extractive Summary>
	<Extractive Summary> =
		In both Table 2 and Table 3, we can see the
		sequence-to-sequence models perform better than
		the baseline simulation model
	</Extractive Summary>
	<Extractive Summary> =
		Firstly,
		29% news in our dataset can not be recognized
		by TFED, and the Table 3 shows that the result
		of uncovered data is obvious lower than the cov-
		ered data
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 3, although the performance
		of MSSPM is lower than SSPM on the covered test
		set, its performance is higher than SSPM on the
		uncovered test set
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the data distribution based on the
		competence label
	</Extractive Summary>
	<Extractive Summary> =
		2, we decided to reduce the compe-
		tence prediction problem to a binary classiﬁcation
		setting (as per Table 3), where we identify whether
		ASAE is one of the institutions responsible to han-
		dle the complaint or not
	</Extractive Summary>
</Paper ID=ument754>


<Paper ID=ument754> <Table ID =4>
	<Abstractive Summary> =
		50
		Table 4:
		Slot level entity matching macro average
		scores grouped by divisions77
		19
		Table 4:
		Model performance for identifying 26
		Wikipedia events in the news data32
		Table 4: Model Perplexity
		event types315
		Table 4: Comparison of pipeline method and multi-task
		learning method (MSSPM)64
		Table 4: Economic activity prediction results
		Predicted
		I
		II
		III
		IV
		V
		VI
		VII
		VIII
		IX
		X
		Z
		Actual
		I
		43
		5
		4
		0
		11
		0
		0
		4
		4
		0
		17
		II
		0
		384
		127
		6
		61
		0
		1
		15
		30
		0
		78
		III
		0
		68
		7,036
		2
		89
		0
		5
		40
		209
		4
		205
		IV
		0
		10
		10
		26
		17
		0
		0
		6
		8
		0
		30
		V
		1
		22
		141
		5
		1,845
		0
		9
		56
		63
		0
		113
		VI
		0
		0
		0
		0
		0
		0
		0
		1
		0
		0
		0
		VII
		0
		0
		7
		0
		12
		0
		864
		58
		114
		0
		101
		VIII
		1
		7
		122
		4
		57
		0
		49
		1,502
		268
		6
		259
		IX
		1
		9
		379
		4
		35
		0
		53
		166
		4,895
		6
		380
		X
		0
		4
		22
		1
		11
		0
		13
		62
		83
		114
		56
		Z
		11
		65
		480
		9
		155
		0
		108
		356
		824
		25
		1,388
		Table 5: Economic activity prediction confusion matrix using SVM (top-1)
		The training process was allowed to run for a
		maximum of 20 epochs
		While words such
		as “order” and “item” appear in the complaint
		group, the presence of expression tags, speciﬁ-
		65
		1
		2
		3
		4
		5
		6
		7
		0
		2,000
		4,000
		4,793
		1,573
		623
		219
		47
		7
		3
		Number of Tags
		Posts
		Figure 2: Multilabel Count Distribution
		Table 4: Common Words used by Tagged Posts
		Tag
		Common Words
		Inquiry
		Availability inquiry
		ba, available, meron, color, stock
		Price inquiry
		ba, much, shipping, price, magkano, fee
		Speciﬁcation inquiry
		ba, pwede, compatible
		Contact details inquiry
		ba, store, warranty, contact
		Promo inquiry
		free, ba, shipping, sale
		Delivery inquiry
		ba, order, day, ilang, delivery
		Payment method
		inquiry
		cod, ba, installment, pwede, cash, delivery
		Deﬁnition inquiry
		go, ano, jbl, ba
		Process inquiry
		ba, order, paano, item
		Product recommendation
		request
		pwede, printer, hi, item, one, thank
		Request (action-directive)
		order, please, ba, item, thank, sana
		Clariﬁcation
		ba, warranty, order, lang, hindi
		Warranty inquiry
		warranty, ba, item, paano
		Inquiry (others)
		ba, original, order, lang, bakit
		Complaint
		Service complaint
		order, item, bakit, naman, wala
		Product complaint
		lang, item, ba, hindi, bakit
		Price complaint
		shipping, mahal, fee, price, bakit, lang
		Delivery complaint
		order, bakit, wala, day, item
		Expression
		Agreement / Accept /
		Yes-answer
		thank, ok, yes, opo
		Opening
		hi, ba, hello, thank, lang, order, pwede
		Thanking
		thank, order, ba, hello
		Expression
		order, ba, naman, hindi, thank, sana
		Transaction
		Purchase
		order, thank, hello, sana
		Order cancellation
		order, cancel, lang
		Return / Exchange /
		Refund
		item, paano, ba, order, return
		Other
		Backchannel
		ba, naman, sabi, please
		Follow-up
		hi, order, item, wala, follow
		Other
		order, item, ba, lang
		cally, Thanking may differentiate the negative
		implications of a complaint from a transaction
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 provides slot level en-
		tity matching scores grouped by divisions and as
		overall
	</Extractive Summary>
	<Extractive Summary> =
		As expected, the recall rates of the Wikipedia-
		based models over the news data (Table 4) are
		lower than those achieved over Wikipedia data
	</Extractive Summary>
	<Extractive Summary> =
		event (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Perplexity results in Table 4 also ver-
		iﬁes the promising model design direction as well
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 4, the performance of multi-
		task learning is clearly better than the pipeline
		method, which conﬁrms our assumption that these
		two tasks are highly related and the joint learning
		improves both of their results
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 summarizes the scores obtained for this
		task, where Acc@k and Macro-F1@k are accuracy
		and macro-F1 scores, respectively, when consider-
		ing that the classiﬁer has made a correct prediction
		54
		if any of the k most conﬁdently predicted classes
		(top-k) corresponds to the target label
	</Extractive Summary>
	<Extractive Summary> =
		Many observations can be made from Table 4 as
		to the possible relation between tags
	</Extractive Summary>
</Paper ID=ument754>


<Paper ID=ument754> <Table ID =5>
	<Abstractive Summary> =
		Based
		Model w/
		Fasttext
		Model w/
		ELMo
		0
		74
		372
		454
		490
		1
		52
		43
		47
		40
		2
		81
		70
		52
		39
		3
		62
		17
		21
		13
		4
		80
		4
		11
		7
		5
		63
		5
		9
		4
		6+
		91
		12
		4
		13
		# Matched
		503
		539
		601
		606
		# Missed
		227
		191
		129
		124
		# Wrong
		67
		64
		27
		13
		Table 5: Number of entity matching errors in pre-
		dicted&matched transactions74
		Table 5: Average precision over the top-20 predicted
		events in the news evaluation data016)
		Table 5: Ablation Study of SSPM64
		Table 4: Economic activity prediction results
		Predicted
		I
		II
		III
		IV
		V
		VI
		VII
		VIII
		IX
		X
		Z
		Actual
		I
		43
		5
		4
		0
		11
		0
		0
		4
		4
		0
		17
		II
		0
		384
		127
		6
		61
		0
		1
		15
		30
		0
		78
		III
		0
		68
		7,036
		2
		89
		0
		5
		40
		209
		4
		205
		IV
		0
		10
		10
		26
		17
		0
		0
		6
		8
		0
		30
		V
		1
		22
		141
		5
		1,845
		0
		9
		56
		63
		0
		113
		VI
		0
		0
		0
		0
		0
		0
		0
		1
		0
		0
		0
		VII
		0
		0
		7
		0
		12
		0
		864
		58
		114
		0
		101
		VIII
		1
		7
		122
		4
		57
		0
		49
		1,502
		268
		6
		259
		IX
		1
		9
		379
		4
		35
		0
		53
		166
		4,895
		6
		380
		X
		0
		4
		22
		1
		11
		0
		13
		62
		83
		114
		56
		Z
		11
		65
		480
		9
		155
		0
		108
		356
		824
		25
		1,388
		Table 5: Economic activity prediction confusion matrix using SVM (top-1)
		The training process was allowed to run for a
		maximum of 20 epochs The best-
		66
		Table 5: Results from the best DA classiﬁcation models
		as an application of this work
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 provides
		these statistics (in the upper block) as well as the
		numbers of missed and wrong transactions of each
		model (in the lower block)
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the precision of the two models,
		compared to a baseline of randomly-selected sen-
		13
		Model
		Precision
		Random sentences
		0
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 5 and Table 6, we found
		that the model performance drops in all the abla-
		tion experiments as expected
	</Extractive Summary>
	<Extractive Summary> =
		The
		SVM confusion matrix is shown in Table 5 and is
		complemented by the per-class precision and re-
		call metrics displayed in Table 6
	</Extractive Summary>
</Paper ID=ument754>


<Paper ID=ument754> <Table ID =6>
	<Abstractive Summary> =
		030)
		Table 6: Ablation Study of MSSPM41
		Table 6: Economic activity prediction precision and re-
		call per class (top-1)
		Classiﬁer
		Acc
		Macro-F1
		Random (stratiﬁed)
		0
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5 and Table 6, we found
		that the model performance drops in all the abla-
		tion experiments as expected
	</Extractive Summary>
</Paper ID=ument754>


<Paper ID=ument754> <Table ID =7>
	<Abstractive Summary> =
		62
		Table 7: Infraction severity prediction results
		tion
	</Abstractive Summary>
</Paper ID=ument754>


<Paper ID=ument754> <Table ID =8>
	<Abstractive Summary> =
		95
		8,371
		3,089
		Other
		153
		2,984
		8,000
		Table 8: Infraction severity prediction confusion matrix
		using SVM
		Although the accuracy and macro-F1 scores
		are not low, there is considerable room for im-
		provement in this particular task
	</Abstractive Summary>
	<Extractive Summary> =
		By analyzing the confusion matrix shown in
		Table 8, it is possible to observe that class “Admin-
		istrative infringement” and “Others” have a con-
		siderable number of cases where the prediction is
		swapped
	</Extractive Summary>
</Paper ID=ument754>


<Paper ID=ument754> <Table ID =9>
	<Abstractive Summary> =
		73
		Table 9: Competence prediction results
		7
	</Abstractive Summary>
</Paper ID=ument754>


<Paper ID=ument754> <Table ID =10>
	<Abstractive Summary> =
		Predicted
		ASAE
		Other
		Actual
		ASAE
		12,408
		2,243
		Other
		2,662
		6,644
		Table 10: Competence prediction confusion matrix us-
		ing SVM
		It should be noted that our results show that it is
		possible, to a large extent, to derive ASAE’s com-
		petence directly from the complaint text (with a
		recall of 85%)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10
		presents the confusion matrix for this task and
		shows there is a considerable amount of cases
		where the prediction is incorrect
	</Extractive Summary>
</Paper ID=ument754>


<Paper ID=ument756> <Table ID =1>
	<Abstractive Summary> =
		Word
		Token
		22/05/2019
		<L0D8O2>
		TR04
		<L2D2O0>
		10,000
		<L0D5O1>
		Table 1: Examples of word transformation algorithm
		We implemented our models using Tensorﬂow
		framework
	</Abstractive Summary>
	<Extractive Summary> =
		It replaces
		the word with a token specifying the count of let-
		ters (L), digits (D) and punctuation characters (O)
		it contains (see Table 1 for examples)
	</Extractive Summary>
</Paper ID=ument756>


<Paper ID=ument756> <Table ID =2>
	<Abstractive Summary> =
		As explained in the previous sec-
		tions, named entities occurring within the docu-
		Relation Type
		Description
		NONE
		Entities are not in the same
		transaction
		SAME DIVISION
		Entities are in the same division
		OTHER DIVISION
		One entity is in Sender while
		other is in the Receiver division
		RECEIVER OF
		One entity is in Receiver while
		other is in the Process Details
		division
		SENDER OF
		One entity is in Sender while
		other is in the Process Details
		division
		Table 2: Possible relation types and their description
		ments might have been stated multiple times by
		the author in different places of the document
	</Abstractive Summary>
	<Extractive Summary> =
		different relation labels (detailed in Table 2) in or-
		der to distinguish the divisions to which the enti-
		ties belong within a transaction
	</Extractive Summary>
</Paper ID=ument756>


<Paper ID=ument756> <Table ID =3>
	<Abstractive Summary> =
		35
		Table 3: F1 scores of binary relation extraction step
		In order to measure the performance of our
		transaction extraction stage, we used a slot level
		entity matching evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 gives relation extraction
		scores of our model both with FastText and ELMo
		embeddings
	</Extractive Summary>
</Paper ID=ument756>


<Paper ID=ument756> <Table ID =4>
	<Abstractive Summary> =
		50
		Table 4:
		Slot level entity matching macro average
		scores grouped by divisions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 provides slot level en-
		tity matching scores grouped by divisions and as
		overall
	</Extractive Summary>
</Paper ID=ument756>


<Paper ID=ument756> <Table ID =5>
	<Abstractive Summary> =
		Based
		Model w/
		Fasttext
		Model w/
		ELMo
		0
		74
		372
		454
		490
		1
		52
		43
		47
		40
		2
		81
		70
		52
		39
		3
		62
		17
		21
		13
		4
		80
		4
		11
		7
		5
		63
		5
		9
		4
		6+
		91
		12
		4
		13
		# Matched
		503
		539
		601
		606
		# Missed
		227
		191
		129
		124
		# Wrong
		67
		64
		27
		13
		Table 5: Number of entity matching errors in pre-
		dicted&matched transactions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 provides
		these statistics (in the upper block) as well as the
		numbers of missed and wrong transactions of each
		model (in the lower block)
	</Extractive Summary>
</Paper ID=ument756>


<Paper ID=ument757> <Table ID =1>
	<Abstractive Summary> =
		5)
		Table 1: Data size (number of sentences) for the three
		models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 indi-
		cates the statistics of the resulting datasets, which
		will be released as part of this work
	</Extractive Summary>
	<Extractive Summary> =
		We kept the original train/test split (see
		Table 1) and denote this data set as SentiFM-
		binary
	</Extractive Summary>
</Paper ID=ument757>


<Paper ID=ument757> <Table ID =2>
	<Abstractive Summary> =
		438
		10627
		Facebook
		302
		6827
		Qualcomm
		120
		3332
		FedEx
		67
		1808
		Anadarko Petroleum
		91
		1478
		Xilinx
		53
		569
		MGM Resorts International
		32
		463
		Accenture
		24
		442
		Allergan
		35
		421
		Campbell Soup Company
		27
		307
		Table 2:
		Number of articles and sentences in the
		News-2019 evaluation data
	</Abstractive Summary>
	<Extractive Summary> =
		Speciﬁcally, we selected the 10 S&P companies
		with the largest number of events from 2019 men-
		tioned in their Wikipedia page (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		To this end, the top 20 model
		predictions of SentiFM-binary and Extended-
		wiki for the companies in Table 2 were annotated
		by three co-authors of this work
	</Extractive Summary>
</Paper ID=ument757>


<Paper ID=ument757> <Table ID =3>
	<Abstractive Summary> =
		94
		Table 3: Model performance on its test set (upper) and
		on the Extended-wiki test set (lower)
		Model
		Recall
		Avg
	</Abstractive Summary>
	<Extractive Summary> =
		As shown
		in Table 3, all models reach high performance
		when tested on the same type of data used in
		training
	</Extractive Summary>
	<Extractive Summary> =
		Next, we evaluate these models on
		the Extended-wiki test set (see Table 3)
	</Extractive Summary>
</Paper ID=ument757>


<Paper ID=ument757> <Table ID =4>
	<Abstractive Summary> =
		77
		19
		Table 4:
		Model performance for identifying 26
		Wikipedia events in the news data
	</Abstractive Summary>
	<Extractive Summary> =
		As expected, the recall rates of the Wikipedia-
		based models over the news data (Table 4) are
		lower than those achieved over Wikipedia data
	</Extractive Summary>
	<Extractive Summary> =
		event (Table 4)
	</Extractive Summary>
</Paper ID=ument757>


<Paper ID=ument757> <Table ID =5>
	<Abstractive Summary> =
		74
		Table 5: Average precision over the top-20 predicted
		events in the news evaluation data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the precision of the two models,
		compared to a baseline of randomly-selected sen-
		14
		Model
		Precision
		Random sentences
		0
	</Extractive Summary>
</Paper ID=ument757>


<Paper ID=ument758> <Table ID =1>
	<Abstractive Summary> =
		03
		Table 1: VAD scores and their correlation with a broad
		set of economic indicators (excerpt)
	</Abstractive Summary>
	<Extractive Summary> =
		The linearly in-
		terpolated series of the emotion scores are highly
		correlated with a broad set of economic data in
		their respective geographic area (see Table 1): we
		compiled data sets for the Euro area and the US
		covering a measure of the change in the real econ-
		omy (approximated by industrial production), in-
		ﬂation, unemployment and interest rates
	</Extractive Summary>
</Paper ID=ument758>


<Paper ID=ument759> <Table ID =1>
	<Abstractive Summary> =
		01
		bankruptcy
		Table 1: Event Types (target variables)
		Figure 1: Time structure of the model
		Figure 2: Corporate event sequence forecasting example: AT&T
		26
		• M denotes the size of memory, and H de-
		notes the size of forecasting horizon, both are
		measured in terms of the number of time win-
		dows
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Formal Deﬁnition
		Let’s formally deﬁne the problem as,
		E(i)j = g|K|
		k=0(S(i)jk)
		(1)
		y(i)q = fM
		j=0(E(i)j)
		(2)
		y(i)q ∈ Y H
		(i)
		(3)
		where,
		• y denotes the event types in Table 1, Y H de-
		notes the event sequence during H, and se-
		quence Y H = [y1,
	</Extractive Summary>
</Paper ID=ument759>


<Paper ID=ument759> <Table ID =2>
	<Abstractive Summary> =
		14%
		Table 2: Model performance (precise evaluation, F1%)
		Event Type
		MCMC
		Baseline
		GRU
		GRU attention
		business combination and
		restructuring
		22
	</Abstractive Summary>
	<Extractive Summary> =
		In both Table 2 and Table 3, we can see the
		sequence-to-sequence models perform better than
		the baseline simulation model
	</Extractive Summary>
</Paper ID=ument759>


<Paper ID=ument759> <Table ID =3>
	<Abstractive Summary> =
		34%
		Table 3: Model performance (fuzzy evaluation, F1%)
		Event Type
		MCMC
		Baseline
		GRU
		GRU attention
		Perplexity
		175
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Discussion
		We show our precise model performance in Table
		2, fuzzy model performance in Table 3, and model
		perplexity in Table 4
	</Extractive Summary>
	<Extractive Summary> =
		In both Table 2 and Table 3, we can see the
		sequence-to-sequence models perform better than
		the baseline simulation model
	</Extractive Summary>
</Paper ID=ument759>


<Paper ID=ument759> <Table ID =4>
	<Abstractive Summary> =
		32
		Table 4: Model Perplexity
		event types
	</Abstractive Summary>
	<Extractive Summary> =
		Perplexity results in Table 4 also ver-
		iﬁes the promising model design direction as well
	</Extractive Summary>
</Paper ID=ument759>


<Paper ID=ument76> <Table ID =1>
	<Abstractive Summary> =
		70
		Table 1: The comparison between the proposed methods LLMap and RGP, and the MUSE supervised method
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 1 shows the average of precision over the 10
		random splits @k = 1, 5 and 10
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		shows how much increasing the non-linearity in
		the mapping by increasing the number of neu-
		rons from 1 to 4 contributes to the overall pre-
		cision
	</Extractive Summary>
</Paper ID=ument76>


<Paper ID=ument76> <Table ID =2>
	<Abstractive Summary> =
		07
		Table 2: The comparison between the proposed method
		LLMap and MUSE on pre-split train and test dictionar-
		ies for any and all senses recovery by the algorithms
		for precision@5
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the precision@5 for the pre-split
		dictionaries
	</Extractive Summary>
</Paper ID=ument76>


<Paper ID=ument760> <Table ID =1>
	<Abstractive Summary> =
		348
		Table 1:
		Results on test set compared with base-
		lines; the results in this table and following tables have
		proven signiﬁcant with p < 0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 is divided into three parts
	</Extractive Summary>
</Paper ID=ument760>


<Paper ID=ument760> <Table ID =2>
	<Abstractive Summary> =
		330
		Table 2: Different Text Input Forms in SSPM
	</Abstractive Summary>
</Paper ID=ument760>


<Paper ID=ument760> <Table ID =3>
	<Abstractive Summary> =
		305
		Table 3: Result on different sets of test data
	</Abstractive Summary>
	<Extractive Summary> =
		Firstly,
		29% news in our dataset can not be recognized
		by TFED, and the Table 3 shows that the result
		of uncovered data is obvious lower than the cov-
		ered data
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 3, although the performance
		of MSSPM is lower than SSPM on the covered test
		set, its performance is higher than SSPM on the
		uncovered test set
	</Extractive Summary>
</Paper ID=ument760>


<Paper ID=ument760> <Table ID =4>
	<Abstractive Summary> =
		315
		Table 4: Comparison of pipeline method and multi-task
		learning method (MSSPM)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, the performance of multi-
		task learning is clearly better than the pipeline
		method, which conﬁrms our assumption that these
		two tasks are highly related and the joint learning
		improves both of their results
	</Extractive Summary>
</Paper ID=ument760>


<Paper ID=ument760> <Table ID =5>
	<Abstractive Summary> =
		016)
		Table 5: Ablation Study of SSPM
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5 and Table 6, we found
		that the model performance drops in all the abla-
		tion experiments as expected
	</Extractive Summary>
</Paper ID=ument760>


<Paper ID=ument760> <Table ID =6>
	<Abstractive Summary> =
		030)
		Table 6: Ablation Study of MSSPM
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5 and Table 6, we found
		that the model performance drops in all the abla-
		tion experiments as expected
	</Extractive Summary>
</Paper ID=ument760>


<Paper ID=ument761> <Table ID =1>
	<Abstractive Summary> =
		441
		Table 1: Results of baselines and proposed methods on the test set (input time window is 40 minutes, and prediction
		delay is 5 minutes, we observe similar result in other time settings)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, all the three proposed
		05min
		10min
		15min
		20min
		25min
		30min
		Pred Delay
		10min
		20min
		30min
		40min
		50min
		60min
		Input Time
		68
	</Extractive Summary>
</Paper ID=ument761>


<Paper ID=ument761> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: Impact of selection number in each group in
		BHAM-Category
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, the
		BHAM-Category performs best when the selec-
		tion number is 3 in all currency pairs
	</Extractive Summary>
</Paper ID=ument761>


<Paper ID=ument762> <Table ID =1>
	<Abstractive Summary> =
		00
		Table 1: Economic activity class distribution
		Class
		# compl
		%
		Crime
		8,086
		5
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the distribution for economic ac-
		tivities
	</Extractive Summary>
	<Extractive Summary> =
		As mentioned previously, and plainly observ-
		able in Table 1, this classiﬁcation problem is very
		imbalanced
	</Extractive Summary>
	<Extractive Summary> =
		Table 10
		presents the confusion matrix for this task and
		shows there is a considerable amount of cases
		where the prediction is incorrect
	</Extractive Summary>
	<Extractive Summary> =
		Predicted
		ASAE
		Other
		Actual
		ASAE
		12,408
		2,243
		Other
		2,662
		6,644
		Table 10: Competence prediction confusion matrix us-
		ing SVM
		It should be noted that our results show that it is
		possible, to a large extent, to derive ASAE’s com-
		petence directly from the complaint text (with a
		recall of 85%)
	</Extractive Summary>
</Paper ID=ument762>


<Paper ID=ument762> <Table ID =2>
	<Abstractive Summary> =
		00
		Table 2: Infraction severity class distribution
		The complaints are evenly distributed across
		time, roughly 14,000 per year, with a slight in-
		crease towards the last 5 years
	</Abstractive Summary>
	<Extractive Summary> =
		Instead of predicting infractions, how-
		ever, we focus on their severity, in a three-layered
		framework (as shown in Table 2)
	</Extractive Summary>
</Paper ID=ument762>


<Paper ID=ument762> <Table ID =3>
	<Abstractive Summary> =
		00
		Table 3: Competence class distribution (binary setting)
		analysis reveals that more densely populated areas
		generate more complaints, as expected
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the data distribution based on the
		competence label
	</Extractive Summary>
	<Extractive Summary> =
		2, we decided to reduce the compe-
		tence prediction problem to a binary classiﬁcation
		setting (as per Table 3), where we identify whether
		ASAE is one of the institutions responsible to han-
		dle the complaint or not
	</Extractive Summary>
</Paper ID=ument762>


<Paper ID=ument762> <Table ID =4>
	<Abstractive Summary> =
		64
		Table 4: Economic activity prediction results
		Predicted
		I
		II
		III
		IV
		V
		VI
		VII
		VIII
		IX
		X
		Z
		Actual
		I
		43
		5
		4
		0
		11
		0
		0
		4
		4
		0
		17
		II
		0
		384
		127
		6
		61
		0
		1
		15
		30
		0
		78
		III
		0
		68
		7,036
		2
		89
		0
		5
		40
		209
		4
		205
		IV
		0
		10
		10
		26
		17
		0
		0
		6
		8
		0
		30
		V
		1
		22
		141
		5
		1,845
		0
		9
		56
		63
		0
		113
		VI
		0
		0
		0
		0
		0
		0
		0
		1
		0
		0
		0
		VII
		0
		0
		7
		0
		12
		0
		864
		58
		114
		0
		101
		VIII
		1
		7
		122
		4
		57
		0
		49
		1,502
		268
		6
		259
		IX
		1
		9
		379
		4
		35
		0
		53
		166
		4,895
		6
		380
		X
		0
		4
		22
		1
		11
		0
		13
		62
		83
		114
		56
		Z
		11
		65
		480
		9
		155
		0
		108
		356
		824
		25
		1,388
		Table 5: Economic activity prediction confusion matrix using SVM (top-1)
		The training process was allowed to run for a
		maximum of 20 epochs
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 summarizes the scores obtained for this
		task, where Acc@k and Macro-F1@k are accuracy
		and macro-F1 scores, respectively, when consider-
		ing that the classiﬁer has made a correct prediction
		55
		if any of the k most conﬁdently predicted classes
		(top-k) corresponds to the target label
	</Extractive Summary>
</Paper ID=ument762>


<Paper ID=ument762> <Table ID =5>
	<Abstractive Summary> =
		64
		Table 4: Economic activity prediction results
		Predicted
		I
		II
		III
		IV
		V
		VI
		VII
		VIII
		IX
		X
		Z
		Actual
		I
		43
		5
		4
		0
		11
		0
		0
		4
		4
		0
		17
		II
		0
		384
		127
		6
		61
		0
		1
		15
		30
		0
		78
		III
		0
		68
		7,036
		2
		89
		0
		5
		40
		209
		4
		205
		IV
		0
		10
		10
		26
		17
		0
		0
		6
		8
		0
		30
		V
		1
		22
		141
		5
		1,845
		0
		9
		56
		63
		0
		113
		VI
		0
		0
		0
		0
		0
		0
		0
		1
		0
		0
		0
		VII
		0
		0
		7
		0
		12
		0
		864
		58
		114
		0
		101
		VIII
		1
		7
		122
		4
		57
		0
		49
		1,502
		268
		6
		259
		IX
		1
		9
		379
		4
		35
		0
		53
		166
		4,895
		6
		380
		X
		0
		4
		22
		1
		11
		0
		13
		62
		83
		114
		56
		Z
		11
		65
		480
		9
		155
		0
		108
		356
		824
		25
		1,388
		Table 5: Economic activity prediction confusion matrix using SVM (top-1)
		The training process was allowed to run for a
		maximum of 20 epochs
	</Abstractive Summary>
	<Extractive Summary> =
		The
		SVM confusion matrix is shown in Table 5 and is
		complemented by the per-class precision and re-
		call metrics displayed in Table 6
	</Extractive Summary>
</Paper ID=ument762>


<Paper ID=ument762> <Table ID =6>
	<Abstractive Summary> =
		41
		Table 6: Economic activity prediction precision and re-
		call per class (top-1)
		Classiﬁer
		Acc
		Macro-F1
		Random (stratiﬁed)
		0
	</Abstractive Summary>
</Paper ID=ument762>


<Paper ID=ument762> <Table ID =7>
	<Abstractive Summary> =
		62
		Table 7: Infraction severity prediction results
		tion
	</Abstractive Summary>
</Paper ID=ument762>


<Paper ID=ument762> <Table ID =8>
	<Abstractive Summary> =
		95
		8,371
		3,089
		Other
		153
		2,984
		8,000
		Table 8: Infraction severity prediction confusion matrix
		using SVM
		Although the accuracy and macro-F1 scores
		are not low, there is considerable room for im-
		provement in this particular task
	</Abstractive Summary>
	<Extractive Summary> =
		By analyzing the confusion matrix shown in
		Table 8, it is possible to observe that class “Admin-
		istrative infringement” and “Others” have a con-
		siderable number of cases where the prediction is
		swapped
	</Extractive Summary>
</Paper ID=ument762>


<Paper ID=ument762> <Table ID =9>
	<Abstractive Summary> =
		73
		Table 9: Competence prediction results
		7
	</Abstractive Summary>
</Paper ID=ument762>


<Paper ID=ument762> <Table ID =10>
	<Abstractive Summary> =
		Predicted
		ASAE
		Other
		Actual
		ASAE
		12,408
		2,243
		Other
		2,662
		6,644
		Table 10: Competence prediction confusion matrix us-
		ing SVM
		It should be noted that our results show that it is
		possible, to a large extent, to derive ASAE’s com-
		petence directly from the complaint text (with a
		recall of 85%)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10
		presents the confusion matrix for this task and
		shows there is a considerable amount of cases
		where the prediction is incorrect
	</Extractive Summary>
</Paper ID=ument762>


<Paper ID=ument763> <Table ID =1>
	<Abstractive Summary> =
		In the second iteration,
		the non-occurring
		63
		Table 1: Tags used to annotate LazadaQA-Taglish-7k
	</Abstractive Summary>
	<Extractive Summary> =
		Tags used for the study are listed on Table 1 ac-
		companied by examples where the tags apply
	</Extractive Summary>
</Paper ID=ument763>


<Paper ID=ument763> <Table ID =2>
	<Abstractive Summary> =
		First
		solve for pj
		pj =
		1
		Nn
		N
		�
		i=1
		nij, 1 =
		k
		�
		j=1
		pj
		(1)
		Table 2: LazadaQA-Taglish-7k
		Tag Distribution
		Tag
		Occurrence
		Speciﬁcation Inquiry
		4143
		Opening
		971
		Inquiry (others)
		684
		Thanking
		679
		Other
		396
		Product Complaint
		389
		Delivery Complaint
		386
		Delivery Inquiry
		362
		Availability Inquiry
		351
		Process Inquiry
		347
		Price Inquiry
		265
		Expression
		175
		Request
		168
		Service Complaint
		131
		Payment Method
		Inquiry
		107
		Warranty Inquiry
		101
		Return / Exchange /
		Refund
		82
		Price Inquiry
		65
		Contact Details Inquiry
		65
		Backchannel
		61
		Deﬁnition Inquiry
		60
		Follow-up
		57
		Price Complaint
		51
		Clariﬁcation
		51
		Product Recommendation
		Request
		41
		Purchase
		33
		Order Cancellation
		30
		Agreement / Accept /
		Yes-answer
		16
		where pj is the proportion of all assignments to the
		j-th tag
	</Abstractive Summary>
</Paper ID=ument763>


<Paper ID=ument763> <Table ID =3>
	<Abstractive Summary> =
		Lastly, plug the values of P and Pe
		into the following equation to get the value of κ:
		κ = P − Pe
		1 − Pe
		(5)
		65
		Table 3: Kappa Scores from Highest to Lowest Relia-
		bility
		Label
		Kappa
		1
		Opening
		0
	</Abstractive Summary>
</Paper ID=ument763>


<Paper ID=ument763> <Table ID =4>
	<Abstractive Summary> =
		While words such
		as “order” and “item” appear in the complaint
		group, the presence of expression tags, speciﬁ-
		66
		1
		2
		3
		4
		5
		6
		7
		0
		2,000
		4,000
		4,793
		1,573
		623
		219
		47
		7
		3
		Number of Tags
		Posts
		Figure 2: Multilabel Count Distribution
		Table 4: Common Words used by Tagged Posts
		Tag
		Common Words
		Inquiry
		Availability inquiry
		ba, available, meron, color, stock
		Price inquiry
		ba, much, shipping, price, magkano, fee
		Speciﬁcation inquiry
		ba, pwede, compatible
		Contact details inquiry
		ba, store, warranty, contact
		Promo inquiry
		free, ba, shipping, sale
		Delivery inquiry
		ba, order, day, ilang, delivery
		Payment method
		inquiry
		cod, ba, installment, pwede, cash, delivery
		Deﬁnition inquiry
		go, ano, jbl, ba
		Process inquiry
		ba, order, paano, item
		Product recommendation
		request
		pwede, printer, hi, item, one, thank
		Request (action-directive)
		order, please, ba, item, thank, sana
		Clariﬁcation
		ba, warranty, order, lang, hindi
		Warranty inquiry
		warranty, ba, item, paano
		Inquiry (others)
		ba, original, order, lang, bakit
		Complaint
		Service complaint
		order, item, bakit, naman, wala
		Product complaint
		lang, item, ba, hindi, bakit
		Price complaint
		shipping, mahal, fee, price, bakit, lang
		Delivery complaint
		order, bakit, wala, day, item
		Expression
		Agreement / Accept /
		Yes-answer
		thank, ok, yes, opo
		Opening
		hi, ba, hello, thank, lang, order, pwede
		Thanking
		thank, order, ba, hello
		Expression
		order, ba, naman, hindi, thank, sana
		Transaction
		Purchase
		order, thank, hello, sana
		Order cancellation
		order, cancel, lang
		Return / Exchange /
		Refund
		item, paano, ba, order, return
		Other
		Backchannel
		ba, naman, sabi, please
		Follow-up
		hi, order, item, wala, follow
		Other
		order, item, ba, lang
		cally, Thanking may differentiate the negative
		implications of a complaint from a transaction
	</Abstractive Summary>
	<Extractive Summary> =
		Many observations can be made from Table 4 as
		to the possible relation between tags
	</Extractive Summary>
</Paper ID=ument763>


<Paper ID=ument763> <Table ID =5>
	<Abstractive Summary> =
		The best-
		67
		Table 5: Results from the best DA classiﬁcation models
		as an application of this work
	</Abstractive Summary>
</Paper ID=ument763>


<Paper ID=ument764> <Table ID =1>
	<Abstractive Summary> =
		cz/hindi-visual-genome/
		wat-2019-multimodal-task
		1
		Lang
		Train
		Dev
		DevTest
		Test
		JE
		3,008,500
		1,790
		1,784
		1,812
		JC
		672,315
		2,090
		2,148
		2,107
		Table 1: Statistics for ASPEC
		• Domain and language pairs
		WAT is the world’s first workshop that
		targets scientific paper domain, and Chi-
		nese↔Japanese and
		Korean↔Japanese
		language pairs We tried the combination
		ASPEC-JE
		NTCIR-JE
		Pairs (train)
		1,000,000
		1,387,713
		Pairs (dev)
		1790
		2000
		Pairs (devtest)
		1784
		-
		Pairs (test)
		1812
		2300
		Vocab (ja)
		3084
		2966
		Vocab (en)
		291
		98
		Table 1: Numbers of sentence pairs and vocabulary of
		ASPEC-JE and NTCIR-JE
		Formality
		Japanese sentence and transcription
		Informal
		駅の 近くに たくさんの お店が ある。
		eki-no chikaku-ni takusan-no omise-ga aru
		Polite
		駅の 近くに たくさんの お店が あります。
		eki-no chikaku-ni takusan-no omise-ga arimasu
		Formal
		駅の 近くに たくさんの お店が ございます。
		eki-no chikaku-ni takusan-no omise-ga gozaimasu
		Table 1: Three sentences meaning “There are many
		shops near the train station”, in different levels of for-
		mality
		For example, when speaking with family, close
		friends, or others of equal social status, the infor-
		mal ある (aru “there are”) is used
		Table 1: An example of the Japanese-English business conversation parallel corpus In the
		63
		3
		200
		201
		202
		203
		204
		205
		206
		207
		208
		209
		210
		211
		212
		213
		214
		215
		216
		217
		218
		219
		220
		221
		222
		223
		224
		225
		226
		227
		228
		229
		230
		231
		232
		233
		234
		235
		236
		237
		238
		239
		240
		241
		242
		243
		244
		245
		246
		247
		248
		249
		250
		251
		252
		253
		254
		255
		256
		257
		258
		259
		260
		261
		262
		263
		264
		265
		266
		267
		268
		269
		270
		271
		272
		273
		274
		275
		276
		277
		278
		279
		280
		281
		282
		283
		284
		285
		286
		287
		288
		289
		290
		291
		292
		293
		294
		295
		296
		297
		298
		299
		Nature of corpus
		Name of Corpus
		Number of instances/items
		Training
		Englsih-Hindi
		28,929
		(Text data)
		Image data
		28,929
		Test (Evaluation Set)
		English to Hindi
		1595
		(Text data)
		Image data
		1595
		Test (Challenge Set)
		English to Hindi
		1400
		(Text data)
		Image data
		1400
		Validation
		English-Hindi
		998
		(Text data)
		Image data
		998
		Table 1: Corpus Statistics (Nakazawa et al3M)
		Development
		1,000 (my: 36,688, en: 25,538)
		1,000 (km: 33,604, en: 25,538)
		Test
		1,018 (my: 37,519, en: 26,236)
		1,018 (km: 34,238, en: 26,236)
		Table 1: Statistics of our preprocessed parallel data8k
		Table 1: Statistics on our in-domain parallel data for
		the Russian–Japanese task
		Using this algorithm, it is possible
		to represent a sentence as a subword sequence
		through as ﬁxed-size vocabulary and to solve the
		Task
		Dataset
		Train
		Dev
		Test
		ASPEC
		En-Ja
		3,008,500
		1,790
		1,812
		Zh-Ja
		672,315
		2,090
		2,107
		En-Ja
		1,000,000
		2,000
		5,668
		JPC2
		Ko-Ja
		1,000,000
		2,000
		5,230
		Zh-Ja
		1,000,000
		2,000
		5,204
		Table 1: Statistics of parallel sentences (sentence)
		problem of unknown words and rare words ef-
		fectively7K
		Table 1: Statistics of our preprocessed parallel data05 
		Table 1:  BLEU scores by changing initial distortion 
		weight in Myanmar-English bidirectional 
		translations
		Set
		# Sentences
		Train
		3,008,500
		(bitext)
		(1,500,000)
		(synthetic)
		(1,508,500)
		Dev
		1,790
		Devtest
		1,784
		Test
		1,812
		Table 1: Numbers of sentences in ASPEC corpus
		Section 261 M
		Table 1: Dataset4
		Table 1: Effect of noisy channel reranking when eval-
		uating on the validation set
		3,998
		3,518
		3,752
		Devtest
		4,014
		3,753
		3,483
		Test
		3,277
		2,898
		—–
		Table 1:
		Data statistics of Timely Disclosure Docu-
		ments Corpus4M
		mono
		Table 1: Training dataset used for ilmulti model
		Table 1: Statistics of our processed parallel data
		Items
		2,845
		Texts
		1,153
		DevTest
		Items
		2,900
		Texts
		1,114
		Test
		Items
		2,129
		Texts
		1,148
		Table 1: Corpus sizes37 
		Table 1: Results of subtasks 
		Japanese-English subtask: 
		The baseline model is a vanilla Transformer 
		model with the first 1M data001
		Batch size in tokens
		4000
		Update frequency
		1
		Table 1: JPO model settings
		2)
		Table 1:
		Statistics on our experimental data sets (af-
		ter tokenizing and lowercasing)3k
		Table 1: Statistics on our in-domain parallel data647579
		Table 1: RIBES score of Tamil–English and English–
		Tamil System submitted by our team at WAT 20191 M
		D-Test
		998
		4922
		4695
		E-Test
		1595
		7852
		7535
		C-Test
		1400
		8185
		8665
		Table 1: Statistics of our data: the number of sen-
		tences and tokenscz/
		hindi-visual-genome/wat-2019-multimodal-task
		182
		Figure 1: System Architecture
		Dataset distribution
		Items
		Training set
		28932
		Development set
		998
		Evaluation set
		1595
		Challenge set
		1400
		Table 1: Hindi Visual Genome dataset details6
		Table 1:
		Statistics of training bitexts007 
		Table 1:  Statistics of Datasets
		Table 1: Example of AddLabel and InsertLabel13
		Table 1: Results of Japanese-Vietnamese NMT systems
		We then modify the baseline architecture with the
		alternative proposed in Section 3py
		Table 1: Statistics about the size, content and diacritics
		usage of (Fadel et al
	</Abstractive Summary>
	<Extractive Summary> =
		8k
		Table 10:
		In-Domain data for the Russian–
		Japanese task
	</Extractive Summary>
	<Extractive Summary> =
		Refer to
		Table 10 for the statistics of the in-domain
		parallel corpora
	</Extractive Summary>
	<Extractive Summary> =
		shtml
		ASPEC
		JPC
		System ID
		System
		Type
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-ko
		ko-ja
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		ATLAS V14 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PAT-Transer 2009 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		RBMT X
		J-Beijing 7 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		Hohrai 2011 (Commercial system)
		RBMT
		✓
		✓
		✓
		RBMT X
		J Soul 9 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		Korai 2011 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		AIAYN
		Google’s implementation of “Attention Is All You Need”
		NMT
		✓
		✓
		Table 11: Baseline Systems I
		7
		JIJI
		IITB
		ALT
		System ID
		System
		Type
		ja-en
		en-ja
		hi-en
		en-hi
		hi-ja
		ja-hi
		my-en
		en-my
		km-en
		en-km
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		Table 12: Baseline Systems II
		NewsCommentary
		TDDC
		EnTam
		Multimodal
		System ID
		System
		Type
		ru-ja
		ja-ru
		ja-en
		ta-en
		en-ta
		en-hi
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		NMT T2T
		Tensor2Tensor’s Transformer
		NMT
		✓
		✓
		✓
		✓
		NMT OT
		OpenNMT-py’s Transformer
		NMT
		✓
		Online X
		Azure Custom Translator
		Other
		✓
		Table 13: Baseline Systems III
		8
		model) for Chinese segmentation
	</Extractive Summary>
	<Extractive Summary> =
		(–20%)
		Table 14: The JPO adequacy criterion
		confidence interval
	</Extractive Summary>
	<Extractive Summary> =
		2
		Evaluation Criterion
		Table 14 shows the JPO adequacy criterion
		from 5 to 1
	</Extractive Summary>
	<Extractive Summary> =
		6
		Participants
		Table 15 shows the participants in WAT2019
	</Extractive Summary>
	<Extractive Summary> =
		Table 16 shows tasks for which
		each team submitted results by the deadline
	</Extractive Summary>
	<Extractive Summary> =
		For multi-modal task, Table 19 shows the
		manual evaluation scores for all valid system
		submissions
	</Extractive Summary>
	<Extractive Summary> =
		,Ltd
		China
		sarah
		Rakuten Institute of Technology
		Japan
		683
		National Institute of Technology Silchar
		India
		KNU_Hyundai
		Kangwon National University
		Korea
		NITSNLP
		National Institute of Technology Silchar
		India
		ryan
		Kakao Brain
		Korea
		PUP-IND
		Punjabi University Patiala
		India
		FBAI
		Facebook AI Research
		USA
		AISTAI
		National Institute of Advanced Industrial Science and Technology
		Japan
		SYSTRAN
		SYSTRAN
		France
		NHK-NES
		NHK & NHK Engineering System
		Japan
		geoduck
		Microsoft Research
		USA
		LTRC-MT
		IIIT Hyderabad
		India
		ykkd
		The University of Tokyo
		Japan
		IDIAP
		Idiap Research Institute
		Switzerland
		NLPRL
		Indian Institute of Technology (BHU) Varanasi
		India
		Table 15: List of participants in WAT2019
		Team ID
		ASPEC
		JPC
		TDDC
		JIJI
		NCPD
		EJ
		JE
		CJ
		JC
		EJ
		JE
		CJ
		JC
		Ko-J
		J-Ko
		JE
		EJ
		JE
		RJ
		JR
		TMU
		✓
		✓
		NTT
		✓
		✓
		✓
		NICT-2
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		srcb
		✓
		✓
		✓
		✓
		sarah
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		KNU_Hyundai
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		ryan
		✓
		✓
		✓
		✓
		✓
		✓
		AISTAI
		✓
		SYSTRAN
		✓
		✓
		NHK-NES
		✓
		✓
		geoduck
		✓
		ykkd
		✓
		Team ID
		Mixed-domain tasks
		Mutimodal task
		ALT
		IITB
		UFAL (EnTam)
		EV/CH
		EM
		ME
		E-Kh
		Kh-E
		EH
		HE
		ET
		TE
		EH
		NICT
		✓
		✓
		NICT-4
		✓
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		UCSYNLP
		✓
		✓
		UCSMNLP
		✓
		✓
		cvit
		✓
		✓
		✓
		✓
		sarah
		✓
		683
		✓
		NITSNLP
		✓
		PUP-IND
		✓
		FBAI
		✓
		✓
		LTRC-MT
		✓
		IDIAP
		✓
		NLPRL
		✓
		✓
		Table 16: Submissions for each task by each team
	</Extractive Summary>
	<Extractive Summary> =
		42
		Table 17: JPO adequacy evaluation results in detail (1)
	</Extractive Summary>
	<Extractive Summary> =
		22
		Table 18: JPO adequacy evaluation results in detail (2)
	</Extractive Summary>
	<Extractive Summary> =
		94
		Table 19: Manual evaluation result for WAT Multi-Modal Tasks
	</Extractive Summary>
	<Extractive Summary> =
		Table 19 thus lists also the
		“Reference”
	</Extractive Summary>
	<Extractive Summary> =
		The tables
		also include results by the organizers’ base-
		lines, which are listed in Table 13
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the sizes of the training set of
		both datasets
	</Extractive Summary>
	<Extractive Summary> =
		Table 10 shows another example of the MT
		output from the internal formality-aware NMT
		model and the corresponding internal NMT base-
		line trained without formality annotations
	</Extractive Summary>
	<Extractive Summary> =
		Reference
		円卓の騎士たちはキラーラビットと戦う。
		entaku-no kishitachi-wa kir¯a rabitto-to tatakau
		NMT Model
		MT Output and Transcription
		Baseline
		アーサー王の騎士はキラーウサギと戦います。
		¯as¯a ¯o-no kishitachi-wa kir¯a usagi-to tatakaimasu
		Formality-Aware - Informal
		キングアーサーの騎士たちはキラーウサギと戦う。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakau
		Formality-Aware - Polite
		キングアーサーの騎士たちはキラーウサギと戦います。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakaimasu
		Formality-Aware - Formal
		キングアーサーの騎士たちはキラーウサギと戦いをいたします。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakai-wo itashimasu
		Table 10: Example output from internal NMT baseline model and formality-aware NMT model, when each for-
		mality level is attached to the source segment
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows
		an example of the corpus
	</Extractive Summary>
	<Extractive Summary> =
		In the text-only translation track, we
		have considered only source-target corresponding
		sentences as shown in Table 1 to build the dic-
		tionary, vocabulary size of dimension 8300, 7984
		using the OpenNMT toolkit
	</Extractive Summary>
	<Extractive Summary> =
		Re-
		fer to Table 1 for an overview of the in-domain
		parallel corpora and the data splits
	</Extractive Summary>
	<Extractive Summary> =
		3
		Korean-Japanese
		Table 10 shows the translation performance of
		JPC2 dataset for Korean and Japanese as JPC2 Ko-
		Ja
	</Extractive Summary>
	<Extractive Summary> =
		04
		1 of 3
		Table 10:
		BLEU score for Korean→Japanese sub-
		tasks on leaderboard
		The Ko-Ja translation task has only a paten sub-
		task as JPC2, and we only participated in tasks for
		Korean to Japanese (Ko→Ja)
	</Extractive Summary>
	<Extractive Summary> =
		04
		Table 11: Method ablation for JPC2 Ko→Ja sub-task
		Sub-task
		Adequacy
		ASPEC (Ja→En)
		4
	</Extractive Summary>
	<Extractive Summary> =
		65
		Table 12: Adequacy Evaluation of Our Model
		positioning
	</Extractive Summary>
	<Extractive Summary> =
		Table 12
		shows the adequacy performance for the sub-tasks
		we participated in
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows BLEU scores by 
		changing various initial distortion weights in 
		Myanmar-English bidirectional translations
	</Extractive Summary>
	<Extractive Summary> =
		6 as shown in 
		Table 1, we found that the translation result did 
		not 
		improve 
		significantly 
		compared 
		with 
		baseline
	</Extractive Summary>
	<Extractive Summary> =
		Set
		Category
		# Sentences
		Train
		texts
		448,472
		items
		955,523
		Dev
		texts
		1,153
		items
		2,845
		Devtest
		texts
		1,114
		items
		2,900
		Test
		texts
		1,148
		items
		2,129
		Table 10: Number of sentences in timely disclosure
		document corpus: We split training set into two cate-
		gories
	</Extractive Summary>
	<Extractive Summary> =
		34
		Table 11: Case-sensitive BLEU scores of provided
		blind test sets: All scores were calculated by ofﬁcial
		evaluation server
	</Extractive Summary>
	<Extractive Summary> =
		Table 13 shows the example
		translations of the baseline and ﬁne-tuned sys-
		tems10
	</Extractive Summary>
	<Extractive Summary> =
		47
		(1)
		Table 12: Ofﬁcial results of our submitted systems for timely disclosure subtask: Shown rank is only ordered
		among constrained submissions
	</Extractive Summary>
	<Extractive Summary> =
		Fine-tuned
		Based on historical data, comparable assets and estimates in the engineering report
		Table 13: Example translations of baseline and ﬁne-tuned system: Example was picked from devtest set
	</Extractive Summary>
	<Extractive Summary> =
		Japanese
		実績値、類似建物の修繕費水準、ER の修繕更新費等を考慮し査定
		English
		Based on historical data, comparable assets and estimates in the engineering report
		Table 14: Example of sentence pair contained in the training set
	</Extractive Summary>
	<Extractive Summary> =
		Table 14 shows the
		sentence pair in the training set that was the most
		similar to the previous example
	</Extractive Summary>
	<Extractive Summary> =
		We found a sen-
		tence pair on the English side that is identical as
		the reference in Table 13, and the Japanese side
		is also quite similar
	</Extractive Summary>
	<Extractive Summary> =
		4
		Submissions and human evaluations
		Table 12 shows our submissions and their human
		evaluation scores
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the detail of each corpus
	</Extractive Summary>
	<Extractive Summary> =
		Firstly, we trained the multiple NMT
		models with different combinations of ﬁve cor-
		pora as shown in Table 1, and evaluated these
		NMT models with an ofﬁcial test-set, in which
		the number of data was 2000
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 re-
		ports the statistics of the full corpus
	</Extractive Summary>
	<Extractive Summary> =
		2 
		Results  
		As shown in Table 1, we rank 1st in the direction 
		of 
		Japanese-English, 
		Japanese-Chinese 
		and 
		Chinese-Japanese, and 2nd in one English-
		Japanese
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the size of
		train/development/test splits used in our experi-
		ments
	</Extractive Summary>
	<Extractive Summary> =
		, 2017) but trained on dif-
		ferent training corpora (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Each item in Table 1 comprises of a
		source text in English, its translation in Hindi,
		the image and a rectangular region in the im-
		age
	</Extractive Summary>
	<Extractive Summary> =
		3
		(Yandex)9
		Statistics of the training bitexts are shown by
		Table 1, summarising for each language the total
		number of sentences, running words, vocabulary
		size and average sentence length
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows data statistics used for the 
		experiments
	</Extractive Summary>
	<Extractive Summary> =
		The
		empirical results are shown in Table 1 for
		Japanese-Vietnamese and in Table 3 for English-
		Vietnamese
	</Extractive Summary>
	<Extractive Summary> =
		The results in Table 10 show that training the
		model with diacritization compared to without
		diacritization improves marginally by 0
	</Extractive Summary>
	<Extractive Summary> =
		com/AliOsm/
		translation-over-diacritization
		Table 10: Translation over Diacritization (ToD) results
		on the test set
		Model
		Training
		Time
		Model
		Size
		Best BLEU
		Score
		Without
		29 Hours
		285MB
		33
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =2>
	<Abstractive Summary> =
		The correspondence be-
		tween field IDs and field names, along with the
		Lang
		Train
		Dev
		DevTest
		Test-N
		zh-ja
		1,000,000
		2,000
		2,000
		5,204
		ko-ja
		1,000,000
		2,000
		2,000
		5,230
		en-ja
		1,000,000
		2,000
		2,000
		5,668
		Lang
		Test-N1
		Test-N2
		Test-N3
		Test-EP
		zh-ja
		2,000
		3,000
		204
		1,151
		ko-ja
		2,000
		3,000
		230
		–
		en-ja
		2,000
		3,000
		668
		–
		Table 2: Statistics for JPC
		frequency and occurrence ratios for the train-
		ing data, are described in the README file
		of ASPEC-JE06
		-
		Table 2: BLEU scores for various models
		Japanese Target
		リストの一番下にある番号がリストから削除されます。
		risuto-no ichiban shita-ni aru bang¯o-ga risuto kara sakujo saremasu
		Table 2: Attaching a single token to the beginning of an English training data source sentence, based on the
		predicted formality of the Japanese target side
		proach on multiple data sets and show that it suc-
		cessfully produces sentences in the requested level
		of formality
		The whole construction process was supervised by
		a person who satisﬁes the following conditions to
		guarantee the conversations to be natural:
		• has the experience of being engaged in lan-
		Scene
		Scenarios
		Sentences
		JA → EN
		face-to-face
		165
		5,068
		phone call
		77
		2,329
		general chatting
		101
		3,321
		meeting
		106
		3,561
		training
		16
		608
		presentation
		4
		113
		sum
		469
		15,000
		EN → JA
		face-to-face
		158
		4,876
		phone call
		99
		2,949
		general chatting
		102
		2,988
		meeting
		103
		3,315
		training
		9
		326
		presentation
		15
		546
		sum
		486
		15,000
		Table 2: Statistics for the corpus, where JA → EN rep-
		resents scenarios which are written in Japanese then
		translated into English and EN → JA represents sce-
		narios constructed in the reverse way682060
		Table 2: BLEU, RIBES and AMFM scores result of
		participated teams for multi-modal translation track1M
		Table 2:
		Statistics of our preprocessed monolin-
		gual data43%)
		2,834,970
		2,104,896
		total
		169,871
		3,984,038
		2,776,397
		Table 2: Statistics on our in-domain parallel data for
		the Tamil–English task72
		1 of 2
		None
		Table 2:
		BLEU score for English-Japanese tasks on
		leaderboard
		38M
		Table 2:
		Statistics of our preprocessed monolingual
		data 
		 
		 
		Corpus 
		Translation pairs  
		ALT  
		230,240 
		NER (Raw) 
		14313 
		NER (clear) 
		14310 
		Table 2:  Data statistics of the NER corpus3
		Mini-batch size
		4K tokens
		Update frequency
		128 batches
		Beam search (n-best)
		6 best
		Table 2: Hyper-parameters for the scientiﬁc paper task:
		Our basic hyper-parameters are identical to Trans-
		former “big” setting80
		BT-JIJI
		Table 2: BLEU scores for Japanese→English translation tasks3
		Table 2:
		Combining BT and ST yields better BLEU
		score than BT and ST5
		Table 2: General translation accuracy of each system
		on the concatenated data (ITEM+TEXT)707060
		-
		-
		Table 2: Translation evaluation scores on IIT-Bombay Hindi-English and UFAL English-Tamil test setsorg/project/truecase/
		139
		Table 2: This table describes the results of WAT 2019 evaluation of our submitted systems & compared with the
		best system submissions of WAT 2019 & the previous year
		Ensemble of 4 models
		Table 2: Summary of system settings92 
		Table 2: Technical point contributions 
		                                                           
		1 http://nlp45
		Table 2: JPO task results71
		Table 2: BLEU scores for ASPEC-JE test set using the Transformer (model) based NMT1
		Batch size
		6144
		Table 2: Hyper parameter values of transformer mod-
		els39
		Table 2: BLEU score of Tamil–English and English–
		Tamil System submitted by our team at WAT 201950
		Table 2: WAT 2019 official automatic and manual evaluation results for English→Hindi (HINDEN) tasks
		on the E-Test (EV, upper part) and C-Test (CH, lower part), complemented with our automatic scores68
		Table 2: Results obtained in Evaluation Test Set5
		661
		Table 2: Statistics of development and test sets 
		 
		Domain 
		Number of Word 
		Myanmar 
		Syllable 
		tokens 
		Myanmar 
		English 
		ALT  
		698,347 
		436,923 
		1,138,297 
		UCSY 
		2,966,666 
		2,255,630 
		6,455,588 
		Total 
		36,650,13 
		2,692,553 
		6,569,417 
		Table 2:  Training Details Information8K
		Yelp
		560K
		38K
		IMDB
		25K
		25K
		Table 2: Sentiment Analysis Datasets
		Train
		dev2010
		tst2010
		Number of words
		1015
		36
		25
		Table 2: The number of Japanese OOV words replaced
		by their synonymscom/mishkal
		217
		Table 2: DER/WER comparison of the different FFNN models on the test set
		DER/WER
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		Basic model
		9
	</Abstractive Summary>
	<Extractive Summary> =
		The automatic scores for the multi-modal
		task along with the WAT evaluation server
		BLEU scores are provided in Table 20
	</Extractive Summary>
	<Extractive Summary> =
		00
		Table 20: Multi-Modal Task automatic evaluation results
	</Extractive Summary>
	<Extractive Summary> =
		2
		Statistical Significance Testing of
		Pairwise Evaluation between
		Submissions
		Table 21 shows the results of statistical signifi-
		cance testing of aspec-ja-en subtasks, Table 22
		shows that of JIJI subtasks, Table 23 shows
		that of TDDC subtasks
	</Extractive Summary>
	<Extractive Summary> =
		The results are shown
		in Table 24
	</Extractive Summary>
	<Extractive Summary> =
		On the other hand, some translations seem
		to be fitted to sentences of TDDC which are
		27
		KNU_Hyundai (3173)
		NICT-2 (3086)
		srcb (3205)
		NTT (3225)
		- ≫ ≫
		KNU_Hyundai (3173)
		- ≫
		NICT-2 (3086)
		>
		NICT-2 (3182)
		srcb (3212)
		AISTAI (3251)
		KNU_Hyundai (3172)
		NTT (3236)
		≫ ≫ ≫ ≫
		NICT-2 (3182)
		-
		≫ ≫
		srcb (3212)
		> ≫
		AISTAI (3251)
		-
		Table 21: Statistical significance testing of the aspec-ja-en (left) and aspec-en-ja (right) Pairwise scores
	</Extractive Summary>
	<Extractive Summary> =
		NHK-NES (2883)
		sarah (2793)
		sarah (2813)
		NHK-NES (2884) ≫ ≫ ≫
		NHK-NES (2883)
		≫ ≫
		sarah (2793)
		≫
		NHK-NES (2885)
		sarah (2814)
		sarah (2815)
		NHK-NES (2886) ≫ ≫ ≫
		NHK-NES (2885)
		≫ ≫
		sarah (2814)
		≫
		Table 22: Statistical significance testing of the jiji-ja-en (left) and jiji-en-ja (right) Pairwise scores
	</Extractive Summary>
	<Extractive Summary> =
		sarah (2807)
		NTT (3002)
		NICT-2 (3081)
		sarah (2811)
		geoduck (3197)
		geoduck (3216)
		ORGANIZER (3264) ≫ ≫ ≫ ≫ ≫ ≫
		sarah (2807)
		≫ ≫ ≫ ≫ ≫
		NTT (3002)
		-
		≫ ≫ ≫
		NICT-2 (3081)
		≫ ≫ ≫
		sarah (2811)
		-
		≫
		geoduck (3197)
		≫
		NTT (3005)
		sarah (2808)
		NICT-2 (3084)
		sarah (2812)
		geoduck (3200)
		geoduck (3217)
		ORGANIZER (3265)
		- ≫ ≫ ≫ ≫ ≫
		NTT (3005)
		≫ ≫ ≫ ≫ ≫
		sarah (2808)
		-
		≫ ≫ ≫
		NICT-2 (3084)
		≫ ≫ ≫
		sarah (2812)
		≫ ≫
		geoduck (3200)
		≫
		Table 23: Statistical significance testing of the tddc-itm-ja-en (left) and tddc-txt-ja-en (right) Pairwise
		scores
	</Extractive Summary>
	<Extractive Summary> =
		185
		Table 24: The Fleiss’ kappa values for the pairwise evaluation results
	</Extractive Summary>
	<Extractive Summary> =
		0 %)
		Table 25: Appropriateness of source English cap-
		tions in the 4032 assessments collected for the
		multi-modal track
	</Extractive Summary>
	<Extractive Summary> =
		The results in Table 25 indicate that for a
		surprisingly high number of items we did not
		receive any answer
	</Extractive Summary>
	<Extractive Summary> =
		00
		Table 26: ASPEC en-ja submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		1901
		NMT
		NO
		26
	</Extractive Summary>
	<Extractive Summary> =
		500
		Table 27: ASPEC ja-en submissions
		33
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		3262
		NMT
		NO
		38
	</Extractive Summary>
	<Extractive Summary> =
		500
		Table 28: TDDC ITM ja-en submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		3263
		NMT
		NO
		24
	</Extractive Summary>
	<Extractive Summary> =
		250
		Table 29: TDDC TXT ja-en submissions
		34
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		juman
		kytea
		mecab
		juman
		kytea
		mecab
		juman
		kytea
		mecab
		NMT
		1904
		NMT
		NO
		16
	</Extractive Summary>
	<Extractive Summary> =
		The fact that the mid-gated model tends to ig-
		nore the second and third types of unknown words
		does not contradict to the result in Table 2, since
		even though other models translate the second and
		third type in their own way, the result is not ex-
		actly the correct answer and it is ignored in the
		BLEU scores
	</Extractive Summary>
	<Extractive Summary> =
		For example, in Table 2 the sufﬁx ます (masu)
		at the end of the Japanese target sentence is a com-
		mon politeness marker that identiﬁes this as a po-
		lite sentence
	</Extractive Summary>
	<Extractive Summary> =
		Moreover, from Table 2, 3 and 4, it is observed
		that when translating English to Hindi our multi-
		modal translation outperforms our text only trans-
		lation as well as our Hindi-only image caption-
		ing
	</Extractive Summary>
	<Extractive Summary> =
		1
		English-Japanese
		Table 2 indicates the BLEU score and rank of the
		system we submitted in the ASPEC and JPC2 sub-
		tasks of WAT 2019
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the selected set of the
		7We mixed ASPEC and JParaCrawl by upsampling AS-
		PEC twice
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2, we show that these two approaches
		can be combined and yield better performance
		than either method individually
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 reports the case-sensitive sacreBLEU
		scores of the NMT systems and the production
		systems without translation memory
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the hyperparameters
		of the model, training, and test
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 2 shows our model performance for the
		patent task
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, our multilingual result
		did not show improvement in the NMT systems,
		falling behind the unidirectional model by not
		more than 2 BLEU points for single decoding
	</Extractive Summary>
	<Extractive Summary> =
		Except this two experiments, the following,
		a series of experiments are all performed using
		“OpenNMT-py” as shown in Table 2, thus, we do
		not mention it every time
	</Extractive Summary>
	<Extractive Summary> =
		But
		we
		found
		that
		the
		big
		transformer
		model (Transformer (big) in Table 2) training by
		“Tensor2Tensor” outperforms the reported models
		training by “OpenNMT-py” (the fourth line in Ta-
		ble 2), especially for English-to-Japanese (42
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2, we show that comb-
		ing these techniques can lead to signiﬁcant im-
		provements of over 1
	</Extractive Summary>
	<Extractive Summary> =
		4, all evalua-
		tion BLEU scores given in Table 2 are evalu-
		ated by WAT 2019 ofﬁcial automatic evaluation
		system14
	</Extractive Summary>
	<Extractive Summary> =
		, 2015)
		scores of our submitted systems are shown in Ta-
		ble 1, Table 2, and Table 3 resepectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 shows the scores ob-
		tained by our system on the Evaluation Test
		Set and Challenge Test Set respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 illustrates statistics of the develop-
		ment and test sets extracted from the correspond-
		ing JaRuNC corpora
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the data about the training 
		detail
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the number of OOV words replaced
		by their synonyms
	</Extractive Summary>
	<Extractive Summary> =
		Although the idea of dia-
		critizing each character independently is counter-
		intuitive, the results of the FFNN models on the
		test set (shown in Table 2) are very promising
		with the embeddings model having an obvious ad-
		vantage over the basic and 100-hot models and
		performing much better than the best rule-based
		diacritization system Mishkal3 among the sys-
		tems reviewed by (Fadel et al
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =3>
	<Abstractive Summary> =
		The
		corpus consists of Chinese-Japanese, Korean-
		Japanese and English-Japanese patent de-
		scriptions whose International Patent Classi-
		2
		Disclosure
		Train
		Dev
		DevTest
		Test
		Period
		Texts
		Items
		Texts
		Items
		Texts
		Items
		2016-01-01 to
		1,089,346
		-
		-
		-
		-
		-
		-
		2017-12-31
		(614,817)
		-
		-
		-
		-
		-
		-
		2018-01-01 to
		314,649
		1,153
		2,845
		1,114
		2,900
		1,153
		2,129
		2018-06-30
		(218,495)
		(1,148)
		(2,650)
		(1,111)
		(2,671)
		(1,135)
		(1,763)
		Table 3: Statistics for TDDC (The number of unique sentences)
		fication (IPC) sections are chemistry, electric-
		ity, mechanical engineering, and physicshtml
		Time
		GPU1
		GPU2
		Basic
		43h
		4GB
		4GB
		Mid-Gated
		40h
		8GB
		4GB
		Multi-Attention
		37h
		8GB
		4GB
		Table 3: The actual training time and GPU overhead of
		each model If any of these verbs are present we con-
		sider the sentence to be formal, if not then we pro-
		ceed to looking for the polite verb forms, then the
		46
		Formality
		Verb forms
		Informal
		だ, だった, じゃない, じゃなかった, だろう,
		da, datta, janai, janakatta, darou,
		だから, だけど, だって, だっけ, そうだ, ようだ
		dakara, dakedo, datte, dakke, souda, youda
		Polite
		です, でした, ない, なかった, ます, ました, ません,
		desu, deshita, nai, nakatta, masu, mashita, masen,
		ましょう, でしょう, ください, なさい, である, ではない
		mashou, deshou, kudasai, nasai, dearu, dewanai
		Formal
		ございます, いらっしゃいます, おります, なさいます, 致します,
		gozaimasu, irrashaimasu, orimasu, nasaimasu, itashimasu,
		ご覧になります, 拝見します, お目に掛かります,
		goranninarimasu, haikenshimasu, omenikakarimasu,
		おいでになります, 伺います, 参ります, 存知します, 存じ上げます,
		oideninarimasu, ukagaimasu, mairimasu, zonjishimasu, zonjiagemasu,
		召し上がります, 頂く, 頂きます, 頂いて, 差しあげます,
		meshiagemasu, itadaku, itadakimasu, itadaite, sashiagemasu,
		下さいます, おっしゃいます, 申し上げます
		kudasaimasu, osshaimasu, moushiagemasu
		Table 3: Common verbs and sufﬁxes for each level of formality, used as identifying heuristics In the original conversation, the
		utterance is from Speaker 2 to Speaker 1, and
		57
		Data Set
		Devel
		Eval
		Train
		BSD
		1000
		1000
		28,000
		AMI
		1000
		1000
		108,499
		ON
		1000
		1000
		26,439
		Total
		162,938
		Table 3: Training, development and evaluation data
		statistics615290
		Table 3: BLEU, RIBES and AMFM scores result of
		participated teams for text-only translation track
		Order
		From
		To
		Graph
		1
		◌្ + ដ
		◌្ + ត
		◌�
		2
		◌ំ + ◌ា
		◌ា + ◌ំ
		◌ាំ
		3
		◌ុ + ◌ាំ
		◌៉ + ◌ាំ
		◌ុាំ
		◌ាំ + ◌ុ
		◌ាំ + ◌៉
		4
		V + S[S]
		S[S] + V
		-
		5
		WS + SS
		SS + WS
		-
		Table 3: Khmer Text normalization rules, where
		”V” is Vowel, ”S” is subscript (subscript sign + a
		consonant) and [S] refer to one or zero subscript,
		WS is west subscript, and SS is south subscript88
		Table 3: Method ablation for ASPEC En-Ja sub-task
		En→Ja and Ja→En, respectively0001
		Table 3: Parameters for training TLM 
		 
		 
		 
		Bilingual 
		Lexicon 
		Translation pairs 
		English  
		 54674 
		Myanmar  
		 35532 
		Total 
		 90206 
		Table 3:  Data statistics of the Bilingual Lexicon1
		Table 3: Comparison of translation performance on
		changing subword size68
		Table 3: BLEU scores for English→Japanese translation tasks1
		Table 3: BLEU scores of systems trained only on the
		provided parallel datasets8
		Table 3:
		BLEU results of our proposed approach on
		the evaluation data sets (ITEM and TEXT)
		Source
		#pairs
		type
		UFAL EnTam
		160K
		en-ta
		Leipzig Newscrawl
		300K
		ta mono
		Kaggle Indian Politics News
		300K
		en mono
		Table 3: Training dataset used for UFAL English-Tamil
		task40 §
		Submitted model
		Table 3: Results of ASPEC tasks57 
		Table 3: Results of different data combination001
		Batch size in tokens
		14336
		Update frequency
		2
		Table 3: JPX model settings
		various components in the pipeline are propa-
		gated00
		Table 3: Evaluation results: BLEU scores783550
		Table 3: AM-FM score of Tamil–English and English–
		Tamil System submitted by our team at WAT 201955
		Table 3: Results obtained in Challenge Test Set
		VH NH AN RH 明日大統領が到着します
		; Президент приезжает завтра
		Table 3: French-German sentence pair with frequency
		constraints3 
		Mini-batch size   
		64 
		Table 3:  Hyper-parameter of NMT models4
		Table 3: Sentiment Analysis Results93)
		Table 3: Results of English-Vietnamese NMT systems
		For comparison purpose, English texts are split
		into sub-words using Sennrich’s BPE methods10%
		Table 3: Extra training dataset statistics
		Extra Train
		Words Count
		22
	</Abstractive Summary>
	<Extractive Summary> =
		250
		Table 30: JIJI en-ja submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		1905
		NMT
		NO
		16
	</Extractive Summary>
	<Extractive Summary> =
		750
		Table 31: JIJI ja-en submissions
		35
		Proceedings of the 6th Workshop on Asian Translation, pages 36–44
		Hong Kong, China, November 4, 2019
	</Extractive Summary>
	<Extractive Summary> =
		If none of the verb forms in
		Table 3 are present in the sentence it is ignored
	</Extractive Summary>
	<Extractive Summary> =
		However, we have attained lower BLEU,
		RIBES and AMFM scores than other teams in
		text-only and Hindi-only image captioning trans-
		lation track as shown in Table 3 and 4 respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the experimental results of our
		proposed approach using the translation memory,
		reporting the sacreBLEU scores on the ITEM and
		TEXT evaluation data sets
	</Extractive Summary>
	<Extractive Summary> =
		3
		Model
		Using similar models settings as (i) JPX model
		in Table 3 with 32,000 subwords tokens at 100%
		character coverage (BASE) and (ii) the JIJI model
		in Table 5 with 10,000 subwords tokens at 100%
		character coverage (MINI), we train one model
		each to compare (i) vs (ii) in the Mixed-domain
		Task
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 3 demonstrates the BLEU scores of
		our baseline Only GV model and proposed
		TMU model on News Commentary Ja→Ru9 and
		Ru→Ja10 test data for News Commentary shared
		task
	</Extractive Summary>
	<Extractive Summary> =
		, 2015)
		scores of our submitted systems are shown in Ta-
		ble 1, Table 2, and Table 3 resepectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 shows the scores ob-
		tained by our system on the Evaluation Test
		Set and Challenge Test Set respectively
	</Extractive Summary>
	<Extractive Summary> =
		In Ta-
		ble 2 and Table 3, TOT, HIC, and MMT rep-
		resents the text-only translation sub task sys-
		tem, automatic image caption generation sys-
		tem of Hindi-only image captioning sub task
		and the multi-modal translation (using both
		the image and the text) sub task system re-
		spectively
	</Extractive Summary>
	<Extractive Summary> =
		Thus, the Japanese-
		Russian parallel sentence with corresponding side
		constraints illustrated in Table 3 is used in training
		to feed the model
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the settings of network 
		hyper-parameters for NMT models
	</Extractive Summary>
	<Extractive Summary> =
		The
		empirical results are shown in Table 1 for
		Japanese-Vietnamese and in Table 3 for English-
		Vietnamese
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 summarizes those results
		and the scores from other systems (Nguyen and
		Chiang, 2017; Huang et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the statistics for the extra
		training dataset
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =4>
	<Abstractive Summary> =
		Therefore, the set of source documents for
		training, development, development-test and
		3
		Lang
		Train
		Dev
		DevTest
		Test
		en-ja
		200,000
		2,000
		2,000
		2,000
		Table 4: Statistics for JIJI Corpus
		Lang
		Train
		Dev
		Test
		Mono
		hi-en
		1,492,827
		520
		2,507
		–
		hi-ja
		152,692
		1,566
		2,000
		–
		hi
		–
		–
		–
		45,075,279
		Table 5: Statistics for IITB Corpus
		Table 4: Translation example by three models83
		Table 4: Evaluation scores of labels produced by the
		formality classiﬁer compared to gold test set labels for
		each formality category (n=150)00
		Table 4: NMT and SMT experiments using the conver-
		sational corpora335390
		Table 4: RIBES, AMFM scores result of participated
		teams for Hindi-only image captioning track98
		1e-09 --clip-norm 5 --sync-sgd
		--exponential-smoothing
		Table 4:
		Parameters of Marian used for training
		our NMT systems72
		Table 4: Method ablation for JPC2 En-Ja sub-task
		over the baseline0001
		--eval bleu true
		Table 4: Parameters for training NMT On the other hand, from 
		Myanmar to English translation, the PBSMT 
		with reranking is better than baseline PBSMT in 
		Data Set 
		#sentences 
		TRAIN 
		ALT 
		NER 
		Bilingual 
		 112570 
		18088 
		14310 
		80172 
		DEV 
		 1000 
		TEST 
		1018 
		Table 4:  Statistics of data sets7
		Table 4: Comparison of translation performance on
		changing the methods of building synthetic data11
		1/2
		Table 4: Ofﬁcial results for the newswire translation tasks of WAT 2019: For JIJI-EJ task, we show the BLEU,
		RIBES, and AMFM scores with KyTea tokenizer6
		Table 4: BLEU scores of systems trained using addi-
		tional monolingual data08
		Table 4:
		BLEU results on the evaluation data sets
		(ITEM and TEXT)
		Table 4: Examples from the test set of correctly translated samples97 (-)
		Table 4: Results of TDDC task71 
		Table 4: Technical point contributions 
		Japanese-Chinese subtask: 
		For this subtask, we utilized only the data in 
		ASPEC, no data augmentation was used34
		-
		Table 4: JPX task results B is the test data
		unknown tokens set covered by concatenated vo-
		cabulary of Jiji and News Commentary pivot paral-
		167
		Ja→Ru
		Ru→Ja
		A (Only GV)
		B (TMU)
		A (Only GV)
		B (TMU)
		#tokens
		#types
		#tokens
		#types
		#tokens
		#types
		#tokens
		#types
		Coverage in data
		1,467
		1,220
		2,072
		1,751
		481
		362
		596
		450
		Correctly translated
		85
		65
		191
		147
		26
		21
		31
		24
		Table 4: The coverage of tokens from additional pivot parallel data and the number of correctly translated tokens
		and types of distinct words by each system calculated for test set
		186
		Input Image and Text
		Reference and Output by different Model Types
		Woman standing on tennis court
		Reference: टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		TOT: टेिनस कोटर् पर मनुष्य
		Transliteration: tenis kort par manushy
		Translation: A man on a tennis court
		HIC:
		एक व्यिक् टेिनस खेल रहा है
		Transliteration: ek vyakti tenis khel raha hai
		Translation: A person playing tennis
		MMT:
		टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		Translation: A woman standing on a tennis court
		Table 4: Sample 1 input and output
		Input Image and Text
		Reference and Output by different Model Types
		man stand on skateboard
		Reference: आदमी स्के टबोडर् पर खड़ा है
		Transliteration: aadmee sketabord par khada hai
		TOT: स्के टबोडर् पर मनुष्य
		Transliteration: sketabord par manushy
		Translation: Man on skateboard
		HIC:
		व्यिक् एक स्के टबोडर् पर
		Transliteration: vyakti ek sketabord par
		Translation: A person on a skateboard
		MMT:
		व्यिक् स्के टबोडर् पर खड़ा है
		Transliteration: vyakti sketabord par khada hai
		Translation: A person standing on a skateboard
		Table 5: Sample 2 input and output
		Input Image and Text
		Reference and Output by different Model Types
		A big tv on a stand
		Reference: एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		TOT: एक सेलफोन पर एक बड़ा सा वैन
		Transliteration: ek selaphon par ek bada sa vain
		Translation: A big van on a cellphone
		HIC:
		इमारत के िकनारे पर एक दीवार
		Transliteration: imaarat ke kinaare par ek deevaar
		Translation: A wall on the side of the building
		MMT:
		एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		Translation: A big tv on a stand
		Table 6: Sample 3 input and output
		Kyunghyun Cho, Bart Van Merriënboer, Caglar
		Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
		Holger Schwenk,
		and Yoshua Bengio36
		Table 4: BLEU score on JaRuNC testset657564 
		Table 4:  Myanmar to English Translation4
		Table 4: Sentiment matching translation accuracy
		Train
		tst2012
		tst2013
		Number of words
		5342
		84
		93
		Table 4: The number of rare words in which their af-
		ﬁxes are detached from the English texts in the SAA
		algorithm, 2019) and machine learning based
		approaches (Belinkov and Glass, 2015; Abandah
		219
		Table 4: DER/WER comparison of the different RNN models on the test set
		DER/WER
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		Without Extra Train Dataset
		Basic model
		2
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, the translation is the same
		3http://www
	</Extractive Summary>
	<Extractive Summary> =
		For example, the Japanese word “データベース”
		which means “database” in English, is a token of
		the second sentence in Table 4 after tokenization
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 contains a precision-recall evaluation of
		our formality classiﬁer, showing strong F1 scores
		for all three classes
	</Extractive Summary>
	<Extractive Summary> =
		, 2001) are shown
		in Table 4 along with several ablation experiments
		on training NMT and SMT systems using only the
		BSD data, all 3 conversational corpora, and ex-
		cluding the BSD corpus from the training data
	</Extractive Summary>
	<Extractive Summary> =
		2
		Subword size/Vocabulary size
		Table 4 shows the BLEU scores when we
		changed the number of subwords obtained from
		sentencepiece
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the ofﬁcial results of our
		submission to WAT 2019
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the results of our submission
		systems on the devtest and test data sets, where
		Fairseq provides the result of an ensemble with 4
		replicas
	</Extractive Summary>
	<Extractive Summary> =
		2
		Qualitative Samples
		The qualitative samples from Table 4 indicate
		en→ta comparable to ta→en, despite the imbal-
		ance in BLEU scores
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 4 shows our model performance for the
		timely disclosure task
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows token and type coverage and cor-
		rectly translated tokens and types of distinct words
		on test data for A and B, respectively
	</Extractive Summary>
	<Extractive Summary> =
		Three sample inputs with the dif-
		ferent forms of an ambiguous word “stand”
		from the challenge test set and their outputs
		are shown in Table 4, Table 5 and Table 6
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 and Table 5 show the different evaluation 
		metrics for Myanmar-English and English-
		Myanmar translation pairs
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 counts the number of sentiment an-
		notation matches
	</Extractive Summary>
	<Extractive Summary> =
		The results of the RNN
		models on the test set (shown in Table 4) are much
		better than the FFNN models by about 67%
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =5>
	<Abstractive Summary> =
		Therefore, the set of source documents for
		training, development, development-test and
		3
		Lang
		Train
		Dev
		DevTest
		Test
		en-ja
		200,000
		2,000
		2,000
		2,000
		Table 4: Statistics for JIJI Corpus
		Lang
		Train
		Dev
		Test
		Mono
		hi-en
		1,492,827
		520
		2,507
		–
		hi-ja
		152,692
		1,566
		2,000
		–
		hi
		–
		–
		–
		45,075,279
		Table 5: Statistics for IITB Corpus
		Table 5: A translation result of the noised sentence
		Dataset
		Train
		Test
		Proprietary
		23,781,990
		300
		ASPEC
		1,000,000
		1,812
		JESC
		3,237,376
		2,001
		KFTT
		329,882
		1,160
		Table 5: Parallel training and test data set sizes in num-
		ber of sentences
		397
		Table 5: BLEU and ChrF2 scores for all three evalua-
		tion data sets using the NMT system trained on all data
		Table 5:
		Best performance examples in English to
		Hindi multi-modal translation We also gener-
		ated 100-best translation hypotheses with our
		71
		Feature
		Description
		L2R (7)
		Scores given by each of the 7 left-to-right Marian models
		LEX (4)
		Sentence-level translation probabilities, for both translation directions
		LM (2)
		Scores given by the language models used by the Moses baseline systems
		LEN (2)
		Difference between the length of the source sentence and the length of the translation hy-
		pothesis, and its absolute value
		Table 5: Set of features used by our reranking systems30
		1 of 3
		Table 5:
		BLEU score for Chinese-Japanese tasks on
		leaderboard
		datasets01
		Table 5: Results (BLEU-cased) of our MT systems on the test set04 
		Table 5:  BLEU, RIBES and AMFM scores for PBSMT, PBSMT with reranking9
		Table 5: Comparison of translation performance on
		changing mini-batch size (update frequency) for each
		update in NMT training
		Table 5: Example results of JIJI-JE tasks translated with JIJI- and Equivalent-style NMT
		Omitted-
		Added-
		Task
		Tag (Style)
		words
		words
		JIJI-JE
		JIJI-style
		187
		-
		Table 5: My→En leaderboard5
		Testset:
		Devtest
		Annotators:
		25
		Tasks:
		50
		Redundancy:
		1
		Task per Annotator:
		2
		Data points:
		8308
		Table 5: Human Evaluation Campaign Parameters
		ITEM
		TEXT
		ALL
		HREF
		73
		Table 5: Failure cases among translated samples  
		 
		Table 5: Technical point contributions 
		Chinese-Japanese subtask: 
		The baseline model is a vanilla Transformer 
		model with all ASPEC data001
		Batch size in tokens
		4000
		Update frequency
		1
		Table 5: JIJI model settings
		glish sentences were combined and fed into Sen-
		tencePiece for training BPE subword units)
		Table 5: Examples of translating [unknown tokens] included in pivot parallel data C from Russian into Japanese
		186
		Input Image and Text
		Reference and Output by different Model Types
		Woman standing on tennis court
		Reference: टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		TOT: टेिनस कोटर् पर मनुष्य
		Transliteration: tenis kort par manushy
		Translation: A man on a tennis court
		HIC:
		एक व्यिक् टेिनस खेल रहा है
		Transliteration: ek vyakti tenis khel raha hai
		Translation: A person playing tennis
		MMT:
		टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		Translation: A woman standing on a tennis court
		Table 4: Sample 1 input and output
		Input Image and Text
		Reference and Output by different Model Types
		man stand on skateboard
		Reference: आदमी स्के टबोडर् पर खड़ा है
		Transliteration: aadmee sketabord par khada hai
		TOT: स्के टबोडर् पर मनुष्य
		Transliteration: sketabord par manushy
		Translation: Man on skateboard
		HIC:
		व्यिक् एक स्के टबोडर् पर
		Transliteration: vyakti ek sketabord par
		Translation: A person on a skateboard
		MMT:
		व्यिक् स्के टबोडर् पर खड़ा है
		Transliteration: vyakti sketabord par khada hai
		Translation: A person standing on a skateboard
		Table 5: Sample 2 input and output
		Input Image and Text
		Reference and Output by different Model Types
		A big tv on a stand
		Reference: एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		TOT: एक सेलफोन पर एक बड़ा सा वैन
		Transliteration: ek selaphon par ek bada sa vain
		Translation: A big van on a cellphone
		HIC:
		इमारत के िकनारे पर एक दीवार
		Transliteration: imaarat ke kinaare par ek deevaar
		Translation: A wall on the side of the building
		MMT:
		एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		Translation: A big tv on a stand
		Table 6: Sample 3 input and output
		Kyunghyun Cho, Bart Van Merriënboer, Caglar
		Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
		Holger Schwenk,
		and Yoshua Bengio41
		-
		Table 5: BLEU score using a batch size = 6,144 with
		3 GPUs698507 
		Table 5:  : English to Myanmar Translation88
		Table 5: BLEU scores
		Train
		tst2012
		tst2013
		Number of words
		1889
		37
		41
		Table 5:
		The number of English OOV words are re-
		placed by their synonyms83%
		Table 5: DER/WER comparison showing the effect of the weights averaging technique on BNG model
		DER/WER
		Averaged
		Epochs
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		Without
		extra
		train
		dataset
		1
		2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows a
		noise-added example and its translations
	</Extractive Summary>
	<Extractive Summary> =
		To further analyze the best and worst per-
		formance of multi-modal translation in compari-
		son to text-only and Hindi-only image captioning,
		sample predicted sentences on challenge test data,
		reference target sentences and Google translation
		on same test data are considered in Table 5, 6
	</Extractive Summary>
	<Extractive Summary> =
		In
		Table 5, our multi-modal NMT system provides
		perfect prediction like reference target sentence,
		Google translation and close to text-only trans-
		lation but wrong translation in Hindi-only image
		captioning
	</Extractive Summary>
	<Extractive Summary> =
		As listed in
		Table 5, it includes the scores given by 7 left-
		to-right NMT models independently trained
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 presents the results of BLEU for each
		Zh→Ja subtask of the method used in this pa-
		per
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the examples of NMT out-
		puts adapted to the JIJI- and Equivalent-styles in
		the ofﬁcial tasks
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the eval-
		uation campaign parameters
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 indicates failure cases, many of which
		shows under-translation phenomena, when all
		source tokens do not have corresponding translated
		tokens in generated translation
	</Extractive Summary>
	<Extractive Summary> =
		The results of applying thus technologies 
		was in Table 5 (results after averaging of last 8 
		checkpoints, best in two models)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Model
		Using similar models settings as (i) JPX model
		in Table 3 with 32,000 subwords tokens at 100%
		character coverage (BASE) and (ii) the JIJI model
		in Table 5 with 10,000 subwords tokens at 100%
		character coverage (MINI), we train one model
		each to compare (i) vs (ii) in the Mixed-domain
		Task
	</Extractive Summary>
	<Extractive Summary> =
		By adding Jiji and News
		Commentary corpora, we define the vocabulary set
		newly covered by the test data vocabulary as C as
		follows:
		C = (T ∩ P) − G
		(3)
		Table 5 shows translation examples of only GV
		and TMU systems
	</Extractive Summary>
	<Extractive Summary> =
		Three sample inputs with the dif-
		ferent forms of an ambiguous word “stand”
		from the challenge test set and their outputs
		are shown in Table 4, Table 5 and Table 6
	</Extractive Summary>
	<Extractive Summary> =
		Validation
		sets are used to select our best performing net-
		works, while results shown in Table 5 are com-
		puted for the ofﬁcial test sets
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 and Table 5 show the different evaluation 
		metrics for Myanmar-English and English-
		Myanmar translation pairs
	</Extractive Summary>
	<Extractive Summary> =
		Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		To
		show the effect of the weights averaging techni-
		que, Table 5 reports the DER/WER statistics rela-
		ted to the BNG model after averaging its weights
		over the last 1, 5, 10, and 20 epochs
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =6>
	<Abstractive Summary> =
		6
		ALT and UCSY Corpus
		The parallel data for Myanmar-English trans-
		lation tasks at WAT2019 consists of two cor-
		Corpus
		Train
		Dev
		Test
		ALT
		18,088
		1,000
		1,018
		UCSY
		204,539
		–
		–
		All
		222,627
		1,000
		1,018
		Table 6: Statistics for the data used in Myanmar-
		English translation tasks
		pora, the ALT corpus and UCSY corpus
		Type
		Src
		Ref
		Basic
		Mid-Gated
		Multi-
		Attention
		1
		ア ン タ ゴ ニ ス
		ティック
		antagonistic
		antagonistic
		antagonistic
		antagonistic
		エコマティAX
		Ecomatie AX
		ecomater AX
		Ecomatey AX
		ecomate AX
		2
		福島大学
		Fukushima
		University
		Fukushima
		University
		Fukushima
		University
		Fukushima
		University
		長谷山俊郎
		Toshio
		Hasegawa
		Nagayama
		Yamato
		IGNORED
		Nakayama
		友ケ島
		Tomoga Island
		Fiken Island
		IGNORED
		IGNORED
		3
		嘔気・嘔吐
		nausea
		and
		vomiting
		air and vomit-
		ing
		air and vomit-
		ing
		air and vomit-
		ing
		捏造
		fabrication
		structure
		IGNORED
		construction
		Table 6: Examples of translations of unknown words3
		Table 6: KyTea-tokenized BLEU, comparing baseline NMT, rule-based conversion and formality-aware NMT
		models on heldout test
		F1
		Dataset
		Model
		Informal
		Polite
		Formal
		Internal
		Baseline NMT
		0
		Table 6:
		Worst performance example in English to
		Hindi multi-modal translation0
		Table 6: Official BLEU scores for our MT systems on the official test set of the tasks92
		Table 6: Method ablation for ASPEC Zh→Ja sub-task
		Method
		BLEU
		Baseline (ASPEC)
		340001
		--eval bleu true
		Table 6: Parameters for training UNMT  
		Table 6: Comparison between my-en translation results2
		Table 6: Results of incorporating ensembling and R2L
		re-ranking techniques55
		Table 6: Further human evaluation results, which is the
		average number of words per 100 words9
		-
		Table 6: En→My leaderboard60
		Table 6: Human Evaluation results04†
		Table 6: Automated evaluation scores on the UFAL En-
		Tam v237 
		Table 6: Technical point contributions 
		4 
		Conclusion 
		In this paper, we described our NMT system, 
		which is based on Transformer model75
		Table 6: JIJI task results
		186
		Input Image and Text
		Reference and Output by different Model Types
		Woman standing on tennis court
		Reference: टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		TOT: टेिनस कोटर् पर मनुष्य
		Transliteration: tenis kort par manushy
		Translation: A man on a tennis court
		HIC:
		एक व्यिक् टेिनस खेल रहा है
		Transliteration: ek vyakti tenis khel raha hai
		Translation: A person playing tennis
		MMT:
		टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		Translation: A woman standing on a tennis court
		Table 4: Sample 1 input and output
		Input Image and Text
		Reference and Output by different Model Types
		man stand on skateboard
		Reference: आदमी स्के टबोडर् पर खड़ा है
		Transliteration: aadmee sketabord par khada hai
		TOT: स्के टबोडर् पर मनुष्य
		Transliteration: sketabord par manushy
		Translation: Man on skateboard
		HIC:
		व्यिक् एक स्के टबोडर् पर
		Transliteration: vyakti ek sketabord par
		Translation: A person on a skateboard
		MMT:
		व्यिक् स्के टबोडर् पर खड़ा है
		Transliteration: vyakti sketabord par khada hai
		Translation: A person standing on a skateboard
		Table 5: Sample 2 input and output
		Input Image and Text
		Reference and Output by different Model Types
		A big tv on a stand
		Reference: एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		TOT: एक सेलफोन पर एक बड़ा सा वैन
		Transliteration: ek selaphon par ek bada sa vain
		Translation: A big van on a cellphone
		HIC:
		इमारत के िकनारे पर एक दीवार
		Transliteration: imaarat ke kinaare par ek deevaar
		Translation: A wall on the side of the building
		MMT:
		एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		Translation: A big tv on a stand
		Table 6: Sample 3 input and output
		Kyunghyun Cho, Bart Van Merriënboer, Caglar
		Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
		Holger Schwenk,
		and Yoshua Bengio9
		Table 6: Sentiment word translation performance on
		the test sets
		Table 6: Examples of outputs from the English→Vietnamese translation systems with the proposed methods
		So, to make a fair comparison with (Belinkov and
		Glass, 2015)’s system, an auxiliary dataset is built
		from the MSA part of the Tashkeela Corpus using
		the same extraction and cleaning method proposed
		220
		Table 6: Comparing the BNG model with (Barqawi and Zerrouki, 2017) in terms of DER/WER on the test set
		DER/WER
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		(Fadel et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 gives some examples
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows
		48
		our results on the test set using BLEU
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 6, the rule-based method
		yields improvements on the informal test portion
		for all models except ASPEC where performance
		remains the same
	</Extractive Summary>
	<Extractive Summary> =
		2
		Performance of formality-aware NMT
		The BLEU scores in Table 6 show that on the
		overall test set, our formality-aware NMT mod-
		els show an improvement over the baseline NMT
		models
	</Extractive Summary>
	<Extractive Summary> =
		However, in Table 6, prediction of
		source word “court” is inappropriate like Google
		translation, text-only translation and wrong trans-
		lation in Hindi-only image captioning
	</Extractive Summary>
	<Extractive Summary> =
		7
		Results
		Table 6 presents the results for different ver-
		sions of our SMT and NMT systems
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows the effectiveness of these tech-
		niques
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows
		the average number of words per 100 words of
		the three evaluators
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows the mean scores for ITEM, TEXT
		and ALL (ITEM+TEXT) for each system
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 6 shows our model performance on the
		newswire task
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows the
		results comparison with Shakkala
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =7>
	<Abstractive Summary> =
		4
		Corpus
		Train
		Dev
		Test
		ALT
		18,088
		1,000
		1,018
		ECCC
		104,660
		–
		–
		All
		122,748
		1,000
		1,018
		Table 7: Statistics for the data used in Khmer-
		English translation tasks
		• The ECCC corpus consists of 100 thou-
		sand Khmer-English parallel sentences
		extracted from document pairs of Khmer-
		English bi-lingual records in Extraordi-
		nary Chambers in the Court of Cam-
		bodia, collected by National Institute of
		Posts, Telecoms & ICT, Cambodia74
		Table 7: F1 scores for each formality category when comparing predicted labels for MT output and reference
		translation77
		Table 7: Method ablation for ASPEC Ja→Zh sub-task
		PEC Ja→Zh subtask showed a 346
		Table 7: Translation performance comparison when we
		incorporate additional training data JParaCrawl
		Table 7: Translation examples of each system on the devtest data set (ITEM) that obtain a higher evaluation score
		than the human reference94
		Table 7: Mixed-domain Task Results
		Table 7 shows the result of our English to Myan-
		mar models
		Seq2Seq
		这是个精
		精
		精明
		明
		明的
		的
		的举动。 (smart)
		Ref-Pos
		这是个精
		精
		精明
		明
		明的
		的
		的行动。 (smart)
		Ref-Neg
		这是个狡
		狡
		狡猾
		猾
		猾的
		的
		的举动。 (cunning)
		AddLabel-Pos
		这是个精
		精
		精明
		明
		明的
		的
		的举动。 (smart)
		AddLabel-Neg
		太狡
		狡
		狡猾
		猾
		猾了。 (cunning)
		Table 7: Sentiment translation examples65%
		Table 7: Comparing the BNG model with (Belinkov and Glass, 2015) in terms of DER/WER on the test set
		DER/WER
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		Classical Arabic Testing Dataset Results
		Our best model
		1
	</Abstractive Summary>
	<Extractive Summary> =
		Our F1 comparison in Table 7 shows to what ex-
		tent the formality-aware NMT output matches the
		predicted formality level of the reference transla-
		tion when the system is provided with the correct
		input label
	</Extractive Summary>
	<Extractive Summary> =
		5
		Unconstrained setting
		Table 7 shows the “Ensemble (4) + R2L (4)”
		results that were trained by ASPEC or AS-
		PEC+JParaCrawl
	</Extractive Summary>
	<Extractive Summary> =
		5
		Ofﬁcial Result
		We ﬁrst planned to submit the unconstrained set-
		ting results (the second row in Table 7) as our pri-
		mary results
	</Extractive Summary>
	<Extractive Summary> =
		Therefore,
		we submitted the constrained setting results (the
		ﬁrst row in Table 7) as our primary results
	</Extractive Summary>
	<Extractive Summary> =
		We illustrate some example translations, gener-
		ated by our methods when given different source
		sentiment labels in Table 7, together with baseline
		Seq2Seq translations and reference translations
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =8>
	<Abstractive Summary> =
		C-Test was pro-
		vided only for the task itself: the source side
		Tokens
		Dataset
		Items
		English
		Hindi
		Training Set
		28,932
		143,178
		136,722
		D-Test
		998
		4,922
		4,695
		E-Test (EV)
		1,595
		7,852
		7,535
		C-Test (CH)
		1,400
		8,185
		8,665
		Table 8: Data for the English→Hindi multi-modal
		translation task7%
		Table 8: Percentage of each training data set classiﬁed
		as informal, polite, or formal
		’informal’ is attached to the source segment30
		Table 8: Method ablation for JPC2 Zh→Ja sub-task
		translation, multi-sourcing, and r2l re-ranking
		to increase the BLEU score by 049
		(2)
		Table 8: Ofﬁcial results of our submitted systems for ASPEC subtask: For En-Ja direction, we show BLEU scores
		with JUMAN tokenizer
		Table 8: Translation examples of each system on the devtest data set (TEXT) that are highly evaluated by human
		annotatorscom
		221
		Table 8: Comparing the BNG model with (Abandah et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows the ofﬁcial results of our submis-
		sions computed in the evaluation server
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =9>
	<Abstractive Summary> =
		43%)
		2,834,970
		2,104,896
		total
		169,871
		3,984,038
		2,776,397
		Table 9: Data for the Tamil↔English task
		Reference
		謝ることなんかなにもないさ
		ayamaru koto nanka nanimo nai sa
		NMT Model
		MT Output and Transcription
		Baseline
		謝ることなんて何もない
		ayamaru koto nante nanimo nai
		Formality-Aware - Informal
		別に謝ることはない
		betsu ni ayamaru koto wa nai
		Formality-Aware - Polite
		謝ることはありません
		ayamaru koto wa arimasen
		Formality-Aware - Formal
		お詫びのことは何もございません
		owabi no koto wa nanimo gozaimasen
		Table 9: Example output from JESC NMT baseline model and formality-aware NMT model, when each formality
		level is attached to the source segment33
		Table 9: Method ablation for JPC2 Ja→Zh sub-task
		Sub-task
		BLEU
		BLEU rank
		JPC2 (Ko→Ja)
		7323
		Table 9: Performance comparison when we incorporate
		additional training data JParaCrawl: Scores here were
		obtained from evaluation server54%
		Table 9: Vocab size for all sequences types before and
		after BPE step
		Language
		Vocab Size
		Before BPE
		After BPE
		English
		113K
		31K
		Original Arabic
		224K
		32K
		Diacritized Arabic
		402K
		186K
		Diacritics Forms
		41K
		15K
		the OPUS8 project
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Analysis and Examples
		To show some concrete examples of our formality-
		aware translations, Table 9 contains an exam-
		ple of the MT output from the JESC formality-
		aware NMT model and the corresponding JESC
		NMT baseline trained without formality annota-
		tions
	</Extractive Summary>
	<Extractive Summary> =
		Table 9 shows the re-
		sults
	</Extractive Summary>
	<Extractive Summary> =
		Table 9 shows the number of tokens be-
		fore and after BPE step for English, Original Ara-
		bic and Diacritized Arabic as well as the Diacritics
		forms when removing the Arabic characters
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =10>
	<Abstractive Summary> =
		8k
		Table 10:
		In-Domain data for the Russian–
		Japanese task
		Reference
		円卓の騎士たちはキラーラビットと戦う。
		entaku-no kishitachi-wa kir¯a rabitto-to tatakau
		NMT Model
		MT Output and Transcription
		Baseline
		アーサー王の騎士はキラーウサギと戦います。
		¯as¯a ¯o-no kishitachi-wa kir¯a usagi-to tatakaimasu
		Formality-Aware - Informal
		キングアーサーの騎士たちはキラーウサギと戦う。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakau
		Formality-Aware - Polite
		キングアーサーの騎士たちはキラーウサギと戦います。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakaimasu
		Formality-Aware - Formal
		キングアーサーの騎士たちはキラーウサギと戦いをいたします。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakai-wo itashimasu
		Table 10: Example output from internal NMT baseline model and formality-aware NMT model, when each for-
		mality level is attached to the source segment04
		1 of 3
		Table 10:
		BLEU score for Korean→Japanese sub-
		tasks on leaderboard
		The Ko-Ja translation task has only a paten sub-
		task as JPC2, and we only participated in tasks for
		Korean to Japanese (Ko→Ja)
		Set
		Category
		# Sentences
		Train
		texts
		448,472
		items
		955,523
		Dev
		texts
		1,153
		items
		2,845
		Devtest
		texts
		1,114
		items
		2,900
		Test
		texts
		1,148
		items
		2,129
		Table 10: Number of sentences in timely disclosure
		document corpus: We split training set into two cate-
		goriescom/AliOsm/
		translation-over-diacritization
		Table 10: Translation over Diacritization (ToD) results
		on the test set
		Model
		Training
		Time
		Model
		Size
		Best BLEU
		Score
		Without
		29 Hours
		285MB
		33
	</Abstractive Summary>
	<Extractive Summary> =
		Refer to
		Table 10 for the statistics of the in-domain
		parallel corpora
	</Extractive Summary>
	<Extractive Summary> =
		Table 10 shows another example of the MT
		output from the internal formality-aware NMT
		model and the corresponding internal NMT base-
		line trained without formality annotations
	</Extractive Summary>
	<Extractive Summary> =
		3
		Korean-Japanese
		Table 10 shows the translation performance of
		JPC2 dataset for Korean and Japanese as JPC2 Ko-
		Ja
	</Extractive Summary>
	<Extractive Summary> =
		The results in Table 10 show that training the
		model with diacritization compared to without
		diacritization improves marginally by 0
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =11>
	<Abstractive Summary> =
		shtml
		ASPEC
		JPC
		System ID
		System
		Type
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-ko
		ko-ja
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		ATLAS V14 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PAT-Transer 2009 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		RBMT X
		J-Beijing 7 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		Hohrai 2011 (Commercial system)
		RBMT
		✓
		✓
		✓
		RBMT X
		J Soul 9 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		Korai 2011 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		AIAYN
		Google’s implementation of “Attention Is All You Need”
		NMT
		✓
		✓
		Table 11: Baseline Systems I
		7
		JIJI
		IITB
		ALT
		System ID
		System
		Type
		ja-en
		en-ja
		hi-en
		en-hi
		hi-ja
		ja-hi
		my-en
		en-my
		km-en
		en-km
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		Table 12: Baseline Systems II
		NewsCommentary
		TDDC
		EnTam
		Multimodal
		System ID
		System
		Type
		ru-ja
		ja-ru
		ja-en
		ta-en
		en-ta
		en-hi
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		NMT T2T
		Tensor2Tensor’s Transformer
		NMT
		✓
		✓
		✓
		✓
		NMT OT
		OpenNMT-py’s Transformer
		NMT
		✓
		Online X
		Azure Custom Translator
		Other
		✓
		Table 13: Baseline Systems III
		8
		model) for Chinese segmentation04
		Table 11: Method ablation for JPC2 Ko→Ja sub-task
		Sub-task
		Adequacy
		ASPEC (Ja→En)
		434
		Table 11: Case-sensitive BLEU scores of provided
		blind test sets: All scores were calculated by ofﬁcial
		evaluation server
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =12>
	<Abstractive Summary> =
		shtml
		ASPEC
		JPC
		System ID
		System
		Type
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-ko
		ko-ja
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		ATLAS V14 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PAT-Transer 2009 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		RBMT X
		J-Beijing 7 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		Hohrai 2011 (Commercial system)
		RBMT
		✓
		✓
		✓
		RBMT X
		J Soul 9 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		Korai 2011 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		AIAYN
		Google’s implementation of “Attention Is All You Need”
		NMT
		✓
		✓
		Table 11: Baseline Systems I
		7
		JIJI
		IITB
		ALT
		System ID
		System
		Type
		ja-en
		en-ja
		hi-en
		en-hi
		hi-ja
		ja-hi
		my-en
		en-my
		km-en
		en-km
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		Table 12: Baseline Systems II
		NewsCommentary
		TDDC
		EnTam
		Multimodal
		System ID
		System
		Type
		ru-ja
		ja-ru
		ja-en
		ta-en
		en-ta
		en-hi
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		NMT T2T
		Tensor2Tensor’s Transformer
		NMT
		✓
		✓
		✓
		✓
		NMT OT
		OpenNMT-py’s Transformer
		NMT
		✓
		Online X
		Azure Custom Translator
		Other
		✓
		Table 13: Baseline Systems III
		8
		model) for Chinese segmentation65
		Table 12: Adequacy Evaluation of Our Model
		positioning47
		(1)
		Table 12: Ofﬁcial results of our submitted systems for timely disclosure subtask: Shown rank is only ordered
		among constrained submissions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 12
		shows the adequacy performance for the sub-tasks
		we participated in
	</Extractive Summary>
	<Extractive Summary> =
		4
		Submissions and human evaluations
		Table 12 shows our submissions and their human
		evaluation scores
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =13>
	<Abstractive Summary> =
		shtml
		ASPEC
		JPC
		System ID
		System
		Type
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-ko
		ko-ja
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		ATLAS V14 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PAT-Transer 2009 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		RBMT X
		J-Beijing 7 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		Hohrai 2011 (Commercial system)
		RBMT
		✓
		✓
		✓
		RBMT X
		J Soul 9 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		Korai 2011 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		AIAYN
		Google’s implementation of “Attention Is All You Need”
		NMT
		✓
		✓
		Table 11: Baseline Systems I
		7
		JIJI
		IITB
		ALT
		System ID
		System
		Type
		ja-en
		en-ja
		hi-en
		en-hi
		hi-ja
		ja-hi
		my-en
		en-my
		km-en
		en-km
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		Table 12: Baseline Systems II
		NewsCommentary
		TDDC
		EnTam
		Multimodal
		System ID
		System
		Type
		ru-ja
		ja-ru
		ja-en
		ta-en
		en-ta
		en-hi
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		NMT T2T
		Tensor2Tensor’s Transformer
		NMT
		✓
		✓
		✓
		✓
		NMT OT
		OpenNMT-py’s Transformer
		NMT
		✓
		Online X
		Azure Custom Translator
		Other
		✓
		Table 13: Baseline Systems III
		8
		model) for Chinese segmentation
		Fine-tuned
		Based on historical data, comparable assets and estimates in the engineering report
		Table 13: Example translations of baseline and ﬁne-tuned system: Example was picked from devtest set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 13 shows the example
		translations of the baseline and ﬁne-tuned sys-
		tems10
	</Extractive Summary>
	<Extractive Summary> =
		We found a sen-
		tence pair on the English side that is identical as
		the reference in Table 13, and the Japanese side
		is also quite similar
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =14>
	<Abstractive Summary> =
		(–20%)
		Table 14: The JPO adequacy criterion
		confidence interval
		Japanese
		実績値、類似建物の修繕費水準、ER の修繕更新費等を考慮し査定
		English
		Based on historical data, comparable assets and estimates in the engineering report
		Table 14: Example of sentence pair contained in the training set
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Evaluation Criterion
		Table 14 shows the JPO adequacy criterion
		from 5 to 1
	</Extractive Summary>
	<Extractive Summary> =
		Table 14 shows the
		sentence pair in the training set that was the most
		similar to the previous example
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =15>
	<Abstractive Summary> =
		,Ltd
		China
		sarah
		Rakuten Institute of Technology
		Japan
		683
		National Institute of Technology Silchar
		India
		KNU_Hyundai
		Kangwon National University
		Korea
		NITSNLP
		National Institute of Technology Silchar
		India
		ryan
		Kakao Brain
		Korea
		PUP-IND
		Punjabi University Patiala
		India
		FBAI
		Facebook AI Research
		USA
		AISTAI
		National Institute of Advanced Industrial Science and Technology
		Japan
		SYSTRAN
		SYSTRAN
		France
		NHK-NES
		NHK & NHK Engineering System
		Japan
		geoduck
		Microsoft Research
		USA
		LTRC-MT
		IIIT Hyderabad
		India
		ykkd
		The University of Tokyo
		Japan
		IDIAP
		Idiap Research Institute
		Switzerland
		NLPRL
		Indian Institute of Technology (BHU) Varanasi
		India
		Table 15: List of participants in WAT2019
		Team ID
		ASPEC
		JPC
		TDDC
		JIJI
		NCPD
		EJ
		JE
		CJ
		JC
		EJ
		JE
		CJ
		JC
		Ko-J
		J-Ko
		JE
		EJ
		JE
		RJ
		JR
		TMU
		✓
		✓
		NTT
		✓
		✓
		✓
		NICT-2
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		srcb
		✓
		✓
		✓
		✓
		sarah
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		KNU_Hyundai
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		ryan
		✓
		✓
		✓
		✓
		✓
		✓
		AISTAI
		✓
		SYSTRAN
		✓
		✓
		NHK-NES
		✓
		✓
		geoduck
		✓
		ykkd
		✓
		Team ID
		Mixed-domain tasks
		Mutimodal task
		ALT
		IITB
		UFAL (EnTam)
		EV/CH
		EM
		ME
		E-Kh
		Kh-E
		EH
		HE
		ET
		TE
		EH
		NICT
		✓
		✓
		NICT-4
		✓
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		UCSYNLP
		✓
		✓
		UCSMNLP
		✓
		✓
		cvit
		✓
		✓
		✓
		✓
		sarah
		✓
		683
		✓
		NITSNLP
		✓
		PUP-IND
		✓
		FBAI
		✓
		✓
		LTRC-MT
		✓
		IDIAP
		✓
		NLPRL
		✓
		✓
		Table 16: Submissions for each task by each team
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Participants
		Table 15 shows the participants in WAT2019
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =16>
	<Abstractive Summary> =
		,Ltd
		China
		sarah
		Rakuten Institute of Technology
		Japan
		683
		National Institute of Technology Silchar
		India
		KNU_Hyundai
		Kangwon National University
		Korea
		NITSNLP
		National Institute of Technology Silchar
		India
		ryan
		Kakao Brain
		Korea
		PUP-IND
		Punjabi University Patiala
		India
		FBAI
		Facebook AI Research
		USA
		AISTAI
		National Institute of Advanced Industrial Science and Technology
		Japan
		SYSTRAN
		SYSTRAN
		France
		NHK-NES
		NHK & NHK Engineering System
		Japan
		geoduck
		Microsoft Research
		USA
		LTRC-MT
		IIIT Hyderabad
		India
		ykkd
		The University of Tokyo
		Japan
		IDIAP
		Idiap Research Institute
		Switzerland
		NLPRL
		Indian Institute of Technology (BHU) Varanasi
		India
		Table 15: List of participants in WAT2019
		Team ID
		ASPEC
		JPC
		TDDC
		JIJI
		NCPD
		EJ
		JE
		CJ
		JC
		EJ
		JE
		CJ
		JC
		Ko-J
		J-Ko
		JE
		EJ
		JE
		RJ
		JR
		TMU
		✓
		✓
		NTT
		✓
		✓
		✓
		NICT-2
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		srcb
		✓
		✓
		✓
		✓
		sarah
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		KNU_Hyundai
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		ryan
		✓
		✓
		✓
		✓
		✓
		✓
		AISTAI
		✓
		SYSTRAN
		✓
		✓
		NHK-NES
		✓
		✓
		geoduck
		✓
		ykkd
		✓
		Team ID
		Mixed-domain tasks
		Mutimodal task
		ALT
		IITB
		UFAL (EnTam)
		EV/CH
		EM
		ME
		E-Kh
		Kh-E
		EH
		HE
		ET
		TE
		EH
		NICT
		✓
		✓
		NICT-4
		✓
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		UCSYNLP
		✓
		✓
		UCSMNLP
		✓
		✓
		cvit
		✓
		✓
		✓
		✓
		sarah
		✓
		683
		✓
		NITSNLP
		✓
		PUP-IND
		✓
		FBAI
		✓
		✓
		LTRC-MT
		✓
		IDIAP
		✓
		NLPRL
		✓
		✓
		Table 16: Submissions for each task by each team
	</Abstractive Summary>
	<Extractive Summary> =
		Table 16 shows tasks for which
		each team submitted results by the deadline
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =17>
	<Abstractive Summary> =
		42
		Table 17: JPO adequacy evaluation results in detail (1)
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =18>
	<Abstractive Summary> =
		22
		Table 18: JPO adequacy evaluation results in detail (2)
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =19>
	<Abstractive Summary> =
		94
		Table 19: Manual evaluation result for WAT Multi-Modal Tasks
	</Abstractive Summary>
	<Extractive Summary> =
		For multi-modal task, Table 19 shows the
		manual evaluation scores for all valid system
		submissions
	</Extractive Summary>
	<Extractive Summary> =
		Table 19 thus lists also the
		“Reference”
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =20>
	<Abstractive Summary> =
		00
		Table 20: Multi-Modal Task automatic evaluation results
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =21>
	<Abstractive Summary> =
		On the other hand, some translations seem
		to be fitted to sentences of TDDC which are
		27
		KNU_Hyundai (3173)
		NICT-2 (3086)
		srcb (3205)
		NTT (3225)
		- ≫ ≫
		KNU_Hyundai (3173)
		- ≫
		NICT-2 (3086)
		>
		NICT-2 (3182)
		srcb (3212)
		AISTAI (3251)
		KNU_Hyundai (3172)
		NTT (3236)
		≫ ≫ ≫ ≫
		NICT-2 (3182)
		-
		≫ ≫
		srcb (3212)
		> ≫
		AISTAI (3251)
		-
		Table 21: Statistical significance testing of the aspec-ja-en (left) and aspec-en-ja (right) Pairwise scores
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Statistical Significance Testing of
		Pairwise Evaluation between
		Submissions
		Table 21 shows the results of statistical signifi-
		cance testing of aspec-ja-en subtasks, Table 22
		shows that of JIJI subtasks, Table 23 shows
		that of TDDC subtasks
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =22>
	<Abstractive Summary> =
		NHK-NES (2883)
		sarah (2793)
		sarah (2813)
		NHK-NES (2884) ≫ ≫ ≫
		NHK-NES (2883)
		≫ ≫
		sarah (2793)
		≫
		NHK-NES (2885)
		sarah (2814)
		sarah (2815)
		NHK-NES (2886) ≫ ≫ ≫
		NHK-NES (2885)
		≫ ≫
		sarah (2814)
		≫
		Table 22: Statistical significance testing of the jiji-ja-en (left) and jiji-en-ja (right) Pairwise scores
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Statistical Significance Testing of
		Pairwise Evaluation between
		Submissions
		Table 21 shows the results of statistical signifi-
		cance testing of aspec-ja-en subtasks, Table 22
		shows that of JIJI subtasks, Table 23 shows
		that of TDDC subtasks
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =23>
	<Abstractive Summary> =
		sarah (2807)
		NTT (3002)
		NICT-2 (3081)
		sarah (2811)
		geoduck (3197)
		geoduck (3216)
		ORGANIZER (3264) ≫ ≫ ≫ ≫ ≫ ≫
		sarah (2807)
		≫ ≫ ≫ ≫ ≫
		NTT (3002)
		-
		≫ ≫ ≫
		NICT-2 (3081)
		≫ ≫ ≫
		sarah (2811)
		-
		≫
		geoduck (3197)
		≫
		NTT (3005)
		sarah (2808)
		NICT-2 (3084)
		sarah (2812)
		geoduck (3200)
		geoduck (3217)
		ORGANIZER (3265)
		- ≫ ≫ ≫ ≫ ≫
		NTT (3005)
		≫ ≫ ≫ ≫ ≫
		sarah (2808)
		-
		≫ ≫ ≫
		NICT-2 (3084)
		≫ ≫ ≫
		sarah (2812)
		≫ ≫
		geoduck (3200)
		≫
		Table 23: Statistical significance testing of the tddc-itm-ja-en (left) and tddc-txt-ja-en (right) Pairwise
		scores
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Statistical Significance Testing of
		Pairwise Evaluation between
		Submissions
		Table 21 shows the results of statistical signifi-
		cance testing of aspec-ja-en subtasks, Table 22
		shows that of JIJI subtasks, Table 23 shows
		that of TDDC subtasks
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =24>
	<Abstractive Summary> =
		185
		Table 24: The Fleiss’ kappa values for the pairwise evaluation results
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =25>
	<Abstractive Summary> =
		0 %)
		Table 25: Appropriateness of source English cap-
		tions in the 4032 assessments collected for the
		multi-modal track
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 25 indicate that for a
		surprisingly high number of items we did not
		receive any answer
	</Extractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =26>
	<Abstractive Summary> =
		00
		Table 26: ASPEC en-ja submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		1901
		NMT
		NO
		26
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =27>
	<Abstractive Summary> =
		500
		Table 27: ASPEC ja-en submissions
		33
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		3262
		NMT
		NO
		38
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =28>
	<Abstractive Summary> =
		500
		Table 28: TDDC ITM ja-en submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		3263
		NMT
		NO
		24
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =29>
	<Abstractive Summary> =
		250
		Table 29: TDDC TXT ja-en submissions
		34
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		juman
		kytea
		mecab
		juman
		kytea
		mecab
		juman
		kytea
		mecab
		NMT
		1904
		NMT
		NO
		16
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =30>
	<Abstractive Summary> =
		250
		Table 30: JIJI en-ja submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		1905
		NMT
		NO
		16
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument764> <Table ID =31>
	<Abstractive Summary> =
		750
		Table 31: JIJI ja-en submissions
		35
		Proceedings of the 6th Workshop on Asian Translation, pages 36–44
		Hong Kong, China, November 4, 2019
	</Abstractive Summary>
</Paper ID=ument764>


<Paper ID=ument766> <Table ID =1>
	<Abstractive Summary> =
		©️2019 Association for Computational Linguistics
		Lang
		Train
		Dev
		DevTest
		Test
		JE
		3,008,500
		1,790
		1,784
		1,812
		JC
		672,315
		2,090
		2,148
		2,107
		Table 1: Statistics for ASPEC
		• Domain and language pairs
		WAT is the world’s first workshop that
		targets scientific paper domain, and Chi-
		nese↔Japanese and
		Korean↔Japanese
		language pairs
	</Abstractive Summary>
	<Extractive Summary> =
		8k
		Table 10:
		In-Domain data for the Russian–
		Japanese task
	</Extractive Summary>
	<Extractive Summary> =
		Refer to
		Table 10 for the statistics of the in-domain
		parallel corpora
	</Extractive Summary>
	<Extractive Summary> =
		shtml
		ASPEC
		JPC
		System ID
		System
		Type
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-ko
		ko-ja
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		ATLAS V14 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PAT-Transer 2009 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		RBMT X
		J-Beijing 7 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		Hohrai 2011 (Commercial system)
		RBMT
		✓
		✓
		✓
		RBMT X
		J Soul 9 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		Korai 2011 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		AIAYN
		Google’s implementation of “Attention Is All You Need”
		NMT
		✓
		✓
		Table 11: Baseline Systems I
		7
		JIJI
		IITB
		ALT
		System ID
		System
		Type
		ja-en
		en-ja
		hi-en
		en-hi
		hi-ja
		ja-hi
		my-en
		en-my
		km-en
		en-km
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		Table 12: Baseline Systems II
		NewsCommentary
		TDDC
		EnTam
		Multimodal
		System ID
		System
		Type
		ru-ja
		ja-ru
		ja-en
		ta-en
		en-ta
		en-hi
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		NMT T2T
		Tensor2Tensor’s Transformer
		NMT
		✓
		✓
		✓
		✓
		NMT OT
		OpenNMT-py’s Transformer
		NMT
		✓
		Online X
		Azure Custom Translator
		Other
		✓
		Table 13: Baseline Systems III
		8
		model) for Chinese segmentation
	</Extractive Summary>
	<Extractive Summary> =
		(–20%)
		Table 14: The JPO adequacy criterion
		confidence interval
	</Extractive Summary>
	<Extractive Summary> =
		2
		Evaluation Criterion
		Table 14 shows the JPO adequacy criterion
		from 5 to 1
	</Extractive Summary>
	<Extractive Summary> =
		6
		Participants
		Table 15 shows the participants in WAT2019
	</Extractive Summary>
	<Extractive Summary> =
		Table 16 shows tasks for which
		each team submitted results by the deadline
	</Extractive Summary>
	<Extractive Summary> =
		For multi-modal task, Table 19 shows the
		manual evaluation scores for all valid system
		submissions
	</Extractive Summary>
	<Extractive Summary> =
		,Ltd
		China
		sarah
		Rakuten Institute of Technology
		Japan
		683
		National Institute of Technology Silchar
		India
		KNU_Hyundai
		Kangwon National University
		Korea
		NITSNLP
		National Institute of Technology Silchar
		India
		ryan
		Kakao Brain
		Korea
		PUP-IND
		Punjabi University Patiala
		India
		FBAI
		Facebook AI Research
		USA
		AISTAI
		National Institute of Advanced Industrial Science and Technology
		Japan
		SYSTRAN
		SYSTRAN
		France
		NHK-NES
		NHK & NHK Engineering System
		Japan
		geoduck
		Microsoft Research
		USA
		LTRC-MT
		IIIT Hyderabad
		India
		ykkd
		The University of Tokyo
		Japan
		IDIAP
		Idiap Research Institute
		Switzerland
		NLPRL
		Indian Institute of Technology (BHU) Varanasi
		India
		Table 15: List of participants in WAT2019
		Team ID
		ASPEC
		JPC
		TDDC
		JIJI
		NCPD
		EJ
		JE
		CJ
		JC
		EJ
		JE
		CJ
		JC
		Ko-J
		J-Ko
		JE
		EJ
		JE
		RJ
		JR
		TMU
		✓
		✓
		NTT
		✓
		✓
		✓
		NICT-2
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		srcb
		✓
		✓
		✓
		✓
		sarah
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		KNU_Hyundai
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		ryan
		✓
		✓
		✓
		✓
		✓
		✓
		AISTAI
		✓
		SYSTRAN
		✓
		✓
		NHK-NES
		✓
		✓
		geoduck
		✓
		ykkd
		✓
		Team ID
		Mixed-domain tasks
		Mutimodal task
		ALT
		IITB
		UFAL (EnTam)
		EV/CH
		EM
		ME
		E-Kh
		Kh-E
		EH
		HE
		ET
		TE
		EH
		NICT
		✓
		✓
		NICT-4
		✓
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		UCSYNLP
		✓
		✓
		UCSMNLP
		✓
		✓
		cvit
		✓
		✓
		✓
		✓
		sarah
		✓
		683
		✓
		NITSNLP
		✓
		PUP-IND
		✓
		FBAI
		✓
		✓
		LTRC-MT
		✓
		IDIAP
		✓
		NLPRL
		✓
		✓
		Table 16: Submissions for each task by each team
	</Extractive Summary>
	<Extractive Summary> =
		42
		Table 17: JPO adequacy evaluation results in detail (1)
	</Extractive Summary>
	<Extractive Summary> =
		22
		Table 18: JPO adequacy evaluation results in detail (2)
	</Extractive Summary>
	<Extractive Summary> =
		94
		Table 19: Manual evaluation result for WAT Multi-Modal Tasks
	</Extractive Summary>
	<Extractive Summary> =
		Table 19 thus lists also the
		“Reference”
	</Extractive Summary>
	<Extractive Summary> =
		The tables
		also include results by the organizers’ base-
		lines, which are listed in Table 13
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =2>
	<Abstractive Summary> =
		The correspondence be-
		tween field IDs and field names, along with the
		Lang
		Train
		Dev
		DevTest
		Test-N
		zh-ja
		1,000,000
		2,000
		2,000
		5,204
		ko-ja
		1,000,000
		2,000
		2,000
		5,230
		en-ja
		1,000,000
		2,000
		2,000
		5,668
		Lang
		Test-N1
		Test-N2
		Test-N3
		Test-EP
		zh-ja
		2,000
		3,000
		204
		1,151
		ko-ja
		2,000
		3,000
		230
		–
		en-ja
		2,000
		3,000
		668
		–
		Table 2: Statistics for JPC
		frequency and occurrence ratios for the train-
		ing data, are described in the README file
		of ASPEC-JE
	</Abstractive Summary>
	<Extractive Summary> =
		The automatic scores for the multi-modal
		task along with the WAT evaluation server
		BLEU scores are provided in Table 20
	</Extractive Summary>
	<Extractive Summary> =
		00
		Table 20: Multi-Modal Task automatic evaluation results
	</Extractive Summary>
	<Extractive Summary> =
		2
		Statistical Significance Testing of
		Pairwise Evaluation between
		Submissions
		Table 21 shows the results of statistical signifi-
		cance testing of aspec-ja-en subtasks, Table 22
		shows that of JIJI subtasks, Table 23 shows
		that of TDDC subtasks
	</Extractive Summary>
	<Extractive Summary> =
		The results are shown
		in Table 24
	</Extractive Summary>
	<Extractive Summary> =
		On the other hand, some translations seem
		to be fitted to sentences of TDDC which are
		27
		KNU_Hyundai (3173)
		NICT-2 (3086)
		srcb (3205)
		NTT (3225)
		- ≫ ≫
		KNU_Hyundai (3173)
		- ≫
		NICT-2 (3086)
		>
		NICT-2 (3182)
		srcb (3212)
		AISTAI (3251)
		KNU_Hyundai (3172)
		NTT (3236)
		≫ ≫ ≫ ≫
		NICT-2 (3182)
		-
		≫ ≫
		srcb (3212)
		> ≫
		AISTAI (3251)
		-
		Table 21: Statistical significance testing of the aspec-ja-en (left) and aspec-en-ja (right) Pairwise scores
	</Extractive Summary>
	<Extractive Summary> =
		NHK-NES (2883)
		sarah (2793)
		sarah (2813)
		NHK-NES (2884) ≫ ≫ ≫
		NHK-NES (2883)
		≫ ≫
		sarah (2793)
		≫
		NHK-NES (2885)
		sarah (2814)
		sarah (2815)
		NHK-NES (2886) ≫ ≫ ≫
		NHK-NES (2885)
		≫ ≫
		sarah (2814)
		≫
		Table 22: Statistical significance testing of the jiji-ja-en (left) and jiji-en-ja (right) Pairwise scores
	</Extractive Summary>
	<Extractive Summary> =
		sarah (2807)
		NTT (3002)
		NICT-2 (3081)
		sarah (2811)
		geoduck (3197)
		geoduck (3216)
		ORGANIZER (3264) ≫ ≫ ≫ ≫ ≫ ≫
		sarah (2807)
		≫ ≫ ≫ ≫ ≫
		NTT (3002)
		-
		≫ ≫ ≫
		NICT-2 (3081)
		≫ ≫ ≫
		sarah (2811)
		-
		≫
		geoduck (3197)
		≫
		NTT (3005)
		sarah (2808)
		NICT-2 (3084)
		sarah (2812)
		geoduck (3200)
		geoduck (3217)
		ORGANIZER (3265)
		- ≫ ≫ ≫ ≫ ≫
		NTT (3005)
		≫ ≫ ≫ ≫ ≫
		sarah (2808)
		-
		≫ ≫ ≫
		NICT-2 (3084)
		≫ ≫ ≫
		sarah (2812)
		≫ ≫
		geoduck (3200)
		≫
		Table 23: Statistical significance testing of the tddc-itm-ja-en (left) and tddc-txt-ja-en (right) Pairwise
		scores
	</Extractive Summary>
	<Extractive Summary> =
		185
		Table 24: The Fleiss’ kappa values for the pairwise evaluation results
	</Extractive Summary>
	<Extractive Summary> =
		0 %)
		Table 25: Appropriateness of source English cap-
		tions in the 4032 assessments collected for the
		multi-modal track
	</Extractive Summary>
	<Extractive Summary> =
		The results in Table 25 indicate that for a
		surprisingly high number of items we did not
		receive any answer
	</Extractive Summary>
	<Extractive Summary> =
		00
		Table 26: ASPEC en-ja submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		1901
		NMT
		NO
		26
	</Extractive Summary>
	<Extractive Summary> =
		500
		Table 27: ASPEC ja-en submissions
		33
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		3262
		NMT
		NO
		38
	</Extractive Summary>
	<Extractive Summary> =
		500
		Table 28: TDDC ITM ja-en submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		3263
		NMT
		NO
		24
	</Extractive Summary>
	<Extractive Summary> =
		250
		Table 29: TDDC TXT ja-en submissions
		34
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		juman
		kytea
		mecab
		juman
		kytea
		mecab
		juman
		kytea
		mecab
		NMT
		1904
		NMT
		NO
		16
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =3>
	<Abstractive Summary> =
		The
		corpus consists of Chinese-Japanese, Korean-
		Japanese and English-Japanese patent de-
		scriptions whose International Patent Classi-
		2
		Disclosure
		Train
		Dev
		DevTest
		Test
		Period
		Texts
		Items
		Texts
		Items
		Texts
		Items
		2016-01-01 to
		1,089,346
		-
		-
		-
		-
		-
		-
		2017-12-31
		(614,817)
		-
		-
		-
		-
		-
		-
		2018-01-01 to
		314,649
		1,153
		2,845
		1,114
		2,900
		1,153
		2,129
		2018-06-30
		(218,495)
		(1,148)
		(2,650)
		(1,111)
		(2,671)
		(1,135)
		(1,763)
		Table 3: Statistics for TDDC (The number of unique sentences)
		fication (IPC) sections are chemistry, electric-
		ity, mechanical engineering, and physics
	</Abstractive Summary>
	<Extractive Summary> =
		250
		Table 30: JIJI en-ja submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		1905
		NMT
		NO
		16
	</Extractive Summary>
	<Extractive Summary> =
		750
		Table 31: JIJI ja-en submissions
		35
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =4>
	<Abstractive Summary> =
		Therefore, the set of source documents for
		training, development, development-test and
		3
		Lang
		Train
		Dev
		DevTest
		Test
		en-ja
		200,000
		2,000
		2,000
		2,000
		Table 4: Statistics for JIJI Corpus
		Lang
		Train
		Dev
		Test
		Mono
		hi-en
		1,492,827
		520
		2,507
		–
		hi-ja
		152,692
		1,566
		2,000
		–
		hi
		–
		–
		–
		45,075,279
		Table 5: Statistics for IITB Corpus
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =5>
	<Abstractive Summary> =
		Therefore, the set of source documents for
		training, development, development-test and
		3
		Lang
		Train
		Dev
		DevTest
		Test
		en-ja
		200,000
		2,000
		2,000
		2,000
		Table 4: Statistics for JIJI Corpus
		Lang
		Train
		Dev
		Test
		Mono
		hi-en
		1,492,827
		520
		2,507
		–
		hi-ja
		152,692
		1,566
		2,000
		–
		hi
		–
		–
		–
		45,075,279
		Table 5: Statistics for IITB Corpus
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =6>
	<Abstractive Summary> =
		6
		ALT and UCSY Corpus
		The parallel data for Myanmar-English trans-
		lation tasks at WAT2019 consists of two cor-
		Corpus
		Train
		Dev
		Test
		ALT
		18,088
		1,000
		1,018
		UCSY
		204,539
		–
		–
		All
		222,627
		1,000
		1,018
		Table 6: Statistics for the data used in Myanmar-
		English translation tasks
		pora, the ALT corpus and UCSY corpus
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =7>
	<Abstractive Summary> =
		4
		Corpus
		Train
		Dev
		Test
		ALT
		18,088
		1,000
		1,018
		ECCC
		104,660
		–
		–
		All
		122,748
		1,000
		1,018
		Table 7: Statistics for the data used in Khmer-
		English translation tasks
		• The ECCC corpus consists of 100 thou-
		sand Khmer-English parallel sentences
		extracted from document pairs of Khmer-
		English bi-lingual records in Extraordi-
		nary Chambers in the Court of Cam-
		bodia, collected by National Institute of
		Posts, Telecoms & ICT, Cambodia
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =8>
	<Abstractive Summary> =
		C-Test was pro-
		vided only for the task itself: the source side
		Tokens
		Dataset
		Items
		English
		Hindi
		Training Set
		28,932
		143,178
		136,722
		D-Test
		998
		4,922
		4,695
		E-Test (EV)
		1,595
		7,852
		7,535
		C-Test (CH)
		1,400
		8,185
		8,665
		Table 8: Data for the English→Hindi multi-modal
		translation task
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =9>
	<Abstractive Summary> =
		43%)
		2,834,970
		2,104,896
		total
		169,871
		3,984,038
		2,776,397
		Table 9: Data for the Tamil↔English task
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =10>
	<Abstractive Summary> =
		8k
		Table 10:
		In-Domain data for the Russian–
		Japanese task
	</Abstractive Summary>
	<Extractive Summary> =
		Refer to
		Table 10 for the statistics of the in-domain
		parallel corpora
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =11>
	<Abstractive Summary> =
		shtml
		ASPEC
		JPC
		System ID
		System
		Type
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-ko
		ko-ja
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		ATLAS V14 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PAT-Transer 2009 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		RBMT X
		J-Beijing 7 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		Hohrai 2011 (Commercial system)
		RBMT
		✓
		✓
		✓
		RBMT X
		J Soul 9 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		Korai 2011 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		AIAYN
		Google’s implementation of “Attention Is All You Need”
		NMT
		✓
		✓
		Table 11: Baseline Systems I
		7
		JIJI
		IITB
		ALT
		System ID
		System
		Type
		ja-en
		en-ja
		hi-en
		en-hi
		hi-ja
		ja-hi
		my-en
		en-my
		km-en
		en-km
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		Table 12: Baseline Systems II
		NewsCommentary
		TDDC
		EnTam
		Multimodal
		System ID
		System
		Type
		ru-ja
		ja-ru
		ja-en
		ta-en
		en-ta
		en-hi
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		NMT T2T
		Tensor2Tensor’s Transformer
		NMT
		✓
		✓
		✓
		✓
		NMT OT
		OpenNMT-py’s Transformer
		NMT
		✓
		Online X
		Azure Custom Translator
		Other
		✓
		Table 13: Baseline Systems III
		8
		model) for Chinese segmentation
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =12>
	<Abstractive Summary> =
		shtml
		ASPEC
		JPC
		System ID
		System
		Type
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-ko
		ko-ja
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		ATLAS V14 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PAT-Transer 2009 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		RBMT X
		J-Beijing 7 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		Hohrai 2011 (Commercial system)
		RBMT
		✓
		✓
		✓
		RBMT X
		J Soul 9 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		Korai 2011 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		AIAYN
		Google’s implementation of “Attention Is All You Need”
		NMT
		✓
		✓
		Table 11: Baseline Systems I
		7
		JIJI
		IITB
		ALT
		System ID
		System
		Type
		ja-en
		en-ja
		hi-en
		en-hi
		hi-ja
		ja-hi
		my-en
		en-my
		km-en
		en-km
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		Table 12: Baseline Systems II
		NewsCommentary
		TDDC
		EnTam
		Multimodal
		System ID
		System
		Type
		ru-ja
		ja-ru
		ja-en
		ta-en
		en-ta
		en-hi
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		NMT T2T
		Tensor2Tensor’s Transformer
		NMT
		✓
		✓
		✓
		✓
		NMT OT
		OpenNMT-py’s Transformer
		NMT
		✓
		Online X
		Azure Custom Translator
		Other
		✓
		Table 13: Baseline Systems III
		8
		model) for Chinese segmentation
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =13>
	<Abstractive Summary> =
		shtml
		ASPEC
		JPC
		System ID
		System
		Type
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-en
		en-ja
		ja-zh
		zh-ja
		ja-ko
		ko-ja
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		✓
		✓
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		ATLAS V14 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PAT-Transer 2009 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		RBMT X
		J-Beijing 7 (Commercial system)
		RBMT
		✓
		✓
		✓
		✓
		RBMT X
		Hohrai 2011 (Commercial system)
		RBMT
		✓
		✓
		✓
		RBMT X
		J Soul 9 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		Korai 2011 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		AIAYN
		Google’s implementation of “Attention Is All You Need”
		NMT
		✓
		✓
		Table 11: Baseline Systems I
		7
		JIJI
		IITB
		ALT
		System ID
		System
		Type
		ja-en
		en-ja
		hi-en
		en-hi
		hi-ja
		ja-hi
		my-en
		en-my
		km-en
		en-km
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Phrase
		Moses’ Phrase-based SMT
		SMT
		✓
		✓
		✓
		✓
		✓
		✓
		SMT Hiero
		Moses’ Hierarchical Phrase-based SMT
		SMT
		✓
		✓
		SMT S2T
		Moses’ String-to-Tree Syntax-based SMT and Berkeley parser
		SMT
		✓
		SMT T2S
		Moses’ Tree-to-String Syntax-based SMT and Berkeley parser
		SMT
		✓
		RBMT X
		The Honyaku V15 (Commercial system)
		RBMT
		✓
		✓
		RBMT X
		PC-Transer V13 (Commercial system)
		RBMT
		✓
		✓
		Online X
		Google translate
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Online X
		Bing translator
		Other
		✓
		✓
		✓
		✓
		✓
		✓
		Table 12: Baseline Systems II
		NewsCommentary
		TDDC
		EnTam
		Multimodal
		System ID
		System
		Type
		ru-ja
		ja-ru
		ja-en
		ta-en
		en-ta
		en-hi
		NMT
		OpenNMT’s NMT with attention
		NMT
		✓
		NMT T2T
		Tensor2Tensor’s Transformer
		NMT
		✓
		✓
		✓
		✓
		NMT OT
		OpenNMT-py’s Transformer
		NMT
		✓
		Online X
		Azure Custom Translator
		Other
		✓
		Table 13: Baseline Systems III
		8
		model) for Chinese segmentation
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =14>
	<Abstractive Summary> =
		(–20%)
		Table 14: The JPO adequacy criterion
		confidence interval
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Evaluation Criterion
		Table 14 shows the JPO adequacy criterion
		from 5 to 1
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =15>
	<Abstractive Summary> =
		,Ltd
		China
		sarah
		Rakuten Institute of Technology
		Japan
		683
		National Institute of Technology Silchar
		India
		KNU_Hyundai
		Kangwon National University
		Korea
		NITSNLP
		National Institute of Technology Silchar
		India
		ryan
		Kakao Brain
		Korea
		PUP-IND
		Punjabi University Patiala
		India
		FBAI
		Facebook AI Research
		USA
		AISTAI
		National Institute of Advanced Industrial Science and Technology
		Japan
		SYSTRAN
		SYSTRAN
		France
		NHK-NES
		NHK & NHK Engineering System
		Japan
		geoduck
		Microsoft Research
		USA
		LTRC-MT
		IIIT Hyderabad
		India
		ykkd
		The University of Tokyo
		Japan
		IDIAP
		Idiap Research Institute
		Switzerland
		NLPRL
		Indian Institute of Technology (BHU) Varanasi
		India
		Table 15: List of participants in WAT2019
		Team ID
		ASPEC
		JPC
		TDDC
		JIJI
		NCPD
		EJ
		JE
		CJ
		JC
		EJ
		JE
		CJ
		JC
		Ko-J
		J-Ko
		JE
		EJ
		JE
		RJ
		JR
		TMU
		✓
		✓
		NTT
		✓
		✓
		✓
		NICT-2
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		srcb
		✓
		✓
		✓
		✓
		sarah
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		KNU_Hyundai
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		ryan
		✓
		✓
		✓
		✓
		✓
		✓
		AISTAI
		✓
		SYSTRAN
		✓
		✓
		NHK-NES
		✓
		✓
		geoduck
		✓
		ykkd
		✓
		Team ID
		Mixed-domain tasks
		Mutimodal task
		ALT
		IITB
		UFAL (EnTam)
		EV/CH
		EM
		ME
		E-Kh
		Kh-E
		EH
		HE
		ET
		TE
		EH
		NICT
		✓
		✓
		NICT-4
		✓
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		UCSYNLP
		✓
		✓
		UCSMNLP
		✓
		✓
		cvit
		✓
		✓
		✓
		✓
		sarah
		✓
		683
		✓
		NITSNLP
		✓
		PUP-IND
		✓
		FBAI
		✓
		✓
		LTRC-MT
		✓
		IDIAP
		✓
		NLPRL
		✓
		✓
		Table 16: Submissions for each task by each team
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Participants
		Table 15 shows the participants in WAT2019
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =16>
	<Abstractive Summary> =
		,Ltd
		China
		sarah
		Rakuten Institute of Technology
		Japan
		683
		National Institute of Technology Silchar
		India
		KNU_Hyundai
		Kangwon National University
		Korea
		NITSNLP
		National Institute of Technology Silchar
		India
		ryan
		Kakao Brain
		Korea
		PUP-IND
		Punjabi University Patiala
		India
		FBAI
		Facebook AI Research
		USA
		AISTAI
		National Institute of Advanced Industrial Science and Technology
		Japan
		SYSTRAN
		SYSTRAN
		France
		NHK-NES
		NHK & NHK Engineering System
		Japan
		geoduck
		Microsoft Research
		USA
		LTRC-MT
		IIIT Hyderabad
		India
		ykkd
		The University of Tokyo
		Japan
		IDIAP
		Idiap Research Institute
		Switzerland
		NLPRL
		Indian Institute of Technology (BHU) Varanasi
		India
		Table 15: List of participants in WAT2019
		Team ID
		ASPEC
		JPC
		TDDC
		JIJI
		NCPD
		EJ
		JE
		CJ
		JC
		EJ
		JE
		CJ
		JC
		Ko-J
		J-Ko
		JE
		EJ
		JE
		RJ
		JR
		TMU
		✓
		✓
		NTT
		✓
		✓
		✓
		NICT-2
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		srcb
		✓
		✓
		✓
		✓
		sarah
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		KNU_Hyundai
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		ryan
		✓
		✓
		✓
		✓
		✓
		✓
		AISTAI
		✓
		SYSTRAN
		✓
		✓
		NHK-NES
		✓
		✓
		geoduck
		✓
		ykkd
		✓
		Team ID
		Mixed-domain tasks
		Mutimodal task
		ALT
		IITB
		UFAL (EnTam)
		EV/CH
		EM
		ME
		E-Kh
		Kh-E
		EH
		HE
		ET
		TE
		EH
		NICT
		✓
		✓
		NICT-4
		✓
		✓
		✓
		✓
		NICT-5
		✓
		✓
		✓
		UCSYNLP
		✓
		✓
		UCSMNLP
		✓
		✓
		cvit
		✓
		✓
		✓
		✓
		sarah
		✓
		683
		✓
		NITSNLP
		✓
		PUP-IND
		✓
		FBAI
		✓
		✓
		LTRC-MT
		✓
		IDIAP
		✓
		NLPRL
		✓
		✓
		Table 16: Submissions for each task by each team
	</Abstractive Summary>
	<Extractive Summary> =
		Table 16 shows tasks for which
		each team submitted results by the deadline
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =17>
	<Abstractive Summary> =
		42
		Table 17: JPO adequacy evaluation results in detail (1)
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =18>
	<Abstractive Summary> =
		22
		Table 18: JPO adequacy evaluation results in detail (2)
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =19>
	<Abstractive Summary> =
		94
		Table 19: Manual evaluation result for WAT Multi-Modal Tasks
	</Abstractive Summary>
	<Extractive Summary> =
		For multi-modal task, Table 19 shows the
		manual evaluation scores for all valid system
		submissions
	</Extractive Summary>
	<Extractive Summary> =
		Table 19 thus lists also the
		“Reference”
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =20>
	<Abstractive Summary> =
		00
		Table 20: Multi-Modal Task automatic evaluation results
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =21>
	<Abstractive Summary> =
		On the other hand, some translations seem
		to be fitted to sentences of TDDC which are
		27
		KNU_Hyundai (3173)
		NICT-2 (3086)
		srcb (3205)
		NTT (3225)
		- ≫ ≫
		KNU_Hyundai (3173)
		- ≫
		NICT-2 (3086)
		>
		NICT-2 (3182)
		srcb (3212)
		AISTAI (3251)
		KNU_Hyundai (3172)
		NTT (3236)
		≫ ≫ ≫ ≫
		NICT-2 (3182)
		-
		≫ ≫
		srcb (3212)
		> ≫
		AISTAI (3251)
		-
		Table 21: Statistical significance testing of the aspec-ja-en (left) and aspec-en-ja (right) Pairwise scores
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Statistical Significance Testing of
		Pairwise Evaluation between
		Submissions
		Table 21 shows the results of statistical signifi-
		cance testing of aspec-ja-en subtasks, Table 22
		shows that of JIJI subtasks, Table 23 shows
		that of TDDC subtasks
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =22>
	<Abstractive Summary> =
		NHK-NES (2883)
		sarah (2793)
		sarah (2813)
		NHK-NES (2884) ≫ ≫ ≫
		NHK-NES (2883)
		≫ ≫
		sarah (2793)
		≫
		NHK-NES (2885)
		sarah (2814)
		sarah (2815)
		NHK-NES (2886) ≫ ≫ ≫
		NHK-NES (2885)
		≫ ≫
		sarah (2814)
		≫
		Table 22: Statistical significance testing of the jiji-ja-en (left) and jiji-en-ja (right) Pairwise scores
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Statistical Significance Testing of
		Pairwise Evaluation between
		Submissions
		Table 21 shows the results of statistical signifi-
		cance testing of aspec-ja-en subtasks, Table 22
		shows that of JIJI subtasks, Table 23 shows
		that of TDDC subtasks
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =23>
	<Abstractive Summary> =
		sarah (2807)
		NTT (3002)
		NICT-2 (3081)
		sarah (2811)
		geoduck (3197)
		geoduck (3216)
		ORGANIZER (3264) ≫ ≫ ≫ ≫ ≫ ≫
		sarah (2807)
		≫ ≫ ≫ ≫ ≫
		NTT (3002)
		-
		≫ ≫ ≫
		NICT-2 (3081)
		≫ ≫ ≫
		sarah (2811)
		-
		≫
		geoduck (3197)
		≫
		NTT (3005)
		sarah (2808)
		NICT-2 (3084)
		sarah (2812)
		geoduck (3200)
		geoduck (3217)
		ORGANIZER (3265)
		- ≫ ≫ ≫ ≫ ≫
		NTT (3005)
		≫ ≫ ≫ ≫ ≫
		sarah (2808)
		-
		≫ ≫ ≫
		NICT-2 (3084)
		≫ ≫ ≫
		sarah (2812)
		≫ ≫
		geoduck (3200)
		≫
		Table 23: Statistical significance testing of the tddc-itm-ja-en (left) and tddc-txt-ja-en (right) Pairwise
		scores
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Statistical Significance Testing of
		Pairwise Evaluation between
		Submissions
		Table 21 shows the results of statistical signifi-
		cance testing of aspec-ja-en subtasks, Table 22
		shows that of JIJI subtasks, Table 23 shows
		that of TDDC subtasks
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =24>
	<Abstractive Summary> =
		185
		Table 24: The Fleiss’ kappa values for the pairwise evaluation results
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =25>
	<Abstractive Summary> =
		0 %)
		Table 25: Appropriateness of source English cap-
		tions in the 4032 assessments collected for the
		multi-modal track
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 25 indicate that for a
		surprisingly high number of items we did not
		receive any answer
	</Extractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =26>
	<Abstractive Summary> =
		00
		Table 26: ASPEC en-ja submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		1901
		NMT
		NO
		26
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =27>
	<Abstractive Summary> =
		500
		Table 27: ASPEC ja-en submissions
		33
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		3262
		NMT
		NO
		38
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =28>
	<Abstractive Summary> =
		500
		Table 28: TDDC ITM ja-en submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		3263
		NMT
		NO
		24
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =29>
	<Abstractive Summary> =
		250
		Table 29: TDDC TXT ja-en submissions
		34
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		juman
		kytea
		mecab
		juman
		kytea
		mecab
		juman
		kytea
		mecab
		NMT
		1904
		NMT
		NO
		16
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =30>
	<Abstractive Summary> =
		250
		Table 30: JIJI en-ja submissions
		System
		ID
		Type
		RSRC
		BLEU
		RIBES
		AMFM
		Pair
		NMT
		1905
		NMT
		NO
		16
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument766> <Table ID =31>
	<Abstractive Summary> =
		750
		Table 31: JIJI ja-en submissions
		35
	</Abstractive Summary>
</Paper ID=ument766>


<Paper ID=ument767> <Table ID =1>
	<Abstractive Summary> =
		We tried the combination
		ASPEC-JE
		NTCIR-JE
		Pairs (train)
		1,000,000
		1,387,713
		Pairs (dev)
		1790
		2000
		Pairs (devtest)
		1784
		-
		Pairs (test)
		1812
		2300
		Vocab (ja)
		3084
		2966
		Vocab (en)
		291
		98
		Table 1: Numbers of sentence pairs and vocabulary of
		ASPEC-JE and NTCIR-JE
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the sizes of the training set of
		both datasets
	</Extractive Summary>
</Paper ID=ument767>


<Paper ID=ument767> <Table ID =2>
	<Abstractive Summary> =
		06
		-
		Table 2: BLEU scores for various models
	</Abstractive Summary>
	<Extractive Summary> =
		The fact that the mid-gated model tends to ig-
		nore the second and third types of unknown words
		does not contradict to the result in Table 2, since
		even though other models translate the second and
		third type in their own way, the result is not ex-
		actly the correct answer and it is ignored in the
		BLEU scores
	</Extractive Summary>
</Paper ID=ument767>


<Paper ID=ument767> <Table ID =3>
	<Abstractive Summary> =
		html
		Time
		GPU1
		GPU2
		Basic
		43h
		4GB
		4GB
		Mid-Gated
		40h
		8GB
		4GB
		Multi-Attention
		37h
		8GB
		4GB
		Table 3: The actual training time and GPU overhead of
		each model
	</Abstractive Summary>
</Paper ID=ument767>


<Paper ID=ument767> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Translation example by three models
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, the translation is the same
		3http://www
	</Extractive Summary>
	<Extractive Summary> =
		For example, the Japanese word “データベース”
		which means “database” in English, is a token of
		the second sentence in Table 4 after tokenization
	</Extractive Summary>
</Paper ID=ument767>


<Paper ID=ument767> <Table ID =5>
	<Abstractive Summary> =
		Table 5: A translation result of the noised sentence
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows a
		noise-added example and its translations
	</Extractive Summary>
</Paper ID=ument767>


<Paper ID=ument767> <Table ID =6>
	<Abstractive Summary> =
		Type
		Src
		Ref
		Basic
		Mid-Gated
		Multi-
		Attention
		1
		ア ン タ ゴ ニ ス
		ティック
		antagonistic
		antagonistic
		antagonistic
		antagonistic
		エコマティAX
		Ecomatie AX
		ecomater AX
		Ecomatey AX
		ecomate AX
		2
		福島大学
		Fukushima
		University
		Fukushima
		University
		Fukushima
		University
		Fukushima
		University
		長谷山俊郎
		Toshio
		Hasegawa
		Nagayama
		Yamato
		IGNORED
		Nakayama
		友ケ島
		Tomoga Island
		Fiken Island
		IGNORED
		IGNORED
		3
		嘔気・嘔吐
		nausea
		and
		vomiting
		air and vomit-
		ing
		air and vomit-
		ing
		air and vomit-
		ing
		捏造
		fabrication
		structure
		IGNORED
		construction
		Table 6: Examples of translations of unknown words
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 gives some examples
	</Extractive Summary>
</Paper ID=ument767>


<Paper ID=ument768> <Table ID =1>
	<Abstractive Summary> =
		Formality
		Japanese sentence and transcription
		Informal
		駅の 近くに たくさんの お店が ある。
		eki-no chikaku-ni takusan-no omise-ga aru
		Polite
		駅の 近くに たくさんの お店が あります。
		eki-no chikaku-ni takusan-no omise-ga arimasu
		Formal
		駅の 近くに たくさんの お店が ございます。
		eki-no chikaku-ni takusan-no omise-ga gozaimasu
		Table 1: Three sentences meaning “There are many
		shops near the train station”, in different levels of for-
		mality
		For example, when speaking with family, close
		friends, or others of equal social status, the infor-
		mal ある (aru “there are”) is used
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10 shows another example of the MT
		output from the internal formality-aware NMT
		model and the corresponding internal NMT base-
		line trained without formality annotations
	</Extractive Summary>
	<Extractive Summary> =
		Reference
		円卓の騎士たちはキラーラビットと戦う。
		entaku-no kishitachi-wa kir¯a rabitto-to tatakau
		NMT Model
		MT Output and Transcription
		Baseline
		アーサー王の騎士はキラーウサギと戦います。
		¯as¯a ¯o-no kishitachi-wa kir¯a usagi-to tatakaimasu
		Formality-Aware - Informal
		キングアーサーの騎士たちはキラーウサギと戦う。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakau
		Formality-Aware - Polite
		キングアーサーの騎士たちはキラーウサギと戦います。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakaimasu
		Formality-Aware - Formal
		キングアーサーの騎士たちはキラーウサギと戦いをいたします。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakai-wo itashimasu
		Table 10: Example output from internal NMT baseline model and formality-aware NMT model, when each for-
		mality level is attached to the source segment
	</Extractive Summary>
</Paper ID=ument768>


<Paper ID=ument768> <Table ID =2>
	<Abstractive Summary> =
		Japanese Target
		リストの一番下にある番号がリストから削除されます。
		risuto-no ichiban shita-ni aru bang¯o-ga risuto kara sakujo saremasu
		Table 2: Attaching a single token to the beginning of an English training data source sentence, based on the
		predicted formality of the Japanese target side
		proach on multiple data sets and show that it suc-
		cessfully produces sentences in the requested level
		of formality
	</Abstractive Summary>
	<Extractive Summary> =
		For example, in Table 2 the sufﬁx ます (masu)
		at the end of the Japanese target sentence is a com-
		mon politeness marker that identiﬁes this as a po-
		lite sentence
	</Extractive Summary>
</Paper ID=ument768>


<Paper ID=ument768> <Table ID =3>
	<Abstractive Summary> =
		If any of these verbs are present we con-
		sider the sentence to be formal, if not then we pro-
		ceed to looking for the polite verb forms, then the
		47
		Formality
		Verb forms
		Informal
		だ, だった, じゃない, じゃなかった, だろう,
		da, datta, janai, janakatta, darou,
		だから, だけど, だって, だっけ, そうだ, ようだ
		dakara, dakedo, datte, dakke, souda, youda
		Polite
		です, でした, ない, なかった, ます, ました, ません,
		desu, deshita, nai, nakatta, masu, mashita, masen,
		ましょう, でしょう, ください, なさい, である, ではない
		mashou, deshou, kudasai, nasai, dearu, dewanai
		Formal
		ございます, いらっしゃいます, おります, なさいます, 致します,
		gozaimasu, irrashaimasu, orimasu, nasaimasu, itashimasu,
		ご覧になります, 拝見します, お目に掛かります,
		goranninarimasu, haikenshimasu, omenikakarimasu,
		おいでになります, 伺います, 参ります, 存知します, 存じ上げます,
		oideninarimasu, ukagaimasu, mairimasu, zonjishimasu, zonjiagemasu,
		召し上がります, 頂く, 頂きます, 頂いて, 差しあげます,
		meshiagemasu, itadaku, itadakimasu, itadaite, sashiagemasu,
		下さいます, おっしゃいます, 申し上げます
		kudasaimasu, osshaimasu, moushiagemasu
		Table 3: Common verbs and sufﬁxes for each level of formality, used as identifying heuristics
	</Abstractive Summary>
	<Extractive Summary> =
		If none of the verb forms in
		Table 3 are present in the sentence it is ignored
	</Extractive Summary>
</Paper ID=ument768>


<Paper ID=ument768> <Table ID =4>
	<Abstractive Summary> =
		83
		Table 4: Evaluation scores of labels produced by the
		formality classiﬁer compared to gold test set labels for
		each formality category (n=150)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 contains a precision-recall evaluation of
		our formality classiﬁer, showing strong F1 scores
		for all three classes
	</Extractive Summary>
</Paper ID=ument768>


<Paper ID=ument768> <Table ID =5>
	<Abstractive Summary> =
		Dataset
		Train
		Test
		Proprietary
		23,781,990
		300
		ASPEC
		1,000,000
		1,812
		JESC
		3,237,376
		2,001
		KFTT
		329,882
		1,160
		Table 5: Parallel training and test data set sizes in num-
		ber of sentences
		3
	</Abstractive Summary>
</Paper ID=ument768>


<Paper ID=ument768> <Table ID =6>
	<Abstractive Summary> =
		3
		Table 6: KyTea-tokenized BLEU, comparing baseline NMT, rule-based conversion and formality-aware NMT
		models on heldout test
		F1
		Dataset
		Model
		Informal
		Polite
		Formal
		Internal
		Baseline NMT
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows
		49
		our results on the test set using BLEU
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 6, the rule-based method
		yields improvements on the informal test portion
		for all models except ASPEC where performance
		remains the same
	</Extractive Summary>
	<Extractive Summary> =
		2
		Performance of formality-aware NMT
		The BLEU scores in Table 6 show that on the
		overall test set, our formality-aware NMT mod-
		els show an improvement over the baseline NMT
		models
	</Extractive Summary>
</Paper ID=ument768>


<Paper ID=ument768> <Table ID =7>
	<Abstractive Summary> =
		74
		Table 7: F1 scores for each formality category when comparing predicted labels for MT output and reference
		translation
	</Abstractive Summary>
	<Extractive Summary> =
		Our F1 comparison in Table 7 shows to what ex-
		tent the formality-aware NMT output matches the
		predicted formality level of the reference transla-
		tion when the system is provided with the correct
		input label
	</Extractive Summary>
</Paper ID=ument768>


<Paper ID=ument768> <Table ID =8>
	<Abstractive Summary> =
		7%
		Table 8: Percentage of each training data set classiﬁed
		as informal, polite, or formal
		’informal’ is attached to the source segment
	</Abstractive Summary>
</Paper ID=ument768>


<Paper ID=ument768> <Table ID =9>
	<Abstractive Summary> =
		Reference
		謝ることなんかなにもないさ
		ayamaru koto nanka nanimo nai sa
		NMT Model
		MT Output and Transcription
		Baseline
		謝ることなんて何もない
		ayamaru koto nante nanimo nai
		Formality-Aware - Informal
		別に謝ることはない
		betsu ni ayamaru koto wa nai
		Formality-Aware - Polite
		謝ることはありません
		ayamaru koto wa arimasen
		Formality-Aware - Formal
		お詫びのことは何もございません
		owabi no koto wa nanimo gozaimasen
		Table 9: Example output from JESC NMT baseline model and formality-aware NMT model, when each formality
		level is attached to the source segment
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Analysis and Examples
		To show some concrete examples of our formality-
		aware translations, Table 9 contains an exam-
		ple of the MT output from the JESC formality-
		aware NMT model and the corresponding JESC
		NMT baseline trained without formality annota-
		tions
	</Extractive Summary>
</Paper ID=ument768>


<Paper ID=ument768> <Table ID =10>
	<Abstractive Summary> =
		Reference
		円卓の騎士たちはキラーラビットと戦う。
		entaku-no kishitachi-wa kir¯a rabitto-to tatakau
		NMT Model
		MT Output and Transcription
		Baseline
		アーサー王の騎士はキラーウサギと戦います。
		¯as¯a ¯o-no kishitachi-wa kir¯a usagi-to tatakaimasu
		Formality-Aware - Informal
		キングアーサーの騎士たちはキラーウサギと戦う。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakau
		Formality-Aware - Polite
		キングアーサーの騎士たちはキラーウサギと戦います。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakaimasu
		Formality-Aware - Formal
		キングアーサーの騎士たちはキラーウサギと戦いをいたします。
		kingu ¯as¯a-no kishitachi-wa kir¯a usagi-to tatakai-wo itashimasu
		Table 10: Example output from internal NMT baseline model and formality-aware NMT model, when each for-
		mality level is attached to the source segment
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10 shows another example of the MT
		output from the internal formality-aware NMT
		model and the corresponding internal NMT base-
		line trained without formality annotations
	</Extractive Summary>
</Paper ID=ument768>


<Paper ID=ument769> <Table ID =1>
	<Abstractive Summary> =
		Table 1: An example of the Japanese-English business conversation parallel corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		an example of the corpus
	</Extractive Summary>
</Paper ID=ument769>


<Paper ID=ument769> <Table ID =2>
	<Abstractive Summary> =
		The whole construction process was supervised by
		a person who satisﬁes the following conditions to
		guarantee the conversations to be natural:
		• has the experience of being engaged in lan-
		Scene
		Scenarios
		Sentences
		JA → EN
		face-to-face
		165
		5,068
		phone call
		77
		2,329
		general chatting
		101
		3,321
		meeting
		106
		3,561
		training
		16
		608
		presentation
		4
		113
		sum
		469
		15,000
		EN → JA
		face-to-face
		158
		4,876
		phone call
		99
		2,949
		general chatting
		102
		2,988
		meeting
		103
		3,315
		training
		9
		326
		presentation
		15
		546
		sum
		486
		15,000
		Table 2: Statistics for the corpus, where JA → EN rep-
		resents scenarios which are written in Japanese then
		translated into English and EN → JA represents sce-
		narios constructed in the reverse way
	</Abstractive Summary>
</Paper ID=ument769>


<Paper ID=ument769> <Table ID =3>
	<Abstractive Summary> =
		In the original conversation, the
		utterance is from Speaker 2 to Speaker 1, and
		58
		Data Set
		Devel
		Eval
		Train
		BSD
		1000
		1000
		28,000
		AMI
		1000
		1000
		108,499
		ON
		1000
		1000
		26,439
		Total
		162,938
		Table 3: Training, development and evaluation data
		statistics
	</Abstractive Summary>
</Paper ID=ument769>


<Paper ID=ument769> <Table ID =4>
	<Abstractive Summary> =
		00
		Table 4: NMT and SMT experiments using the conver-
		sational corpora
	</Abstractive Summary>
	<Extractive Summary> =
		, 2001) are shown
		in Table 4 along with several ablation experiments
		on training NMT and SMT systems using only the
		BSD data, all 3 conversational corpora, and ex-
		cluding the BSD corpus from the training data
	</Extractive Summary>
</Paper ID=ument769>


<Paper ID=ument769> <Table ID =5>
	<Abstractive Summary> =
		97
		Table 5: BLEU and ChrF2 scores for all three evalua-
		tion data sets using the NMT system trained on all data
	</Abstractive Summary>
</Paper ID=ument769>


<Paper ID=ument77> <Table ID =1>
	<Abstractive Summary> =
		836
		ar
		bg
		ca
		cs
		da
		de
		el
		en
		es
		et
		fa
		ﬁ
		fr
		he
		hi
		hr
		hu
		id
		it
		ja
		ko
		la
		lv
		nl
		no
		pl
		pt
		ro
		ru
		sk
		sl
		sv
		sw
		th
		tr
		uk
		ur
		vi
		zh
		MLDoc
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		NLI
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		NER
		✓
		✓
		✓
		✓
		✓
		POS
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Parsing
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		✓
		Table 1: The 39 languages used in the 5 tasks
	</Abstractive Summary>
</Paper ID=ument77>


<Paper ID=ument77> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: MLDoc experiments
	</Abstractive Summary>
</Paper ID=ument77>


<Paper ID=ument77> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: XNLI experiments
	</Abstractive Summary>
</Paper ID=ument77>


<Paper ID=ument77> <Table ID =4>
	<Abstractive Summary> =
		03
		Table 4: NER tagging experiments
	</Abstractive Summary>
</Paper ID=ument77>


<Paper ID=ument77> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: POS tagging
	</Abstractive Summary>
</Paper ID=ument77>


<Paper ID=ument77> <Table ID =6>
	<Abstractive Summary> =
		9
		Table 6:
		Dependency parsing results by language
		(UAS/LAS)
	</Abstractive Summary>
</Paper ID=ument77>


<Paper ID=ument770> <Table ID =1>
	<Abstractive Summary> =
		In the
		64
		3
		200
		201
		202
		203
		204
		205
		206
		207
		208
		209
		210
		211
		212
		213
		214
		215
		216
		217
		218
		219
		220
		221
		222
		223
		224
		225
		226
		227
		228
		229
		230
		231
		232
		233
		234
		235
		236
		237
		238
		239
		240
		241
		242
		243
		244
		245
		246
		247
		248
		249
		250
		251
		252
		253
		254
		255
		256
		257
		258
		259
		260
		261
		262
		263
		264
		265
		266
		267
		268
		269
		270
		271
		272
		273
		274
		275
		276
		277
		278
		279
		280
		281
		282
		283
		284
		285
		286
		287
		288
		289
		290
		291
		292
		293
		294
		295
		296
		297
		298
		299
		Nature of corpus
		Name of Corpus
		Number of instances/items
		Training
		Englsih-Hindi
		28,929
		(Text data)
		Image data
		28,929
		Test (Evaluation Set)
		English to Hindi
		1595
		(Text data)
		Image data
		1595
		Test (Challenge Set)
		English to Hindi
		1400
		(Text data)
		Image data
		1400
		Validation
		English-Hindi
		998
		(Text data)
		Image data
		998
		Table 1: Corpus Statistics (Nakazawa et al
	</Abstractive Summary>
	<Extractive Summary> =
		In the text-only translation track, we
		have considered only source-target corresponding
		sentences as shown in Table 1 to build the dic-
		tionary, vocabulary size of dimension 8300, 7984
		using the OpenNMT toolkit
	</Extractive Summary>
</Paper ID=ument770>


<Paper ID=ument770> <Table ID =2>
	<Abstractive Summary> =
		682060
		Table 2: BLEU, RIBES and AMFM scores result of
		participated teams for multi-modal translation track
	</Abstractive Summary>
	<Extractive Summary> =
		Moreover, from Table 2, 3 and 4, it is observed
		that when translating English to Hindi our multi-
		modal translation outperforms our text only trans-
		lation as well as our Hindi-only image caption-
		ing
	</Extractive Summary>
</Paper ID=ument770>


<Paper ID=ument770> <Table ID =3>
	<Abstractive Summary> =
		615290
		Table 3: BLEU, RIBES and AMFM scores result of
		participated teams for text-only translation track
	</Abstractive Summary>
	<Extractive Summary> =
		However, we have attained lower BLEU,
		RIBES and AMFM scores than other teams in
		text-only and Hindi-only image captioning trans-
		lation track as shown in Table 3 and 4 respectively
	</Extractive Summary>
</Paper ID=ument770>


<Paper ID=ument770> <Table ID =4>
	<Abstractive Summary> =
		335390
		Table 4: RIBES, AMFM scores result of participated
		teams for Hindi-only image captioning track
	</Abstractive Summary>
</Paper ID=ument770>


<Paper ID=ument770> <Table ID =5>
	<Abstractive Summary> =
		Table 5:
		Best performance examples in English to
		Hindi multi-modal translation
	</Abstractive Summary>
	<Extractive Summary> =
		To further analyze the best and worst per-
		formance of multi-modal translation in compari-
		son to text-only and Hindi-only image captioning,
		sample predicted sentences on challenge test data,
		reference target sentences and Google translation
		on same test data are considered in Table 5, 6
	</Extractive Summary>
	<Extractive Summary> =
		In
		Table 5, our multi-modal NMT system provides
		perfect prediction like reference target sentence,
		Google translation and close to text-only trans-
		lation but wrong translation in Hindi-only image
		captioning
	</Extractive Summary>
</Paper ID=ument770>


<Paper ID=ument770> <Table ID =6>
	<Abstractive Summary> =
		Table 6:
		Worst performance example in English to
		Hindi multi-modal translation
	</Abstractive Summary>
	<Extractive Summary> =
		However, in Table 6, prediction of
		source word “court” is inappropriate like Google
		translation, text-only translation and wrong trans-
		lation in Hindi-only image captioning
	</Extractive Summary>
</Paper ID=ument770>


<Paper ID=ument771> <Table ID =1>
	<Abstractive Summary> =
		3M)
		Development
		1,000 (my: 36,688, en: 25,538)
		1,000 (km: 33,604, en: 25,538)
		Test
		1,018 (my: 37,519, en: 26,236)
		1,018 (km: 34,238, en: 26,236)
		Table 1: Statistics of our preprocessed parallel data
	</Abstractive Summary>
</Paper ID=ument771>


<Paper ID=ument771> <Table ID =2>
	<Abstractive Summary> =
		1M
		Table 2:
		Statistics of our preprocessed monolin-
		gual data
	</Abstractive Summary>
</Paper ID=ument771>


<Paper ID=ument771> <Table ID =3>
	<Abstractive Summary> =
		Order
		From
		To
		Graph
		1
		◌្ + ដ
		◌្ + ត
		◌�
		2
		◌ំ + ◌ា
		◌ា + ◌ំ
		◌ាំ
		3
		◌ុ + ◌ាំ
		◌៉ + ◌ាំ
		◌ុាំ
		◌ាំ + ◌ុ
		◌ាំ + ◌៉
		4
		V + S[S]
		S[S] + V
		-
		5
		WS + SS
		SS + WS
		-
		Table 3: Khmer Text normalization rules, where
		”V” is Vowel, ”S” is subscript (subscript sign + a
		consonant) and [S] refer to one or zero subscript,
		WS is west subscript, and SS is south subscript
	</Abstractive Summary>
</Paper ID=ument771>


<Paper ID=ument771> <Table ID =4>
	<Abstractive Summary> =
		98
		1e-09 --clip-norm 5 --sync-sgd
		--exponential-smoothing
		Table 4:
		Parameters of Marian used for training
		our NMT systems
	</Abstractive Summary>
</Paper ID=ument771>


<Paper ID=ument771> <Table ID =5>
	<Abstractive Summary> =
		We also gener-
		ated 100-best translation hypotheses with our
		72
		Feature
		Description
		L2R (7)
		Scores given by each of the 7 left-to-right Marian models
		LEX (4)
		Sentence-level translation probabilities, for both translation directions
		LM (2)
		Scores given by the language models used by the Moses baseline systems
		LEN (2)
		Difference between the length of the source sentence and the length of the translation hy-
		pothesis, and its absolute value
		Table 5: Set of features used by our reranking systems
	</Abstractive Summary>
	<Extractive Summary> =
		As listed in
		Table 5, it includes the scores given by 7 left-
		to-right NMT models independently trained
	</Extractive Summary>
</Paper ID=ument771>


<Paper ID=ument771> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: Official BLEU scores for our MT systems on the official test set of the tasks
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Results
		Table 6 presents the results for different ver-
		sions of our SMT and NMT systems
	</Extractive Summary>
</Paper ID=ument771>


<Paper ID=ument772> <Table ID =1>
	<Abstractive Summary> =
		8k
		Table 1: Statistics on our in-domain parallel data for
		the Russian–Japanese task
	</Abstractive Summary>
	<Extractive Summary> =
		Re-
		fer to Table 1 for an overview of the in-domain
		parallel corpora and the data splits
	</Extractive Summary>
</Paper ID=ument772>


<Paper ID=ument772> <Table ID =2>
	<Abstractive Summary> =
		43%)
		2,834,970
		2,104,896
		total
		169,871
		3,984,038
		2,776,397
		Table 2: Statistics on our in-domain parallel data for
		the Tamil–English task
	</Abstractive Summary>
</Paper ID=ument772>


<Paper ID=ument773> <Table ID =1>
	<Abstractive Summary> =
		Using this algorithm, it is possible
		to represent a sentence as a subword sequence
		through as ﬁxed-size vocabulary and to solve the
		Task
		Dataset
		Train
		Dev
		Test
		ASPEC
		En-Ja
		3,008,500
		1,790
		1,812
		Zh-Ja
		672,315
		2,090
		2,107
		En-Ja
		1,000,000
		2,000
		5,668
		JPC2
		Ko-Ja
		1,000,000
		2,000
		5,230
		Zh-Ja
		1,000,000
		2,000
		5,204
		Table 1: Statistics of parallel sentences (sentence)
		problem of unknown words and rare words ef-
		fectively
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Korean-Japanese
		Table 10 shows the translation performance of
		JPC2 dataset for Korean and Japanese as JPC2 Ko-
		Ja
	</Extractive Summary>
	<Extractive Summary> =
		04
		1 of 3
		Table 10:
		BLEU score for Korean→Japanese sub-
		tasks on leaderboard
		The Ko-Ja translation task has only a paten sub-
		task as JPC2, and we only participated in tasks for
		Korean to Japanese (Ko→Ja)
	</Extractive Summary>
	<Extractive Summary> =
		04
		Table 11: Method ablation for JPC2 Ko→Ja sub-task
		Sub-task
		Adequacy
		ASPEC (Ja→En)
		4
	</Extractive Summary>
	<Extractive Summary> =
		65
		Table 12: Adequacy Evaluation of Our Model
		positioning
	</Extractive Summary>
	<Extractive Summary> =
		Table 12
		shows the adequacy performance for the sub-tasks
		we participated in
	</Extractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =2>
	<Abstractive Summary> =
		72
		1 of 2
		None
		Table 2:
		BLEU score for English-Japanese tasks on
		leaderboard
		3
	</Abstractive Summary>
	<Extractive Summary> =
		1
		English-Japanese
		Table 2 indicates the BLEU score and rank of the
		system we submitted in the ASPEC and JPC2 sub-
		tasks of WAT 2019
	</Extractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =3>
	<Abstractive Summary> =
		88
		Table 3: Method ablation for ASPEC En-Ja sub-task
		En→Ja and Ja→En, respectively
	</Abstractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =4>
	<Abstractive Summary> =
		72
		Table 4: Method ablation for JPC2 En-Ja sub-task
		over the baseline
	</Abstractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =5>
	<Abstractive Summary> =
		30
		1 of 3
		Table 5:
		BLEU score for Chinese-Japanese tasks on
		leaderboard
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 presents the results of BLEU for each
		Zh→Ja subtask of the method used in this pa-
		per
	</Extractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =6>
	<Abstractive Summary> =
		92
		Table 6: Method ablation for ASPEC Zh→Ja sub-task
		Method
		BLEU
		Baseline (ASPEC)
		34
	</Abstractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =7>
	<Abstractive Summary> =
		77
		Table 7: Method ablation for ASPEC Ja→Zh sub-task
		PEC Ja→Zh subtask showed a 34
	</Abstractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =8>
	<Abstractive Summary> =
		30
		Table 8: Method ablation for JPC2 Zh→Ja sub-task
		translation, multi-sourcing, and r2l re-ranking
		to increase the BLEU score by 0
	</Abstractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =9>
	<Abstractive Summary> =
		33
		Table 9: Method ablation for JPC2 Ja→Zh sub-task
		Sub-task
		BLEU
		BLEU rank
		JPC2 (Ko→Ja)
		73
	</Abstractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =10>
	<Abstractive Summary> =
		04
		1 of 3
		Table 10:
		BLEU score for Korean→Japanese sub-
		tasks on leaderboard
		The Ko-Ja translation task has only a paten sub-
		task as JPC2, and we only participated in tasks for
		Korean to Japanese (Ko→Ja)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Korean-Japanese
		Table 10 shows the translation performance of
		JPC2 dataset for Korean and Japanese as JPC2 Ko-
		Ja
	</Extractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =11>
	<Abstractive Summary> =
		04
		Table 11: Method ablation for JPC2 Ko→Ja sub-task
		Sub-task
		Adequacy
		ASPEC (Ja→En)
		4
	</Abstractive Summary>
</Paper ID=ument773>


<Paper ID=ument773> <Table ID =12>
	<Abstractive Summary> =
		65
		Table 12: Adequacy Evaluation of Our Model
		positioning
	</Abstractive Summary>
	<Extractive Summary> =
		Table 12
		shows the adequacy performance for the sub-tasks
		we participated in
	</Extractive Summary>
</Paper ID=ument773>


<Paper ID=ument774> <Table ID =1>
	<Abstractive Summary> =
		7K
		Table 1: Statistics of our preprocessed parallel data
	</Abstractive Summary>
</Paper ID=ument774>


<Paper ID=ument774> <Table ID =2>
	<Abstractive Summary> =
		8M
		Table 2:
		Statistics of our preprocessed monolingual
		data
	</Abstractive Summary>
</Paper ID=ument774>


<Paper ID=ument774> <Table ID =3>
	<Abstractive Summary> =
		0001
		Table 3: Parameters for training TLM
	</Abstractive Summary>
</Paper ID=ument774>


<Paper ID=ument774> <Table ID =4>
	<Abstractive Summary> =
		0001
		--eval bleu true
		Table 4: Parameters for training NMT
	</Abstractive Summary>
</Paper ID=ument774>


<Paper ID=ument774> <Table ID =5>
	<Abstractive Summary> =
		01
		Table 5: Results (BLEU-cased) of our MT systems on the test set
	</Abstractive Summary>
</Paper ID=ument774>


<Paper ID=ument774> <Table ID =6>
	<Abstractive Summary> =
		0001
		--eval bleu true
		Table 6: Parameters for training UNMT
	</Abstractive Summary>
</Paper ID=ument774>


<Paper ID=ument775> <Table ID =1>
	<Abstractive Summary> =
		05 
		Table 1:  BLEU scores by changing initial distortion 
		weight in Myanmar-English bidirectional 
		translations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows BLEU scores by 
		changing various initial distortion weights in 
		Myanmar-English bidirectional translations
	</Extractive Summary>
	<Extractive Summary> =
		6 as shown in 
		Table 1, we found that the translation result did 
		not 
		improve 
		significantly 
		compared 
		with 
		baseline
	</Extractive Summary>
</Paper ID=ument775>


<Paper ID=ument775> <Table ID =2>
	<Abstractive Summary> =
		Corpus 
		Translation pairs  
		ALT  
		230,240 
		NER (Raw) 
		14313 
		NER (clear) 
		14310 
		Table 2:  Data statistics of the NER corpus
	</Abstractive Summary>
</Paper ID=ument775>


<Paper ID=ument775> <Table ID =3>
	<Abstractive Summary> =
		Bilingual 
		Lexicon 
		Translation pairs 
		English  
		 54674 
		Myanmar  
		 35532 
		Total 
		 90206 
		Table 3:  Data statistics of the Bilingual Lexicon
	</Abstractive Summary>
</Paper ID=ument775>


<Paper ID=ument775> <Table ID =4>
	<Abstractive Summary> =
		On the other hand, from 
		Myanmar to English translation, the PBSMT 
		with reranking is better than baseline PBSMT in 
		Data Set 
		#sentences 
		TRAIN 
		ALT 
		NER 
		Bilingual 
		 112570 
		18088 
		14310 
		80172 
		DEV 
		 1000 
		TEST 
		1018 
		Table 4:  Statistics of data sets
	</Abstractive Summary>
</Paper ID=ument775>


<Paper ID=ument775> <Table ID =5>
	<Abstractive Summary> =
		04 
		Table 5:  BLEU, RIBES and AMFM scores for PBSMT, PBSMT with reranking
	</Abstractive Summary>
</Paper ID=ument775>


<Paper ID=ument775> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Comparison between my-en translation results
	</Abstractive Summary>
</Paper ID=ument775>


<Paper ID=ument776> <Table ID =1>
	<Abstractive Summary> =
		Set
		# Sentences
		Train
		3,008,500
		(bitext)
		(1,500,000)
		(synthetic)
		(1,508,500)
		Dev
		1,790
		Devtest
		1,784
		Test
		1,812
		Table 1: Numbers of sentences in ASPEC corpus
		Section 2
	</Abstractive Summary>
	<Extractive Summary> =
		Set
		Category
		# Sentences
		Train
		texts
		448,472
		items
		955,523
		Dev
		texts
		1,153
		items
		2,845
		Devtest
		texts
		1,114
		items
		2,900
		Test
		texts
		1,148
		items
		2,129
		Table 10: Number of sentences in timely disclosure
		document corpus: We split training set into two cate-
		gories
	</Extractive Summary>
	<Extractive Summary> =
		34
		Table 11: Case-sensitive BLEU scores of provided
		blind test sets: All scores were calculated by ofﬁcial
		evaluation server
	</Extractive Summary>
	<Extractive Summary> =
		Table 13 shows the example
		translations of the baseline and ﬁne-tuned sys-
		tems10
	</Extractive Summary>
	<Extractive Summary> =
		47
		(1)
		Table 12: Ofﬁcial results of our submitted systems for timely disclosure subtask: Shown rank is only ordered
		among constrained submissions
	</Extractive Summary>
	<Extractive Summary> =
		Fine-tuned
		Based on historical data, comparable assets and estimates in the engineering report
		Table 13: Example translations of baseline and ﬁne-tuned system: Example was picked from devtest set
	</Extractive Summary>
	<Extractive Summary> =
		Japanese
		実績値、類似建物の修繕費水準、ER の修繕更新費等を考慮し査定
		English
		Based on historical data, comparable assets and estimates in the engineering report
		Table 14: Example of sentence pair contained in the training set
	</Extractive Summary>
	<Extractive Summary> =
		Table 14 shows the
		sentence pair in the training set that was the most
		similar to the previous example
	</Extractive Summary>
	<Extractive Summary> =
		We found a sen-
		tence pair on the English side that is identical as
		the reference in Table 13, and the Japanese side
		is also quite similar
	</Extractive Summary>
	<Extractive Summary> =
		4
		Submissions and human evaluations
		Table 12 shows our submissions and their human
		evaluation scores
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =2>
	<Abstractive Summary> =
		3
		Mini-batch size
		4K tokens
		Update frequency
		128 batches
		Beam search (n-best)
		6 best
		Table 2: Hyper-parameters for the scientiﬁc paper task:
		Our basic hyper-parameters are identical to Trans-
		former “big” setting
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the selected set of the
		7We mixed ASPEC and JParaCrawl by upsampling AS-
		PEC twice
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Comparison of translation performance on
		changing subword size
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Comparison of translation performance on
		changing the methods of building synthetic data
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Subword size/Vocabulary size
		Table 4 shows the BLEU scores when we
		changed the number of subwords obtained from
		sentencepiece
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =5>
	<Abstractive Summary> =
		9
		Table 5: Comparison of translation performance on
		changing mini-batch size (update frequency) for each
		update in NMT training
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the results
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =6>
	<Abstractive Summary> =
		2
		Table 6: Results of incorporating ensembling and R2L
		re-ranking techniques
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the effectiveness of these tech-
		niques
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =7>
	<Abstractive Summary> =
		6
		Table 7: Translation performance comparison when we
		incorporate additional training data JParaCrawl
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Unconstrained setting
		Table 7 shows the “Ensemble (4) + R2L (4)”
		results that were trained by ASPEC or AS-
		PEC+JParaCrawl
	</Extractive Summary>
	<Extractive Summary> =
		5
		Ofﬁcial Result
		We ﬁrst planned to submit the unconstrained set-
		ting results (the second row in Table 7) as our pri-
		mary results
	</Extractive Summary>
	<Extractive Summary> =
		Therefore,
		we submitted the constrained setting results (the
		ﬁrst row in Table 7) as our primary results
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =8>
	<Abstractive Summary> =
		49
		(2)
		Table 8: Ofﬁcial results of our submitted systems for ASPEC subtask: For En-Ja direction, we show BLEU scores
		with JUMAN tokenizer
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows the ofﬁcial results of our submis-
		sions computed in the evaluation server
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =9>
	<Abstractive Summary> =
		23
		Table 9: Performance comparison when we incorporate
		additional training data JParaCrawl: Scores here were
		obtained from evaluation server
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 shows the re-
		sults
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =10>
	<Abstractive Summary> =
		Set
		Category
		# Sentences
		Train
		texts
		448,472
		items
		955,523
		Dev
		texts
		1,153
		items
		2,845
		Devtest
		texts
		1,114
		items
		2,900
		Test
		texts
		1,148
		items
		2,129
		Table 10: Number of sentences in timely disclosure
		document corpus: We split training set into two cate-
		gories
	</Abstractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =11>
	<Abstractive Summary> =
		34
		Table 11: Case-sensitive BLEU scores of provided
		blind test sets: All scores were calculated by ofﬁcial
		evaluation server
	</Abstractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =12>
	<Abstractive Summary> =
		47
		(1)
		Table 12: Ofﬁcial results of our submitted systems for timely disclosure subtask: Shown rank is only ordered
		among constrained submissions
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Submissions and human evaluations
		Table 12 shows our submissions and their human
		evaluation scores
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =13>
	<Abstractive Summary> =
		Fine-tuned
		Based on historical data, comparable assets and estimates in the engineering report
		Table 13: Example translations of baseline and ﬁne-tuned system: Example was picked from devtest set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 13 shows the example
		translations of the baseline and ﬁne-tuned sys-
		tems10
	</Extractive Summary>
	<Extractive Summary> =
		We found a sen-
		tence pair on the English side that is identical as
		the reference in Table 13, and the Japanese side
		is also quite similar
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument776> <Table ID =14>
	<Abstractive Summary> =
		Japanese
		実績値、類似建物の修繕費水準、ER の修繕更新費等を考慮し査定
		English
		Based on historical data, comparable assets and estimates in the engineering report
		Table 14: Example of sentence pair contained in the training set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 14 shows the
		sentence pair in the training set that was the most
		similar to the previous example
	</Extractive Summary>
</Paper ID=ument776>


<Paper ID=ument777> <Table ID =1>
	<Abstractive Summary> =
		61 M
		Table 1: Dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the detail of each corpus
	</Extractive Summary>
	<Extractive Summary> =
		Firstly, we trained the multiple NMT
		models with different combinations of ﬁve cor-
		pora as shown in Table 1, and evaluated these
		NMT models with an ofﬁcial test-set, in which
		the number of data was 2000
	</Extractive Summary>
</Paper ID=ument777>


<Paper ID=ument777> <Table ID =2>
	<Abstractive Summary> =
		80
		BT-JIJI
		Table 2: BLEU scores for Japanese→English translation tasks
	</Abstractive Summary>
</Paper ID=ument777>


<Paper ID=ument777> <Table ID =3>
	<Abstractive Summary> =
		68
		Table 3: BLEU scores for English→Japanese translation tasks
	</Abstractive Summary>
</Paper ID=ument777>


<Paper ID=ument777> <Table ID =4>
	<Abstractive Summary> =
		11
		1/2
		Table 4: Ofﬁcial results for the newswire translation tasks of WAT 2019: For JIJI-EJ task, we show the BLEU,
		RIBES, and AMFM scores with KyTea tokenizer
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the ofﬁcial results of our
		submission to WAT 2019
	</Extractive Summary>
</Paper ID=ument777>


<Paper ID=ument777> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Example results of JIJI-JE tasks translated with JIJI- and Equivalent-style NMT
		Omitted-
		Added-
		Task
		Tag (Style)
		words
		words
		JIJI-JE
		JIJI-style
		18
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the examples of NMT out-
		puts adapted to the JIJI- and Equivalent-styles in
		the ofﬁcial tasks
	</Extractive Summary>
</Paper ID=ument777>


<Paper ID=ument777> <Table ID =6>
	<Abstractive Summary> =
		55
		Table 6: Further human evaluation results, which is the
		average number of words per 100 words
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows
		the average number of words per 100 words of
		the three evaluators
	</Extractive Summary>
</Paper ID=ument777>


<Paper ID=ument778> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: Effect of noisy channel reranking when eval-
		uating on the validation set
	</Abstractive Summary>
</Paper ID=ument778>


<Paper ID=ument778> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2:
		Combining BT and ST yields better BLEU
		score than BT and ST
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we show that these two approaches
		can be combined and yield better performance
		than either method individually
	</Extractive Summary>
</Paper ID=ument778>


<Paper ID=ument778> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: BLEU scores of systems trained only on the
		provided parallel datasets
	</Abstractive Summary>
</Paper ID=ument778>


<Paper ID=ument778> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: BLEU scores of systems trained using addi-
		tional monolingual data
	</Abstractive Summary>
</Paper ID=ument778>


<Paper ID=ument778> <Table ID =5>
	<Abstractive Summary> =
		7
		-
		Table 5: My→En leaderboard5
	</Abstractive Summary>
</Paper ID=ument778>


<Paper ID=ument778> <Table ID =6>
	<Abstractive Summary> =
		9
		-
		Table 6: En→My leaderboard6
	</Abstractive Summary>
</Paper ID=ument778>


<Paper ID=ument779> <Table ID =1>
	<Abstractive Summary> =
		3,998
		3,518
		3,752
		Devtest
		4,014
		3,753
		3,483
		Test
		3,277
		2,898
		—–
		Table 1:
		Data statistics of Timely Disclosure Docu-
		ments Corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 re-
		ports the statistics of the full corpus
	</Extractive Summary>
</Paper ID=ument779>


<Paper ID=ument779> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: General translation accuracy of each system
		on the concatenated data (ITEM+TEXT)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 reports the case-sensitive sacreBLEU
		scores of the NMT systems and the production
		systems without translation memory
	</Extractive Summary>
</Paper ID=ument779>


<Paper ID=ument779> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3:
		BLEU results of our proposed approach on
		the evaluation data sets (ITEM and TEXT)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the experimental results of our
		proposed approach using the translation memory,
		reporting the sacreBLEU scores on the ITEM and
		TEXT evaluation data sets
	</Extractive Summary>
</Paper ID=ument779>


<Paper ID=ument779> <Table ID =4>
	<Abstractive Summary> =
		08
		Table 4:
		BLEU results on the evaluation data sets
		(ITEM and TEXT)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results of our submission
		systems on the devtest and test data sets, where
		Fairseq provides the result of an ensemble with 4
		replicas
	</Extractive Summary>
</Paper ID=ument779>


<Paper ID=ument779> <Table ID =5>
	<Abstractive Summary> =
		Testset:
		Devtest
		Annotators:
		25
		Tasks:
		50
		Redundancy:
		1
		Task per Annotator:
		2
		Data points:
		8308
		Table 5: Human Evaluation Campaign Parameters
		ITEM
		TEXT
		ALL
		HREF
		73
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the eval-
		uation campaign parameters
	</Extractive Summary>
</Paper ID=ument779>


<Paper ID=ument779> <Table ID =6>
	<Abstractive Summary> =
		0
		Table 6: Human Evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the mean scores for ITEM, TEXT
		and ALL (ITEM+TEXT) for each system
	</Extractive Summary>
</Paper ID=ument779>


<Paper ID=ument779> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Translation examples of each system on the devtest data set (ITEM) that obtain a higher evaluation score
		than the human reference
	</Abstractive Summary>
</Paper ID=ument779>


<Paper ID=ument779> <Table ID =8>
	<Abstractive Summary> =
		Table 8: Translation examples of each system on the devtest data set (TEXT) that are highly evaluated by human
		annotators
	</Abstractive Summary>
</Paper ID=ument779>


<Paper ID=ument78> <Table ID =1>
	<Abstractive Summary> =
		01†
		Table 1: Experimental results on the Chinese-English translation task
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Overall Performance
		Table 1 shows the overall experimental results
	</Extractive Summary>
	<Extractive Summary> =
		According to the reported performance of our
		framework shown in Table 1, we only considered
		IDDA in all subsequent experiments
	</Extractive Summary>
</Paper ID=ument78>


<Paper ID=ument78> <Table ID =2>
	<Abstractive Summary> =
		02
		Table 2:
		Experimental results of comparing IDDA
		with its two variants
	</Abstractive Summary>
</Paper ID=ument78>


<Paper ID=ument78> <Table ID =3>
	<Abstractive Summary> =
		Inspecting this
		Model
		Translation
		Src
		这(zh`e) 是(sh`ı) 第(d`ı) 一(y¯i) 种(zhˇong)
		直立(zh´ıl`ı) 行走(x´ıngzˇou) 的(de)
		灵长类(l´ıngzhˇangl`ei) 动物(d`ongw`u)
		Ref
		that was the ﬁrst upright primate
		MFT
		this is the ﬁrst animal to walk upright
		KD
		this is the ﬁrst growing primate
		WDCD
		this is the ﬁrst primate walking around
		IDDA-1
		this is the ﬁrst upright - walking primate
		IDDA-2
		this is the ﬁrst upright - walking primate
		IDDA-3
		this is the ﬁrst primates walking upright
		IDDA-4
		this is the ﬁrst upright primate
		IDDA-5
		this is the ﬁrst upright primate
		IDDA-6
		this is the ﬁrst upright primate
		IDDA-7
		this is the ﬁrst upright primate
		Table 3: Translation examples of different NMT mod-
		els
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Case Study
		Table 3 displays the 1-best translations of a sam-
		pled test sentence generated by MFT, KD, WDCD,
		and IDDA at different iterations
	</Extractive Summary>
</Paper ID=ument78>


<Paper ID=ument78> <Table ID =4>
	<Abstractive Summary> =
		39†
		Table 4: Experimental results of the English-German translation task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows experimental results
	</Extractive Summary>
</Paper ID=ument78>


<Paper ID=ument78> <Table ID =5>
	<Abstractive Summary> =
		91
		Table 5: Experimental results of IDDA using different
		conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the ﬁnal experimental results,
		which are in line with our expectations and verify
		the validity of our designs
	</Extractive Summary>
</Paper ID=ument78>


<Paper ID=ument780> <Table ID =1>
	<Abstractive Summary> =
		4M
		mono
		Table 1: Training dataset used for ilmulti model
	</Abstractive Summary>
</Paper ID=ument780>


<Paper ID=ument780> <Table ID =2>
	<Abstractive Summary> =
		707060
		-
		-
		Table 2: Translation evaluation scores on IIT-Bombay Hindi-English and UFAL English-Tamil test sets
	</Abstractive Summary>
</Paper ID=ument780>


<Paper ID=ument780> <Table ID =3>
	<Abstractive Summary> =
		Source
		#pairs
		type
		UFAL EnTam
		160K
		en-ta
		Leipzig Newscrawl
		300K
		ta mono
		Kaggle Indian Politics News
		300K
		en mono
		Table 3: Training dataset used for UFAL English-Tamil
		task
	</Abstractive Summary>
</Paper ID=ument780>


<Paper ID=ument780> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Examples from the test set of correctly translated samples
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Qualitative Samples
		The qualitative samples from Table 4 indicate
		en→ta comparable to ta→en, despite the imbal-
		ance in BLEU scores
	</Extractive Summary>
</Paper ID=ument780>


<Paper ID=ument780> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Failure cases among translated samples
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 indicates failure cases, many of which
		shows under-translation phenomena, when all
		source tokens do not have corresponding translated
		tokens in generated translation
	</Extractive Summary>
</Paper ID=ument780>


<Paper ID=ument780> <Table ID =6>
	<Abstractive Summary> =
		04†
		Table 6: Automated evaluation scores on the UFAL En-
		Tam v2
	</Abstractive Summary>
</Paper ID=ument780>


<Paper ID=ument781> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Statistics of our processed parallel data
	</Abstractive Summary>
</Paper ID=ument781>


<Paper ID=ument781> <Table ID =2>
	<Abstractive Summary> =
		org/project/truecase/
		140
		Table 2: This table describes the results of WAT 2019 evaluation of our submitted systems & compared with the
		best system submissions of WAT 2019 & the previous year
	</Abstractive Summary>
</Paper ID=ument781>


<Paper ID=ument782> <Table ID =1>
	<Abstractive Summary> =
		Items
		2,845
		Texts
		1,153
		DevTest
		Items
		2,900
		Texts
		1,114
		Test
		Items
		2,129
		Texts
		1,148
		Table 1: Corpus sizes
	</Abstractive Summary>
</Paper ID=ument782>


<Paper ID=ument782> <Table ID =2>
	<Abstractive Summary> =
		Ensemble of 4 models
		Table 2: Summary of system settings
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the hyperparameters
		of the model, training, and test
	</Extractive Summary>
</Paper ID=ument782>


<Paper ID=ument782> <Table ID =3>
	<Abstractive Summary> =
		40 §
		Submitted model
		Table 3: Results of ASPEC tasks
	</Abstractive Summary>
</Paper ID=ument782>


<Paper ID=ument782> <Table ID =4>
	<Abstractive Summary> =
		97 (-)
		Table 4: Results of TDDC task
	</Abstractive Summary>
</Paper ID=ument782>


<Paper ID=ument783> <Table ID =1>
	<Abstractive Summary> =
		37 
		Table 1: Results of subtasks 
		Japanese-English subtask: 
		The baseline model is a vanilla Transformer 
		model with the first 1M data
	</Abstractive Summary>
	<Extractive Summary> =
		2 
		Results  
		As shown in Table 1, we rank 1st in the direction 
		of 
		Japanese-English, 
		Japanese-Chinese 
		and 
		Chinese-Japanese, and 2nd in one English-
		Japanese
	</Extractive Summary>
</Paper ID=ument783>


<Paper ID=ument783> <Table ID =2>
	<Abstractive Summary> =
		92 
		Table 2: Technical point contributions 
		                                                           
		1 http://nlp
	</Abstractive Summary>
</Paper ID=ument783>


<Paper ID=ument783> <Table ID =3>
	<Abstractive Summary> =
		57 
		Table 3: Results of different data combination
	</Abstractive Summary>
</Paper ID=ument783>


<Paper ID=ument783> <Table ID =4>
	<Abstractive Summary> =
		71 
		Table 4: Technical point contributions 
		Japanese-Chinese subtask: 
		For this subtask, we utilized only the data in 
		ASPEC, no data augmentation was used
	</Abstractive Summary>
</Paper ID=ument783>


<Paper ID=ument783> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Technical point contributions 
		Chinese-Japanese subtask: 
		The baseline model is a vanilla Transformer 
		model with all ASPEC data
	</Abstractive Summary>
	<Extractive Summary> =
		The results of applying thus technologies 
		was in Table 5 (results after averaging of last 8 
		checkpoints, best in two models)
	</Extractive Summary>
</Paper ID=ument783>


<Paper ID=ument783> <Table ID =6>
	<Abstractive Summary> =
		37 
		Table 6: Technical point contributions 
		4 
		Conclusion 
		In this paper, we described our NMT system, 
		which is based on Transformer model
	</Abstractive Summary>
</Paper ID=ument783>


<Paper ID=ument784> <Table ID =1>
	<Abstractive Summary> =
		001
		Batch size in tokens
		4000
		Update frequency
		1
		Table 1: JPO model settings
		2
	</Abstractive Summary>
</Paper ID=ument784>


<Paper ID=ument784> <Table ID =2>
	<Abstractive Summary> =
		45
		Table 2: JPO task results
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 2 shows our model performance for the
		patent task
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, our multilingual result
		did not show improvement in the NMT systems,
		falling behind the unidirectional model by not
		more than 2 BLEU points for single decoding
	</Extractive Summary>
</Paper ID=ument784>


<Paper ID=ument784> <Table ID =3>
	<Abstractive Summary> =
		001
		Batch size in tokens
		14336
		Update frequency
		2
		Table 3: JPX model settings
		various components in the pipeline are propa-
		gated
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Model
		Using similar models settings as (i) JPX model
		in Table 3 with 32,000 subwords tokens at 100%
		character coverage (BASE) and (ii) the JIJI model
		in Table 5 with 10,000 subwords tokens at 100%
		character coverage (MINI), we train one model
		each to compare (i) vs (ii) in the Mixed-domain
		Task
	</Extractive Summary>
</Paper ID=ument784>


<Paper ID=ument784> <Table ID =4>
	<Abstractive Summary> =
		34
		-
		Table 4: JPX task results
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 4 shows our model performance for the
		timely disclosure task
	</Extractive Summary>
</Paper ID=ument784>


<Paper ID=ument784> <Table ID =5>
	<Abstractive Summary> =
		001
		Batch size in tokens
		4000
		Update frequency
		1
		Table 5: JIJI model settings
		glish sentences were combined and fed into Sen-
		tencePiece for training BPE subword units
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Model
		Using similar models settings as (i) JPX model
		in Table 3 with 32,000 subwords tokens at 100%
		character coverage (BASE) and (ii) the JIJI model
		in Table 5 with 10,000 subwords tokens at 100%
		character coverage (MINI), we train one model
		each to compare (i) vs (ii) in the Mixed-domain
		Task
	</Extractive Summary>
</Paper ID=ument784>


<Paper ID=ument784> <Table ID =6>
	<Abstractive Summary> =
		75
		Table 6: JIJI task results
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 6 shows our model performance on the
		newswire task
	</Extractive Summary>
</Paper ID=ument784>


<Paper ID=ument784> <Table ID =7>
	<Abstractive Summary> =
		94
		Table 7: Mixed-domain Task Results
		Table 7 shows the result of our English to Myan-
		mar models
	</Abstractive Summary>
</Paper ID=ument784>


<Paper ID=ument785> <Table ID =1>
	<Abstractive Summary> =
		)
		Table 1:
		Statistics on our experimental data sets (af-
		ter tokenizing and lowercasing)
	</Abstractive Summary>
</Paper ID=ument785>


<Paper ID=ument785> <Table ID =2>
	<Abstractive Summary> =
		71
		Table 2: BLEU scores for ASPEC-JE test set using the Transformer (model) based NMT
	</Abstractive Summary>
	<Extractive Summary> =
		Except this two experiments, the following,
		a series of experiments are all performed using
		“OpenNMT-py” as shown in Table 2, thus, we do
		not mention it every time
	</Extractive Summary>
	<Extractive Summary> =
		But
		we
		found
		that
		the
		big
		transformer
		model (Transformer (big) in Table 2) training by
		“Tensor2Tensor” outperforms the reported models
		training by “OpenNMT-py” (the fourth line in Ta-
		ble 2), especially for English-to-Japanese (42
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2, we show that comb-
		ing these techniques can lead to signiﬁcant im-
		provements of over 1
	</Extractive Summary>
	<Extractive Summary> =
		4, all evalua-
		tion BLEU scores given in Table 2 are evalu-
		ated by WAT 2019 ofﬁcial automatic evaluation
		system14
	</Extractive Summary>
</Paper ID=ument785>


<Paper ID=ument786> <Table ID =1>
	<Abstractive Summary> =
		3k
		Table 1: Statistics on our in-domain parallel data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the size of
		train/development/test splits used in our experi-
		ments
	</Extractive Summary>
	<Extractive Summary> =
		, 2017) but trained on dif-
		ferent training corpora (Table 1)
	</Extractive Summary>
</Paper ID=ument786>


<Paper ID=ument786> <Table ID =2>
	<Abstractive Summary> =
		1
		Batch size
		6144
		Table 2: Hyper parameter values of transformer mod-
		els
	</Abstractive Summary>
</Paper ID=ument786>


<Paper ID=ument786> <Table ID =3>
	<Abstractive Summary> =
		00
		Table 3: Evaluation results: BLEU scores
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 3 demonstrates the BLEU scores of
		our baseline Only GV model and proposed
		TMU model on News Commentary Ja→Ru9 and
		Ru→Ja10 test data for News Commentary shared
		task
	</Extractive Summary>
</Paper ID=ument786>


<Paper ID=ument786> <Table ID =4>
	<Abstractive Summary> =
		B is the test data
		unknown tokens set covered by concatenated vo-
		cabulary of Jiji and News Commentary pivot paral-
		168
		Ja→Ru
		Ru→Ja
		A (Only GV)
		B (TMU)
		A (Only GV)
		B (TMU)
		#tokens
		#types
		#tokens
		#types
		#tokens
		#types
		#tokens
		#types
		Coverage in data
		1,467
		1,220
		2,072
		1,751
		481
		362
		596
		450
		Correctly translated
		85
		65
		191
		147
		26
		21
		31
		24
		Table 4: The coverage of tokens from additional pivot parallel data and the number of correctly translated tokens
		and types of distinct words by each system calculated for test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows token and type coverage and cor-
		rectly translated tokens and types of distinct words
		on test data for A and B, respectively
	</Extractive Summary>
</Paper ID=ument786>


<Paper ID=ument786> <Table ID =5>
	<Abstractive Summary> =
		)
		Table 5: Examples of translating [unknown tokens] included in pivot parallel data C from Russian into Japanese
	</Abstractive Summary>
	<Extractive Summary> =
		By adding Jiji and News
		Commentary corpora, we define the vocabulary set
		newly covered by the test data vocabulary as C as
		follows:
		C = (T ∩ P) − G
		(3)
		Table 5 shows translation examples of only GV
		and TMU systems
	</Extractive Summary>
</Paper ID=ument786>


<Paper ID=ument787> <Table ID =1>
	<Abstractive Summary> =
		647579
		Table 1: RIBES score of Tamil–English and English–
		Tamil System submitted by our team at WAT 2019
	</Abstractive Summary>
</Paper ID=ument787>


<Paper ID=ument787> <Table ID =2>
	<Abstractive Summary> =
		39
		Table 2: BLEU score of Tamil–English and English–
		Tamil System submitted by our team at WAT 2019
	</Abstractive Summary>
	<Extractive Summary> =
		, 2015)
		scores of our submitted systems are shown in Ta-
		ble 1, Table 2, and Table 3 resepectively
	</Extractive Summary>
</Paper ID=ument787>


<Paper ID=ument787> <Table ID =3>
	<Abstractive Summary> =
		783550
		Table 3: AM-FM score of Tamil–English and English–
		Tamil System submitted by our team at WAT 2019
	</Abstractive Summary>
	<Extractive Summary> =
		, 2015)
		scores of our submitted systems are shown in Ta-
		ble 1, Table 2, and Table 3 resepectively
	</Extractive Summary>
</Paper ID=ument787>


<Paper ID=ument788> <Table ID =1>
	<Abstractive Summary> =
		1 M
		D-Test
		998
		4922
		4695
		E-Test
		1595
		7852
		7535
		C-Test
		1400
		8185
		8665
		Table 1: Statistics of our data: the number of sen-
		tences and tokens
	</Abstractive Summary>
</Paper ID=ument788>


<Paper ID=ument788> <Table ID =2>
	<Abstractive Summary> =
		50
		Table 2: WAT 2019 official automatic and manual evaluation results for English→Hindi (HINDEN) tasks
		on the E-Test (EV, upper part) and C-Test (CH, lower part), complemented with our automatic scores
	</Abstractive Summary>
</Paper ID=ument788>


<Paper ID=ument789> <Table ID =1>
	<Abstractive Summary> =
		cz/
		hindi-visual-genome/wat-2019-multimodal-task
		183
		Figure 1: System Architecture
		Dataset distribution
		Items
		Training set
		28932
		Development set
		998
		Evaluation set
		1595
		Challenge set
		1400
		Table 1: Hindi Visual Genome dataset details
	</Abstractive Summary>
	<Extractive Summary> =
		Each item in Table 1 comprises of a
		source text in English, its translation in Hindi,
		the image and a rectangular region in the im-
		age
	</Extractive Summary>
</Paper ID=ument789>


<Paper ID=ument789> <Table ID =2>
	<Abstractive Summary> =
		68
		Table 2: Results obtained in Evaluation Test Set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 shows the scores ob-
		tained by our system on the Evaluation Test
		Set and Challenge Test Set respectively
	</Extractive Summary>
</Paper ID=ument789>


<Paper ID=ument789> <Table ID =3>
	<Abstractive Summary> =
		55
		Table 3: Results obtained in Challenge Test Set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 shows the scores ob-
		tained by our system on the Evaluation Test
		Set and Challenge Test Set respectively
	</Extractive Summary>
	<Extractive Summary> =
		In Ta-
		ble 2 and Table 3, TOT, HIC, and MMT rep-
		resents the text-only translation sub task sys-
		tem, automatic image caption generation sys-
		tem of Hindi-only image captioning sub task
		and the multi-modal translation (using both
		the image and the text) sub task system re-
		spectively
	</Extractive Summary>
</Paper ID=ument789>


<Paper ID=ument789> <Table ID =4>
	<Abstractive Summary> =
		187
		Input Image and Text
		Reference and Output by different Model Types
		Woman standing on tennis court
		Reference: टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		TOT: टेिनस कोटर् पर मनुष्य
		Transliteration: tenis kort par manushy
		Translation: A man on a tennis court
		HIC:
		एक व्यिक् टेिनस खेल रहा है
		Transliteration: ek vyakti tenis khel raha hai
		Translation: A person playing tennis
		MMT:
		टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		Translation: A woman standing on a tennis court
		Table 4: Sample 1 input and output
		Input Image and Text
		Reference and Output by different Model Types
		man stand on skateboard
		Reference: आदमी स्के टबोडर् पर खड़ा है
		Transliteration: aadmee sketabord par khada hai
		TOT: स्के टबोडर् पर मनुष्य
		Transliteration: sketabord par manushy
		Translation: Man on skateboard
		HIC:
		व्यिक् एक स्के टबोडर् पर
		Transliteration: vyakti ek sketabord par
		Translation: A person on a skateboard
		MMT:
		व्यिक् स्के टबोडर् पर खड़ा है
		Transliteration: vyakti sketabord par khada hai
		Translation: A person standing on a skateboard
		Table 5: Sample 2 input and output
		Input Image and Text
		Reference and Output by different Model Types
		A big tv on a stand
		Reference: एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		TOT: एक सेलफोन पर एक बड़ा सा वैन
		Transliteration: ek selaphon par ek bada sa vain
		Translation: A big van on a cellphone
		HIC:
		इमारत के िकनारे पर एक दीवार
		Transliteration: imaarat ke kinaare par ek deevaar
		Translation: A wall on the side of the building
		MMT:
		एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		Translation: A big tv on a stand
		Table 6: Sample 3 input and output
		Kyunghyun Cho, Bart Van Merriënboer, Caglar
		Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
		Holger Schwenk,
		and Yoshua Bengio
	</Abstractive Summary>
	<Extractive Summary> =
		Three sample inputs with the dif-
		ferent forms of an ambiguous word “stand”
		from the challenge test set and their outputs
		are shown in Table 4, Table 5 and Table 6
	</Extractive Summary>
</Paper ID=ument789>


<Paper ID=ument789> <Table ID =5>
	<Abstractive Summary> =
		187
		Input Image and Text
		Reference and Output by different Model Types
		Woman standing on tennis court
		Reference: टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		TOT: टेिनस कोटर् पर मनुष्य
		Transliteration: tenis kort par manushy
		Translation: A man on a tennis court
		HIC:
		एक व्यिक् टेिनस खेल रहा है
		Transliteration: ek vyakti tenis khel raha hai
		Translation: A person playing tennis
		MMT:
		टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		Translation: A woman standing on a tennis court
		Table 4: Sample 1 input and output
		Input Image and Text
		Reference and Output by different Model Types
		man stand on skateboard
		Reference: आदमी स्के टबोडर् पर खड़ा है
		Transliteration: aadmee sketabord par khada hai
		TOT: स्के टबोडर् पर मनुष्य
		Transliteration: sketabord par manushy
		Translation: Man on skateboard
		HIC:
		व्यिक् एक स्के टबोडर् पर
		Transliteration: vyakti ek sketabord par
		Translation: A person on a skateboard
		MMT:
		व्यिक् स्के टबोडर् पर खड़ा है
		Transliteration: vyakti sketabord par khada hai
		Translation: A person standing on a skateboard
		Table 5: Sample 2 input and output
		Input Image and Text
		Reference and Output by different Model Types
		A big tv on a stand
		Reference: एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		TOT: एक सेलफोन पर एक बड़ा सा वैन
		Transliteration: ek selaphon par ek bada sa vain
		Translation: A big van on a cellphone
		HIC:
		इमारत के िकनारे पर एक दीवार
		Transliteration: imaarat ke kinaare par ek deevaar
		Translation: A wall on the side of the building
		MMT:
		एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		Translation: A big tv on a stand
		Table 6: Sample 3 input and output
		Kyunghyun Cho, Bart Van Merriënboer, Caglar
		Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
		Holger Schwenk,
		and Yoshua Bengio
	</Abstractive Summary>
	<Extractive Summary> =
		Three sample inputs with the dif-
		ferent forms of an ambiguous word “stand”
		from the challenge test set and their outputs
		are shown in Table 4, Table 5 and Table 6
	</Extractive Summary>
</Paper ID=ument789>


<Paper ID=ument789> <Table ID =6>
	<Abstractive Summary> =
		187
		Input Image and Text
		Reference and Output by different Model Types
		Woman standing on tennis court
		Reference: टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		TOT: टेिनस कोटर् पर मनुष्य
		Transliteration: tenis kort par manushy
		Translation: A man on a tennis court
		HIC:
		एक व्यिक् टेिनस खेल रहा है
		Transliteration: ek vyakti tenis khel raha hai
		Translation: A person playing tennis
		MMT:
		टेिनस कोटर् पर खड़ी मिहला
		Transliteration: tenis kort par khadee mahila
		Translation: A woman standing on a tennis court
		Table 4: Sample 1 input and output
		Input Image and Text
		Reference and Output by different Model Types
		man stand on skateboard
		Reference: आदमी स्के टबोडर् पर खड़ा है
		Transliteration: aadmee sketabord par khada hai
		TOT: स्के टबोडर् पर मनुष्य
		Transliteration: sketabord par manushy
		Translation: Man on skateboard
		HIC:
		व्यिक् एक स्के टबोडर् पर
		Transliteration: vyakti ek sketabord par
		Translation: A person on a skateboard
		MMT:
		व्यिक् स्के टबोडर् पर खड़ा है
		Transliteration: vyakti sketabord par khada hai
		Translation: A person standing on a skateboard
		Table 5: Sample 2 input and output
		Input Image and Text
		Reference and Output by different Model Types
		A big tv on a stand
		Reference: एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		TOT: एक सेलफोन पर एक बड़ा सा वैन
		Transliteration: ek selaphon par ek bada sa vain
		Translation: A big van on a cellphone
		HIC:
		इमारत के िकनारे पर एक दीवार
		Transliteration: imaarat ke kinaare par ek deevaar
		Translation: A wall on the side of the building
		MMT:
		एक स्टैंड पर एक बड़ा टीवी
		Transliteration: ek staind par ek bada teevee
		Translation: A big tv on a stand
		Table 6: Sample 3 input and output
		Kyunghyun Cho, Bart Van Merriënboer, Caglar
		Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
		Holger Schwenk,
		and Yoshua Bengio
	</Abstractive Summary>
</Paper ID=ument789>


<Paper ID=ument79> <Table ID =1>
	<Abstractive Summary> =
		Table 1: In this sample, NMT systems with different
		implementations present diverse translation errors for
		one long sentence
	</Abstractive Summary>
</Paper ID=ument79>


<Paper ID=ument79> <Table ID =2>
	<Abstractive Summary> =
		17
		Table 2: BLEU score for the representative models in multi-agent training on NIST Chinese-English translation
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2, we see that the model’s
		performance consistently improves as the number
		of agents increases, and we observe that 1) The
		four baseline systems with different implementa-
		tions present diverse translation quality, in particu-
		lar Rel with reﬁned position encoding achieves the
		best performance
	</Extractive Summary>
	<Extractive Summary> =
		As reported in Table 2, although the
		single Rel model achieves the best performance
		compared to the other three single agents, the d
		× 2 (47
	</Extractive Summary>
</Paper ID=ument79>


<Paper ID=ument79> <Table ID =3>
	<Abstractive Summary> =
		27
		Table 3: BLEU score on IWSLT 2014 German-English
		translation
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 3, we observe that both KD-4 and Rel-
		4 models achieve the best result on this dataset
		(35
	</Extractive Summary>
</Paper ID=ument79>


<Paper ID=ument79> <Table ID =4>
	<Abstractive Summary> =
		67
		Table 4:
		BLEU score on newstest2014 for WMT
		English-German translation
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 4, we
		can see that another related work, multi-agent dual
		learning (Wang et al
	</Extractive Summary>
</Paper ID=ument79>


<Paper ID=ument79> <Table ID =5>
	<Abstractive Summary> =
		60
		Table 5: BLEU score for the L2R model in multi-
		agent training on large-scale corpus of Chinese-English
		translation
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 5, we
		can see that multi-agent learning can increase the
		average baseline BLEU score of several test sets
		by 0
	</Extractive Summary>
</Paper ID=ument79>


<Paper ID=ument79> <Table ID =6>
	<Abstractive Summary> =
		62
		Table 6: Accuracy of subject-verb agreement (SVA)
		and word sense disambiguation (WSD) for different
		models
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 6, we can see that the L2R training
		with multiple agents improves the accuracy both
		on SVA and WSD, and we observe that: 1) The
		L2R improves its ability of SVA with the help of
		the other agents signiﬁcantly
	</Extractive Summary>
</Paper ID=ument79>


<Paper ID=ument790> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1:
		Statistics of training bitexts
	</Abstractive Summary>
	<Extractive Summary> =
		3
		(Yandex)9
		Statistics of the training bitexts are shown by
		Table 1, summarising for each language the total
		number of sentences, running words, vocabulary
		size and average sentence length
	</Extractive Summary>
</Paper ID=ument790>


<Paper ID=ument790> <Table ID =2>
	<Abstractive Summary> =
		5
		661
		Table 2: Statistics of development and test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 illustrates statistics of the develop-
		ment and test sets extracted from the correspond-
		ing JaRuNC corpora
	</Extractive Summary>
</Paper ID=ument790>


<Paper ID=ument790> <Table ID =3>
	<Abstractive Summary> =
		VH NH AN RH 明日大統領が到着します
		; Президент приезжает завтра
		Table 3: French-German sentence pair with frequency
		constraints
	</Abstractive Summary>
	<Extractive Summary> =
		Thus, the Japanese-
		Russian parallel sentence with corresponding side
		constraints illustrated in Table 3 is used in training
		to feed the model
	</Extractive Summary>
</Paper ID=ument790>


<Paper ID=ument790> <Table ID =4>
	<Abstractive Summary> =
		36
		Table 4: BLEU score on JaRuNC testset
	</Abstractive Summary>
</Paper ID=ument790>


<Paper ID=ument790> <Table ID =5>
	<Abstractive Summary> =
		41
		-
		Table 5: BLEU score using a batch size = 6,144 with
		3 GPUs
	</Abstractive Summary>
	<Extractive Summary> =
		Validation
		sets are used to select our best performing net-
		works, while results shown in Table 5 are com-
		puted for the ofﬁcial test sets
	</Extractive Summary>
</Paper ID=ument790>


<Paper ID=ument791> <Table ID =1>
	<Abstractive Summary> =
		007 
		Table 1:  Statistics of Datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows data statistics used for the 
		experiments
	</Extractive Summary>
</Paper ID=ument791>


<Paper ID=ument791> <Table ID =2>
	<Abstractive Summary> =
		Domain 
		Number of Word 
		Myanmar 
		Syllable 
		tokens 
		Myanmar 
		English 
		ALT  
		698,347 
		436,923 
		1,138,297 
		UCSY 
		2,966,666 
		2,255,630 
		6,455,588 
		Total 
		36,650,13 
		2,692,553 
		6,569,417 
		Table 2:  Training Details Information
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the data about the training 
		detail
	</Extractive Summary>
</Paper ID=ument791>


<Paper ID=ument791> <Table ID =3>
	<Abstractive Summary> =
		3 
		Mini-batch size   
		64 
		Table 3:  Hyper-parameter of NMT models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the settings of network 
		hyper-parameters for NMT models
	</Extractive Summary>
</Paper ID=ument791>


<Paper ID=ument791> <Table ID =4>
	<Abstractive Summary> =
		657564 
		Table 4:  Myanmar to English Translation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 and Table 5 show the different evaluation 
		metrics for Myanmar-English and English-
		Myanmar translation pairs
	</Extractive Summary>
</Paper ID=ument791>


<Paper ID=ument791> <Table ID =5>
	<Abstractive Summary> =
		698507 
		Table 5:  : English to Myanmar Translation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 and Table 5 show the different evaluation 
		metrics for Myanmar-English and English-
		Myanmar translation pairs
	</Extractive Summary>
</Paper ID=ument791>


<Paper ID=ument792> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example of AddLabel and InsertLabel
	</Abstractive Summary>
</Paper ID=ument792>


<Paper ID=ument792> <Table ID =2>
	<Abstractive Summary> =
		8K
		Yelp
		560K
		38K
		IMDB
		25K
		25K
		Table 2: Sentiment Analysis Datasets
	</Abstractive Summary>
</Paper ID=ument792>


<Paper ID=ument792> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Sentiment Analysis Results
	</Abstractive Summary>
</Paper ID=ument792>


<Paper ID=ument792> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4: Sentiment matching translation accuracy
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 counts the number of sentiment an-
		notation matches
	</Extractive Summary>
</Paper ID=ument792>


<Paper ID=ument792> <Table ID =5>
	<Abstractive Summary> =
		88
		Table 5: BLEU scores
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5)
	</Extractive Summary>
</Paper ID=ument792>


<Paper ID=ument792> <Table ID =6>
	<Abstractive Summary> =
		9
		Table 6: Sentiment word translation performance on
		the test sets
	</Abstractive Summary>
</Paper ID=ument792>


<Paper ID=ument792> <Table ID =7>
	<Abstractive Summary> =
		Seq2Seq
		这是个精
		精
		精明
		明
		明的
		的
		的举动。 (smart)
		Ref-Pos
		这是个精
		精
		精明
		明
		明的
		的
		的行动。 (smart)
		Ref-Neg
		这是个狡
		狡
		狡猾
		猾
		猾的
		的
		的举动。 (cunning)
		AddLabel-Pos
		这是个精
		精
		精明
		明
		明的
		的
		的举动。 (smart)
		AddLabel-Neg
		太狡
		狡
		狡猾
		猾
		猾了。 (cunning)
		Table 7: Sentiment translation examples
	</Abstractive Summary>
	<Extractive Summary> =
		We illustrate some example translations, gener-
		ated by our methods when given different source
		sentiment labels in Table 7, together with baseline
		Seq2Seq translations and reference translations
	</Extractive Summary>
</Paper ID=ument792>


<Paper ID=ument793> <Table ID =1>
	<Abstractive Summary> =
		13
		Table 1: Results of Japanese-Vietnamese NMT systems
		We then modify the baseline architecture with the
		alternative proposed in Section 3
	</Abstractive Summary>
	<Extractive Summary> =
		The
		empirical results are shown in Table 1 for
		Japanese-Vietnamese and in Table 3 for English-
		Vietnamese
	</Extractive Summary>
</Paper ID=ument793>


<Paper ID=ument793> <Table ID =2>
	<Abstractive Summary> =
		Train
		dev2010
		tst2010
		Number of words
		1015
		36
		25
		Table 2: The number of Japanese OOV words replaced
		by their synonyms
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the number of OOV words replaced
		by their synonyms
	</Extractive Summary>
</Paper ID=ument793>


<Paper ID=ument793> <Table ID =3>
	<Abstractive Summary> =
		93)
		Table 3: Results of English-Vietnamese NMT systems
		For comparison purpose, English texts are split
		into sub-words using Sennrich’s BPE methods
	</Abstractive Summary>
	<Extractive Summary> =
		The
		empirical results are shown in Table 1 for
		Japanese-Vietnamese and in Table 3 for English-
		Vietnamese
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 summarizes those results
		and the scores from other systems (Nguyen and
		Chiang, 2017; Huang et al
	</Extractive Summary>
</Paper ID=ument793>


<Paper ID=ument793> <Table ID =4>
	<Abstractive Summary> =
		Train
		tst2012
		tst2013
		Number of words
		5342
		84
		93
		Table 4: The number of rare words in which their af-
		ﬁxes are detached from the English texts in the SAA
		algorithm
	</Abstractive Summary>
</Paper ID=ument793>


<Paper ID=ument793> <Table ID =5>
	<Abstractive Summary> =
		Train
		tst2012
		tst2013
		Number of words
		1889
		37
		41
		Table 5:
		The number of English OOV words are re-
		placed by their synonyms
	</Abstractive Summary>
</Paper ID=ument793>


<Paper ID=ument793> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Examples of outputs from the English→Vietnamese translation systems with the proposed methods
	</Abstractive Summary>
</Paper ID=ument793>


<Paper ID=ument794> <Table ID =1>
	<Abstractive Summary> =
		py
		Table 1: Statistics about the size, content and diacritics
		usage of (Fadel et al
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 10 show that training the
		model with diacritization compared to without
		diacritization improves marginally by 0
	</Extractive Summary>
	<Extractive Summary> =
		com/AliOsm/
		translation-over-diacritization
		Table 10: Translation over Diacritization (ToD) results
		on the test set
		Model
		Training
		Time
		Model
		Size
		Best BLEU
		Score
		Without
		29 Hours
		285MB
		33
	</Extractive Summary>
</Paper ID=ument794>


<Paper ID=ument794> <Table ID =2>
	<Abstractive Summary> =
		com/mishkal
		218
		Table 2: DER/WER comparison of the different FFNN models on the test set
		DER/WER
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		Basic model
		9
	</Abstractive Summary>
	<Extractive Summary> =
		Although the idea of dia-
		critizing each character independently is counter-
		intuitive, the results of the FFNN models on the
		test set (shown in Table 2) are very promising
		with the embeddings model having an obvious ad-
		vantage over the basic and 100-hot models and
		performing much better than the best rule-based
		diacritization system Mishkal3 among the sys-
		tems reviewed by (Fadel et al
	</Extractive Summary>
</Paper ID=ument794>


<Paper ID=ument794> <Table ID =3>
	<Abstractive Summary> =
		10%
		Table 3: Extra training dataset statistics
		Extra Train
		Words Count
		22
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the statistics for the extra
		training dataset
	</Extractive Summary>
</Paper ID=ument794>


<Paper ID=ument794> <Table ID =4>
	<Abstractive Summary> =
		, 2019) and machine learning based
		approaches (Belinkov and Glass, 2015; Abandah
		220
		Table 4: DER/WER comparison of the different RNN models on the test set
		DER/WER
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		Without Extra Train Dataset
		Basic model
		2
	</Abstractive Summary>
	<Extractive Summary> =
		The results of the RNN
		models on the test set (shown in Table 4) are much
		better than the FFNN models by about 67%
	</Extractive Summary>
</Paper ID=ument794>


<Paper ID=ument794> <Table ID =5>
	<Abstractive Summary> =
		83%
		Table 5: DER/WER comparison showing the effect of the weights averaging technique on BNG model
		DER/WER
		Averaged
		Epochs
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		Without
		extra
		train
		dataset
		1
		2
	</Abstractive Summary>
	<Extractive Summary> =
		To
		show the effect of the weights averaging techni-
		que, Table 5 reports the DER/WER statistics rela-
		ted to the BNG model after averaging its weights
		over the last 1, 5, 10, and 20 epochs
	</Extractive Summary>
</Paper ID=ument794>


<Paper ID=ument794> <Table ID =6>
	<Abstractive Summary> =
		So, to make a fair comparison with (Belinkov and
		Glass, 2015)’s system, an auxiliary dataset is built
		from the MSA part of the Tashkeela Corpus using
		the same extraction and cleaning method proposed
		221
		Table 6: Comparing the BNG model with (Barqawi and Zerrouki, 2017) in terms of DER/WER on the test set
		DER/WER
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		(Fadel et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the
		results comparison with Shakkala
	</Extractive Summary>
</Paper ID=ument794>


<Paper ID=ument794> <Table ID =7>
	<Abstractive Summary> =
		65%
		Table 7: Comparing the BNG model with (Belinkov and Glass, 2015) in terms of DER/WER on the test set
		DER/WER
		w/ case ending
		w/o case ending
		w/ case ending
		w/o case ending
		Including ‘no diacritic’
		Excluding ‘no diacritic’
		Classical Arabic Testing Dataset Results
		Our best model
		1
	</Abstractive Summary>
</Paper ID=ument794>


<Paper ID=ument794> <Table ID =8>
	<Abstractive Summary> =
		com
		222
		Table 8: Comparing the BNG model with (Abandah et al
	</Abstractive Summary>
</Paper ID=ument794>


<Paper ID=ument794> <Table ID =9>
	<Abstractive Summary> =
		54%
		Table 9: Vocab size for all sequences types before and
		after BPE step
		Language
		Vocab Size
		Before BPE
		After BPE
		English
		113K
		31K
		Original Arabic
		224K
		32K
		Diacritized Arabic
		402K
		186K
		Diacritics Forms
		41K
		15K
		the OPUS8 project
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 shows the number of tokens be-
		fore and after BPE step for English, Original Ara-
		bic and Diacritized Arabic as well as the Diacritics
		forms when removing the Arabic characters
	</Extractive Summary>
</Paper ID=ument794>


<Paper ID=ument794> <Table ID =10>
	<Abstractive Summary> =
		com/AliOsm/
		translation-over-diacritization
		Table 10: Translation over Diacritization (ToD) results
		on the test set
		Model
		Training
		Time
		Model
		Size
		Best BLEU
		Score
		Without
		29 Hours
		285MB
		33
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 10 show that training the
		model with diacritization compared to without
		diacritization improves marginally by 0
	</Extractive Summary>
</Paper ID=ument794>


<Paper ID=ument795> <Table ID =1>
	<Abstractive Summary> =
		The traces are generated on the
		training set, and the question-answering system is
		5
		Table 1: Game statistics
		Slice of life
		Horror
		QA/Source
		Target
		QA
		Source
		Target
		TextWorld
		9:05
		Lurking Horror
		Afﬂicted
		Anchorhead
		Vocab size
		788
		297
		773
		761
		2256
		Branching factor
		122
		677
		-
		947
		1918
		Number of rooms
		10
		7
		25
		18
		28
		Completion steps
		5
		25
		289
		20
		39
		Words per obs5
		Table 1: Evaluation results on the FB20K (Unseen set-
		ting) and Filtered setting
		22
		Table 1: Comparison with state-of-the art methods for K = 7 and T = 2491
		Table 1: Out-of-the-box scenario results, in BLEU
		scores
		The dataset contains more
		words than exist in the English language as there
		Parameter
		Value
		Tokens
		2,183,079,274
		Unique words
		13,744,507
		Tok’ less than 5 incidence
		11,430,003
		Tok’ less than 100 incidence
		13,436,432
		Table 1: Wikipedia dump statistic In order to suit the
		task, the baseline architecture extends the state-of-
		Table 1: F1 and exact match (EM) score of BERT when
		evaluated on answer retrieval over contexts of varying
		size09 %
		Table 1: Comparison between ESSENTIA and FSA baseline on paraphrase extraction
		of co-occurrences296
		Table 1: The leaderboard performance of the submit-
		ted systems for the explanation regeneration task on
		the held out test set
		Organisms; living things live in their habitat;
		their home
		Consumers eat other organisms
		Table 1: An example of Explanation Regeneration
		relevant explanation sentences needed to answer
		the speciﬁc question4017
		Table 1:
		Base MAP scoring - where the Python
		Baseline1e9 is the same as the original Python Baseline,
		but with the evaluate Granite is a type of
		Answer igneous rock
		Explanation
		[rank 1] igneous rocks or minerals are formed from
		magma or lava cooling
		[rank 2] igneous is a kind of rock
		[rank 3] a type is synonymous with a kind
		[not in gold expl] rock is hard
		[not in gold expl] to cause the formation of means to form
		[not in gold expl] metamorphic rock is a kind of rock
		[not in gold expl] cooling or colder means removing or
		reducing or decreasing heat or temperature
		Table 1: Example depicting the Multi-Hop Inference
		Explanation Regeneration Task5846
		Table 1: MAP score on the development set675
		Table 1: Spearman correlations between human-based judgements and similarity obtained using different embed-
		dings learned on more than 634 millions of tokens19
		Table 1: Accuracies of different models on the time
		classiﬁcation task
		134
		Dataset
		Trusted (# Docs)
		Satire (# Docs)
		Hoax (# Docs)
		Propaganda (# Docs)
		LUN-train
		GN except ‘APW’ and ‘WPB’ (9,995)
		The Onion (14,047)
		American News (6,942)
		Activist Report (17,870)
		LUN-test
		GN only ‘APW’ and ‘WPB’ (750)
		The Borowitz Report, Clickhole (750)
		DC Gazette (750)
		The Natural News (750)
		SLN
		The Toronto Star, The NY Times (180)
		The Onion, The Beaverton (180)
		-
		-
		RPN
		WSJ, NBC, etc (75)
		The Onion, The Beaverton, etc (75)
		-
		-
		Table 1: Statistics about different dataset sources1
		Data Description
		We collected full text, all sections including ab-
		stract, introduction, and experiments, of 5,850 pa-
		pers in the proceedings of ACM SIGKDD 1994–
		2015, IEEE ICDM 2001–2015, The Web Confer-
		Table 1: Neighboring words for concept typing First, we describe the data sets
		154
		Intent
		Utterances with slot labels
		searchFlight
		ﬁnd me a ﬂight from [origin](Paris) to [destination](New York)
		searchFlight
		I need a ﬂight leaving [date](this weekend) to [destination](Berlin)
		searchFlight
		show me ﬂights to go to [destination](new york) leaving [date](this evening)
		Table 1: Examples of the SNIPS Dataset For
		example, in the COL module, the prediction is
		computed by the using following equations:
		160
		Table 1: Example Databases
		Database
		Table Name
		Column Names
		department store
		customers
		customer id
		customer name
		customer address
		customer orders
		order id
		customer id
		order date
		coffee shop
		shop
		shop id
		address
		num of staff
		P num
		COL = P
		�
		Wnum
		1
		Hnum
		Q/COL
		⊤ + Wnum
		2
		Hnum
		HS/COL
		⊤�
		P val
		COL = P
		�
		Wval
		1 Hval
		Q/COL
		⊤ + Wval
		2 Hval
		HS/COL
		⊤
		+ Wval
		3 HCOL⊤�
		,
		where “num” means the number of columns, “val”
		means the index(es) of the column(s), “Q” means
		question, “HS” means path history, “COL” means
		column, P(U)
		=
		softmax(V tanh(U)) is a
		probability distribution given score U and param-
		eter V, the W’s are learnable parameters, and
		the H1/2’s are conditional embeddings deﬁned as
		H1/2 = softmax(H1WH⊤
		2 )H1 Our
		approach is quite suitable for downstream applica-
		Gene
		Phenotype
		Disease
		CA1
		Hypothermia
		Ischemia
		CA1
		Neuronal loss
		Ischemia
		CA1
		Hyperglycemia
		Ischemia
		Gene
		Drug
		Disease
		BTK
		Ibrutinib
		Chronic
		lymphocytic leukemia
		BTK
		Acalabrutinib
		Chronic
		lymphocytic leukemia
		Table 1:
		Example paths for query types ‘Gene-
		Phenotype-Disease’,
		‘Gene-Drug-Disease’ showing
		the ability to reason over 1-hop7
		Table 1: Node matching results: percentage of CKG con-
		taining a correct match (higher is better); percentage of test
		nodes correctly resolved (higher is better), spuriously merged
		with some KG node and spuriously added to KG as a new
		node (lower is better)
		recover the one (or none) ﬁnal match for each
		method With coref-
		erence resolution, the graph is further enriched se-
		mantically, linking nodes that refer to the same
		Table 1: Library of pre-deﬁned Generators
		Symbol
		Description
		gCR
		Coreference Resolution generator
		gEL
		X
		Entity Linker to the resource X
		gNER
		Named Entity Recognizer
		gOIE
		Open Information Extractor
		gπ
		provider of an embedding function
		pi
		entity in the text2
		cross-sentence relations
		yes
		no
		Table 1: Data statistics
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Evaluation results and discussion
		In Table 1, our method shows a signiﬁcant im-
		provement on FB20K
	</Extractive Summary>
	<Extractive Summary> =
		We obtained bet-
		ter results with logistic regression (reported in
		Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		The other methods, mainly deep learning (DL)
		in Table 1 use millions of training examples from
		Wikipedia’s anchor links and corresponding entit-
		ies
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 sums up our experimental res-
		ults (averaged P @1 is also referred as accuracy
		(Sun et al
	</Extractive Summary>
	<Extractive Summary> =
		“L+S+M” corresponds to the same results in Table 1
		and “L+S+M+T” is the setting with gold transcriptions
		added to the training set
	</Extractive Summary>
	<Extractive Summary> =
		1% of the words appear
		in the corpus less than ﬁve times as can be seen in
		Table 1 and only 2
	</Extractive Summary>
	<Extractive Summary> =
		1
		Mining Paraphrases
		Table 1 compares the performance of ESSENTIA
		with the FSA baseline for paraphrase mining
	</Extractive Summary>
	<Extractive Summary> =
		It also effectively forces
		all submissions to include a ranking over all ex-
		planations - a simple ﬁx (with the Python Baseline
		rescored in Table 1) will be submitted via GitHub
	</Extractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Task Description
		The MIER-19 task (Jansen and Ustalov, 2019) fo-
		cused on computing a ranked list of explanation
		sentences (as shown in Table 1) for a question and
		its correct answer (QA) from an unordered collec-
		tion of explanation sentences
	</Extractive Summary>
	<Extractive Summary> =
		How-
		ever, more often than not, a QA lexically matches
		with irrelevant candidate explanation sentences
		(consider the latter explanation sentences in the
		example in Table 1) resulting in semantic drift
	</Extractive Summary>
	<Extractive Summary> =
		We consider a variant of our model that ranks
		individual facts directly rather than paths (BERT
		Re-ranker in the Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the different results ob-
		tained with our joint learning approach (with and
		without antonyms), separate results for ﬁrst order
		and second order representations, and word2vec
		(skip-gram with negative sampling) with and with-
		out retroﬁtting
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the statistics
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows a few examples
	</Extractive Summary>
	<Extractive Summary> =
		In Table 1 and Figure 1 respectively, we show
		an example of the databases and the graph con-
		161
		Figure 1: An example of the constructed graph for
		databases in Table 1
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows two examples of ranked 1-hop
		paths for query types, ‘Gene-Phenotype-Disease’
		and ‘Gene-Drug-Disease’
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 provides a brief sum-
		mary of both datasets
	</Extractive Summary>
</Paper ID=ument795>


<Paper ID=ument795> <Table ID =2>
	<Abstractive Summary> =
		In both do-
		7
		Table 2: Results for the slice of life games0
		Table 2: Evaluation results on the FB15K (Seen set-
		ting)
		Table 2:
		Computing times rounded to the minute5
		Table 2: Results with transcriptions, in BLEU scores
		36
		Types
		Examples
		Inﬂections
		car:cars, go:going
		Derivation
		able:unable, life:lifeless
		Lexicography
		sea:water, clean:dirty
		Encyclopedia
		spain:madrid, dog:bark
		Table 2: Examples for the four different types of anal-
		ogy questions
		All four types are balanced, i31
		Table 2: Supporting facts identiﬁcation: Harmonic
		mean (F1), Precision and Recall
		Model
		F1
		P
		R
		Baseline (Yang et al One class
		55
		Domain
		Example paraphrases
		Restaurant search
		recommend a good place
		suggest a place
		Restaurant reservation
		get me a place
		get me a spot
		Get directions
		show me the way
		get me directions
		Get weather
		need the weather
		want the weather
		Request ride
		ﬁnd a taxi
		need an uber
		Share location
		share my location
		send my location
		Hotel Wi-Fi
		log onto the Wi-Fi
		connect to Wi-Fi
		Hotel checkout
		extend our checkout
		have a late checkout
		Table 2: Examples of domain-speciﬁc paraphrases35
		Table 2: Explanation reconstruction performance broken down by the level of lexical overlap a given fact has with
		the question and/or answer3273
		Table 2: Comparison of the Relevance Learners with
		different dataset preparation techniques without re-
		ranking
		It can be observed in Table 2 that the two class
		classiﬁcation head with context performs best and
		BERT Large outperforms XLNET Large for this
		particular task96
		Table 2: MAP scoring of new methods08
		Table 2: Explanation table types (21 of 63 in total) sorted by the proportion of their occurrence for their respective
		sentences participating in at least 1% of the training and development set QA explanations4017
		Table 2: MAP score on the hidden test set447
		Table 2: Cosine similarity measures for different word pairs979
		Table 2: Ranking results9
		Table 2: 2-way classiﬁcation results on SLN For synonym(X, Y ), we merge node X and
		Y in H: if neither was in H, we create a new iso-
		lated node named “X, Y ”; if one of them existed
		in H, we update the name of the existing node as
		“X, Y ”; if both existed, we merge their ancestor
		nodes as the new ancestor node AX ∪ AY , and we
		143
		Table 2: Data science concept examples extracted
		by two complementary phrase mining tools Utterances
		Train
		1,310
		SSL candidate (unlablled)
		11,774
		Dev
		700
		Test
		700
		Table 2: SNIPS data statistics0%
		Table 2: Comparison of different methods with respect
		to ﬁnal exact matching accuracy and COL module ac-
		curacy on development set5M
		Table 2: Triple Data set statistics5
		Table 2: Comparison with previous methods for rela-
		tion extraction on SCIERC dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 show results for the slice of
		life and horror domains, respectively
	</Extractive Summary>
	<Extractive Summary> =
		In Table 25, our method shows a better MR than
		the previous models
	</Extractive Summary>
	<Extractive Summary> =
		We report this in Table 2, along with
		some experimental computing times
	</Extractive Summary>
	<Extractive Summary> =
		The results shown in Table 2 are consistent with
		previous work: adding transcriptions further en-
		hance the system performance
	</Extractive Summary>
	<Extractive Summary> =
		We observe a signiﬁcant drop
		in performance (≈20% F1 score) when we opti-
		mise the baseline only for supporting facts identi-
		ﬁcation (see Baseline Replication in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		This results
		in a notable drop of F1 score (see Table 2) rein-
		forcing the evidence that explicit semantic infor-
		mation encoded in relational form contributes to-
		wards the performance of the model
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 lists
		some domains and examples of domain-speciﬁc
		paraphrases detected by ESSENTIA
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows that for all models submitted to
		the shared task, the largest contributor to model
		performance is from locating explanation sentences
		that have 2 or more shared words with the question
		or answer
	</Extractive Summary>
	<Extractive Summary> =
		The eval-
		uation in Table 2 shows that higher overall expla-
		nation regeneration performance does not neces-
		sarily imply better multi-hop performance
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2 are the results of our
		evaluation on the validation set
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, these optimisation steps
		increased the Python Baseline score signiﬁcantly,
		without introducing algorithmic complexity
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 lists
		prominent explanation table types used in at least
		1% of the training and development explanations
	</Extractive Summary>
	<Extractive Summary> =
		To form pairs of unrelated entities, ﬁrst, we cre-
		ate lists of living and nonliving entities using the
		KINDOF explanation table type (one among 61
		explanation tables with a few shown in Table 2),
		where sentences are of the pattern “[LHS] is a
		kind of [RHS]”
	</Extractive Summary>
	<Extractive Summary> =
		To illustrate the impact the joint learning ap-
		proach has on the embeddings space, Table 2 pro-
		vides examples showing the impact our approach
		has on the cosine similarity of different kinds of
		word pairs
	</Extractive Summary>
	<Extractive Summary> =
		6
		Results
		Table 2 shows the quantitative results for the two
		way classiﬁcation between satirical and trusted
		news articles
	</Extractive Summary>
	<Extractive Summary> =
		Table 2
		shows examples of data science concepts the tools
		extracted
	</Extractive Summary>
	<Extractive Summary> =
		synonym
		sibling
		ancestor
		# unique concept pairs
		41
		234
		138
		# extractions
		1,966
		1,379
		381
		AutoPhrase, some 1-gram and n-gram high qual-
		ity phrase are in Table 2a
	</Extractive Summary>
	<Extractive Summary> =
		For results of SchBase,
		some acronyms and typical abbreviation expan-
		sions we selected are in Table 2b
	</Extractive Summary>
	<Extractive Summary> =
		Overall dataset
		statistics is given in Table 2, intent distributions in
		Train, SSL, Dev and Test set are shown in Figure
		3
	</Extractive Summary>
	<Extractive Summary> =
		First system (“Baseline”)
		is trained only on training data in Table 2, with-
		out applying SSL
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		Extraction Performance
		Table 2 and Table 3
		compare our concept network CTN with baselines
		for SCIERC and SemEval18, respectively
	</Extractive Summary>
</Paper ID=ument795>


<Paper ID=ument795> <Table ID =3>
	<Abstractive Summary> =
		45
		Table 3: Results for horror games
		Table 3: Entities in the FB15K and their descriptions,
		and the relation “dubbing performance/actor” holds
		between Entity1 and 2 In our classiﬁ-
		cation experiments, each document is represented
		Dataset
		Class
		train
		test
		AGs news
		4
		3000
		1900
		DBpedia
		14
		40000
		5000
		Yelp reviews
		5
		130000
		10000
		Yelp polarity
		2
		280000
		19000
		Yahoo
		10
		140000
		5000
		Amazon review
		5
		600000
		130000
		Amazon polarity
		2
		1800000
		200000
		Table 3:
		datasets information, columns are the num-
		ber of classes in the dataset, number of samples in the
		training set and number of samples in the testing set,
		respectively
		47
		Table 3: Precision, Recall and F1 score of the DGN
		model with different values of k assigned to the pre-
		ﬁltering algorithmidf
		JS
		ASU
		RDAI
		COR
		Top 5 Ranked Rows
		Marked Gold
		27%
		32%
		46%
		36%
		49%
		Highly Relevant
		19%
		22%
		26%
		27%
		20%
		Possibly Relevant or Topical
		40%
		25%
		16%
		30%
		24%
		Not Relevant
		14%
		22%
		13%
		7%
		7%
		Manual Relevance@5 (Gold+HR)
		46%
		54%
		72%
		63%
		79%
		Top 10 Ranked Rows
		Marked Gold
		17%
		23%
		31%
		29%
		34%
		Highly Relevant
		15%
		15%
		21%
		26%
		17%
		Possibly Relevant or Topical
		45%
		34%
		25%
		30%
		33%
		Not Relevant
		33%
		29%
		22%
		15%
		16%
		Manual Relevance@10 (Gold+HR)
		32%
		38%
		52%
		55%
		51%
		Top 20 Ranked Rows
		Marked Gold
		11%
		14%
		18%
		18%
		20%
		Highly Relevant
		13%
		12%
		18%
		23%
		19%
		Possibly Relevant or Topical
		40%
		39%
		29%
		38%
		40%
		Not Relevant
		36%
		35%
		35%
		22%
		21%
		Manual Relevance@20 (Gold+HR)
		24%
		26%
		36%
		41%
		39%
		Table 3: Manually rated relevance judgements for the top 5, top 10, and top 20 ranked rows, across 14 randomly
		sampled questions3665
		Table 3: Comparison of Relevance Learners with Iter-
		ative Re-ranking till depth N
		Explanation Roles
		BERT
		N=15
		CENTRAL
		00439
		Table 3: Contribution of Explanation Roles - Dev-Set
		MAP per role (computed by ﬁltering explanations of
		other roles out of the gold explanation list then com-
		puting the MAP as per normal)
		wards being able to build longer explanations as
		our semantic relevance methods become more so-
		phisticated, 2017) were extracted from q and e sentences
		Table 3: 76 feature categories for explanation ranking3%
		Table 3: Results of preliminary analysis for the impact
		proxy95
		Table 3: 4-way classiﬁcation results for different mod-
		els, “models”) used in the pat-
		Table 3: Performance of concept typing71
		Table 3: Transductive results on SNIPS data
		Exp 1
		Exp 2
		query types
		4
		6
		avg no of paths
		6
		33
		min no of paths
		2
		10
		total paths
		573
		3356
		Table 3: Path data set statistics used in experiment 1
		and 26
		Table 3: Comparison with previous methods for rela-
		tion extraction on SemEval18 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 show results for the slice of
		life and horror domains, respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the entities
		in the FB15K and their descriptions, and the rela-
		tion “dubbing performance/actor” holds between
		Entity1 and 2
	</Extractive Summary>
	<Extractive Summary> =
		, 2015) and
		they are listed in Table 3
		Each document from the datasets was converted
		to a vector in the size of the embedding
	</Extractive Summary>
	<Extractive Summary> =
		The results reported in Table 3 take into account
		the combined performance of the full pipeline with
		different hyperparameters assigned to the preﬁlter-
		ing algorithm
	</Extractive Summary>
	<Extractive Summary> =
		The results of this manual analysis are shown
		in Table 3, presented as proportions of the top-N
		ranked rows for each model
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, the propor-
		tion marked gold is equivalent to the Precision@N
		metric in Table 4, but measured using 14 ques-
		tions instead of the entire test set
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, we compare the Rel-
		evance Learners before and after Iterative Re-
		ranking
	</Extractive Summary>
	<Extractive Summary> =
		4 These representations re-
		sulted in 76 feature categories shown in Table 3
		which are used to generate (q, a, e) triplet instance
		one-hot encoded feature vectors
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows that the
		accuracy of concept typing (a 4-class classiﬁ-
		cation task) is 0
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		Extraction Performance
		Table 2 and Table 3
		compare our concept network CTN with baselines
		for SCIERC and SemEval18, respectively
	</Extractive Summary>
</Paper ID=ument795>


<Paper ID=ument795> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Accuracy percentage results of Mikolov Analogy test and the four main question types created in the
		Gladkova Analogy test33
		Precision@N
		Precision@1
		541
		55%
		67%
		63%
		79%
		79%
		Precision@2
		541
		43%
		49%
		47%
		63%
		69%
		Precision@3
		541
		36%
		44%
		44%
		54%
		59%
		Precision@4
		541
		31%
		37%
		41%
		48%
		53%
		Precision@5
		541
		27%
		32%
		39%
		43%
		48%
		Precision@10
		541
		17%
		21%
		27%
		29%
		34%
		Precision@20
		541
		11%
		13%
		18%
		18%
		21%
		Table 4: Explanation reconstruction performance broken down by the explanatory role of facts, table knowledge
		types, and using a Precision@N metric000586
		Table 4:
		MAP for different Explanation Roles for
		BERT trained with classiﬁcation head and context, be-
		fore and after re-ranking till N=15
		Figure 1: MAP v/s Length of the Gold Explanation
		Table 5 shows the MAP scores for the best mod-
		els on both the Validation and the hidden Test set4
		Table 4: Mean Average Precision (mAP) percentage
		scores for Elementary Science QA explanation sen-
		tence ranking from only pairwise LTR (row 1) and as a
		hybrid system with rules (row 2) on development and
		test datasets, respectively145
		Table 4: Correlation results963 (207/8)
		Table 4: False type predictions in red01
		0
		0
		0
		0
		0
		0
		0
		Table 4: Hits@n and MAP@n scores for Embed+predrank, Embed+cosine scoring functions comparing to baseline
		for top 100, 300, 500 predictions given by KBC model5
		Table 4: Ablation study of isolated contribution of each
		rule
	</Abstractive Summary>
	<Extractive Summary> =
		As can be observed from Table 4, Word2Vec
		achieved the best results in both Mikolov and
		Gladkova analogy tests
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, the propor-
		tion marked gold is equivalent to the Precision@N
		metric in Table 4, but measured using 14 ques-
		tions instead of the entire test set
	</Extractive Summary>
	<Extractive Summary> =
		“Marked gold” performance is equivalent to
		Precision@N performance in Table 4 with 14 samples instead of 541
	</Extractive Summary>
	<Extractive Summary> =
		All Top-20 values are
		within 1% of the full test sample in Table 4
		of facts for assessing coverage and completeness,
		but that assessing relevance of highly ranked facts
		is still best accomplished by including at least a
		modest manual evaluation
	</Extractive Summary>
	<Extractive Summary> =
		3
		Additional Performance Evaluation
		Additional automatic performance evaluations are
		shown in Table 4, which includes evaluation by
		73
		Questions
		Baseline
		Team
		Metric
		N
		tf
	</Extractive Summary>
	<Extractive Summary> =
		1
		Performance by Explanatory Roles
		Table 4 shows performance broken down by the
		explanatory role of the facts being analysed
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows explanation reconstruction per-
		formance by table knowledge type
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 compares the MAP for different expla-
		nation roles before and after iterative re-ranking
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results and Discussion
		Table 4 shows the elementary science QA expla-
		nation sentence ranking results from the ofﬁcial
		MIER-19 scoring program in terms of mAP
	</Extractive Summary>
	<Extractive Summary> =
		Table 4a gives two of
		the 27 MajVot’s false predictions
	</Extractive Summary>
	<Extractive Summary> =
		Table 4b shows
		three of the 8 false cases among 215 predictions
	</Extractive Summary>
	<Extractive Summary> =
		According to results in Table 4, Embed+cosine
		scoring
		function
		yields
		better
		Hits@n
		and
		MAP@n scores than Embed+pred rank in gen-
		eral
	</Extractive Summary>
</Paper ID=ument795>


<Paper ID=ument795> <Table ID =5>
	<Abstractive Summary> =
		69
		Table 5: Average results across the different classiﬁcation algorithms2962
		Table 5: Validation and Test MAP for the best Rele-
		vance Learner, Reranked and the provide baseline mod-
		els
		81
		Gold Explanation
		Predicted Explanation
		heat means temperature increases
		adding heat means increasing temperature
		sunlight is a kind of solar energy
		the sun is the source of solar energy called sunlight
		look at means observe
		observe means see
		a kitten is a kind of young; baby cat
		a kitten is a young; baby cat
		Table 6: Similar Explanations present in top 30
		Gold Explanation
		Predicted Explanation
		an animal is a kind of living thing
		an animal is a kind of organism
		an organism is a living thing
		a frog is a kind of aquatic animal
		a frog is a kind of amphibian
		an amphibian is a kind of animal
		a leaf is a part of a tree
		a leaf is a part of a,green plant
		a tree is a kind of plant
		Table 7: Single-hop and Multi-hop Errors in top 30
		Gold Explanation
		to reduce means to decrease
		Predicted Explanation
		to lower means to decrease
		less means a reduced amount
		Table 8: Errors due to Sentence Vectors in top 30
		direct lexical or semantic overlap with the
		question and correct answer14
		Table 5: Ablation results of the LTR SVMrank system
		in terms of percentage mAP with feature groups (from
		seven feature types considered) incrementally added098
		Table 5: Feature selection results For result of
		144
		Table 5: Number of concepts of each type12
		Table 5: Spearman ranking correlation (rs) for Rp-Rt
		ranks of paths in ground truth and their relative ranking
		in predicted paths, Rp-Rr ranks of randomly permuted
		paths and their relative ranking in predicted paths, Rt-
		Rr ranks of paths in ground truth and their relative
		ranking in randomly permuted paths88%
		123
		-
		-
		-
		Table 5: Accuracy (TP = true positives) per relationship
		type on testing data
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen from Table 5,
		Word2Vec achieved the best average accuracy re-
		sults on ﬁve datasets out of seven though the dif-
		ferences vs
	</Extractive Summary>
	<Extractive Summary> =
		Emb2 did not achieve
		the best accuracy scores but from Table 5 Emb2
		had more stable results than the baselines as the
		standard deviation of its ranking is much lower
		than Line, Node2Vec and Word2Vec
	</Extractive Summary>
	<Extractive Summary> =
		000586
		Table 4:
		MAP for different Explanation Roles for
		BERT trained with classiﬁcation head and context, be-
		fore and after re-ranking till N=15
		Figure 1: MAP v/s Length of the Gold Explanation
		Table 5 shows the MAP scores for the best mod-
		els on both the Validation and the hidden Test set
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the number of concepts of each type
		we have for hierarchy induction
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the Spearman ranking correla-
		tion (rs) for Rp-Rt ranks of paths in ground truth
		(Rt) and their relative ranking in predicted paths
		(Rp), Rp-Rr ranks of randomly permuted paths
		(Rr) and their relative ranking in predicted paths
		(Rp), Rt-Rr ranks of paths in ground truth (Rt)
		and their relative ranking in randomly permuted
		paths (Rr)
	</Extractive Summary>
	<Extractive Summary> =
		Relation Type Sensitivity
		Table 5 shows the
		ability of CTN to identify each type of relation-
		ship that is labeled in the ground truth data
	</Extractive Summary>
</Paper ID=ument795>


<Paper ID=ument795> <Table ID =6>
	<Abstractive Summary> =
		49
		Table 6: Statistical signiﬁcance results between the best
		conﬁguration of the proposed algorithm, Emb2, and the
		baselines2962
		Table 5: Validation and Test MAP for the best Rele-
		vance Learner, Reranked and the provide baseline mod-
		els
		81
		Gold Explanation
		Predicted Explanation
		heat means temperature increases
		adding heat means increasing temperature
		sunlight is a kind of solar energy
		the sun is the source of solar energy called sunlight
		look at means observe
		observe means see
		a kitten is a kind of young; baby cat
		a kitten is a young; baby cat
		Table 6: Similar Explanations present in top 30
		Gold Explanation
		Predicted Explanation
		an animal is a kind of living thing
		an animal is a kind of organism
		an organism is a living thing
		a frog is a kind of aquatic animal
		a frog is a kind of amphibian
		an amphibian is a kind of animal
		a leaf is a part of a tree
		a leaf is a part of a,green plant
		a tree is a kind of plant
		Table 7: Single-hop and Multi-hop Errors in top 30
		Gold Explanation
		to reduce means to decrease
		Predicted Explanation
		to lower means to decrease
		less means a reduced amount
		Table 8: Errors due to Sentence Vectors in top 30
		direct lexical or semantic overlap with the
		question and correct answer
		$Problem
		$Method
		$Object
		$Metric
		Count (predicted)
		52
		104
		9
		50
		Count (ground truth)
		53
		100
		13
		49
		Table 6: Number of relations for each type
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 presents the statistical signiﬁcance re-
		sults between all baselines and Emb2
	</Extractive Summary>
	<Extractive Summary> =
		For example in Table 6, the
		predicted explanations were present in top
		30
	</Extractive Summary>
	<Extractive Summary> =
		Table 6
		gives the number of relation tuples we extracted
		for each type
	</Extractive Summary>
</Paper ID=ument795>


<Paper ID=ument795> <Table ID =7>
	<Abstractive Summary> =
		2962
		Table 5: Validation and Test MAP for the best Rele-
		vance Learner, Reranked and the provide baseline mod-
		els
		81
		Gold Explanation
		Predicted Explanation
		heat means temperature increases
		adding heat means increasing temperature
		sunlight is a kind of solar energy
		the sun is the source of solar energy called sunlight
		look at means observe
		observe means see
		a kitten is a kind of young; baby cat
		a kitten is a young; baby cat
		Table 6: Similar Explanations present in top 30
		Gold Explanation
		Predicted Explanation
		an animal is a kind of living thing
		an animal is a kind of organism
		an organism is a living thing
		a frog is a kind of aquatic animal
		a frog is a kind of amphibian
		an amphibian is a kind of animal
		a leaf is a part of a tree
		a leaf is a part of a,green plant
		a tree is a kind of plant
		Table 7: Single-hop and Multi-hop Errors in top 30
		Gold Explanation
		to reduce means to decrease
		Predicted Explanation
		to lower means to decrease
		less means a reduced amount
		Table 8: Errors due to Sentence Vectors in top 30
		direct lexical or semantic overlap with the
		question and correct answer It is not fair to compare with
		taxonomy construction methods because we are
		targeting a different problem, that is to generate
		a concept hierarchy of facets with three kinds of
		Table 7: Comparing HiGrowth with baselines on
		building hierarchy from data science literature
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 7 we can see the model makes both
		kinds of errors
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 7, Hi-
		Growth consistently outperforms TAXI on all three
		kinds of paths: it improves synonym detection by
		3
	</Extractive Summary>
</Paper ID=ument795>


<Paper ID=ument795> <Table ID =8>
	<Abstractive Summary> =
		2962
		Table 5: Validation and Test MAP for the best Rele-
		vance Learner, Reranked and the provide baseline mod-
		els
		81
		Gold Explanation
		Predicted Explanation
		heat means temperature increases
		adding heat means increasing temperature
		sunlight is a kind of solar energy
		the sun is the source of solar energy called sunlight
		look at means observe
		observe means see
		a kitten is a kind of young; baby cat
		a kitten is a young; baby cat
		Table 6: Similar Explanations present in top 30
		Gold Explanation
		Predicted Explanation
		an animal is a kind of living thing
		an animal is a kind of organism
		an organism is a living thing
		a frog is a kind of aquatic animal
		a frog is a kind of amphibian
		an amphibian is a kind of animal
		a leaf is a part of a tree
		a leaf is a part of a,green plant
		a tree is a kind of plant
		Table 7: Single-hop and Multi-hop Errors in top 30
		Gold Explanation
		to reduce means to decrease
		Predicted Explanation
		to lower means to decrease
		less means a reduced amount
		Table 8: Errors due to Sentence Vectors in top 30
		direct lexical or semantic overlap with the
		question and correct answer
	</Abstractive Summary>
	<Extractive Summary> =
		The use of sentence vectors for similarity
		introduces errors shown in Table 8, where
		the correct explanation contains “reduce”, but
		the predicted explanations which have simi-
		lar words like “lower”, “less” and “reduced
		amount”, are ranked higher
	</Extractive Summary>
</Paper ID=ument795>


<Paper ID=ument795> <Table ID =9>
	<Abstractive Summary> =
		The algorithm gives
		importance to the relevance score, vector
		Gold Explanation
		temperature rise means become warmer
		Predicted Explanation
		warm up means increase temperature
		warmer means greater; higher in temperature
		Table 9: Model unable to understand ordering in Lexi-
		cal Glue in top 30
		similarity with previously selected explana-
		tions and vector similarity with the question
		answer pair
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 is
		one such instance
	</Extractive Summary>
</Paper ID=ument795>


<Paper ID=ument797> <Table ID =1>
	<Abstractive Summary> =
		The traces are generated on the
		training set, and the question-answering system is
		6
		Table 1: Game statistics
		Slice of life
		Horror
		QA/Source
		Target
		QA
		Source
		Target
		TextWorld
		9:05
		Lurking Horror
		Afﬂicted
		Anchorhead
		Vocab size
		788
		297
		773
		761
		2256
		Branching factor
		122
		677
		-
		947
		1918
		Number of rooms
		10
		7
		25
		18
		28
		Completion steps
		5
		25
		289
		20
		39
		Words per obs
	</Abstractive Summary>
</Paper ID=ument797>


<Paper ID=ument797> <Table ID =2>
	<Abstractive Summary> =
		In both do-
		8
		Table 2: Results for the slice of life games
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 show results for the slice of
		life and horror domains, respectively
	</Extractive Summary>
</Paper ID=ument797>


<Paper ID=ument797> <Table ID =3>
	<Abstractive Summary> =
		45
		Table 3: Results for horror games
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 and Table 3 show results for the slice of
		life and horror domains, respectively
	</Extractive Summary>
</Paper ID=ument797>


<Paper ID=ument798> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Evaluation results on the FB20K (Unseen set-
		ting) and Filtered setting
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Evaluation results and discussion
		In Table 1, our method shows a signiﬁcant im-
		provement on FB20K
	</Extractive Summary>
</Paper ID=ument798>


<Paper ID=ument798> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Evaluation results on the FB15K (Seen set-
		ting)
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 25, our method shows a better MR than
		the previous models
	</Extractive Summary>
</Paper ID=ument798>


<Paper ID=ument798> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Entities in the FB15K and their descriptions,
		and the relation “dubbing performance/actor” holds
		between Entity1 and 2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the entities
		in the FB15K and their descriptions, and the rela-
		tion “dubbing performance/actor” holds between
		Entity1 and 2
	</Extractive Summary>
</Paper ID=ument798>


<Paper ID=ument799> <Table ID =1>
	<Abstractive Summary> =
		23
		Table 1: Comparison with state-of-the art methods for K = 7 and T = 249
	</Abstractive Summary>
	<Extractive Summary> =
		We obtained bet-
		ter results with logistic regression (reported in
		Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		The other methods, mainly deep learning (DL)
		in Table 1 use millions of training examples from
		Wikipedia’s anchor links and corresponding entit-
		ies
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 sums up our experimental res-
		ults (averaged P @1 is also referred as accuracy
		(Sun et al
	</Extractive Summary>
</Paper ID=ument799>


<Paper ID=ument799> <Table ID =2>
	<Abstractive Summary> =
		Table 2:
		Computing times rounded to the minute
	</Abstractive Summary>
	<Extractive Summary> =
		We report this in Table 2, along with
		some experimental computing times
	</Extractive Summary>
</Paper ID=ument799>


<Paper ID=ument8> <Table ID =1>
	<Abstractive Summary> =
		4
		Table 1: Mean Pearson correlation on STS tasks for
		Deep Learning and Set-based methods using fastText
	</Abstractive Summary>
</Paper ID=ument8>


<Paper ID=ument8> <Table ID =2>
	<Abstractive Summary> =
		Indeed, the performance of max-pooled
		vectors with Spearman correlation approaches or
		Approach
		Time complexity
		MaxPool+SPR
		O(nd + d log d)
		CKA
		O(nd2 + d2)
		DynaMax
		O(n2d)
		SoftCard
		O(n2d)
		WMD
		O(n3 log n · d)
		WMD (relaxed)
		O(n2d)
		Table 2: Computational complexity of some of the set-
		based STS methods discussed in this paper
	</Abstractive Summary>
</Paper ID=ument8>


<Paper ID=ument80> <Table ID =1>
	<Abstractive Summary> =
		1M
		Table 1: Parallel training data statistics
	</Abstractive Summary>
</Paper ID=ument80>


<Paper ID=ument80> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Main results ﬁne-tuned with source-target parallel data
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we report principal results after ﬁne-
		tuning the pre-trained models using source-target
		parallel data
	</Extractive Summary>
	<Extractive Summary> =
		Note that, unlike Table 2, the mul-
		tilingual baselines exclude source→target and
		target→source directions
	</Extractive Summary>
	<Extractive Summary> =
		showing comparable or better ﬁne-tuned perfor-
		mance against plain transfer (see also Table 2)
	</Extractive Summary>
</Paper ID=ument80>


<Paper ID=ument80> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: Pivot adapter variations (German→Czech)
	</Abstractive Summary>
</Paper ID=ument80>


<Paper ID=ument80> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Cross-lingual encoder variations (French→
		German)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 veriﬁes that the noisy input in autoen-
		coding is indeed beneﬁcial to our cross-lingual
		encoder
	</Extractive Summary>
</Paper ID=ument80>


<Paper ID=ument80> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Zero-resource results
	</Abstractive Summary>
	<Extractive Summary> =
		Compared to pivot translation (Table 5), our
		872
		best results are also clearly better in French
		→German and comparable in German→Czech
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows how our pre-trained models
		perform in zero-resource scenarios with the two
		options
	</Extractive Summary>
</Paper ID=ument80>


<Paper ID=ument80> <Table ID =6>
	<Abstractive Summary> =
		6
		Table 6:
		Results ﬁne-tuned with a combination of source-target parallel data and large synthetic data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows our transfer learning results
		ﬁne-tuned with a combination of given parallel
		data and generated synthetic parallel data
	</Extractive Summary>
</Paper ID=ument80>


<Paper ID=ument800> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Out-of-the-box scenario results, in BLEU
		scores
	</Abstractive Summary>
	<Extractive Summary> =
		“L+S+M” corresponds to the same results in Table 1
		and “L+S+M+T” is the setting with gold transcriptions
		added to the training set
	</Extractive Summary>
</Paper ID=ument800>


<Paper ID=ument800> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Results with transcriptions, in BLEU scores
	</Abstractive Summary>
	<Extractive Summary> =
		The results shown in Table 2 are consistent with
		previous work: adding transcriptions further en-
		hance the system performance
	</Extractive Summary>
</Paper ID=ument800>


<Paper ID=ument801> <Table ID =1>
	<Abstractive Summary> =
		The dataset contains more
		words than exist in the English language as there
		Parameter
		Value
		Tokens
		2,183,079,274
		Unique words
		13,744,507
		Tok’ less than 5 incidence
		11,430,003
		Tok’ less than 100 incidence
		13,436,432
		Table 1: Wikipedia dump statistic
	</Abstractive Summary>
	<Extractive Summary> =
		1% of the words appear
		in the corpus less than ﬁve times as can be seen in
		Table 1 and only 2
	</Extractive Summary>
</Paper ID=ument801>


<Paper ID=ument801> <Table ID =2>
	<Abstractive Summary> =
		37
		Types
		Examples
		Inﬂections
		car:cars, go:going
		Derivation
		able:unable, life:lifeless
		Lexicography
		sea:water, clean:dirty
		Encyclopedia
		spain:madrid, dog:bark
		Table 2: Examples for the four different types of anal-
		ogy questions
		All four types are balanced, i
	</Abstractive Summary>
</Paper ID=ument801>


<Paper ID=ument801> <Table ID =3>
	<Abstractive Summary> =
		In our classiﬁ-
		cation experiments, each document is represented
		Dataset
		Class
		train
		test
		AGs news
		4
		3000
		1900
		DBpedia
		14
		40000
		5000
		Yelp reviews
		5
		130000
		10000
		Yelp polarity
		2
		280000
		19000
		Yahoo
		10
		140000
		5000
		Amazon review
		5
		600000
		130000
		Amazon polarity
		2
		1800000
		200000
		Table 3:
		datasets information, columns are the num-
		ber of classes in the dataset, number of samples in the
		training set and number of samples in the testing set,
		respectively
	</Abstractive Summary>
	<Extractive Summary> =
		, 2015) and
		they are listed in Table 3
		Each document from the datasets was converted
		to a vector in the size of the embedding
	</Extractive Summary>
</Paper ID=ument801>


<Paper ID=ument801> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Accuracy percentage results of Mikolov Analogy test and the four main question types created in the
		Gladkova Analogy test
	</Abstractive Summary>
	<Extractive Summary> =
		As can be observed from Table 4, Word2Vec
		achieved the best results in both Mikolov and
		Gladkova analogy tests
	</Extractive Summary>
</Paper ID=ument801>


<Paper ID=ument801> <Table ID =5>
	<Abstractive Summary> =
		69
		Table 5: Average results across the different classiﬁcation algorithms
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen from Table 5,
		Word2Vec achieved the best average accuracy re-
		sults on ﬁve datasets out of seven though the dif-
		ferences vs
	</Extractive Summary>
	<Extractive Summary> =
		Emb2 did not achieve
		the best accuracy scores but from Table 5 Emb2
		had more stable results than the baselines as the
		standard deviation of its ranking is much lower
		than Line, Node2Vec and Word2Vec
	</Extractive Summary>
</Paper ID=ument801>


<Paper ID=ument801> <Table ID =6>
	<Abstractive Summary> =
		49
		Table 6: Statistical signiﬁcance results between the best
		conﬁguration of the proposed algorithm, Emb2, and the
		baselines
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 presents the statistical signiﬁcance re-
		sults between all baselines and Emb2
	</Extractive Summary>
</Paper ID=ument801>


<Paper ID=ument802> <Table ID =1>
	<Abstractive Summary> =
		In order to suit the
		task, the baseline architecture extends the state-of-
		Table 1: F1 and exact match (EM) score of BERT when
		evaluated on answer retrieval over contexts of varying
		size
	</Abstractive Summary>
</Paper ID=ument802>


<Paper ID=ument802> <Table ID =2>
	<Abstractive Summary> =
		31
		Table 2: Supporting facts identiﬁcation: Harmonic
		mean (F1), Precision and Recall
		Model
		F1
		P
		R
		Baseline (Yang et al
	</Abstractive Summary>
	<Extractive Summary> =
		We observe a signiﬁcant drop
		in performance (≈20% F1 score) when we opti-
		mise the baseline only for supporting facts identi-
		ﬁcation (see Baseline Replication in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		This results
		in a notable drop of F1 score (see Table 2) rein-
		forcing the evidence that explicit semantic infor-
		mation encoded in relational form contributes to-
		wards the performance of the model
	</Extractive Summary>
</Paper ID=ument802>


<Paper ID=ument802> <Table ID =3>
	<Abstractive Summary> =
		48
		Table 3: Precision, Recall and F1 score of the DGN
		model with different values of k assigned to the pre-
		ﬁltering algorithm
	</Abstractive Summary>
	<Extractive Summary> =
		The results reported in Table 3 take into account
		the combined performance of the full pipeline with
		different hyperparameters assigned to the preﬁlter-
		ing algorithm
	</Extractive Summary>
</Paper ID=ument802>


<Paper ID=ument803> <Table ID =1>
	<Abstractive Summary> =
		09 %
		Table 1: Comparison between ESSENTIA and FSA baseline on paraphrase extraction
		of co-occurrences
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Mining Paraphrases
		Table 1 compares the performance of ESSENTIA
		with the FSA baseline for paraphrase mining
	</Extractive Summary>
</Paper ID=ument803>


<Paper ID=ument803> <Table ID =2>
	<Abstractive Summary> =
		One class
		56
		Domain
		Example paraphrases
		Restaurant search
		recommend a good place
		suggest a place
		Restaurant reservation
		get me a place
		get me a spot
		Get directions
		show me the way
		get me directions
		Get weather
		need the weather
		want the weather
		Request ride
		ﬁnd a taxi
		need an uber
		Share location
		share my location
		send my location
		Hotel Wi-Fi
		log onto the Wi-Fi
		connect to Wi-Fi
		Hotel checkout
		extend our checkout
		have a late checkout
		Table 2: Examples of domain-speciﬁc paraphrases
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists
		some domains and examples of domain-speciﬁc
		paraphrases detected by ESSENTIA
	</Extractive Summary>
</Paper ID=ument803>


<Paper ID=ument805> <Table ID =1>
	<Abstractive Summary> =
		296
		Table 1: The leaderboard performance of the submit-
		ted systems for the explanation regeneration task on
		the held out test set
	</Abstractive Summary>
</Paper ID=ument805>


<Paper ID=ument805> <Table ID =2>
	<Abstractive Summary> =
		35
		Table 2: Explanation reconstruction performance broken down by the level of lexical overlap a given fact has with
		the question and/or answer
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows that for all models submitted to
		the shared task, the largest contributor to model
		performance is from locating explanation sentences
		that have 2 or more shared words with the question
		or answer
	</Extractive Summary>
	<Extractive Summary> =
		The eval-
		uation in Table 2 shows that higher overall expla-
		nation regeneration performance does not neces-
		sarily imply better multi-hop performance
	</Extractive Summary>
</Paper ID=ument805>


<Paper ID=ument805> <Table ID =3>
	<Abstractive Summary> =
		idf
		JS
		ASU
		RDAI
		COR
		Top 5 Ranked Rows
		Marked Gold
		27%
		32%
		46%
		36%
		49%
		Highly Relevant
		19%
		22%
		26%
		27%
		20%
		Possibly Relevant or Topical
		40%
		25%
		16%
		30%
		24%
		Not Relevant
		14%
		22%
		13%
		7%
		7%
		Manual Relevance@5 (Gold+HR)
		46%
		54%
		72%
		63%
		79%
		Top 10 Ranked Rows
		Marked Gold
		17%
		23%
		31%
		29%
		34%
		Highly Relevant
		15%
		15%
		21%
		26%
		17%
		Possibly Relevant or Topical
		45%
		34%
		25%
		30%
		33%
		Not Relevant
		33%
		29%
		22%
		15%
		16%
		Manual Relevance@10 (Gold+HR)
		32%
		38%
		52%
		55%
		51%
		Top 20 Ranked Rows
		Marked Gold
		11%
		14%
		18%
		18%
		20%
		Highly Relevant
		13%
		12%
		18%
		23%
		19%
		Possibly Relevant or Topical
		40%
		39%
		29%
		38%
		40%
		Not Relevant
		36%
		35%
		35%
		22%
		21%
		Manual Relevance@20 (Gold+HR)
		24%
		26%
		36%
		41%
		39%
		Table 3: Manually rated relevance judgements for the top 5, top 10, and top 20 ranked rows, across 14 randomly
		sampled questions
	</Abstractive Summary>
	<Extractive Summary> =
		The results of this manual analysis are shown
		in Table 3, presented as proportions of the top-N
		ranked rows for each model
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3, the propor-
		tion marked gold is equivalent to the Precision@N
		metric in Table 4, but measured using 14 ques-
		tions instead of the entire test set
	</Extractive Summary>
</Paper ID=ument805>


<Paper ID=ument805> <Table ID =4>
	<Abstractive Summary> =
		33
		Precision@N
		Precision@1
		541
		55%
		67%
		63%
		79%
		79%
		Precision@2
		541
		43%
		49%
		47%
		63%
		69%
		Precision@3
		541
		36%
		44%
		44%
		54%
		59%
		Precision@4
		541
		31%
		37%
		41%
		48%
		53%
		Precision@5
		541
		27%
		32%
		39%
		43%
		48%
		Precision@10
		541
		17%
		21%
		27%
		29%
		34%
		Precision@20
		541
		11%
		13%
		18%
		18%
		21%
		Table 4: Explanation reconstruction performance broken down by the explanatory role of facts, table knowledge
		types, and using a Precision@N metric
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, the propor-
		tion marked gold is equivalent to the Precision@N
		metric in Table 4, but measured using 14 ques-
		tions instead of the entire test set
	</Extractive Summary>
	<Extractive Summary> =
		“Marked gold” performance is equivalent to
		Precision@N performance in Table 4 with 14 samples instead of 541
	</Extractive Summary>
	<Extractive Summary> =
		All Top-20 values are
		within 1% of the full test sample in Table 4
		of facts for assessing coverage and completeness,
		but that assessing relevance of highly ranked facts
		is still best accomplished by including at least a
		modest manual evaluation
	</Extractive Summary>
	<Extractive Summary> =
		3
		Additional Performance Evaluation
		Additional automatic performance evaluations are
		shown in Table 4, which includes evaluation by
		74
		Questions
		Baseline
		Team
		Metric
		N
		tf
	</Extractive Summary>
	<Extractive Summary> =
		1
		Performance by Explanatory Roles
		Table 4 shows performance broken down by the
		explanatory role of the facts being analysed
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows explanation reconstruction per-
		formance by table knowledge type
	</Extractive Summary>
</Paper ID=ument805>


<Paper ID=ument806> <Table ID =1>
	<Abstractive Summary> =
		Organisms; living things live in their habitat;
		their home
		Consumers eat other organisms
		Table 1: An example of Explanation Regeneration
		relevant explanation sentences needed to answer
		the speciﬁc question
	</Abstractive Summary>
</Paper ID=ument806>


<Paper ID=ument806> <Table ID =2>
	<Abstractive Summary> =
		3273
		Table 2: Comparison of the Relevance Learners with
		different dataset preparation techniques without re-
		ranking
		It can be observed in Table 2 that the two class
		classiﬁcation head with context performs best and
		BERT Large outperforms XLNET Large for this
		particular task
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2 are the results of our
		evaluation on the validation set
	</Extractive Summary>
</Paper ID=ument806>


<Paper ID=ument806> <Table ID =3>
	<Abstractive Summary> =
		3665
		Table 3: Comparison of Relevance Learners with Iter-
		ative Re-ranking till depth N
		Explanation Roles
		BERT
		N=15
		CENTRAL
		0
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3, we compare the Rel-
		evance Learners before and after Iterative Re-
		ranking
	</Extractive Summary>
</Paper ID=ument806>


<Paper ID=ument806> <Table ID =4>
	<Abstractive Summary> =
		000586
		Table 4:
		MAP for different Explanation Roles for
		BERT trained with classiﬁcation head and context, be-
		fore and after re-ranking till N=15
		Figure 1: MAP v/s Length of the Gold Explanation
		Table 5 shows the MAP scores for the best mod-
		els on both the Validation and the hidden Test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 compares the MAP for different expla-
		nation roles before and after iterative re-ranking
	</Extractive Summary>
</Paper ID=ument806>


<Paper ID=ument806> <Table ID =5>
	<Abstractive Summary> =
		2962
		Table 5: Validation and Test MAP for the best Rele-
		vance Learner, Reranked and the provide baseline mod-
		els
		82
		Gold Explanation
		Predicted Explanation
		heat means temperature increases
		adding heat means increasing temperature
		sunlight is a kind of solar energy
		the sun is the source of solar energy called sunlight
		look at means observe
		observe means see
		a kitten is a kind of young; baby cat
		a kitten is a young; baby cat
		Table 6: Similar Explanations present in top 30
		Gold Explanation
		Predicted Explanation
		an animal is a kind of living thing
		an animal is a kind of organism
		an organism is a living thing
		a frog is a kind of aquatic animal
		a frog is a kind of amphibian
		an amphibian is a kind of animal
		a leaf is a part of a tree
		a leaf is a part of a,green plant
		a tree is a kind of plant
		Table 7: Single-hop and Multi-hop Errors in top 30
		Gold Explanation
		to reduce means to decrease
		Predicted Explanation
		to lower means to decrease
		less means a reduced amount
		Table 8: Errors due to Sentence Vectors in top 30
		direct lexical or semantic overlap with the
		question and correct answer
	</Abstractive Summary>
	<Extractive Summary> =
		000586
		Table 4:
		MAP for different Explanation Roles for
		BERT trained with classiﬁcation head and context, be-
		fore and after re-ranking till N=15
		Figure 1: MAP v/s Length of the Gold Explanation
		Table 5 shows the MAP scores for the best mod-
		els on both the Validation and the hidden Test set
	</Extractive Summary>
</Paper ID=ument806>


<Paper ID=ument806> <Table ID =6>
	<Abstractive Summary> =
		2962
		Table 5: Validation and Test MAP for the best Rele-
		vance Learner, Reranked and the provide baseline mod-
		els
		82
		Gold Explanation
		Predicted Explanation
		heat means temperature increases
		adding heat means increasing temperature
		sunlight is a kind of solar energy
		the sun is the source of solar energy called sunlight
		look at means observe
		observe means see
		a kitten is a kind of young; baby cat
		a kitten is a young; baby cat
		Table 6: Similar Explanations present in top 30
		Gold Explanation
		Predicted Explanation
		an animal is a kind of living thing
		an animal is a kind of organism
		an organism is a living thing
		a frog is a kind of aquatic animal
		a frog is a kind of amphibian
		an amphibian is a kind of animal
		a leaf is a part of a tree
		a leaf is a part of a,green plant
		a tree is a kind of plant
		Table 7: Single-hop and Multi-hop Errors in top 30
		Gold Explanation
		to reduce means to decrease
		Predicted Explanation
		to lower means to decrease
		less means a reduced amount
		Table 8: Errors due to Sentence Vectors in top 30
		direct lexical or semantic overlap with the
		question and correct answer
	</Abstractive Summary>
	<Extractive Summary> =
		For example in Table 6, the
		predicted explanations were present in top
		30
	</Extractive Summary>
</Paper ID=ument806>


<Paper ID=ument806> <Table ID =7>
	<Abstractive Summary> =
		2962
		Table 5: Validation and Test MAP for the best Rele-
		vance Learner, Reranked and the provide baseline mod-
		els
		82
		Gold Explanation
		Predicted Explanation
		heat means temperature increases
		adding heat means increasing temperature
		sunlight is a kind of solar energy
		the sun is the source of solar energy called sunlight
		look at means observe
		observe means see
		a kitten is a kind of young; baby cat
		a kitten is a young; baby cat
		Table 6: Similar Explanations present in top 30
		Gold Explanation
		Predicted Explanation
		an animal is a kind of living thing
		an animal is a kind of organism
		an organism is a living thing
		a frog is a kind of aquatic animal
		a frog is a kind of amphibian
		an amphibian is a kind of animal
		a leaf is a part of a tree
		a leaf is a part of a,green plant
		a tree is a kind of plant
		Table 7: Single-hop and Multi-hop Errors in top 30
		Gold Explanation
		to reduce means to decrease
		Predicted Explanation
		to lower means to decrease
		less means a reduced amount
		Table 8: Errors due to Sentence Vectors in top 30
		direct lexical or semantic overlap with the
		question and correct answer
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 7 we can see the model makes both
		kinds of errors
	</Extractive Summary>
</Paper ID=ument806>


<Paper ID=ument806> <Table ID =8>
	<Abstractive Summary> =
		2962
		Table 5: Validation and Test MAP for the best Rele-
		vance Learner, Reranked and the provide baseline mod-
		els
		82
		Gold Explanation
		Predicted Explanation
		heat means temperature increases
		adding heat means increasing temperature
		sunlight is a kind of solar energy
		the sun is the source of solar energy called sunlight
		look at means observe
		observe means see
		a kitten is a kind of young; baby cat
		a kitten is a young; baby cat
		Table 6: Similar Explanations present in top 30
		Gold Explanation
		Predicted Explanation
		an animal is a kind of living thing
		an animal is a kind of organism
		an organism is a living thing
		a frog is a kind of aquatic animal
		a frog is a kind of amphibian
		an amphibian is a kind of animal
		a leaf is a part of a tree
		a leaf is a part of a,green plant
		a tree is a kind of plant
		Table 7: Single-hop and Multi-hop Errors in top 30
		Gold Explanation
		to reduce means to decrease
		Predicted Explanation
		to lower means to decrease
		less means a reduced amount
		Table 8: Errors due to Sentence Vectors in top 30
		direct lexical or semantic overlap with the
		question and correct answer
	</Abstractive Summary>
	<Extractive Summary> =
		The use of sentence vectors for similarity
		introduces errors shown in Table 8, where
		the correct explanation contains “reduce”, but
		the predicted explanations which have simi-
		lar words like “lower”, “less” and “reduced
		amount”, are ranked higher
	</Extractive Summary>
</Paper ID=ument806>


<Paper ID=ument806> <Table ID =9>
	<Abstractive Summary> =
		The algorithm gives
		importance to the relevance score, vector
		Gold Explanation
		temperature rise means become warmer
		Predicted Explanation
		warm up means increase temperature
		warmer means greater; higher in temperature
		Table 9: Model unable to understand ordering in Lexi-
		cal Glue in top 30
		similarity with previously selected explana-
		tions and vector similarity with the question
		answer pair
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 is
		one such instance
	</Extractive Summary>
</Paper ID=ument806>


<Paper ID=ument807> <Table ID =1>
	<Abstractive Summary> =
		4017
		Table 1:
		Base MAP scoring - where the Python
		Baseline1e9 is the same as the original Python Baseline,
		but with the evaluate
	</Abstractive Summary>
	<Extractive Summary> =
		It also effectively forces
		all submissions to include a ranking over all ex-
		planations - a simple ﬁx (with the Python Baseline
		rescored in Table 1) will be submitted via GitHub
	</Extractive Summary>
</Paper ID=ument807>


<Paper ID=ument807> <Table ID =2>
	<Abstractive Summary> =
		96
		Table 2: MAP scoring of new methods
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, these optimisation steps
		increased the Python Baseline score signiﬁcantly,
		without introducing algorithmic complexity
	</Extractive Summary>
</Paper ID=ument807>


<Paper ID=ument807> <Table ID =3>
	<Abstractive Summary> =
		0439
		Table 3: Contribution of Explanation Roles - Dev-Set
		MAP per role (computed by ﬁltering explanations of
		other roles out of the gold explanation list then com-
		puting the MAP as per normal)
		wards being able to build longer explanations as
		our semantic relevance methods become more so-
		phisticated
	</Abstractive Summary>
</Paper ID=ument807>


<Paper ID=ument808> <Table ID =1>
	<Abstractive Summary> =
		Granite is a type of
		Answer igneous rock
		Explanation
		[rank 1] igneous rocks or minerals are formed from
		magma or lava cooling
		[rank 2] igneous is a kind of rock
		[rank 3] a type is synonymous with a kind
		[not in gold expl] rock is hard
		[not in gold expl] to cause the formation of means to form
		[not in gold expl] metamorphic rock is a kind of rock
		[not in gold expl] cooling or colder means removing or
		reducing or decreasing heat or temperature
		Table 1: Example depicting the Multi-Hop Inference
		Explanation Regeneration Task
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Task Description
		The MIER-19 task (Jansen and Ustalov, 2019) fo-
		cused on computing a ranked list of explanation
		sentences (as shown in Table 1) for a question and
		its correct answer (QA) from an unordered collec-
		tion of explanation sentences
	</Extractive Summary>
	<Extractive Summary> =
		How-
		ever, more often than not, a QA lexically matches
		with irrelevant candidate explanation sentences
		(consider the latter explanation sentences in the
		example in Table 1) resulting in semantic drift
	</Extractive Summary>
</Paper ID=ument808>


<Paper ID=ument808> <Table ID =2>
	<Abstractive Summary> =
		08
		Table 2: Explanation table types (21 of 63 in total) sorted by the proportion of their occurrence for their respective
		sentences participating in at least 1% of the training and development set QA explanations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists
		prominent explanation table types used in at least
		1% of the training and development explanations
	</Extractive Summary>
	<Extractive Summary> =
		To form pairs of unrelated entities, ﬁrst, we cre-
		ate lists of living and nonliving entities using the
		KINDOF explanation table type (one among 61
		explanation tables with a few shown in Table 2),
		where sentences are of the pattern “[LHS] is a
		kind of [RHS]”
	</Extractive Summary>
</Paper ID=ument808>


<Paper ID=ument808> <Table ID =3>
	<Abstractive Summary> =
		, 2017) were extracted from q and e sentences
		Table 3: 76 feature categories for explanation ranking
	</Abstractive Summary>
	<Extractive Summary> =
		4 These representations re-
		sulted in 76 feature categories shown in Table 3
		which are used to generate (q, a, e) triplet instance
		one-hot encoded feature vectors
	</Extractive Summary>
</Paper ID=ument808>


<Paper ID=ument808> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4: Mean Average Precision (mAP) percentage
		scores for Elementary Science QA explanation sen-
		tence ranking from only pairwise LTR (row 1) and as a
		hybrid system with rules (row 2) on development and
		test datasets, respectively
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results and Discussion
		Table 4 shows the elementary science QA expla-
		nation sentence ranking results from the ofﬁcial
		MIER-19 scoring program in terms of mAP
	</Extractive Summary>
</Paper ID=ument808>


<Paper ID=ument808> <Table ID =5>
	<Abstractive Summary> =
		14
		Table 5: Ablation results of the LTR SVMrank system
		in terms of percentage mAP with feature groups (from
		seven feature types considered) incrementally added
	</Abstractive Summary>
</Paper ID=ument808>


<Paper ID=ument809> <Table ID =1>
	<Abstractive Summary> =
		5846
		Table 1: MAP score on the development set
	</Abstractive Summary>
	<Extractive Summary> =
		We consider a variant of our model that ranks
		individual facts directly rather than paths (BERT
		Re-ranker in the Table 1)
	</Extractive Summary>
</Paper ID=ument809>


<Paper ID=ument809> <Table ID =2>
	<Abstractive Summary> =
		4017
		Table 2: MAP score on the hidden test set
	</Abstractive Summary>
</Paper ID=ument809>


<Paper ID=ument81> <Table ID =1>
	<Abstractive Summary> =
		)
		500
		ellipsis (VP)
		500
		Table 1: Size of test sets: total number of test instances
		and with regard to the distance between sentences re-
		quiring consistency (in the number of sentences)
	</Abstractive Summary>
	<Extractive Summary> =
		)” in Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Another includes
		cases of verb phrase ellipsis in English, which
		does not exist in Russian, thus requires predicting
		the verb when translating into Russian (“ellipsis
		(VP)” in Table 1)
	</Extractive Summary>
</Paper ID=ument81>


<Paper ID=ument81> <Table ID =2>
	<Abstractive Summary> =
		60
		Table 2: BLEU scores
	</Abstractive Summary>
	<Extractive Summary> =
		1
		General results
		The BLEU scores are provided in Table 2 (we
		evaluate translations of 4-sentence fragments)
	</Extractive Summary>
</Paper ID=ument81>


<Paper ID=ument81> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Results on contrastive test sets for speciﬁc
		contextual phenomena (deixis, lexical consistency, el-
		lipsis (inﬂection), and VP ellipsis)
	</Abstractive Summary>
</Paper ID=ument81>


<Paper ID=ument81> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Detailed accuracy on deixis and lexical cohe-
		sion test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 provides scores for deixis and lexi-
		cal cohesion separately for different distances be-
		tween sentences requiring consistency
	</Extractive Summary>
</Paper ID=ument81>


<Paper ID=ument81> <Table ID =5>
	<Abstractive Summary> =
		all
		equal
		better
		worse
		700
		367
		242
		90
		100%
		52%
		35%
		13%
		Table 5: Human evaluation results, comparing DocRe-
		pair with baseline
	</Abstractive Summary>
</Paper ID=ument81>


<Paper ID=ument81> <Table ID =6>
	<Abstractive Summary> =
		2
		Table 6: Results for DocRepair trained on different
		amount of data
	</Abstractive Summary>
	<Extractive Summary> =
		1
		The amount of training data
		Table 6 provides BLEU and consistency scores for
		the DocRepair model trained on different amount
		of data
	</Extractive Summary>
	<Extractive Summary> =
		5m instances randomly chosen from
		monolingual data (Table 6) are different from the
		ones for the model trained on 2
	</Extractive Summary>
</Paper ID=ument81>


<Paper ID=ument81> <Table ID =7>
	<Abstractive Summary> =
		8
		Table 7: Consistency scores for the DocRepair model
		trained on 2
	</Abstractive Summary>
</Paper ID=ument81>


<Paper ID=ument81> <Table ID =8>
	<Abstractive Summary> =
		8
		Table 8: DocRepair trained on 2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows that the quality of the model
		trained on data which came from the parallel part
		is worse than the one trained on monolingual data
	</Extractive Summary>
</Paper ID=ument81>


<Paper ID=ument810> <Table ID =1>
	<Abstractive Summary> =
		675
		Table 1: Spearman correlations between human-based judgements and similarity obtained using different embed-
		dings learned on more than 634 millions of tokens
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the different results ob-
		tained with our joint learning approach (with and
		without antonyms), separate results for ﬁrst order
		and second order representations, and word2vec
		(skip-gram with negative sampling) with and with-
		out retroﬁtting
	</Extractive Summary>
</Paper ID=ument810>


<Paper ID=ument810> <Table ID =2>
	<Abstractive Summary> =
		447
		Table 2: Cosine similarity measures for different word pairs
	</Abstractive Summary>
	<Extractive Summary> =
		To illustrate the impact the joint learning ap-
		proach has on the embeddings space, Table 2 pro-
		vides examples showing the impact our approach
		has on the cosine similarity of different kinds of
		word pairs
	</Extractive Summary>
</Paper ID=ument810>


<Paper ID=ument811> <Table ID =1>
	<Abstractive Summary> =
		19
		Table 1: Accuracies of different models on the time
		classiﬁcation task
	</Abstractive Summary>
</Paper ID=ument811>


<Paper ID=ument811> <Table ID =2>
	<Abstractive Summary> =
		979
		Table 2: Ranking results
	</Abstractive Summary>
</Paper ID=ument811>


<Paper ID=ument811> <Table ID =3>
	<Abstractive Summary> =
		3%
		Table 3: Results of preliminary analysis for the impact
		proxy
	</Abstractive Summary>
</Paper ID=ument811>


<Paper ID=ument811> <Table ID =4>
	<Abstractive Summary> =
		145
		Table 4: Correlation results
	</Abstractive Summary>
</Paper ID=ument811>


<Paper ID=ument811> <Table ID =5>
	<Abstractive Summary> =
		098
		Table 5: Feature selection results
	</Abstractive Summary>
</Paper ID=ument811>


<Paper ID=ument812> <Table ID =1>
	<Abstractive Summary> =
		135
		Dataset
		Trusted (# Docs)
		Satire (# Docs)
		Hoax (# Docs)
		Propaganda (# Docs)
		LUN-train
		GN except ‘APW’ and ‘WPB’ (9,995)
		The Onion (14,047)
		American News (6,942)
		Activist Report (17,870)
		LUN-test
		GN only ‘APW’ and ‘WPB’ (750)
		The Borowitz Report, Clickhole (750)
		DC Gazette (750)
		The Natural News (750)
		SLN
		The Toronto Star, The NY Times (180)
		The Onion, The Beaverton (180)
		-
		-
		RPN
		WSJ, NBC, etc (75)
		The Onion, The Beaverton, etc (75)
		-
		-
		Table 1: Statistics about different dataset sources
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the statistics
	</Extractive Summary>
</Paper ID=ument812>


<Paper ID=ument812> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: 2-way classiﬁcation results on SLN
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results
		Table 2 shows the quantitative results for the two
		way classiﬁcation between satirical and trusted
		news articles
	</Extractive Summary>
</Paper ID=ument812>


<Paper ID=ument812> <Table ID =3>
	<Abstractive Summary> =
		95
		Table 3: 4-way classiﬁcation results for different mod-
		els
	</Abstractive Summary>
</Paper ID=ument812>


<Paper ID=ument813> <Table ID =1>
	<Abstractive Summary> =
		1
		Data Description
		We collected full text, all sections including ab-
		stract, introduction, and experiments, of 5,850 pa-
		pers in the proceedings of ACM SIGKDD 1994–
		2015, IEEE ICDM 2001–2015, The Web Confer-
		Table 1: Neighboring words for concept typing
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows a few examples
	</Extractive Summary>
</Paper ID=ument813>


<Paper ID=ument813> <Table ID =2>
	<Abstractive Summary> =
		For synonym(X, Y ), we merge node X and
		Y in H: if neither was in H, we create a new iso-
		lated node named “X, Y ”; if one of them existed
		in H, we update the name of the existing node as
		“X, Y ”; if both existed, we merge their ancestor
		nodes as the new ancestor node AX ∪ AY , and we
		144
		Table 2: Data science concept examples extracted
		by two complementary phrase mining tools
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2
		shows examples of data science concepts the tools
		extracted
	</Extractive Summary>
	<Extractive Summary> =
		synonym
		sibling
		ancestor
		# unique concept pairs
		41
		234
		138
		# extractions
		1,966
		1,379
		381
		AutoPhrase, some 1-gram and n-gram high qual-
		ity phrase are in Table 2a
	</Extractive Summary>
	<Extractive Summary> =
		For results of SchBase,
		some acronyms and typical abbreviation expan-
		sions we selected are in Table 2b
	</Extractive Summary>
</Paper ID=ument813>


<Paper ID=ument813> <Table ID =3>
	<Abstractive Summary> =
		, “models”) used in the pat-
		Table 3: Performance of concept typing
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows that the
		accuracy of concept typing (a 4-class classiﬁ-
		cation task) is 0
	</Extractive Summary>
</Paper ID=ument813>


<Paper ID=ument813> <Table ID =4>
	<Abstractive Summary> =
		963 (207/8)
		Table 4: False type predictions in red
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4a gives two of
		the 27 MajVot’s false predictions
	</Extractive Summary>
	<Extractive Summary> =
		Table 4b shows
		three of the 8 false cases among 215 predictions
	</Extractive Summary>
</Paper ID=ument813>


<Paper ID=ument813> <Table ID =5>
	<Abstractive Summary> =
		For result of
		145
		Table 5: Number of concepts of each type
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the number of concepts of each type
		we have for hierarchy induction
	</Extractive Summary>
</Paper ID=ument813>


<Paper ID=ument813> <Table ID =6>
	<Abstractive Summary> =
		$Problem
		$Method
		$Object
		$Metric
		Count (predicted)
		52
		104
		9
		50
		Count (ground truth)
		53
		100
		13
		49
		Table 6: Number of relations for each type
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6
		gives the number of relation tuples we extracted
		for each type
	</Extractive Summary>
</Paper ID=ument813>


<Paper ID=ument813> <Table ID =7>
	<Abstractive Summary> =
		It is not fair to compare with
		taxonomy construction methods because we are
		targeting a different problem, that is to generate
		a concept hierarchy of facets with three kinds of
		Table 7: Comparing HiGrowth with baselines on
		building hierarchy from data science literature
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 7, Hi-
		Growth consistently outperforms TAXI on all three
		kinds of paths: it improves synonym detection by
		3
	</Extractive Summary>
</Paper ID=ument813>


<Paper ID=ument814> <Table ID =1>
	<Abstractive Summary> =
		First, we describe the data sets
		155
		Intent
		Utterances with slot labels
		searchFlight
		ﬁnd me a ﬂight from [origin](Paris) to [destination](New York)
		searchFlight
		I need a ﬂight leaving [date](this weekend) to [destination](Berlin)
		searchFlight
		show me ﬂights to go to [destination](new york) leaving [date](this evening)
		Table 1: Examples of the SNIPS Dataset
	</Abstractive Summary>
</Paper ID=ument814>


<Paper ID=ument814> <Table ID =2>
	<Abstractive Summary> =
		Utterances
		Train
		1,310
		SSL candidate (unlablled)
		11,774
		Dev
		700
		Test
		700
		Table 2: SNIPS data statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Overall dataset
		statistics is given in Table 2, intent distributions in
		Train, SSL, Dev and Test set are shown in Figure
		3
	</Extractive Summary>
	<Extractive Summary> =
		First system (“Baseline”)
		is trained only on training data in Table 2, with-
		out applying SSL
	</Extractive Summary>
</Paper ID=ument814>


<Paper ID=ument814> <Table ID =3>
	<Abstractive Summary> =
		71
		Table 3: Transductive results on SNIPS data
	</Abstractive Summary>
</Paper ID=ument814>


<Paper ID=ument815> <Table ID =1>
	<Abstractive Summary> =
		For
		example, in the COL module, the prediction is
		computed by the using following equations:
		161
		Table 1: Example Databases
		Database
		Table Name
		Column Names
		department store
		customers
		customer id
		customer name
		customer address
		customer orders
		order id
		customer id
		order date
		coffee shop
		shop
		shop id
		address
		num of staff
		P num
		COL = P
		�
		Wnum
		1
		Hnum
		Q/COL
		⊤ + Wnum
		2
		Hnum
		HS/COL
		⊤�
		P val
		COL = P
		�
		Wval
		1 Hval
		Q/COL
		⊤ + Wval
		2 Hval
		HS/COL
		⊤
		+ Wval
		3 HCOL⊤�
		,
		where “num” means the number of columns, “val”
		means the index(es) of the column(s), “Q” means
		question, “HS” means path history, “COL” means
		column, P(U)
		=
		softmax(V tanh(U)) is a
		probability distribution given score U and param-
		eter V, the W’s are learnable parameters, and
		the H1/2’s are conditional embeddings deﬁned as
		H1/2 = softmax(H1WH⊤
		2 )H1
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1 and Figure 1 respectively, we show
		an example of the databases and the graph con-
		162
		Figure 1: An example of the constructed graph for
		databases in Table 1
	</Extractive Summary>
</Paper ID=ument815>


<Paper ID=ument815> <Table ID =2>
	<Abstractive Summary> =
		0%
		Table 2: Comparison of different methods with respect
		to ﬁnal exact matching accuracy and COL module ac-
		curacy on development set
	</Abstractive Summary>
</Paper ID=ument815>


<Paper ID=ument816> <Table ID =1>
	<Abstractive Summary> =
		Our
		approach is quite suitable for downstream applica-
		Gene
		Phenotype
		Disease
		CA1
		Hypothermia
		Ischemia
		CA1
		Neuronal loss
		Ischemia
		CA1
		Hyperglycemia
		Ischemia
		Gene
		Drug
		Disease
		BTK
		Ibrutinib
		Chronic
		lymphocytic leukemia
		BTK
		Acalabrutinib
		Chronic
		lymphocytic leukemia
		Table 1:
		Example paths for query types ‘Gene-
		Phenotype-Disease’,
		‘Gene-Drug-Disease’ showing
		the ability to reason over 1-hop
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows two examples of ranked 1-hop
		paths for query types, ‘Gene-Phenotype-Disease’
		and ‘Gene-Drug-Disease’
	</Extractive Summary>
</Paper ID=ument816>


<Paper ID=ument816> <Table ID =2>
	<Abstractive Summary> =
		5M
		Table 2: Triple Data set statistics
	</Abstractive Summary>
</Paper ID=ument816>


<Paper ID=ument816> <Table ID =3>
	<Abstractive Summary> =
		Exp 1
		Exp 2
		query types
		4
		6
		avg no of paths
		6
		33
		min no of paths
		2
		10
		total paths
		573
		3356
		Table 3: Path data set statistics used in experiment 1
		and 2
	</Abstractive Summary>
</Paper ID=ument816>


<Paper ID=ument816> <Table ID =4>
	<Abstractive Summary> =
		01
		0
		0
		0
		0
		0
		0
		0
		Table 4: Hits@n and MAP@n scores for Embed+predrank, Embed+cosine scoring functions comparing to baseline
		for top 100, 300, 500 predictions given by KBC model
	</Abstractive Summary>
	<Extractive Summary> =
		According to results in Table 4, Embed+cosine
		scoring
		function
		yields
		better
		Hits@n
		and
		MAP@n scores than Embed+pred rank in gen-
		eral
	</Extractive Summary>
</Paper ID=ument816>


<Paper ID=ument816> <Table ID =5>
	<Abstractive Summary> =
		12
		Table 5: Spearman ranking correlation (rs) for Rp-Rt
		ranks of paths in ground truth and their relative ranking
		in predicted paths, Rp-Rr ranks of randomly permuted
		paths and their relative ranking in predicted paths, Rt-
		Rr ranks of paths in ground truth and their relative
		ranking in randomly permuted paths
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the Spearman ranking correla-
		tion (rs) for Rp-Rt ranks of paths in ground truth
		(Rt) and their relative ranking in predicted paths
		(Rp), Rp-Rr ranks of randomly permuted paths
		(Rr) and their relative ranking in predicted paths
		(Rp), Rt-Rr ranks of paths in ground truth (Rt)
		and their relative ranking in randomly permuted
		paths (Rr)
	</Extractive Summary>
</Paper ID=ument816>


<Paper ID=ument817> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Node matching results: percentage of CKG con-
		taining a correct match (higher is better); percentage of test
		nodes correctly resolved (higher is better), spuriously merged
		with some KG node and spuriously added to KG as a new
		node (lower is better)
		recover the one (or none) ﬁnal match for each
		method
	</Abstractive Summary>
</Paper ID=ument817>


<Paper ID=ument818> <Table ID =1>
	<Abstractive Summary> =
		With coref-
		erence resolution, the graph is further enriched se-
		mantically, linking nodes that refer to the same
		Table 1: Library of pre-deﬁned Generators
		Symbol
		Description
		gCR
		Coreference Resolution generator
		gEL
		X
		Entity Linker to the resource X
		gNER
		Named Entity Recognizer
		gOIE
		Open Information Extractor
		gπ
		provider of an embedding function
		pi
		entity in the text
	</Abstractive Summary>
</Paper ID=ument818>


<Paper ID=ument819> <Table ID =1>
	<Abstractive Summary> =
		2
		cross-sentence relations
		yes
		no
		Table 1: Data statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 provides a brief sum-
		mary of both datasets
	</Extractive Summary>
</Paper ID=ument819>


<Paper ID=ument819> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Comparison with previous methods for rela-
		tion extraction on SCIERC dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Extraction Performance
		Table 2 and Table 3
		compare our concept network CTN with baselines
		for SCIERC and SemEval18, respectively
	</Extractive Summary>
</Paper ID=ument819>


<Paper ID=ument819> <Table ID =3>
	<Abstractive Summary> =
		6
		Table 3: Comparison with previous methods for rela-
		tion extraction on SemEval18 dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Extraction Performance
		Table 2 and Table 3
		compare our concept network CTN with baselines
		for SCIERC and SemEval18, respectively
	</Extractive Summary>
</Paper ID=ument819>


<Paper ID=ument819> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Ablation study of isolated contribution of each
		rule
	</Abstractive Summary>
</Paper ID=ument819>


<Paper ID=ument819> <Table ID =5>
	<Abstractive Summary> =
		88%
		123
		-
		-
		-
		Table 5: Accuracy (TP = true positives) per relationship
		type on testing data
	</Abstractive Summary>
	<Extractive Summary> =
		Relation Type Sensitivity
		Table 5 shows the
		ability of CTN to identify each type of relation-
		ship that is labeled in the ground truth data
	</Extractive Summary>
</Paper ID=ument819>


<Paper ID=ument82> <Table ID =1>
	<Abstractive Summary> =
		58
		Table 1:
		Evaluation of various phrase composi-
		tion strategies under N-gram phrase partition
	</Abstractive Summary>
	<Extractive Summary> =
		As seen in Table 1, all pro-
		posed phrase composition methods consistently
		outperform TRANSFORMER-BASE baseline, val-
		idating the importance of introducing multi-
		granularity phrase in TRANSFORMER
	</Extractive Summary>
</Paper ID=ument82>


<Paper ID=ument82> <Table ID =2>
	<Abstractive Summary> =
		83
		Table 2: Evaluation of different layers in the encoder,
		which are implemented as self-attention with SANs
		phrase composition under N-gram partition
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 2, reducing the applied layers from
		high-level to low-level consistently increase trans-
		lation quality in terms of BLEU score as well as
		the training speed
	</Extractive Summary>
</Paper ID=ument82>


<Paper ID=ument82> <Table ID =3>
	<Abstractive Summary> =
		97
		Table 3: Evaluation of phrase partition, tag supervision and interaction strategies
	</Abstractive Summary>
	<Extractive Summary> =
		Phrase Partition and Tag Supervision
		As seen
		in Table 3, syntactic phrase partition (Row 3) im-
		proves the model performance over the N-gram
		phrase partition (Row 2), showing that the syn-
		tactic phrase beneﬁts to translation quality
	</Extractive Summary>
	<Extractive Summary> =
		Phrase Interaction
		As observed in Table 3,
		phrase interaction (Row 5-6) consistently im-
		proves performance of translation, proving the
		effectiveness and necessity of enhancing phrase
		level dependencies on phrase representation
	</Extractive Summary>
	<Extractive Summary> =
		For pre-trained NMT encoders, we use the pre-
		trained encoders of model variations in Table 3
		followed by a MLP classiﬁer, which are used to
		894
		(a) Vanilla Multi-Head Self-Attention
		(b) Multi-Granularity Self-Attention
		Figure 3: Visualization of attention examples of the same input sentence: (a) and (b) are produced by the vanilla
		multi-head self-attention and the proposed MG-SA models, respectively
	</Extractive Summary>
</Paper ID=ument82>


<Paper ID=ument82> <Table ID =4>
	<Abstractive Summary> =
		21
		Table 4: Comparing with the existing NMT systems on WMT14 En⇒De and NIST Zh⇒En test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Main Results
		Table 4 lists the results on
		WMT14 En⇒De and NIST Zh⇒En translation
		tasks
	</Extractive Summary>
</Paper ID=ument82>


<Paper ID=ument82> <Table ID =5>
	<Abstractive Summary> =
		31
		Table 5: Accuracies on multi-granularity label prediction tasks
	</Abstractive Summary>
	<Extractive Summary> =
		Results Analysis
		Table 5 lists the prediction ac-
		curacies of ﬁve syntactic labels on test
	</Extractive Summary>
</Paper ID=ument82>


<Paper ID=ument820> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Full-length natural answer generation from the
		question and the factoid answer
		natural sentence is, hence, a natural extension and
		post-processing step of any QA system54
		Table 1:
		Performance on CNN/Daily Mail test set using the full length ROUGE F1 score
		Table 1: Beginning of an example timeline about the
		Syrian civil war59
		Table 1: Comparison of various models using ROUGE-
		1/ROUGE-2 on DUC 2004 with 665 bytes summaries01
		Table 1: ROUGE-L evaluation per product type, 2016), we found
		that the existing models trained on news and
		Wikipedia data produced ungrammatical results,
		and that the size of dataset is insufﬁcient for the
		49
		mean
		min
		25th
		50th
		75th
		max
		Words
		US
		1382
		245
		922
		1254
		1773
		8463
		CA
		1684
		561
		1123
		1498
		2113
		3795
		Sentences
		US
		46
		7
		31
		41
		58
		372
		CA
		47
		12
		31
		42
		59
		137
		Table 1: Text length distributions on preprocessed texts56
		Table 1: Data statistics are broken into three categories
		70
		Dataset
		Train
		Validation
		Test
		CNN/DM
		287 227
		13 368
		11 490
		SAMSum
		14 732
		818
		819
		Table 1: Datasets sizes
		we present details about the new corpus and de-
		scribe how it was created, validated and cleaned2]
		Table 1: Organization structure of this paper: four measures presented in this paper and choices of model designs
		they have inﬂuence oncom/software/BeautifulSoup/
		Language
		ISO 639-1
		gv-snippet gv-crowd
		Number of articles
		English
		en
		4,573
		529
		Spanish
		es
		3,921
		487
		Malagasy
		mg
		2,680
		374
		Bengali
		bn
		2,253
		352
		French
		fr
		2,130
		352
		Portuguese
		pt
		798
		162
		Russian
		ru
		795
		139
		Arabic
		ar
		745
		191
		Italian
		it
		718
		135
		Macedonian
		mk
		701
		138
		Greek
		el
		694
		128
		German
		de
		647
		204
		Japanese
		ja
		424
		75
		Swahili
		sw
		418
		84
		Dutch
		nl
		348
		87
		Other statistics
		Summarized by
		GV authors MTurkers
		Summary languages
		All versions English
		Summary lengths (words)
		-
		40-50
		Article lengths (words)
		150-500
		150-350
		Table 1: Summary of the Global Voices dataset67
		1
		5
		Table 1:
		Position of summary-worthy sentences in
		a document for single-doc (CNN/DM) and multi-doc
		datasets (DUC-04, TAC11)3
		Table 1: Comparison of state-of-the-art summarization systems
		Table 1: Some spans (top) are plausible summary state-
		ments, because they make sense when removed from
		context sentences23
		Table 1: Experimental results of baselines, oracles and
		models on Sentence-Level (S-Level) and Discourse-
		Level (D-Level) segmentation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows a sample of our system input and output
	</Extractive Summary>
	<Extractive Summary> =
		1
		CNN/Daily Mail
		Table 1 shows the experimental results
		on
		CNN/Daily Mail dataset, with extractive models
		in the top block and abstractive models in the bot-
		tom block
	</Extractive Summary>
	<Extractive Summary> =
		Adding reinforcement learning (BERT-ext +
		RL) gives higher performance, which is compet-
		itive with other extractive approaches using pre-
		trained Transformers (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Clustering
		We need to cluster sentences that describe the
		same event (such as the formation of the Syrian
		national council in Table 1) so that the MSC sys-
		tem can generate concise summaries from the re-
		sulting clusters
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 1 compares the quality of EditNet with
		that of several state-of-the-art extractive-only
		or abstractive-only baselines
	</Extractive Summary>
	<Extractive Summary> =
		The
		comparison of EditNet to both EditNetE and
		EditNetA variants provides a strong empirical proof
		that, by utilizing an hybrid decision approach, a
		4The rnn-ext-RL extractor results reported in Table 1 are
		the ones that were reported by (Chen and Bansal, 2018)
	</Extractive Summary>
	<Extractive Summary> =
		In Table 1, we present data statistics of the gold-
		standard sub-sentence highlights
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		presents the size of the dataset split used in our
		experiments
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, positive sen-
		tences in the training data (see §3
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 provides examples of spans that do and do
		not make sense when extracted in this manner
	</Extractive Summary>
	<Extractive Summary> =
		Results are listed in Table 1, all models with
		discourse-level segmentation outperform those
		with sentence-level segmentation, demonstrating
		the effectiveness of our ﬁner-grained means
	</Extractive Summary>
</Paper ID=ument820>


<Paper ID=ument820> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Automatically created dataset samples
		Figure 1: Question trigram distribution of automati-
		cally created dataset
		ity humans bring when generating new sentences,
		we manually annotated 15000 QA pairs, from the
		3
		SQuAD dataset63
		Table 2: Comparison of extractor networks081
		Table 2: Dataset statistics, including number of topics,
		timelines and the average number of sentences to be
		summarized for each topic69
		Table 2: Comparison of various models using ROUGE-
		1/2 on DUC 2004 with 100 words summaries43
		Table 2: ROUGE-{1, 2, L} metrics on the full dataset
		Table 2: ROUGE F-scores (%) of different methods80
		Table 2: ROUGE results on CNN/DM test set at both sentence and sub-sentence level
		Table 2: Example of a dialogue from the collected cor-
		pus
		3
		Dialogues baselines
		The baseline commonly used in the news summa-
		rization task is Lead-3 (See et al62
		Table 2: Detailed statistics of six datasets4K
		Table 2: Data used to train and validate translation and
		summarization models35
		Table 2: Results on the DUC-04 dataset evaluated by
		ROUGE9
		Table 2: Percentage of summary sentences that are faithful,
		grammatical, etc Yeltsin might also appear weak if he had to replace General Grachev
		B
		B
		B
		Table 2: A candidate set for (t1) = “Mr
	</Abstractive Summary>
	<Extractive Summary> =
		However, as Table 2 shows,
		ROUGE scores of lead baselines and extractors
		from previous work in Sentence Rewrite frame-
		work (Chen and Bansal, 2018; Xu and Durrett,
		2019) are almost tie
	</Extractive Summary>
	<Extractive Summary> =
		Each topic also has a set of related
		news articles scraped from the web (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		We hypothesize that this
		is related to the lower compression rate and greater
		size of Crisis (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		These results, to-
		gether with the difference in size and compression
		rate between the datasets observed in Table 2, ex-
		plain why our system outperforms the state of the
		art only on the more compressive Crisis dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 depicts models producing 100 words
		summaries, all depending on hand-crafted fea-
		tures
	</Extractive Summary>
	<Extractive Summary> =
		We report in Table 2 the results published
		by Angelidis and Lapata (2018) on the Oposum
		dataset
	</Extractive Summary>
	<Extractive Summary> =
		com/pcyin/PyRouge
		51
		(Table 2b), but all three supervised methods per-
		form better than the unsupervised baselines, sug-
		gesting that models built using the language of
		US Bills can transfer to other states
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		In Table 2 we report results on the CNN/DM test
		set evaluated by ROUGE (Lin, 2004)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Summarization Results
		We compare our system with strong summariza-
		tion baselines (Table 2 and 3)
	</Extractive Summary>
</Paper ID=ument820>


<Paper ID=ument820> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Manual dataset samples
		Let the question be represented by words Q =
		{q1, q2, 89
		Table 3: Comparison of different methods building up-
		per bound for full model472
		Table 3: Result of our system, the oracles, and comparison systems46∗
		Table 3: ROUGE-1/2 for various methods to build the sentence semantic relation graph
		Table 3: Examples of summary and text like words
		Summary-like
		prohibit, DOD, VA, allow,
		penalty, prohibit, EPA, elim-
		inate, implement, require
		Text-like
		estimate,
		average,
		report,
		rise, section, ﬁnish, percent,
		debate
		6
		Conclusion
		In this paper, we introduced BillSum, the ﬁrst cor-
		pus for legislative summarization57
		-PERSON
		Table 3: Baselines for the dialogues summarization
		signiﬁcant information59
		Table 3: Results of cross-dataset PCR(ηp), CCR(ηc)
		and ROUGE-2 score1
		Table 3: Cross-lingual summarization results with different approaches65
		Table 3: ROUGE results on the TAC-11 dataset04
		Table 3: Results for each merging method495
		Table 3: Spearman’s ρ for our ordinal regression model
		p(α(s)|s), compared both to the inter-annotator agree-
		ment and a simpler logistic regression model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the summary-level ROUGE
		scores of previously explained methods
	</Extractive Summary>
	<Extractive Summary> =
		Sentence Semantic Relation Graph
		Table 3
		shows the results of different methods to create
		the sentence semantic relation graph with various
		thresholds tg
		sim for 665 bytes summaries (we ob-
		tain similar results for 100 words)
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the most summary-
		like and text-like words in bills and resolutions
	</Extractive Summary>
	<Extractive Summary> =
		We can see from Table 3 that the
		training and test set of the same dataset always
		have the highest PCR/CCR score, which indicates
		the distribution between them is the closest based
		on consitituent factors
	</Extractive Summary>
	<Extractive Summary> =
		The {es,fr}-en models have the same
		hyperparameters as those of the base Transformer
		architecture described in Table 3 of Vaswani et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents our results
	</Extractive Summary>
	<Extractive Summary> =
		Does the way sentences are fused affect their
		faithfulness and grammaticality? Table 3 provides
		insights regarding this question
	</Extractive Summary>
</Paper ID=ument820>


<Paper ID=ument820> <Table ID =4>
	<Abstractive Summary> =
		40
		Table 4: The top section displays BLEU and ROGUE scores for the models tested on the manually created test
		dataset64
		Table 4: Comparison of RL training
		Table 4: Beginning of the timeline generated by our ab-
		stractive system with sentence constraint for the time-
		line in Table 124∗
		Table 4: Ablation test
		Figure 3 is an excerpt
		from a particularly difﬁcult example:
		Table 4: ROUGE Scores of Congressional Bills
		Rouge-1
		Rouge-2
		Rouge-L
		Oracle
		4151
		Table 4: Model evaluation on the news corpus test set
		manually evaluated summaries generated by the
		models for 150 news and 100 dialogues23
		Table 4:
		The performance of our base model on
		CNN/DM dataset, test set is broken down based on
		DENSITY and COMPRESSION
		ROUGE score in Table 4
		Table 4: Example system summaries and their human
		reference summary
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Example Timeline
		Table 4 shows an example timeline generated by
		our system
	</Extractive Summary>
	<Extractive Summary> =
		Ablation Study
		We quantify the contribution
		of each module of SemSentSum in Table 4 for
		665 bytes summaries (we obtain similar results for
		100 words)
	</Extractive Summary>
	<Extractive Summary> =
		However, as it is useful to an-
		alyze the precision and recall scores separately,
		both are presented in Table 4 for US Bills and
		in Table 5 for CA Bills
	</Extractive Summary>
	<Extractive Summary> =
		5
		Results
		The results for the news summarization task are
		shown in Table 4 and for the dialogue summariza-
		tion – in Table 5
	</Extractive Summary>
	<Extractive Summary> =
		So the results of compression
		in Table 4 are in line with our expectations, how
		the model represents long documents to get good
		performance in text summarization task remains a
		challenge (Celikyilmaz et al
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4 we show example system summaries
		and a human-written reference summary
	</Extractive Summary>
</Paper ID=ument820>


<Paper ID=ument820> <Table ID =5>
	<Abstractive Summary> =
		50
		Table 5: The top section displays the scores for the models tested on the 500 randomly chosen NewsQA dataset
		Combination Search selects a set of sentences
		16
		Models
		Relevance
		Readability
		Total
		Sentence Rewrite (Chen and Bansal, 2018)
		56
		59
		115
		BERTSUM (Liu, 2019)
		58
		60
		118
		BERT-ext + abs + RL + rerank (ours)
		66
		61
		127
		Table 5: Results of human evaluation010∗
		Table 5: Ablation results on Crisis, showing changes of
		ROUGE align and date selection F122
		(b) Recall Scores
		Table 5: ROUGE Scores of California Bills
		Rouge-1
		Rouge-2
		Rouge-L
		Oracle
		4145
		Table 5: Model evaluation on the dialogues corpus test set
		in Table 74%
		Table 5: Experiment about DENSITY, Pct denotes the
		percentage of ψ(si, S) to �
		si∈D ψ(si, S)
	</Abstractive Summary>
	<Extractive Summary> =
		From the results shown in Table 5, we
		can see that our model is better in relevance com-
		pared to others
	</Extractive Summary>
	<Extractive Summary> =
		However, as it is useful to an-
		alyze the precision and recall scores separately,
		both are presented in Table 4 for US Bills and
		in Table 5 for CA Bills
	</Extractive Summary>
</Paper ID=ument820>


<Paper ID=ument820> <Table ID =6>
	<Abstractive Summary> =
		8
		Table 6: Accuracy Scores(in the range of 0-100) for the
		various models
		0
		10000
		20000
		30000
		40000
		50000
		60000
		70000
		40
		50
		60
		70
		80
		90
		Basline
		2-Encoder Pointer Gen on auto-generated data
		2-Encoder Pointer Gen on augmented data
		Figure 4: Validation Accuracy
		6
		Results
		As shown in table 4, 5, 6 and 7, augmenting the
		manually annotated data with the auto-generated
		data for training leads to signiﬁcant improvements
		for the 2-encoder pointer generator network38
		Table 6: Performance on NYT50 test set using the lim-
		ited length ROUGE recall score24
		Table 6: Spearman correlation of the score difference
		between systems and timeline properties
		Table 6: Statistics of human evaluation of summaries’ quality and ROUGE evaluation of those summaries
		ROUGE-1
		ROUGE-2
		ROUGE-L
		corr
		p-value
		corr
		p-value
		corr
		p-value
		NEWS
		088 ↑
		Table 6: Performance of models equipped with different types of knowledge on CNN/DM dataset
	</Abstractive Summary>
	<Extractive Summary> =
		2
		New York Times corpus
		Table 6 gives the results on NYT50 dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows the breakdown performance on
		CNN/DM based on DENSITY and COMPRESSION
	</Extractive Summary>
</Paper ID=ument820>


<Paper ID=ument820> <Table ID =7>
	<Abstractive Summary> =
		14
		Table 7: Performance on DUC-2002 test set using the
		full length ROUGE F1 score
		Table 7: Rating scale for the readability task1e-5
		Table 7: Pearson’s correlations between human judgement and ROUGE metric
		scribed in Section 416
		Table 7: Results of four models under two types of evaluation settings: IN-DATASET, and CROSS-DATASET
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows that the pointer-generator
		system handles tense agreement and generation of
		new words
	</Extractive Summary>
	<Extractive Summary> =
		Table 7 shows the results of four models under
		two types of evaluation settings: IN-DATASET, and
		CROSS-DATA, and we have the following ﬁndings:
		1) For IN-DATASET setting, comparing the Tag
		and the basic models, we ﬁnd a very simple method
		that assign each sample a domain tag could achieve
		improvement
	</Extractive Summary>
</Paper ID=ument820>


<Paper ID=ument820> <Table ID =8>
	<Abstractive Summary> =
		Table 8: Failure Cases52
		187
		89
		17
		6
		1
		Table 8: Results of the readability evaluation [58/36/42]
		Table 8: Examples of dialogues (Part 1)10
		Table 8: Results of experiments with tags on our base
		model and current state-of-the-art model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows readability results
	</Extractive Summary>
	<Extractive Summary> =
		Table 8 and 9 show a few selected dialogues,
		together with summaries produced by the best
		tested models:
		• DynamicConv + GPT-2 embeddings with
		a separator (trained on news + dialogues),
		• DynamicConv + GPT-2 embeddings (trained
		on news + dialogues),
		• Fast Abs RL (trained on dialogues),
		• Fast Abs RL Enhanced (trained on dia-
		logues),
		• Transformer (trained on news + dialogues)
	</Extractive Summary>
	<Extractive Summary> =
		, for Dialogue 1 in Table 8,
		Fast Abs RL generates the following summary:
		’lilly and lilly are going to eat salmon’
	</Extractive Summary>
	<Extractive Summary> =
		Liu
		(2019) and the results are presented in Table 8, we
		can obtain the following observations:
		1) Random partitioning does not make sense and
		cannot lead to the improvement of performance
	</Extractive Summary>
</Paper ID=ument820>


<Paper ID=ument820> <Table ID =9>
	<Abstractive Summary> =
		[67/55/67]
		Table 9: Examples of dialogues (Part 2)
	</Abstractive Summary>
	<Extractive Summary> =
		An
		interesting example of the misleading behavior
		of the ROUGE metrics is presented in Table 9
		for Dialogue 4, where a wrong summary – ’paul
		and cindy don’t like red roses
	</Extractive Summary>
</Paper ID=ument820>


<Paper ID=ument822> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Full-length natural answer generation from the
		question and the factoid answer
		natural sentence is, hence, a natural extension and
		post-processing step of any QA system
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows a sample of our system input and output
	</Extractive Summary>
</Paper ID=ument822>


<Paper ID=ument822> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Automatically created dataset samples
		Figure 1: Question trigram distribution of automati-
		cally created dataset
		ity humans bring when generating new sentences,
		we manually annotated 15000 QA pairs, from the
		4
		SQuAD dataset
	</Abstractive Summary>
</Paper ID=ument822>


<Paper ID=ument822> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Manual dataset samples
		Let the question be represented by words Q =
		{q1, q2,
	</Abstractive Summary>
</Paper ID=ument822>


<Paper ID=ument822> <Table ID =4>
	<Abstractive Summary> =
		40
		Table 4: The top section displays BLEU and ROGUE scores for the models tested on the manually created test
		dataset
	</Abstractive Summary>
</Paper ID=ument822>


<Paper ID=ument822> <Table ID =5>
	<Abstractive Summary> =
		50
		Table 5: The top section displays the scores for the models tested on the 500 randomly chosen NewsQA dataset
	</Abstractive Summary>
</Paper ID=ument822>


<Paper ID=ument822> <Table ID =6>
	<Abstractive Summary> =
		8
		Table 6: Accuracy Scores(in the range of 0-100) for the
		various models
		0
		10000
		20000
		30000
		40000
		50000
		60000
		70000
		40
		50
		60
		70
		80
		90
		Basline
		2-Encoder Pointer Gen on auto-generated data
		2-Encoder Pointer Gen on augmented data
		Figure 4: Validation Accuracy
		6
		Results
		As shown in table 4, 5, 6 and 7, augmenting the
		manually annotated data with the auto-generated
		data for training leads to signiﬁcant improvements
		for the 2-encoder pointer generator network
	</Abstractive Summary>
</Paper ID=ument822>


<Paper ID=ument822> <Table ID =7>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows that the pointer-generator
		system handles tense agreement and generation of
		new words
	</Extractive Summary>
</Paper ID=ument822>


<Paper ID=ument822> <Table ID =8>
	<Abstractive Summary> =
		Table 8: Failure Cases
	</Abstractive Summary>
</Paper ID=ument822>


<Paper ID=ument823> <Table ID =1>
	<Abstractive Summary> =
		54
		Table 1:
		Performance on CNN/Daily Mail test set using the full length ROUGE F1 score
	</Abstractive Summary>
	<Extractive Summary> =
		1
		CNN/Daily Mail
		Table 1 shows the experimental results
		on
		CNN/Daily Mail dataset, with extractive models
		in the top block and abstractive models in the bot-
		tom block
	</Extractive Summary>
	<Extractive Summary> =
		Adding reinforcement learning (BERT-ext +
		RL) gives higher performance, which is compet-
		itive with other extractive approaches using pre-
		trained Transformers (see Table 1)
	</Extractive Summary>
</Paper ID=ument823>


<Paper ID=ument823> <Table ID =2>
	<Abstractive Summary> =
		63
		Table 2: Comparison of extractor networks
	</Abstractive Summary>
	<Extractive Summary> =
		However, as Table 2 shows,
		ROUGE scores of lead baselines and extractors
		from previous work in Sentence Rewrite frame-
		work (Chen and Bansal, 2018; Xu and Durrett,
		2019) are almost tie
	</Extractive Summary>
</Paper ID=ument823>


<Paper ID=ument823> <Table ID =3>
	<Abstractive Summary> =
		89
		Table 3: Comparison of different methods building up-
		per bound for full model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the summary-level ROUGE
		scores of previously explained methods
	</Extractive Summary>
</Paper ID=ument823>


<Paper ID=ument823> <Table ID =4>
	<Abstractive Summary> =
		64
		Table 4: Comparison of RL training
	</Abstractive Summary>
</Paper ID=ument823>


<Paper ID=ument823> <Table ID =5>
	<Abstractive Summary> =
		Combination Search selects a set of sentences
		17
		Models
		Relevance
		Readability
		Total
		Sentence Rewrite (Chen and Bansal, 2018)
		56
		59
		115
		BERTSUM (Liu, 2019)
		58
		60
		118
		BERT-ext + abs + RL + rerank (ours)
		66
		61
		127
		Table 5: Results of human evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		From the results shown in Table 5, we
		can see that our model is better in relevance com-
		pared to others
	</Extractive Summary>
</Paper ID=ument823>


<Paper ID=ument823> <Table ID =6>
	<Abstractive Summary> =
		38
		Table 6: Performance on NYT50 test set using the lim-
		ited length ROUGE recall score
	</Abstractive Summary>
	<Extractive Summary> =
		2
		New York Times corpus
		Table 6 gives the results on NYT50 dataset
	</Extractive Summary>
</Paper ID=ument823>


<Paper ID=ument823> <Table ID =7>
	<Abstractive Summary> =
		14
		Table 7: Performance on DUC-2002 test set using the
		full length ROUGE F1 score
	</Abstractive Summary>
</Paper ID=ument823>


<Paper ID=ument824> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Beginning of an example timeline about the
		Syrian civil war
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Clustering
		We need to cluster sentences that describe the
		same event (such as the formation of the Syrian
		national council in Table 1) so that the MSC sys-
		tem can generate concise summaries from the re-
		sulting clusters
	</Extractive Summary>
</Paper ID=ument824>


<Paper ID=ument824> <Table ID =2>
	<Abstractive Summary> =
		081
		Table 2: Dataset statistics, including number of topics,
		timelines and the average number of sentences to be
		summarized for each topic
	</Abstractive Summary>
	<Extractive Summary> =
		Each topic also has a set of related
		news articles scraped from the web (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		We hypothesize that this
		is related to the lower compression rate and greater
		size of Crisis (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		These results, to-
		gether with the difference in size and compression
		rate between the datasets observed in Table 2, ex-
		plain why our system outperforms the state of the
		art only on the more compressive Crisis dataset
	</Extractive Summary>
</Paper ID=ument824>


<Paper ID=ument824> <Table ID =3>
	<Abstractive Summary> =
		472
		Table 3: Result of our system, the oracles, and comparison systems
	</Abstractive Summary>
</Paper ID=ument824>


<Paper ID=ument824> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Beginning of the timeline generated by our ab-
		stractive system with sentence constraint for the time-
		line in Table 1
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Example Timeline
		Table 4 shows an example timeline generated by
		our system
	</Extractive Summary>
</Paper ID=ument824>


<Paper ID=ument824> <Table ID =5>
	<Abstractive Summary> =
		010∗
		Table 5: Ablation results on Crisis, showing changes of
		ROUGE align and date selection F1
	</Abstractive Summary>
</Paper ID=ument824>


<Paper ID=ument824> <Table ID =6>
	<Abstractive Summary> =
		24
		Table 6: Spearman correlation of the score difference
		between systems and timeline properties
	</Abstractive Summary>
</Paper ID=ument824>


<Paper ID=ument824> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Rating scale for the readability task
	</Abstractive Summary>
</Paper ID=ument824>


<Paper ID=ument824> <Table ID =8>
	<Abstractive Summary> =
		52
		187
		89
		17
		6
		1
		Table 8: Results of the readability evaluation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows readability results
	</Extractive Summary>
</Paper ID=ument824>


<Paper ID=ument825> <Table ID =1>
	<Abstractive Summary> =
		59
		Table 1: Comparison of various models using ROUGE-
		1/ROUGE-2 on DUC 2004 with 665 bytes summaries
	</Abstractive Summary>
</Paper ID=ument825>


<Paper ID=ument825> <Table ID =2>
	<Abstractive Summary> =
		69
		Table 2: Comparison of various models using ROUGE-
		1/2 on DUC 2004 with 100 words summaries
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 depicts models producing 100 words
		summaries, all depending on hand-crafted fea-
		tures
	</Extractive Summary>
</Paper ID=ument825>


<Paper ID=ument825> <Table ID =3>
	<Abstractive Summary> =
		46∗
		Table 3: ROUGE-1/2 for various methods to build the sentence semantic relation graph
	</Abstractive Summary>
	<Extractive Summary> =
		Sentence Semantic Relation Graph
		Table 3
		shows the results of different methods to create
		the sentence semantic relation graph with various
		thresholds tg
		sim for 665 bytes summaries (we ob-
		tain similar results for 100 words)
	</Extractive Summary>
</Paper ID=ument825>


<Paper ID=ument825> <Table ID =4>
	<Abstractive Summary> =
		24∗
		Table 4: Ablation test
	</Abstractive Summary>
	<Extractive Summary> =
		Ablation Study
		We quantify the contribution
		of each module of SemSentSum in Table 4 for
		665 bytes summaries (we obtain similar results for
		100 words)
	</Extractive Summary>
</Paper ID=ument825>


<Paper ID=ument826> <Table ID =1>
	<Abstractive Summary> =
		01
		Table 1: ROUGE-L evaluation per product type
	</Abstractive Summary>
</Paper ID=ument826>


<Paper ID=ument826> <Table ID =2>
	<Abstractive Summary> =
		43
		Table 2: ROUGE-{1, 2, L} metrics on the full dataset
	</Abstractive Summary>
	<Extractive Summary> =
		We report in Table 2 the results published
		by Angelidis and Lapata (2018) on the Oposum
		dataset
	</Extractive Summary>
</Paper ID=ument826>


<Paper ID=ument827> <Table ID =1>
	<Abstractive Summary> =
		4
		Benchmark Methods
		To establish benchmarks on summarization per-
		formance, we evaluate several extractive summa-
		rization approaches by ﬁrst scoring individual sen-
		tences, then using a selection strategy to pick
		mean
		min
		25th
		50th
		75th
		max
		Words
		US
		1382
		245
		923
		1253
		1758
		8785
		CA
		1684
		561
		1123
		1498
		2113
		3795
		Sentences
		US
		46
		3
		31
		42
		58
		372
		CA
		47
		12
		31
		42
		59
		137
		Table 1: Text length distributions on preprocessed texts
	</Abstractive Summary>
</Paper ID=ument827>


<Paper ID=ument827> <Table ID =2>
	<Abstractive Summary> =
		Table 2: ROUGE F-scores (%) of different methods
	</Abstractive Summary>
	<Extractive Summary> =
		Overall, the performance is lower than on US bills
		(Table 2b), but all three supervised methods per-
		form better than the unsupervised baselines, sug-
		gesting that models built using the language of US
		Bills can transfer to other states
	</Extractive Summary>
</Paper ID=ument827>


<Paper ID=ument827> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Examples of summary and text like words
		Summary-like
		prohibit, DOD, VA, allow,
		penalty, prohibit, EPA, elim-
		inate, implement, require
		Text-like
		estimate,
		average,
		report,
		rise, section, ﬁnish, percent,
		debate
		6
		Conclusion
		In this paper, we introduced BillSum, the ﬁrst cor-
		pus for legislative summarization
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the most summary-
		like and text-like words in bills and resolutions
	</Extractive Summary>
</Paper ID=ument827>


<Paper ID=ument827> <Table ID =4>
	<Abstractive Summary> =
		Table 4: ROUGE Scores of Congressional Bills
		Rouge-1
		Rouge-2
		Rouge-L
		Oracle
		40
	</Abstractive Summary>
	<Extractive Summary> =
		However, as it is useful to an-
		alyze the precision and recall scores separately,
		both are presented in Table 4 for US Bills and
		in Table 5 for CA Bills
	</Extractive Summary>
</Paper ID=ument827>


<Paper ID=ument827> <Table ID =5>
	<Abstractive Summary> =
		62
		(b) Recall Scores
		Table 5: ROUGE Scores of California Bills
		Rouge-1
		Rouge-2
		Rouge-L
		Oracle
		45
	</Abstractive Summary>
	<Extractive Summary> =
		However, as it is useful to an-
		alyze the precision and recall scores separately,
		both are presented in Table 4 for US Bills and
		in Table 5 for CA Bills
	</Extractive Summary>
</Paper ID=ument827>


<Paper ID=ument828> <Table ID =1>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 1 compares the quality of EditNet with
		that of several state-of-the-art extractive-only
		or abstractive-only baselines
	</Extractive Summary>
	<Extractive Summary> =
		The
		comparison of EditNet to both EditNetE and
		EditNetA variants provides a strong empirical proof
		that, by utilizing an hybrid decision approach, a
		4The rnn-ext-RL extractor results reported in Table 1 are
		the ones that were reported by (Chen and Bansal, 2018)
	</Extractive Summary>
</Paper ID=ument828>


<Paper ID=ument829> <Table ID =1>
	<Abstractive Summary> =
		56
		Table 1: Data statistics are broken into three categories
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1, we present data statistics of the gold-
		standard sub-sentence highlights
	</Extractive Summary>
</Paper ID=ument829>


<Paper ID=ument829> <Table ID =2>
	<Abstractive Summary> =
		80
		Table 2: ROUGE results on CNN/DM test set at both sentence and sub-sentence level
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		In Table 2 we report results on the CNN/DM test
		set evaluated by ROUGE (Lin, 2004)
	</Extractive Summary>
</Paper ID=ument829>


<Paper ID=ument83> <Table ID =1>
	<Abstractive Summary> =
		11
		Table 1:
		Empirical measure of output variance Var(r) of
		RC and error signal change ratio βLN, βRC and β (Eq
	</Abstractive Summary>
	<Extractive Summary> =
		Results in Table 1 show that LN weak-
		ens error signal (βLN < 1) but RC strengthens it
		(βRC > 1)
	</Extractive Summary>
	<Extractive Summary> =
		However, results in Table 1 also suggests that the
		self-attention sublayer in the encoder is not strong
		902
		Linear
		Linear
		Linear
		Linear
		Scale & Mask
		MatMul
		SoftMax
		MatMul
		target
		Q
		K
		V
		(a) Self-Attention
		MatMul
		Linear
		target
		Avg Mask
		Linear
		Relu
		Gate Layer
		(b) AAN
		Linear
		Linear
		Linear
		Scale & Mask
		SoftMax
		MatMul
		target
		Q
		K
		V
		source
		Avg Mask
		MatMul
		MatMul
		V
		Linear
		Linear
		(c) Merged attention with simpliﬁed AAN
		Figure 2: An overview of self-attention, AAN and the proposed merged attention with simpliﬁed AAN
	</Extractive Summary>
	<Extractive Summary> =
		Results in Table 1 suggest that DS-Init narrows
		both the variance and different ratios to be ∼1, en-
		suring the stability of gradient back-propagation
	</Extractive Summary>
	<Extractive Summary> =
		5
		Analysis of Training Dynamics
		Our analysis in Figure 1 and Table 1 is based
		on gradients estimated exactly after parameter ini-
		tialization without considering training dynam-
		ics
	</Extractive Summary>
</Paper ID=ument83>


<Paper ID=ument83> <Table ID =2>
	<Abstractive Summary> =
		2M
		159K
		30K
		Table 2: Statistics for different training datasets
	</Abstractive Summary>
</Paper ID=ument83>


<Paper ID=ument83> <Table ID =3>
	<Abstractive Summary> =
		13×‡
		Table 3: Tokenized case-sensitive BLEU (in parentheses: sacreBLEU) on WMT14 En-De translation task
	</Abstractive Summary>
	<Extractive Summary> =
		3
		WMT14 En-De Translation Task
		Table 3 summarizes translation results under dif-
		ferent settings
	</Extractive Summary>
	<Extractive Summary> =
		Re-
		sults in Table 3 show that T2T underperforms DS-
		Init by 0
	</Extractive Summary>
</Paper ID=ument83>


<Paper ID=ument83> <Table ID =4>
	<Abstractive Summary> =
		58
		Table 4: Tokenized case-sensitive BLEU (BLEU) and per-
		plexity (PPL) on training (Train) and development (new-
		stest2013, Dev) set
	</Abstractive Summary>
	<Extractive Summary> =
		Results in Table 4 show that models with DS-Init
		yield the best perplexity on both training and de-
		velopment set, and those with T2T achieve the
		best BLEU on the training set
	</Extractive Summary>
</Paper ID=ument83>


<Paper ID=ument83> <Table ID =5>
	<Abstractive Summary> =
		65×
		Table 5: Translation results on different tasks
	</Abstractive Summary>
</Paper ID=ument83>


<Paper ID=ument83> <Table ID =6>
	<Abstractive Summary> =
		96)
		Table 6: Tokenized case-sensitive BLEU (sacreBLEU) on
		WMT14 En-De translation task
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Comparison with Existing Work
		Table 6 lists the results in big setting and compares
		with current SOTA
	</Extractive Summary>
</Paper ID=ument83>


<Paper ID=ument830> <Table ID =1>
	<Abstractive Summary> =
		71
		Dataset
		Train
		Validation
		Test
		CNN/DM
		287 227
		13 368
		11 490
		SAMSum
		14 732
		818
		819
		Table 1: Datasets sizes
		we present details about the new corpus and de-
		scribe how it was created, validated and cleaned
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		presents the size of the dataset split used in our
		experiments
	</Extractive Summary>
</Paper ID=ument830>


<Paper ID=ument830> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Example of a dialogue from the collected cor-
		pus
		3
		Dialogues baselines
		The baseline commonly used in the news summa-
		rization task is Lead-3 (See et al
	</Abstractive Summary>
</Paper ID=ument830>


<Paper ID=ument830> <Table ID =3>
	<Abstractive Summary> =
		57
		-PERSON
		Table 3: Baselines for the dialogues summarization
		signiﬁcant information
	</Abstractive Summary>
</Paper ID=ument830>


<Paper ID=ument830> <Table ID =4>
	<Abstractive Summary> =
		51
		Table 4: Model evaluation on the news corpus test set
		manually evaluated summaries generated by the
		models for 150 news and 100 dialogues
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		The results for the news summarization task are
		shown in Table 4 and for the dialogue summariza-
		tion – in Table 5
	</Extractive Summary>
</Paper ID=ument830>


<Paper ID=ument830> <Table ID =5>
	<Abstractive Summary> =
		45
		Table 5: Model evaluation on the dialogues corpus test set
		in Table 7
	</Abstractive Summary>
</Paper ID=ument830>


<Paper ID=ument830> <Table ID =6>
	<Abstractive Summary> =
		Table 6: Statistics of human evaluation of summaries’ quality and ROUGE evaluation of those summaries
		ROUGE-1
		ROUGE-2
		ROUGE-L
		corr
		p-value
		corr
		p-value
		corr
		p-value
		NEWS
		0
	</Abstractive Summary>
</Paper ID=ument830>


<Paper ID=ument830> <Table ID =7>
	<Abstractive Summary> =
		1e-5
		Table 7: Pearson’s correlations between human judgement and ROUGE metric
		scribed in Section 4
	</Abstractive Summary>
</Paper ID=ument830>


<Paper ID=ument830> <Table ID =8>
	<Abstractive Summary> =
		[58/36/42]
		Table 8: Examples of dialogues (Part 1)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 and 9 show a few selected dialogues,
		together with summaries produced by the best
		tested models:
		• DynamicConv + GPT-2 embeddings with
		a separator (trained on news + dialogues),
		• DynamicConv + GPT-2 embeddings (trained
		on news + dialogues),
		• Fast Abs RL (trained on dialogues),
		• Fast Abs RL Enhanced (trained on dia-
		logues),
		• Transformer (trained on news + dialogues)
	</Extractive Summary>
	<Extractive Summary> =
		, for Dialogue 1 in Table 8,
		Fast Abs RL generates the following summary:
		’lilly and lilly are going to eat salmon’
	</Extractive Summary>
</Paper ID=ument830>


<Paper ID=ument830> <Table ID =9>
	<Abstractive Summary> =
		[67/55/67]
		Table 9: Examples of dialogues (Part 2)
	</Abstractive Summary>
	<Extractive Summary> =
		An
		interesting example of the misleading behavior
		of the ROUGE metrics is presented in Table 9
		for Dialogue 4, where a wrong summary – ’paul
		and cindy don’t like red roses
	</Extractive Summary>
</Paper ID=ument830>


<Paper ID=ument831> <Table ID =1>
	<Abstractive Summary> =
		2]
		Table 1: Organization structure of this paper: four measures presented in this paper and choices of model designs
		they have inﬂuence on
	</Abstractive Summary>
</Paper ID=ument831>


<Paper ID=ument831> <Table ID =2>
	<Abstractive Summary> =
		62
		Table 2: Detailed statistics of six datasets
	</Abstractive Summary>
</Paper ID=ument831>


<Paper ID=ument831> <Table ID =3>
	<Abstractive Summary> =
		59
		Table 3: Results of cross-dataset PCR(ηp), CCR(ηc)
		and ROUGE-2 score
	</Abstractive Summary>
	<Extractive Summary> =
		We can see from Table 3 that the
		training and test set of the same dataset always
		have the highest PCR/CCR score, which indicates
		the distribution between them is the closest based
		on consitituent factors
	</Extractive Summary>
</Paper ID=ument831>


<Paper ID=ument831> <Table ID =4>
	<Abstractive Summary> =
		23
		Table 4:
		The performance of our base model on
		CNN/DM dataset, test set is broken down based on
		DENSITY and COMPRESSION
		ROUGE score in Table 4
	</Abstractive Summary>
	<Extractive Summary> =
		So the results of compression
		in Table 4 are in line with our expectations, how
		the model represents long documents to get good
		performance in text summarization task remains a
		challenge (Celikyilmaz et al
	</Extractive Summary>
</Paper ID=ument831>


<Paper ID=ument831> <Table ID =5>
	<Abstractive Summary> =
		4%
		Table 5: Experiment about DENSITY, Pct denotes the
		percentage of ψ(si, S) to �
		si∈D ψ(si, S)
	</Abstractive Summary>
</Paper ID=ument831>


<Paper ID=ument831> <Table ID =6>
	<Abstractive Summary> =
		88 ↑
		Table 6: Performance of models equipped with different types of knowledge on CNN/DM dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the breakdown performance on
		CNN/DM based on DENSITY and COMPRESSION
	</Extractive Summary>
</Paper ID=ument831>


<Paper ID=ument831> <Table ID =7>
	<Abstractive Summary> =
		16
		Table 7: Results of four models under two types of evaluation settings: IN-DATASET, and CROSS-DATASET
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the results of four models under
		two types of evaluation settings: IN-DATASET, and
		CROSS-DATA, and we have the following ﬁndings:
		1) For IN-DATASET setting, comparing the Tag
		and the basic models, we ﬁnd a very simple method
		that assign each sample a domain tag could achieve
		improvement
	</Extractive Summary>
</Paper ID=ument831>


<Paper ID=ument831> <Table ID =8>
	<Abstractive Summary> =
		10
		Table 8: Results of experiments with tags on our base
		model and current state-of-the-art model
	</Abstractive Summary>
	<Extractive Summary> =
		Liu
		(2019) and the results are presented in Table 8, we
		can obtain the following observations:
		1) Random partitioning does not make sense and
		cannot lead to the improvement of performance
	</Extractive Summary>
</Paper ID=ument831>


<Paper ID=ument832> <Table ID =1>
	<Abstractive Summary> =
		com/software/BeautifulSoup/
		Language
		ISO 639-1
		gv-snippet gv-crowd
		Number of articles
		English
		en
		4,573
		529
		Spanish
		es
		3,921
		487
		Malagasy
		mg
		2,680
		374
		Bengali
		bn
		2,253
		352
		French
		fr
		2,130
		352
		Portuguese
		pt
		798
		162
		Russian
		ru
		795
		139
		Arabic
		ar
		745
		191
		Italian
		it
		718
		135
		Macedonian
		mk
		701
		138
		Greek
		el
		694
		128
		German
		de
		647
		204
		Japanese
		ja
		424
		75
		Swahili
		sw
		418
		84
		Dutch
		nl
		348
		87
		Other statistics
		Summarized by
		GV authors MTurkers
		Summary languages
		All versions English
		Summary lengths (words)
		-
		40-50
		Article lengths (words)
		150-500
		150-350
		Table 1: Summary of the Global Voices dataset
	</Abstractive Summary>
</Paper ID=ument832>


<Paper ID=ument832> <Table ID =2>
	<Abstractive Summary> =
		4K
		Table 2: Data used to train and validate translation and
		summarization models
	</Abstractive Summary>
</Paper ID=ument832>


<Paper ID=ument832> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Cross-lingual summarization results with different approaches
	</Abstractive Summary>
	<Extractive Summary> =
		The {es,fr}-en models have the same
		hyperparameters as those of the base Transformer
		architecture described in Table 3 of Vaswani et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents our results
	</Extractive Summary>
</Paper ID=ument832>


<Paper ID=ument833> <Table ID =1>
	<Abstractive Summary> =
		67
		1
		5
		Table 1:
		Position of summary-worthy sentences in
		a document for single-doc (CNN/DM) and multi-doc
		datasets (DUC-04, TAC11)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, positive sen-
		tences in the training data (see §3
	</Extractive Summary>
</Paper ID=ument833>


<Paper ID=ument833> <Table ID =2>
	<Abstractive Summary> =
		35
		Table 2: Results on the DUC-04 dataset evaluated by
		ROUGE
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Summarization Results
		We compare our system with strong summariza-
		tion baselines (Table 2 and 3)
	</Extractive Summary>
</Paper ID=ument833>


<Paper ID=ument833> <Table ID =3>
	<Abstractive Summary> =
		65
		Table 3: ROUGE results on the TAC-11 dataset
	</Abstractive Summary>
</Paper ID=ument833>


<Paper ID=ument833> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Example system summaries and their human
		reference summary
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4 we show example system summaries
		and a human-written reference summary
	</Extractive Summary>
</Paper ID=ument833>


<Paper ID=ument834> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Comparison of state-of-the-art summarization systems
	</Abstractive Summary>
</Paper ID=ument834>


<Paper ID=ument834> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2: Percentage of summary sentences that are faithful,
		grammatical, etc
	</Abstractive Summary>
</Paper ID=ument834>


<Paper ID=ument834> <Table ID =3>
	<Abstractive Summary> =
		04
		Table 3: Results for each merging method
	</Abstractive Summary>
	<Extractive Summary> =
		Does the way sentences are fused affect their
		faithfulness and grammaticality? Table 3 provides
		insights regarding this question
	</Extractive Summary>
</Paper ID=ument834>


<Paper ID=ument835> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Some spans (top) are plausible summary state-
		ments, because they make sense when removed from
		context sentences
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 provides examples of spans that do and do
		not make sense when extracted in this manner
	</Extractive Summary>
</Paper ID=ument835>


<Paper ID=ument835> <Table ID =2>
	<Abstractive Summary> =
		Yeltsin might also appear weak if he had to replace General Grachev
		B
		B
		B
		Table 2: A candidate set for (t1) = “Mr
	</Abstractive Summary>
</Paper ID=ument835>


<Paper ID=ument835> <Table ID =3>
	<Abstractive Summary> =
		495
		Table 3: Spearman’s ρ for our ordinal regression model
		p(α(s)|s), compared both to the inter-annotator agree-
		ment and a simpler logistic regression model
	</Abstractive Summary>
</Paper ID=ument835>


<Paper ID=ument836> <Table ID =1>
	<Abstractive Summary> =
		23
		Table 1: Experimental results of baselines, oracles and
		models on Sentence-Level (S-Level) and Discourse-
		Level (D-Level) segmentation
	</Abstractive Summary>
	<Extractive Summary> =
		Results are listed in Table 1, all models with
		discourse-level segmentation outperform those
		with sentence-level segmentation, demonstrating
		the effectiveness of our ﬁner-grained means
	</Extractive Summary>
</Paper ID=ument836>


<Paper ID=ument837> <Table ID =1>
	<Abstractive Summary> =
		55
		Table 1: Label statistics for the SPOT datasets We explain this behavior with the
		statistics of Table 1: “Witness (Salient)” is higher
		in *-EDU datasets compared to *-SENT datasets
		Hyper-parameter
		Value
		Encoder
		type
		LSTM
		rnn hidden size
		100
		layers
		1
		Decoder
		type
		LSTM
		rnn hidden size
		100
		layers
		1
		General
		word vec size
		200
		optimizer
		Adam
		learning rate
		1e−3
		train/validation split
		90/10
		vocabulary size
		30k for SSC, TV merged
		10k for single TV-shows
		Table 1: Hyperparameters67
		4
		46
		Table 1: Key characteristics of the source sentences1
		Table 1: Adaptation to Proﬁciency Level in F0 E2E-ABSA
		sentence
		aspect, aspect sentiment
		Table 1: Different problem settings in ABSA
		whale → wahle
		Table 1: The synthetic noise types applied during training2
		Table 1: Results on dev and test987
		Table 1: Afﬁnity and Speciﬁcity of terms found in
		r/motogp calculated on the word distributions of 10
		sample subreddits6815
		Table 1:
		Impact of smoothing factor ϵ on the Quora
		validation set
		76
		Raw data
		After
		pre-processing
		# reviews
		53,273
		# words
		1,724,842
		529,035
		# unique words
		25,007
		10,102
		Table 1: Summary of dataset84%
		Table 1: Dataset statistics for multiclass datasets1
		Corpus
		Corpus
		Long-term
		Date
		Texts
		Tokens
		Types
		Texts
		Tokens
		Types
		Full
		10,696
		863,227
		32,020
		1,634
		127,644
		11,274
		Train+val
		1,464
		117,947
		9,973
		1,464
		115,227
		10,540
		Test
		150
		11,886
		2,383
		150
		11,738
		2,592
		Table 1: Descriptives of the dating proﬁle corpus
		A total sample of 12,310 dating proﬁles together
		with the indicated desired relationship goal was
		collected from a popular Dutch dating site (see
		Table 1)80
		-
		Table 1: F1 scores over ﬁve runs on CONLL and ONTONOTES test set of ablation experiments
		Paraphrase Questions
		我想知道卡片的开户行
		I would like to know the card’s bank
		您好,请帮我查询一下卡片的开户行
		Hi, please help me checkthe card’s bank
		卡片的开户行请帮我查询一下
		The card’s bank, please help me check it
		卡片的开户行能帮我查询一下吗？
		The card’s bank, can you help me check it?
		Table 1: Example of an question and its paraphrases26
		Table 1: Distribution of the replacement categories
		in the development data, ‘No norm
		Table 1: Performances of the external bibliographic re-
		sources used for matching books on RFR via ISBN
		Table 1: Targeted TV program
		# tweets
		3,745
		# chars per10
		Table 1: Accuracy of models (%) on Seq2seq task23
		Table 1: The word error rates of the different models in relation to the test set
		source
		correct target
		prediction
		1
		joo
		joo
		joo
		2
		ettE
		ett¨a
		ett¨a
		3
		heet
		he
		heet
		4
		uskovah
		uskovat
		uskovat
		5
		n
		niin
		niin
		6 <ettE
		ett¨a
		ett¨a
		7
		sinn
		sinne
		sinne
		8 <ei
		ei
		ei
		9
		ole
		ole
		ole
		10
		,
		,
		,
		11
		kukhaan
		kukaan
		kukaan
		12
		ymm¨art¨anny
		ymm¨art¨anyt
		ymm¨art¨anyt
		13
		menn¨a
		menn¨a
		menn¨a
		14
		           Relevance: irrelevant 
		Table 1: An example of relevance classification
		Table 1: Hyperparameters
		Parameter
		Value
		Number of encoder layers
		2
		Encoder forward cell size
		128
		Encoder backward cell size
		128
		Number of decoder layers
		1
		Decoder cell size
		512
		Input BPE vocab size
		40000
		BPE embedding size
		100
		UPOS embedding size
		100
		Language embedding size
		20
		Dropout rate
		0
		161
		Table 1:
		Examples on the left are considered valid
		usages in simplified Chinese (SC)397
		Table 1: Model performance with threshold selection (2011) explored the problem of identifying
		inﬂuenza epidemics using machine-learning based
		tweet classiﬁers along with search engine trends
		182
		Name
		Label
		Annotation
		Class
		Mention of Non-Tobacco Drugs
		OD
		-1
		Unrelated or Ambiguous Mention
		UM
		0
		Personal or Anecdotal Mention
		PM
		1
		Informative or Advisory Mention
		IM
		2
		Advertisements
		AD
		3
		Table 1: Label and ID associated with each class
		Train
		Dev
		Test
		Total
		FA
		consistent
		3956
		470
		538
		4998
		inconsistent
		28
		4
		2
		GA
		consistent
		3887
		468
		495
		4878
		inconsistent
		16
		6
		6
		B
		consistent
		3138
		400
		416
		4843
		inconsistent
		702
		85
		102
		C
		consistent
		3036
		382
		381
		4523
		inconsistent
		570
		69
		85
		Start
		consistent
		3725
		451
		472
		4924
		inconsistent
		223
		28
		25
		Stub
		consistent
		3863
		470
		492
		4931
		inconsistent
		83
		12
		11
		Total
		—
		23227
		2845
		3025
		29097
		Table 1: A breakdown of our Wikipedia dataset2%
		Table 1: Data set statistics, adverse drug event (AE) and sentiment scores (2016) explore
		constructions of verbal irony in social media texts,
		reporting that detection of contrasting polarities
		is a strong indicator and use sentiment analysis
		212
		Train
		All
		Ironic
		Non-Ironic
		Irony
		1
		2
		3
		# Tweets
		3817
		1901
		1383 (73%)
		316 (17%)
		202 (10%)
		1916
		# Tweets containing emoji
		406
		175
		162 (93%)
		7 (4%)
		6 (3%)
		231
		# Unique emojis
		158
		104
		122
		Test
		# Tweets
		784
		311
		164 (53%)
		85 (27%)
		62 (20%)
		473
		# Tweets containing emoji
		88
		33
		27 (82%)
		3 (9%)
		3 (9%)
		55
		# Unique emojis
		81
		23
		70
		Table 1: Dataset statistics
		Ironic
		Non-ironic
		Emoji
		Count
		Emoji
		Count
		42 (2954
		Table 1: Performance of prior work and proposed model98∗∗
		0
		Table 1: Model performance and signiﬁcance levels with respect to text-only models: ∗ : p ≤ 039
		W-NUT
		930
		–
		–
		Table 1: Number of labels and mean/median distance
		in km between instances and the cluster town center
		Case-agnostic models discard orthographic infor-
		237
		Annotation
		O
		O
		O
		B-ORG
		I-ORG
		E-ORG
		(a)
		Original Sentence
		I
		live
		in
		New
		York
		City
		(b)
		Lower-cased Sentence
		i
		live
		in
		new
		york
		city
		(c)
		Upper-cased Sentence
		I
		LIVE
		IN
		NEW
		YORK
		CITY
		Table 1: Example of Data Augmentation
		mation (how the given text was capitalized), which
		is considered to be highly useful (Robinson et al
		Label
		Train
		Dev
		Test
		Conﬂict
		+1
		14,749
		929
		509
		-1
		177,421
		13,130
		13,576
		Life
		+1
		17,434
		354
		154
		-1
		177,421
		13,130
		13,576
		Table 1: Numbers of the positive and negative samples
		of the LFK datasets
		False
		Table 1: Examples of relations annotated by distant
		supervision
		Phatic
		Could you send me your paper?
		Conative
		X:
		Sure, it’s here: <link>
		Referential
		Y:
		Thanks!
		Phatic
		Table 1: An example of a discussion where all func-
		tions of language are present61
		Table 1: CO-MC F1-scores / intents accuracies of the ﬁrst 100 %, 75 %, 50 %, and 25 % of the tokens of the
		utterances of the test dataset of the cleaned human transcribed full utterances
		training dataset
		95 % conf xx
		@renskedemaessc dm me je
		gsmnummer eens ;-)
		<user> doormail me je gsm-
		nummer eens <emoji>
		<user> mail me your cell-
		phone number once <emoji>
		Table 1: Source and target pairs as parallel data for a machine translation approach Given a word, the
		286
		Method
		Input Text
		Google Translate
		Clean Input
		there is a fat duck swimming in the lake
		湖里 有一只胖鸭子在游泳
		Noisy Input
		there is a fat dack swimming in the leake
		在 leake 里游泳时有一个 胖子
		Spell-Checker
		there is a fat sack swimming in the leak
		在 泄露处 有一个肥胖 袋在游泳
		Grammaly2
		there is a fat dack swimming in the lake
		湖里 游泳很胖
		Ours
		there is a fat duck swimming in the lake
		湖里 有一只胖鸭子在游泳
		Table 1: Illustrative example of spell-checker and contextual denoising
		Label
		Description
		ARGA
		Causer
		ARG0
		Agent or Experiencer or Doer
		ARG1
		Theme or Patient
		ARG2
		Beniﬁciary
		ARG2 ATTR
		Attribute or Quality
		ARG2 LOC
		Physical Location
		ARG2 GOL
		Destination or Goal
		ARG2 SOU
		Source
		ARG3
		Instrument
		ARGM DIR
		Direction
		ARGM LOC
		Location
		ARGM MNR
		Manner
		ARGM EXT
		Extent or Comparison
		ARGM TMP
		Temporal
		ARGM REC
		Reciprocal
		ARGM PRP
		Purpose
		ARGM CAU
		Cause or Reason
		ARGM DIS
		Discourse
		ARGM ADV
		Adverb
		ARGM NEG
		Negative
		ARGM PRX
		Complex Predicate
		Table 1: PropBank Tagset
		Social media data doesn’t conform to the rules
		of spelling, grammar or punctuation @beautifulloser8 i’m about
		to type it up !
		Table 1: Noisy UGC example and its canonical form
		(Gold)
		We make a few comments on this deﬁnition Dog sitting disaster
		Table 1: Each unshaded box shows the titles of a randomly chosen post and the 5 posts most similar to it
		counted the proportion of them that are clustered
		into the same cluster
		Many of
		Snippet
		haaapppyyyy birthday best friend!!
		Love you lots #love
		Word
		‘haaapppyyyy’, ‘birthday’, ‘best’,
		‘friend’, ‘!’, ‘!’, ‘Love’, ‘you’,
		‘lots’, ‘#’, ‘love’
		Wordpiece ‘ha’, ‘##aa’, ‘##pp’, ‘##py’, ‘##y’,
		‘##y’,
		‘##y’,
		‘birthday’,
		‘best’,
		‘friend’, ‘!’, ‘!’, ‘Love’, ‘you’,
		‘lots’, ‘#’, ‘love’
		Domain-
		speciﬁc
		‘happy’,
		‘<elongated>’,
		‘birth-
		day’, ‘best’, ‘friend’, ‘!’, ‘<re-
		peated>’,
		‘Love’,
		‘you’,
		‘lots’,
		‘</hashtag>’, ‘love’, ‘<hashtag>’
		Table 1: Example of an real-world irregular expression
		preprocessed by methods at different levels0)
		Table 1:
		Number of users in SMHD dataset per con-
		dition and the average number of posts per user (with
		std27M
		Table 1: Size of parallel and monolingual data
		Hate Speech
		Table 1:
		Examples where our combined model is able to predict correct label whereas the baseline sentiment
		model fails
		Error type Description
		Example
		Occ
		incorInﬂ
		incorrect inﬂection
		[pracovají → pracují] v továrnˇe
		8 986
		incorBase incorrect word base
		musíš to [posvˇetlit → posvˇetit]
		20 334
		fwFab
		non-emendable, „fabricated“ word
		pokud nechceš slyšet [smášky]
		78
		fwNC
		foreign word
		váza je na [Tisch → stole]
		166
		ﬂex
		supplementary ﬂag used with fwFab and
		jdu do [shopa → obchodu]
		34
		fwNC marking the presence of inﬂection
		wbdPre
		preﬁx separated by a space or preposition w/o space musím to [pˇri pravit → pˇripravit]
		817
		wbdComp wrongly separated compound
		[ˇceský anglický → ˇcesko-anglický] slovník
		92
		wbdOther other word boundary error
		[mocdobˇre → moc dobˇre]; [atak → a tak]
		1326
		stylColl
		colloquial form
		[dobrej → dobrý] ﬁlm
		3 533
		stylOther
		bookish, dialectal, slang, hyper-correct form
		holka s [hnˇedými oˇcimi → hnˇedýma oˇcima]
		156
		agr
		violated agreement rules
		to jsou [hezké → hezcí] chlapci; Jana [ˇctu → ˇcte]
		5 162
		dep
		error in valency
		bojí se [pes → psa]; otázka [ˇcas → ˇcasu]
		6 733
		ref
		error in pronominal reference
		dal jsem to jemu i [jejího → jeho] bratrovi
		344
		vbx
		error in analytical verb form or compound predicate musíš [pˇrijdeš → pˇrijít]; kluci [jsou] bˇehali
		864
		rﬂx
		error in reﬂexive expression
		dívá [∅ → se] na televizi; Pavel [si → se] raduje
		915
		neg
		error in negation
		[p˚ujdu ne → nep˚ujdu] do školy
		111
		lex
		error in lexicon or phraseology
		dopadlo to [pˇrírodnˇe → pˇrirozenˇe]
		3 967
		use
		error in the use of a grammar category
		pošta je [nejvíc blízko → nejblíže]
		1 458
		sec
		secondary error (supplementary ﬂag)
		stará se o [našich holˇciˇckách → naše holˇciˇcky]
		866
		stylColl
		colloquial expression
		vidˇeli jsme [hezký → hezké] holky
		3 533
		stylOther
		bookish, dialectal, slang, hyper-correct expression
		rozbil se mi [hadr]
		156
		stylMark
		redundant discourse marker
		[no]; [teda]; [jo]
		15
		disr
		disrupted construction
		známe [hodné spoustu → spoustu hodných] lidí
		64
		problem
		supplementary label for problematic cases
		175
		unspec
		unspeciﬁed error type
		69 123
		Table 1: Error types used in CzeSL corpus taken from (Jelínek et alhtml
		357
		Word
		Confusion set
		had
		hard head hand gad has ha ad hat
		night
		knight naught nought nights bight might nightie
		then
		them the hen ten than thin thee thew
		haben
		habend halben gaben habe habet haken
		Nacht
		Nachts Nascht Macht Naht Acht Nach Jacht Pacht
		dann
		sann dank denn dünn kann wann bannen kannst
		имел
		им ел им-ел имела имели имело мел умел
		ночь
		ночью ночи дочь мочь ноль новь точь
		затем
		за тем за-тем затеем затеям зятем затеями
		Table 1: Examples of spell-broken confusion sets for
		English, German and Russiancom/topsites/countries/MT
		Source Type & Name
		Query strings/
		Articles
		Online
		Posts
		SNS - Twitter
		1
		38
		SNS - Facebook
		1
		28
		SNS
		-
		Twitter-The
		Malta Independent
		1
		12
		News - Times of Malta
		4
		249
		News - MaltaToday
		4
		175
		News - The Malta Inde-
		pendent
		4
		45
		Table 1: Data sources used for the consolidated dataset
		terms of content published– were selected for each
		newswire mentioned:
		• Overview of the upcoming budget, published
		on the budget day;
		• Near to real-time live updates in commentary
		format, on the budget measures being pre-
		sented for the upcoming year;
		• Overview of the presented budget, published
		after the budget ﬁnishes, on the same day
		and/or the following day3571
		Table 1: Performance of Article Bias Classiﬁer
		When comment labels were utilized for train-
		ing, existing language models could success-
		fully detect the slant of news comments, as dis-
		372
		Figure 1: The proportion of liberal and conservative news comments within the progressive and conser-
		vative media’s comment sections
		played in Table 2
		Singular
		Table 1: Examples from our two domains23
		Table 1: Impact of the parameters on validation perfor-
		mance for the WWII article history The approximated global optimum point
		of SGNS is shifted-PMI (Levy and Goldberg,
		Symbol
		Description
		V
		Vocabulary
		Gi
		Set of sub-words in word i
		i, j
		Index of a word in vocabulary
		D
		Set of co-occurring word pairs
		σ
		Sigmoid function
		⃗ui,⃗vj
		Embedding vector of word i, j
		⃗gi,⃗hj
		Embedding vector of sub-word i, j
		k
		Regularization parameter
		s
		Statistic sub-sampling parameter
		Sij
		Regularization coefﬁcient
		of word pair (i, j)
		Table 1: Summary of the symbols
		2014b)
		Table 1: Example of the lexical substitution tasks
		weights38
		Table 1: Statistics on the French side of the corpora used in our experiments919
		Table 1: Data Set Detail
		Dhar et al
		427
		Sample of detected neologisms
		politics
		pizzagate, drumpf, trumpster, shillary, killary
		news
		antifa, brexit, drumpf, Libruls, redpilling, neonazi
		worldnews
		burkini, brexit, pizzagate, edgelord, petrodollar
		sports
		deﬂategate, handegg, ballboy, skurﬁng, playstyle
		movies
		plothole, stuckmannized, jumpscare, MetaHuman
		gaming
		playerbase, pokestop, jumpscare, hitscan
		Table 1: Subreddit-level detected neologisms
		Accuracy
		BLEU
		Baseline
		55
		432
		Datasets
		DemB
		IMDBs
		AphB
		Task nature
		Structured
		X
		X
		Partially structured
		X
		Language type
		Verbal
		X
		X
		Written
		X
		Lexics
		Complex
		X
		Medium
		X
		Simple
		X
		Syntax
		Complex
		X
		Medium
		X
		Simple
		X
		Table 1: Comparison of the datasets in terms of task
		nature, type of language used to collect the data, lexical
		and syntactic complexity
	</Abstractive Summary>
	<Extractive Summary> =
		For evalu-
		ating the segment-level classiﬁcation performance
		on Yelp’13 and IMDB, we use the SPOT-Yelp and
		SPOT-IMDB datasets, respectively (Angelidis and
		Lapata, 2018), annotated at two levels of gran-
		ularity, namely, sentences (SENT) and Elemen-
		tary Discourse Units (EDUs)1 (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		25, which
		is signiﬁcantly lower than the WR on sentiment
		classiﬁcation datasets (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		The
		Spanish-A2 testset has the highest number of er-
		rors per 100 words among all the L1-Level test-
		sets, as shown in Table 1 in the Appendix
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes three existing research
		problems related to ABSA
	</Extractive Summary>
	<Extractive Summary> =
		Synthetic Noise
		Table 1 describes the four types
		of synthetic noise we used during training
	</Extractive Summary>
	<Extractive Summary> =
		7
		Results
		Table 1 contains results without CNNs for the
		baselines, RNNs, and CRF
	</Extractive Summary>
	<Extractive Summary> =
		A total of 3,228 texts (1,614 texts for each re-
		lationship group; see Table 1) was used for train-
		ing and testing
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 reports results of 4 strong baselines
		that use popular embeddings (column X), further
		adding either the LS representation (Ghaddar and
		Langlais, 2018a) or ours
	</Extractive Summary>
	<Extractive Summary> =
		Not
		surprisingly, there is a correlation visible with the
		frequencies (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		In order to retrieve the author’s
		name(s) associated with the books in the RFR dataset,
		we perform ISBN matching using public APIs on eight
		of them, listed in Table 1 along with the fraction of
		found ISBNs from this dataset
	</Extractive Summary>
	<Extractive Summary> =
		1
		Comparing models on Seq2seq task
		In Table 1 we report the overall character level ac-
		curacy of the 4 best performing models for each
		conﬁguration and experiment: (1) LSTM-Token-
		seq: the model with the Token LSTM + Sequence
		encoder (yellow and blue parts of Figure 1) and
		Token decoder, (2) CNN-Token-seq: the model
		with the Token CNN + Sequence encoder (red
		and blue parts of Figure 1) and Token decoder
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the WERs of the different meth-
		ods
	</Extractive Summary>
	<Extractive Summary> =
		, topic and utterance1 in Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		shows a full list of the hyperparameters used in
		the training procedure
	</Extractive Summary>
	<Extractive Summary> =
		The
		difference between simplified and tradi-
		tional Chinese shown in Table 1 is an
		example
	</Extractive Summary>
	<Extractive Summary> =
		It can also be applied to both
		simplified Chinese and traditional Chinese,
		despite the challenging issue that some er-
		roneous usages of characters in traditional
		texts are considered valid usages in simpli-
		fied texts (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Baseline model performance
		Table 1 shows baseline model performance on the
		relationship disambiguation task and highlights
		model parameters
	</Extractive Summary>
	<Extractive Summary> =
		Overall, from the scores in Table 1 one can see
		that the vector similarity models improve over the
		return all, but that there is much work to be done
		to further improve precision and recall
	</Extractive Summary>
	<Extractive Summary> =
		Post were distributed over the 11 forums as
		shown in Table 1 in terms of number of posts
		and percentage of total (columns 2 and 3); also
		shown are median post length, percentage of posts
		with AEs, and percentages of posts with posi-
		tive/negative/neutral sentiment
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the
		resulting number of labels, and the mean distance
		in km between the new instance coordinates and
		the respective town center
	</Extractive Summary>
	<Extractive Summary> =
		See
		Table 1 (a) for an example annotated sentence
	</Extractive Summary>
	<Extractive Summary> =
		When a sentence is all-lowercased
		or all-uppercased as in Table 1 (b) and (c), each
		word would still correspond to the same en-
		tity
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the staticstics of our generated datasets
	</Extractive Summary>
	<Extractive Summary> =
		In Table 1, we present
		an anecdotal example where all functions are ex-
		pressed in the conversation
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 in Zhang et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 lists some examples of the noisy data
		we are dealing with
	</Extractive Summary>
	<Extractive Summary> =
		There are several beneﬁts of the proposed method:
		• Our method can make accurate corrections
		based on the context and semantic meaning
		of the whole sentence as Table 1 shows
	</Extractive Summary>
	<Extractive Summary> =
		We illustrate
		it with the following example (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Qualitative Evaluation
		Table 1, shows the titles of 3 randomly chosen
		posts and the ﬁve most similar posts to them
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows an example of an irregular expres-
		sion and how it can be preprocessed at word level,
		wordpiece level and at domain level
	</Extractive Summary>
	<Extractive Summary> =
		As the average number of posts per user is roughly
		160 (Table 1), it is reasonable to expect of a model
		to perform well with similar amounts of data avail-
		able
	</Extractive Summary>
	<Extractive Summary> =
		On looking at some reviews in the test dataset
		that our combined model (Se + Sc + Hu + Ha)
		got right and that the baseline sentiment model
		got wrong, we observe that our model deﬁnitively
		helps with identifying the right sentiment for sar-
		castic reviews (Table 1) and some hate reviews, al-
		though we couldn’t ﬁnd many humorous reviews
		in this context
	</Extractive Summary>
	<Extractive Summary> =
		Most context-free spell-checkers
		combine a weighted edit distance and phonetic al-
		gorithms to order suggestions, which produces reli-
		able confusion sets (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 presents details about the social dataset
		collected on the Malta Budget 2018
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summa-
		rizes the results, which shows the BODY-TEXT
		representation of the validation set showed the best
		result
	</Extractive Summary>
	<Extractive Summary> =
		See the ﬁrst two tweets in Table 1,
		for an example of these two uses by the same
		user
	</Extractive Summary>
	<Extractive Summary> =
		As seen on Table 1, when compared against
		the regular encoder, utilizing our edit-sentence ap-
		proach with token-level labels leads to a higher
		F1-Score and accuracy, showing the effectiveness
		of our proposed edit encoder
	</Extractive Summary>
	<Extractive Summary> =
		Table 1
		shows an example of the lexical substitution task
		with a sentence,1 the target word to replace, and
		words of substitution candidates
	</Extractive Summary>
	<Extractive Summary> =
		Ranking Method
		As shown in Table 1, lexical
		substitution ranks substitution candidates of the
		target word based on their paraphrasabilities under
		a given context
	</Extractive Summary>
	<Extractive Summary> =
		OpenTest, for which Table 1 reports some statis-
		tics
	</Extractive Summary>
	<Extractive Summary> =
		37, means that the level of
		mixing between languages in the data is quite high
		(see Table 1 for detail)
		2https://en
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows samples of the top detected ne-
		ologisms for each subreddit
	</Extractive Summary>
</Paper ID=ument837>


<Paper ID=ument837> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: F1 score for segment-level sentiment classiﬁ-
		cation5)
		218k
		Table 2: Results of experiments on formality and sentence suitability72
		1
		68
		Table 2: Key characteristics of collected paraphrases0
		Table 2: Top: Adaptation to L1 Only06
		Table 2: Main results9%
		Table 2: Performance on the IWSLT 2016 translation task with varying rates of natural noise in the test set3
		Table 2: Results of the CNN models000
		‘subreddit’, ‘order’,
		‘account’, ‘game’, ‘issue’
		Table 2: A sample presentation of high afﬁnity terms and low afﬁnity terms from subreddits with high high afﬁnity
		averages (top 1%), and low high afﬁnity averages (bottom 1%)6764
		Table 2:
		Impact of the batch size N on the Quora
		validation set They in-
		tegrate with my Samsung Smart Things Hub
		79
		Label
		Top keywords
		Price
		price, buy, gift, christmas, worth, black friday, money, sale, deal, item
		Integration
		light, control, command, system, integration, thermostat, room, ecosystem, connect
		Sound quality
		speaker, sound, quality, small, music, loud, great, room, bluetooth, volume
		Accuracy
		question, answer, time, response, quick, issue, problem, work, search, good
		Skills
		music, weather, news, alarm, timer, kitchen, morning, reminder, shopping list
		Fun
		fun, family, kid, useful, helpful, great, friend, game, information, question
		Ease of use
		easy, use, set, setup, simple, install, recommend, connect, quick, work
		Playing music
		music, play, song, playlist, favorite, pandora, prime, stream, subscription, beam
		Table 2: 8 merged dimensions and the keywords reveal how people use smart speakers and their perceptions105
		808
		Table 2: Dataset statistics for multilabel datasets29**
		Table 2: Pearson’s r compared to humans84
		Table 2: Mention-level F1 scores
		869
		1,063
		620
		873
		Table 2: Statistics of annotated datasets for question
		paraphrasing
		Table 2: Global system top-k accuracy at the book level)
		# vocab of chars
		1,693
		# vocab of words
		7,727
		Table 2: Statistics of Raw Data
		# B-REFERENCE
		7,871
		# I-REFERENCE
		5,558
		# O
		29,829
		Table 3: Statistics of Reference Tags
		1https://en
		15
		Artj¨arveN
		Artj¨arven
		Artj¨arven
		16
		kirkolt
		kirkolta
		kirkolta
		17
		menn¨ah
		menn¨a¨an
		menn¨a¨an
		18
		sinneh
		sinne
		sinne
		19
		Hiitel¨ah
		Hiitel¨a¨an
		Hiitel¨ass¨a
		Table 2: Examples from input, output and prediction
		phonotactic accuracy makes selection of correct
		analysis from multiple predicted variants more dif-
		ﬁcult, as it is not possible to easily detect mor-
		phologically valid and invalid forms, all sen-
		tences belonging to the same group had the same
		155
		Table 2: Training Dataset Statistics
		English
		German
		Spanish
		Chinese
		Korean
		Dutch
		English
		-
		521
		163
		Table 2: Examples of the computation of character similarities25
		Table 2: Average polysemy statistics”
		Table 2: Examples for each category represented by its label442
		Table 2:
		Intrinsic evaluation results
		2,320
		216
		615
		51
		2,845
		1,037
		137
		360
		313
		169
		308
		Table 2: Occurrence counts for Phase 1 annotations (Que=Question, Neg=Negation, Gen=Generalisation, Spe=
		Speculation, Red=ReducedCertainty; see Section 48%)
		Table 2: Top 10 most frequent emojis in ironic tweets and
		non-ironic tweets along with the count and percentage of each
		emoji03∗∗
		Table 2: Model performance and signiﬁcance levels with respect to text-only model AttCNN: ∗ : p ≤ 009∗
		Table 2: Performance of prior work and of the proposed model with W-NUT and P2C labels2
		Table 2: F1 scores on original, lower-cased, and upper-cased test sets of English Datasets0 (4)
		Table 2: Model performance5
		Table 2: Parameter Settings
		outperform the former over the entire range of re-
		call51%
		Table 2: Analysis per language function54
		Table 2: Intents accuracies / percentages of the used tokens for predicting the intents using the smallest partial
		utterance of the test dataset of the cleaned human transcribed incremental utterances for which the system has a
		conﬁdence of more or equal than 95 %, 90 %, 85 %, and 80 %, if the conﬁdence is not reached, the full utterance is
		used
		training dataset
		ﬁrst 100 %
		ﬁrst 75 %
		ﬁrst 50 %
		ﬁrst 25 %
		human, full
		90117
		Table 2: Dutch parallel corpora data statistics of [MASK] (n)
		Top k
		Size
		1
		3000
		3000
		2
		5
		25
		3
		3
		27
		4
		2
		16
		Total:
		3068
		Table 2: Size of the candidate list
		307
		used as a light verb
		Table 2:
		Rolesets and meanings for the Hindi verb
		xeKa We will
		see in section 5 that despite its simplicity, such
		300
		noisy
		canonical
		ye
		yeah
		##a
		[SPACE]
		im
		i
		�
		MASK
		�
		’
		�
		MASK
		�
		m
		already
		already
		knowing
		knowing
		wa
		what
		##t
		[SPACE]
		Table 2: Independent Alignment of yea im already
		knowing wat u sayin normalized as yeah i’m already
		knowing what you saying
		an alignment allows our model to reach good
		performances
		Original
		Processed
		’:)’, ’:-)’
		<happy>
		’REAL’
		<allcaps>real </allcaps>
		’gooooood’
		good <elongated>
		October 8th
		<date>
		@jeremy
		<user>
		#Christmas
		<hashtag>Christmas </hashtag>
		Table 2: Examples of typical Twitter-speciﬁc expres-
		sions and their preprocessed versions with annotation
		marks
		394
		Table 2: F1 measure averaged over ﬁve runs with different control groups54%
		Table 2: Noise level of ASR generated data ﬁltered
		with different length ratio thresholds22
		Table 2: Testing our hypothesis: Mi refers to the re-
		spective model, P and L indicate using predicted prob-
		abilities and labels respectively1 %
		Table 2: Statistics of the AKCES-GEC dataset – number of documents, sentences, words and error rates
		Corpus
		Dev
		Test
		Train
		EN
		W&I+LOCNESS
		4,384
		4,477
		34,308
		DE
		Falco+MERLIN
		2,503
		2,337
		18,7544
		RU
		RULEC-GEC
		2,500
		5,000
		4,980
		Table 2: Sizes of labelled corpora in no9669
		Table 2: Inter-rater reliability measures for each anno-
		tation type
		A third expert in the domain consolidated the
		annotations to create a ﬁnal dataset8322
		Table 2: Performance of Comment Bias Classiﬁer
		5 In-
		stead, we employ different techniques to obtain
		distantly supervised labels in two domains, as
		376
		Twitter
		Europarl
		Train
		58963
		11249
		Dev
		7370
		1405
		Test
		7370
		1405
		Total
		73703
		14059
		Table 2: Number of instances in our two corpora28
		Table 2: Summary of our results18
		Table 2: Spearman’s rank correlation coefﬁcient of word similarity task on 2
		Table 2: Examples of CEFR-LP5†
		Table 2: BLEU score results for our two benchmark models for the different train-test combinations ”jobnya” (in English: ”the job”)
		Table 2: Type of OOV Tokens
		37
		Table 2: Evaluation of the normalization systems
		least two of the three annotators agree upon were
		selected as normalizations19
		Table 2: Lexical complexity and richness, and syntactic
		complexity of the three datasets
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results
		Sentiment Classiﬁcation:
		Table 2 reports the
		evaluation results on SPOT datasets for both
		sentence- and EDU-level classiﬁcation
	</Extractive Summary>
	<Extractive Summary> =
		The results in
		Table 2 (top) show that all L1-adapted models
		are better than the baseline, with improvements
		ranging from 1
	</Extractive Summary>
	<Extractive Summary> =
		All the models adapted to both
		Level and L1 outperform the models adapted to
		only one of these features, as shown in Table 2
		(bottom)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Main Results
		From Table 2, we surprisingly ﬁnd that only in-
		troducing a simple token-level classiﬁer, namely,
		BERT-Linear, already outperforms the existing
		37
		BERT-GRU
		BERT-TFM
		BERT-CRF
		0
		20
		40
		60
		80
		100
		F1 score
		46
	</Extractive Summary>
	<Extractive Summary> =
		Results
		Table 2 shows the model’s performance
		on data with varying amounts of natural errors
	</Extractive Summary>
	<Extractive Summary> =
		For CNN results, Table 2 shows test set perfor-
		mance
	</Extractive Summary>
	<Extractive Summary> =
		Our results, as shown in Table 2,
		demonstrate that high afﬁnity terms have different
		characteristics across communities
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the effect
		of an increase in the number of negative examples
		in a batch
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the respective top keywords
	</Extractive Summary>
	<Extractive Summary> =
		05) was signiﬁcantly
		stronger than the correlation scores for the regres-
		sion model on these labels (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Comparing Contextualized Embeddings
		Table 2 reports F1 scores on the test portion of
		the 7 datasets we considered, for models trained
		with different embedding combinations
	</Extractive Summary>
	<Extractive Summary> =
		1
		Error analysis
		Table 2 illustrates the general performance of the
		model, with errors marked in bold
	</Extractive Summary>
	<Extractive Summary> =
		Vast ma-
		jority of needed changes are individual insertions,
		replacements or deletions in the word end, as il-
		lustrated in Table 2 at lines 2, 4, 6, 7, 15, 16, 17
		and 18
	</Extractive Summary>
	<Extractive Summary> =
		They help us to model the sub-
		tle nuances in different characters that are composed of
		identical strokes (see examples in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Secondly, if a pair
		of more complex characters have the same edit dis-
		tance as a pair of less complex characters, we want
		the similarity of the more complex characters to be
		slightly higher than that of the less complex char-
		acters (see examples in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Therefore, we utilize character pronunciations
		of all CJK languages (see examples in Table 2),
		which are provided by the Unihan Database
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 report polysemy statistics
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the average NLPD over
		10 runs on both test sets
	</Extractive Summary>
	<Extractive Summary> =
		Here we also see that
		GPs achieve signiﬁcantly better performance than
		RFs, although by a much lower margin compared
		to the results on the primary labels (Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 presents occurrence counts for enti-
		ties and events from Phase 1
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows ten emojis that
		most frequently appear in ironic tweets and non-
		ironic tweets in the English dataset
	</Extractive Summary>
	<Extractive Summary> =
		F1 scores are shown
		in Table 2 and 3
	</Extractive Summary>
	<Extractive Summary> =
		‘Transfer to Twitter’ col-
		umn of Table 2 reports results
	</Extractive Summary>
	<Extractive Summary> =
		2
		Evaluation
		Table 2 presents the performance of the models
		on the test performance for different datasets (i
	</Extractive Summary>
	<Extractive Summary> =
		The
		percentage of controversial parent comments per
		function shown in Table 2 conﬁrms our intuition
		that conative comments are used to reply to con-
		troversial content more often than the other com-
		ments
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 presents the number of parallel sen-
		tences in each genre and the number of words be-
		fore and after normalization3
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 also reveals that the corpus
		amounts to only a few hundred parallel sentences
	</Extractive Summary>
	<Extractive Summary> =
		This has re-
		sulted in a different style of Twitter corpus which
		comprises more noisy and longer tweets (see CER
		in Table 2 vs
	</Extractive Summary>
	<Extractive Summary> =
		When comparing the results of Setups 2 and 3
		with the original WER values of our test set (see
		Table 2) we observe that the normalization task
		was not solved at all as the WER values are al-
		most always higher
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 give a few examples of preprocessed
		words with annotation, where <*>is a designated
		annotation mark
	</Extractive Summary>
	<Extractive Summary> =
		This fact is supported by Table 2, which
		shows that the average error rate of Romani de-
		velopment set is 21
	</Extractive Summary>
	<Extractive Summary> =
		As primary development and test data, we use
		the following learner corpora (Table 2):
		• English: the new W&I+LOCNESS corpus
		(Bryant et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 2, shows the inter-rater reli-
		ability agreement scores for each annotation type
	</Extractive Summary>
	<Extractive Summary> =
		See Table 2 for details about each
		of these datasets, which we make publicly avail-
		able
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 summarizes our best results on each se-
		lected article
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the results for the
		same experiment but with words that occur ﬁve or
		more times in the corpus for a vocabulary size of
		2
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows examples sampled from CEFR-
		LP
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		IMDB is the most
		lexically rich dataset (see Table 2), with the high-
		est ratio of uni-, bi-, and trigrams occuring only
		once
	</Extractive Summary>
	<Extractive Summary> =
		IMDB is the most complex according to various
		measures of syntactic complexity: it has the high-
		est scores with metrics associated with length of
		production unit, amount of subordination, coordi-
		nation, and particular structures, and it also has the
		highest amount of complex sentences (sentences
		of D-level 5-7, as shown in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		AphB has the lowest
		level of syntactic complexity, containing the high-
		est amount of the simplest sentences (D-level 0),
		and lowest scores in other subgroups of syntactic
		features (see Table 2)
	</Extractive Summary>
</Paper ID=ument837>


<Paper ID=ument837> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Review-level (left) and sentence-level (right)
		evaluation results for discovering foodborne illness
		Table 3: Priming questions used for human evaluation of paraphrase adequacy (ParA), paraphrase ﬂuency (ParF ),
		and translation adequacy (NMTA)5
		Table 3: Results on the CoNLL14 testsets for Chinese
		models
		Dataset
		Train
		Dev
		Test
		Total
		LAPTOP
		# sent
		2741
		304
		800
		4245
		# aspect
		2041
		256
		634
		2931
		REST
		# sent
		3490
		387
		2158
		6035
		# aspect
		3893
		413
		2287
		6593
		Table 3: Statistics of datasets27
		Table 3: Performance on IWSLT 2016 de-en test with
		maximal natural noise when training with one noise
		type (top) and three noise types (bottom) This for-
		54
		Table 3: Phone accuracy as a higher % of unicode sub-
		stitutions are made for lookalike ASCII characters062
		Table 3: Coefﬁcient of determination values for linear models trained on community characteristics that predict
		semantic narrowing (left) and semantic broadening (right) of high afﬁnity terms6837
		Table 3:
		Comparison of different loss functions on
		Quora validation set4
		Table 3: Classiﬁcation Accuracy for single label classi-
		ﬁcation62
		Table 3: Accuracy scores by humans and classiﬁca-
		tion models
		# entities
		Dataset
		Domain Types train
		dev
		test
		CONLL
		news
		4
		23499 5942
		5648
		ONTONOTES
		news
		18
		81828 11066 11257
		WNUT17
		tweet
		6
		1975
		836
		1079
		I2B2
		bio
		23
		11791 5453 11360
		FIN
		ﬁnance
		4
		460
		-
		120
		WIKIGOLD
		wikipedia
		4
		-
		-
		3558
		WEBPAGES
		web
		4
		-
		-
		783
		Table 3: Statistics on the datasets used in our ex-
		periments690
		Table 3: The overall performance of different models on four datasets from two domains (bold number in each
		column is the best performance on that dataset))
		# vocab of chars
		1,693
		# vocab of words
		7,727
		Table 2: Statistics of Raw Data
		# B-REFERENCE
		7,871
		# I-REFERENCE
		5,558
		# O
		29,829
		Table 3: Statistics of Reference Tags
		1https://encom/ccliu2/syn-emb
		Table 3: Syntactic Nearest-Neighbour Accuracy (%)
		ISO
		1-NN/5-NN
		Total/Groups
		English
		en
		973
		Table 3: Statistics of datasets73
		Table 3: Spearman’s rank correlation coefﬁcients on word similarity tasks67%
		Table 3: Binary Classiﬁcation accuracies for speciﬁc topic (Personal Health Mention) or general theme (Tobacco-
		related Mentions)545
		Table 3: Point estimate comparison results (Ironic)
		Case 3
		Another day in paradise haha (Ironic)
		Another day in paradise haha
		(Non-Ironic)
		Table 3: Examples of annotated tweets with respect to the different cases7
		Table 3: F1 scores on original, lower-cased, and upper-cased test sets of Non-English Datasets
		Question 2: How effective Caseless, True-
		casing, and Data Augmentation approaches are
		in improving robustness of models? All meth-
		ods show similar levels of performance on lower-
		cased or uppercased text2
		Table 3: P@N for relation extraction in entity pairs with different number of sentences
		0
		262
		referential
		phatic
		emotive
		poetic
		conative
		she has a credible claim
		absolutely agree
		shameful
		feeltheburn
		mind explaining why
		what time was that
		I’m sorry
		barely even human
		inverted triple bern
		keep fooling yourself
		it’s all marketing
		I upvoted you
		epic simply epic
		duality of man
		dude relax
		Table 3: Example of predictions of our semi-supervised approach17
		Table 3: CO-MC F1-scores / intents accuracies of the ﬁrst 100 %, 75 %, 50 %, and 25 % of the tokens of the
		utterances of the test dataset of the human transcribed full utterances
		training dataset
		95 % conf067
		Table 3: Parallel corpora data statistics after new anno-
		tations40
		Table 3: BLEU scores of EN-to-DE tranlsation
		As can be seen, both fairseq model and Google
		Translate suffer from a signiﬁcant performance
		drop on the noisy texts with both natural and syn-
		thetic noise
		Feature
		Argument Identiﬁcation
		P
		R
		f-score
		Predicate
		33
		50
		40
		Headword (HW)
		52
		47
		49
		HeadwordPOS
		33
		50
		40
		Phrasetype (PT)
		41
		34
		37
		Predicate-PT
		42
		65
		51
		Predicate-HW
		55
		49
		51
		Dependency
		78
		78
		78
		Named Entity
		57
		50
		65
		HeadwordPOS-PT
		41
		34
		37
		Headword-PT
		57
		49
		53
		HeadwordPOS(UD)
		32
		50
		39
		UD dependency
		64
		65
		64
		Predicate-language
		43
		65
		52
		Headword-language
		55
		47
		51
		Table 3: Individual feature performance for Argument
		Identiﬁcation
		variance) along i dimension
		301
		Noisy
		Canonical
		ye
		ye
		##a
		##ah
		im
		i
		�
		MASK
		�
		’
		�
		MASK
		�
		m
		already
		already
		knowing
		knowing
		wa
		wh
		##t
		##at
		Table 3: Parallel Alignment of yea im already knowing
		wat u sayin normalized as yeah i’m already knowing
		what you saying
		Noisy
		Gold
		#next mask
		ye
		ye
		0
		##a
		##ah
		0
		im
		i
		2
		�
		MASK
		�
		’
		-
		�
		MASK
		�
		m
		-
		already
		already
		0
		knowing
		knowing
		0
		wa
		wh
		0
		##t
		##at
		0
		Table 4: Parallel Alignment of yea im already knowing
		wat u sayin normalized as yeah i’m already knowing
		what you saying with gold number of next masks for
		each source token
		the overall architecture3†
		Table 3: Result of multi-label emotion classiﬁcation on
		SemEval-2018 pronouns
		I, my, her, your, they
		I’ve never, your thoughts
		Affective
		like, nice, love, bad
		I love
		Social
		friend, boyfriend, girl, guy
		my dad, my girlfriend, my ex
		Biological
		pain, sex, skin, sleep, porn
		your pain, a doctor, a therapist
		Informal
		omg, lol, shit, fuck, cool
		tl dr, holy shit
		Other
		advice, please, reddit
		thank you, your advice
		Table 3:
		Unigrams and bigrams most often given the
		highest weight by attention mechanism in depression
		classiﬁcation07
		Table 3: BLEU scores of models ﬁne-tuned on different data in the Fr→En direction18
		Table 3:
		Model Performances for various combina-
		tions of Sentiment Se (E1), Sarcasm Sc (E2), Humor
		Hu (E3) and Hate Speech Ha (E4)4%
		Table 3: Statistics of available corpora for Grammatical
		Error Correction99
		Table 3: Performance for different confusion sets and
		edit weighting techniques on W&I+LOCNESS Dev2%
		Table 3: Distribution of sentiment polarity annotations
		Polarity Intensity
		Online Posts
		Percentage
		Very Positive
		37
		68
		Table 3: Accuracy (percent of correct predictions) of
		our ﬁne-tuned BERT model, tested both in- and out-of-
		domain81
		Table 3: Spearman’s rank correlation coefﬁcient of word similarity task on 665
		Table 3: Basic statistics in CEFR-LP compared to LS-SE and LS-CIC
		CEFR level
		target
		candidate
		all
		863
		14, 259
		A1
		300
		2, 090
		A2
		190
		2, 856
		B1
		110
		4, 513
		B2
		186
		3, 201
		C1
		30
		648
		C2
		47
		951
		Table 4: Distribution of CEFR levels in CEFR-LP
		a certain candidate
		Table 3: BLEU score results for our three benchmark models on normalized test sets14
		Table 3: Language Identiﬁcation Experiment Result
		For evaluation of lexical normalization, we
		conduct a number of scenario
		Sentence
		Raw
		republicans who don’t want drumpf are voting for hilldawg
		Best system
		republicans who don’t want trump are voting for hillary
		Reference
		republicans who don’t want trump are voting for hillary
		Raw
		this is one of the biggest clickbate news outlets
		Best system
		this is one of the biggest click bait news outlets
		Reference
		this is one of the biggest click bait news outlets
		Raw
		the hillbots have gone full insanity
		Best system
		the hill bots have gone full insanity
		Reference
		the hillary bots have gone full insanity
		Table 3: Normalization examples
		neologisms/canonical-equivalents
		level)
		along
		with using BLEU score (Papineni et al31
		Table 3: Change of feature values, per dataset and per
		level of text alterations
	</Abstractive Summary>
	<Extractive Summary> =
		Foodborne Illness Discovery:
		Table 3 reports
		the evaluation results for both review- and
		sentence-level foodborne classiﬁcation
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the model’s performance on the
		German dataset when training with various mix-
		tures of noise
	</Extractive Summary>
	<Extractive Summary> =
		Semantic Narrowing and Semantic Broaden-
		ing: Table 3 shows that number of comments has
		the strongest correlation to semantic narrowing
		and semantic broadening of high afﬁnity terms,
		achieving R2 values of 0
	</Extractive Summary>
	<Extractive Summary> =
		In contrast, while loyalty and dedication
		have similarly high R2 values when used for mod-
		eling semantic narrowing of high afﬁnity terms as
		shown in Table 3, it is more weakly linked to the
		semantic broadening of high afﬁnity terms
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 and 4 compare the proposed loss with
		the triplet loss with random sampling, TL(Rand)
	</Extractive Summary>
	<Extractive Summary> =
		24), meaning that there was no method
		that performed signiﬁcantly better than any other
		method (see Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Dataset
		Table 3 list the dataset used in this study do-
		main, label size, and number of mentions in
		train/dev/test portions
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 also shows that the
		syntactic information of multiple languages was
		captured by a single embedding model
	</Extractive Summary>
	<Extractive Summary> =
		The nearest neighbours accuracy of our syn-
		tactic embeddings in Table 3 signiﬁcantly outper-
		forms the general purpose language models
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 illustrates the results
		for this experiment
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 illustrates the results for this
		experiment
	</Extractive Summary>
	<Extractive Summary> =
		From Table 3,
		we see that while
		BILSTM+ outperform our methods in the consis-
		consistent
		inconsistent
		RMSE
		r
		RMSE
		r
		BILSTM+
		0
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows some
		example tweets
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the results on NYT dataset regard-
		ing P@100, P@200, P@300 and the mean of three
		settings for each model
	</Extractive Summary>
	<Extractive Summary> =
		We show in Table 3 ex-
		amples of the predictions for the unlabelled dataset
		made by our approach
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows
		the size of the newly annotated data in terms of
		the number of sentences and tokens for each genre
	</Extractive Summary>
	<Extractive Summary> =
		As can be derived from Table 3, TWE remains
		the least noisy genre, whereas the SNS genre is
		now the noisiest one with higher WER and CER
		values
	</Extractive Summary>
	<Extractive Summary> =
		The new annotations as presented in Table 3
		were used for training (Setup 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		All experiment results is summarized in
		Table 3, where we use BLEU score (Papineni
		et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the precision, recall and
		F1 scores of the features for Argument Identiﬁca-
		tion
	</Extractive Summary>
	<Extractive Summary> =
		We also see a signiﬁcant increase in accu-
		racy when we use the combinational feature of
		predicate and its language, as compared to using
		only predicate as a feature (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Evaluation
		Table 3 lists the results of multi-label emotion
		classiﬁcation on SemEval-2018
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows that BERT integrating with
		Twitter-speciﬁc features outperforms both general
		and domain-speciﬁc component model
	</Extractive Summary>
	<Extractive Summary> =
		Some of the top 100 most common unigrams and
		bigrams are presented in Table 3, aggregated un-
		der the most common LIWC categories
	</Extractive Summary>
	<Extractive Summary> =
		2
		Data Augmentation
		As it is common in the ﬁeld, we experimented
		with back translation (third row in Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		5
		External Data
		To explore the effect of other types of noise, we
		ﬁne-tuned our baseline model on different external
		datasets (see the “Unconstrained" rows in Table 3
		and 4)
	</Extractive Summary>
	<Extractive Summary> =
		In the Fr→En direction, we
		submitted the model ﬁne-tuned on merged MTNT
		data, forward translation and fuzzy match data
		(row 5 in Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		5
		Corpora Statistics
		Table 3 indicates that there is a variety of English
		datasets for GEC
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 also presents an average error rate of
		each corpus
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results and analysis
		Confusion sets
		On English data, all proposed
		confusion set generation methods perform better
		than random word substitution (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		The distribution of
		the dataset annotations are represented as follows:
		sentiment polarity in Table 3, sentiment polarity
		intensity in Table 4 and emotion in Tables 5 and 6,
		respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the results of the word similar-
		ity experiment with words that occur two or more
		times in the corpus for a vocabulary size of 6
	</Extractive Summary>
	<Extractive Summary> =
		3
		Analysis of CEFR-LP
		Table 3 shows the basic statistics for CEFR-LP
		compared to those in LS-SE and LS-CIC
	</Extractive Summary>
	<Extractive Summary> =
		BLEU scores for our normalized test sets are
		reported in Table 3a and Table 3b, for the G2P
		and Espeak phonetizers
	</Extractive Summary>
	<Extractive Summary> =
		Additionally,
		these results could be regarded as evidence support-
		ing that our proposed method performs generally
		better for short sentences, as observed in Table 3
		results’ discussion
	</Extractive Summary>
	<Extractive Summary> =
		44 (the detail is in Table 3)
		Language
		prec
		recall
		F1-score
		en
		89
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents three normalization examples,
		with the raw, gold reference, and the output of our
		system
	</Extractive Summary>
	<Extractive Summary> =
		Such a difference is observed in
		all three datasets individually (see Table 3)
	</Extractive Summary>
</Paper ID=ument837>


<Paper ID=ument837> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Results by paraphrasing method57
		Table 4: L1-Level breakdown by error type in relative improvements in F04%
		Table 4:
		The proportion of natural errors caused
		by deleting/inserting/substituting a single character or
		swapping two adjacent characters6%
		Table 4: Results of choosing text spans with the full
		phone number, or a partial match001
		Table 4: Coefﬁcient of determination values for linear and multivariate models trained on community characteris-
		tics that predict rate of new users (δu(si))6789
		Table 4:
		Comparison of different loss functions on
		Quora test set7
		Table 4: Classiﬁcation Accuracy for multi-label classiﬁcation
		rather, music, EVENT, my
		Nature and hobbies
		I, like, ﬁnd, nice, go, friend, eat, movie, cozy, time, watch, couch, delightful, walk, evening, day, good, make, enjoy, music
		Table 4: Translated topic models with assigned topic names747
		Table 4: Transfer learning performance of our pipeline
		model on the intersection datasets (bold number in
		each column is the best performance on that dataset)5
		Table 4: Training Parameters
		5 With this training method, no UPOS
		157
		Table 4: Syntactic Nearest-Neighbour for Language Models (%)
		English
		German
		Spanish
		Chinese
		Korean
		Dutch
		Model
		1-NN/5-NN
		1-NN/5-NN
		1-NN/5-NN
		1-NN/5-NN
		1-NN/5-NN
		1-NN/5-NN
		USE
		71
		We
		use
		the
		pre-trained
		masked
		language
		165
		Table 4:
		Configurations of FASPell684
		Table 4:
		Spearman’s correlation on word similarity
		tasks713
		Table 4: Evaluation scores for the Fine-grained classiﬁcation experiment152†
		Table 4: Results for prediction of secondary labels23
		52%
		Table 4: Fleiss’ κ and percent agreement scores for calculat-
		ing inter-annotator agreement
		Table 4: Some examples of Separate Head-Tail CNN
		corrections compared to PCNN
		4847
		Table 4: Precision, recall and F1 score per function33
		Table 4: Intents accuracies / percentages of the used tokens for predicting the intents using the smallest partial
		utterance of the test dataset of the human transcribed incremental utterances for which the system has a conﬁdence
		of more or equal than 95 %, 90 %, 85 %, and 80 %, if the conﬁdence is not reached, the full utterance is used
		training dataset
		ﬁrst 100 %
		ﬁrst 75 %
		ﬁrst 50 %
		ﬁrst 25 %
		human, full
		83
		jaaa sws toch <emoji> hijzelf is wel tof
		jaaa sws toch <emoji> hij blijft echt leuk
		jaaa sws maar <emoji> hij is gewoon leuk
		jaaa sws dus <emoji> hij is inderdaad tof
		English
		yes anyway but/so <emoji> he/himself is
		really/just/indeed nice/cool
		Table 4: Data augmentation using pretrained embed-
		dings0
		Table 4: SNLI classiﬁcation accuracy with artiﬁcial
		noise and natural noise
		Feature
		Argument Identiﬁcation
		P
		R
		f-score
		Baseline
		56
		53
		55
		with predicate-lang
		57
		54
		55
		+dependency
		81
		76
		78
		Table 4: Accuracy scores for Argument Identiﬁcation
		variance) along i dimension
		301
		Noisy
		Canonical
		ye
		ye
		##a
		##ah
		im
		i
		�
		MASK
		�
		’
		�
		MASK
		�
		m
		already
		already
		knowing
		knowing
		wa
		wh
		##t
		##at
		Table 3: Parallel Alignment of yea im already knowing
		wat u sayin normalized as yeah i’m already knowing
		what you saying
		Noisy
		Gold
		#next mask
		ye
		ye
		0
		##a
		##ah
		0
		im
		i
		2
		�
		MASK
		�
		’
		-
		�
		MASK
		�
		m
		-
		already
		already
		0
		knowing
		knowing
		0
		wa
		wh
		0
		##t
		##at
		0
		Table 4: Parallel Alignment of yea im already knowing
		wat u sayin normalized as yeah i’m already knowing
		what you saying with gold number of next masks for
		each source token
		the overall architecture53)
		Table 4: F1 on binary classiﬁcation for each emotion
		class05
		Table 4: BLEU scores of models ﬁne-tuned on different data in the En→Fr direction40
		Table 4: Model performances during several runs of the baseline and the combined (Se + Sc + Hu + Ha) models25
		0
		Table 4: Language speciﬁc constants for token- and character-level noising operations46
		(c) Russian (RULEC-GEC)
		Table 4: Unsupervised and ﬁne-tuned MAGEC sys-
		tems for English, German and Russian, contrasted with
		systems from related work and spell-checking base-
		lines9%
		Table 4:
		Distribution of sentiment polarity intensity
		annotations
		Emotion
		Online Posts
		Percentage
		Anger
		131
		2348
		Table 4: Spearman/Pearson correlation of sentence similarity on typo/omitted-word setting65
		Table 3: Basic statistics in CEFR-LP compared to LS-SE and LS-CIC
		CEFR level
		target
		candidate
		all
		863
		14, 259
		A1
		300
		2, 090
		A2
		190
		2, 856
		B1
		110
		4, 513
		B2
		186
		3, 201
		C1
		30
		648
		C2
		47
		951
		Table 4: Distribution of CEFR levels in CEFR-LP
		a certain candidate4
		Table 4: BLEU score results comparison on the
		MTNT and Cr#pbank blind test sets50
		Table 4: Lexical Normalization Experimental Result
		Moreover, we investigate the errors by draw-
		ing sample of misclassiﬁed cases
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents empirical results organized by
		paraphrasing method, while Table 5 organizes by
		pivot languages used
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the results
		for the systems adapted to both L1 and Level that
		improved the most in overall F0
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the percentage
		of noised tokens that can be covered by a single
		noising operation
	</Extractive Summary>
	<Extractive Summary> =
		Can the models iden-
		tify the correct text span that contains a phone
		number? Table 4 shows these results for standard
		Text Span ID of Phone Numbers
		Full
		Full+Partial
		Zero pad
		70
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4 we present our results of regression
		and correlation testing
	</Extractive Summary>
	<Extractive Summary> =
		01), and
		a nature-related topic model shows that a nature-
		focused label is important to distinguish relation-
		ship goals, which LIWC is currently lacking (see
		Table 4 and Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		4
		Exploring hyper-parameters
		First, we only change the number of candidates
		in Table 4 to see its effect on spell checking per-
		formance
	</Extractive Summary>
	<Extractive Summary> =
		The reason we set the number of candidates
		c = 4 in Table 4 and no larger is because there is a
		trade-off with time consumption
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 illustrates the results of
		the experiment
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 illustrates
		the results for this experiment
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 summarises the results
	</Extractive Summary>
	<Extractive Summary> =
		7
		Case Study
		In Table 4, we show some of our SHTCNN
		model examples corrections compared to tradi-
		tional PCNN
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4, we illustrate this
		augmentation technique, starting from the user-
		generated text jaaa sws toch :) hij is echt leuk
		(yes anyway <emoji> he is really nice)
	</Extractive Summary>
	<Extractive Summary> =
		In the upper part of Table 4 the standard words
		in the source and target sentences are placed
		in cursive
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 gives the accuracy scores for the system
		using baseline features
	</Extractive Summary>
	<Extractive Summary> =
		For more detailed investigation of the effect of
		domain knowledge, Table 4 shows the result of
		binary classiﬁcation for each emotion class mea-
		sured by F1 score
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in the last row in Table 4, mixed training
		could improve the performance over baseline on
		noisy texts
	</Extractive Summary>
	<Extractive Summary> =
		In the En→Fr direction, the
		double-tuned model with punctuation ﬁxed was
		submitted (row 6 in Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Main results
		We ﬁrst compare the GEC systems
		with simple baselines using a greedy and context
		spell-checking (Table 4); the latter selects the best
		correction suggestion based on the sentence per-
		plexity from a Transformer language model
	</Extractive Summary>
	<Extractive Summary> =
		Roth (2019) for their systems that use authentic
		error-annotated data for training (Table 4b and 4c)
	</Extractive Summary>
	<Extractive Summary> =
		, 2019) from the
		BEA shared task trained on publicly available error-
		annotated corpora (Table 4a)
	</Extractive Summary>
	<Extractive Summary> =
		The distribution of
		the dataset annotations are represented as follows:
		sentiment polarity in Table 3, sentiment polarity
		intensity in Table 4 and emotion in Tables 5 and 6,
		respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 show that OLIVE outperforms in each
		sub-word and word embedding group on both
		typo/omitted-word settings
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the distribution of CEFR levels
		context embedding units
		300
		LSTM hidden/output units
		600
		MLP input units
		1200
		MLP hidden units
		1200
		sentential context units
		600
		target word units
		600
		number of negative samples
		10
		negative sampling rate
		0
	</Extractive Summary>
	<Extractive Summary> =
		These results are displayed in Table 4,
		we also show the performance of the (Michel and
		Neubig, 2018)’s baseline system on such test sets
	</Extractive Summary>
	<Extractive Summary> =
		See Table 4) for detail
	</Extractive Summary>
	<Extractive Summary> =
		The results in Table 4
		show that syntactic features have more predictive
		power than lexical features
	</Extractive Summary>
</Paper ID=ument837>


<Paper ID=ument837> <Table ID =5>
	<Abstractive Summary> =
		4
		Table 5: Results by pivot language1
		Table 5: The performance of a machine translation
		model on the MTNT task7491
		Table 5: Comparison of different loss functions on
		open domain QA dataset validation set59
		Table 5: Performance on different categories of PWPs for different parts of the PWPs
		Method
		Important features
		Human
		seek, date, nice, know, people, undertake, spontaneous,
		sweet, terrace, pretty, enjoy, child wish, family person
		Classiﬁer
		date, spontaneous, let, live, nature, sociable, send, build,
		exercise, nice, independent, again, friendship, sea, girl,
		terrace
		LLR
		quiet, sweet, nothing, nature, fetch, again, proﬁle,
		click, feel free, weekend, sea, people, visit, caring
		Table 5: Translated words not in LIWC’s lexicon or-
		dered by importance for relationship goal identiﬁca-
		tion95
		Table 5: Results
		as in CoNLL 200300
		Table 5: Functional Dissimilarity Scores (Lower is Better)
		Model
		English
		German
		Spanish
		Chinese
		Korean
		Dutch
		BERTavg
		0
		Table 5:
		Speed comparison (ms/sent)025
		Table 5: Correlations between F0868
		Table 5: Average retweets and favorites across classes
		the linguistic cues common among these tweets
		was not considered until now :)
		Love your new show @driverminnie
		+
		+
		Non-ironic (Yes)
		Table 5: Generated ironic examples73
		Table 5: CO-MC F1-scores / intents accuracies of the ﬁrst partial automatically transcribed utterances that have
		equal or more than the ﬁrst 100 %, 75 %, 50 %, and 25 % of the tokens of the utterances of the test dataset of the
		automatically transcribed full utterances
		272
		training dataset
		95 % conf
		jaaa sws maar <emoji> hij is gwn leuk
		jaaa sws dus <emoji> hij is idd tof
		English
		yes anyway but/so <emoji> he is really/
		just/indeed nice/cool
		Table 5: Data augmentation using dictionary of abbre-
		viations4
		Table 5: Classiﬁcation F1 score on MRPC
		4
		Conclusion and Future Work
		In this paper, we present a novel text denois-
		ing algorithm using ready-to-use masked language
		model
		Feature
		Argument Classiﬁcation
		P
		R
		f-score
		Predicate
		06
		09
		06
		Headword (HW)
		18
		10
		13
		HeadwordPOS
		05
		07
		06
		Phrasetype (PT)
		08
		10
		08
		Predicate-PT
		05
		08
		06
		Predicate-HW
		05
		06
		06
		Dependency
		81
		86
		83
		Named Entity
		20
		14
		16
		HeadwordPOS-PT
		07
		09
		08
		Headword-PT
		12
		09
		10
		HeadwordPOS(UD)
		08
		11
		09
		UD dependency
		77
		83
		80
		Predicate-language
		06
		10
		07
		Headword-language
		18
		11
		14
		Table 5: Individual feature performance for Argument
		Classiﬁcation18
		Table 5: Impact of our noise-speciﬁc strategy on the F1
		score (development set) reported with best alignment
		setting
		(782
		Table 5: WMT19 Robustness Leaderboard on Fr→En
		Extended story line: the ﬁrst half of
		the review is negative, and the model
		likely misses the turning point towards
		positive halfway through the review
		Table 5: Examples where the combined model goes wrong and the baseline sentiment model predicts the correct
		sentiment40
		Table 5: Comparison of systems on two English GEC datasets01
		Table 5:
		Comparison with LM-based GEC on the
		CoNLL (M2) and JFLEG (GLEU) test sets for unsuper-
		vised (⋆) and supervised systems trained or ﬁne-tuned
		on different amounts of labelled data3%
		Table 5: Distribution of emotion (6-levels) annotations
		Emotion
		Online Posts
		Percentage
		Anger
		121
		2275
		number of epochs
		10
		Table 5: Context2vec hyper-parameters that show the
		best performance in (Melamud et al
		87
		16
		15
		13
		12
		11
		8
		7
		6
		6
		Table 5: Most frequent normalization replacements on the Cr#pbank test corpus50
		Table 5: Translation Experimental Result
		(BLEU: higher is better, WER: lower is better)
		As integration of aforementioned modules, we
		evaluate the pipeline model by conducting four
		experiments, 1) comparing raw tweets with ﬁnal
		tweets, 2) comparing raw tweets which have been
		translated (without MLF model) into Indonesian
		with ﬁnal tweets, 3) comparing raw tweets which
		422
		have been translated (with MLF Model) into In-
		donesian with ﬁnal tweets, and 4) comparing raw
		tweets which have been normalized and trans-
		lated (with MLF Model) into Indonesian with ﬁnal
		tweets91
		Incorrect (AD)
		DemB
		Table 5: Examples of two features, cond entropy 3gram and C/S, their value change when text samples are mod-
		iﬁed on the level of 20%, 40% and 60%, and associated classiﬁer’s predictions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents empirical results organized by
		paraphrasing method, while Table 5 organizes by
		pivot languages used
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 5, noised training has min-
		imal impact on performance
	</Extractive Summary>
	<Extractive Summary> =
		01), and
		a nature-related topic model shows that a nature-
		focused label is important to distinguish relation-
		ship goals, which LIWC is currently lacking (see
		Table 4 and Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Filtering Speed10
		First, we measure the filtering speed of Chinese
		spell checking in terms of absolute time consump-
		tion per sentence (see Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 clearly
		shows that FASPell is much faster
	</Extractive Summary>
	<Extractive Summary> =
		4
		Network Correlation Results
		Table 5 displays correlation values between graph-
		based semantic similarity metrics of F0
	</Extractive Summary>
	<Extractive Summary> =
		Similarly, Table 5 shows an interest-
		ing trends for the favorites
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 displays the generated ironic and
		non-ironic tweets
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 exempliﬁes this technique
	</Extractive Summary>
	<Extractive Summary> =
		6
		WMT19 Robustness Leaderboard
		We submitted our best constrained systems to
		WMT19 Robustness Leaderboard10, as shown in
		Table 5 and Table 6
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows an evident performance
		boost (3
	</Extractive Summary>
	<Extractive Summary> =
		, 2017)
		(Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Pre-training used the same hyper-parameter set-
		tings of context2vec (Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 provides examples of two
		features, one lexical and one syntactic, their value
		changes when text samples are modiﬁed, and the
		associated change of the classiﬁer’s predictions
	</Extractive Summary>
</Paper ID=ument837>


<Paper ID=ument837> <Table ID =6>
	<Abstractive Summary> =
		45
		Table 6: Results for translation-based rewriting, ordered by decreasing average adequacy (ParA)7480
		Table 6:
		Comparison of different loss functions on
		open domain QA dataset test set
		We studied the quality of learned embeddings
		by examining nearest neighbours in the embed-
		Table 6: Syntactic Nearest-Neighbour on New lan-
		guages (%)
		Lang (ISO)
		1-NN/5-NN
		Total/Group
		French (fr)
		35
		166
		Table 6:
		This table shows spell checking performances on both detection and correction level54
		Table 6: Intents accuracies / percentages of the used tokens for predicting the intents using the ﬁrst partial utterance
		of the test dataset of the automatically transcribed incremental utterances for which the system has a conﬁdence of
		more or equal than 95 %, 90 %, 85 %, and 80 %, if the conﬁdence is not reached, the full utterance is used
		utterances
		INS
		DEL
		SUBS
		SUM
		TWE
		1118
		934
		355
		2407
		SNS
		2483
		2238
		1021
		5742
		SMS
		4209
		508
		758
		5475
		Table 6: Operations needed at the character level to
		normalize the test set for each genre
		Feature
		Argument Classiﬁcation
		P
		R
		f-score
		Baseline
		27
		15
		19
		+dependency
		84
		84
		84
		Table 6: Accuracy scores for Argument Classiﬁcation6
		Table 6: Comparing our systems to the State-of-the-
		art system MoNoise (we report on same development
		dataset reported in MoNoise original paper (last 950
		sentences))
		Model
		F1
		Supranovich and Patsepnia, 2015
		825
		Table 6: WMT19 Robustness Leaderboard on En→Fr
		Flip in sentiment
		Table 6: Examples where the combined and the baseline sentiment models both fail to predict the correct senti-
		ment71
		Table 6: Results on on Falko-Merlin Test Set (German)37
		Table 6: Performance of single MAGEC w/ LM models
		on two groups of errors on respective development sets1%
		Trust
		60
		11%
		Table 6: Distribution of emotion (8-levels) annotations
		The dataset annotation results displayed do not
		fully reﬂect the opinions portrayed by the writers,
		367
		since a large amount of online posts were off-topic
		to the budget (342
		Table 6: GAP scores on LS-SE, LS-CIC and CEFR-LP datasets, where bold denotes the highest scores we talk about it or not WWW44
		Table 6: Examples from our noisy UGC corpus89
		Table 6: Pipeline Experiment Result
		(BLEU: higher is better, WER: lower is better)
		6
		Conclusion and Future Work
		In this paper, we have proposed a pipeline model
		comprising of four modules, i
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows results from our grounded para-
		phrasing experiment in which we compared how
		different translation methods affect monolingual
		rewriting quality
	</Extractive Summary>
	<Extractive Summary> =
		The results are shown in Table 6 (top)
	</Extractive Summary>
	<Extractive Summary> =
		The results
		are shown in Table 6 (bottom)
	</Extractive Summary>
	<Extractive Summary> =
		Indonesian in
		Table 6, the above methods only improved nearest
		neighbours accuracy slightly
	</Extractive Summary>
	<Extractive Summary> =
		2
		Performance
		As shown in Table 6, FASPell achieves state-of-
		the-art F1 performance on both detection level and
		correction level
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 also shows that all the components of
		FASPell contribute effectively to its good perfor-
		mance
	</Extractive Summary>
	<Extractive Summary> =
		280
		Table 6 shows the number of insertions (INS),
		deletions (DEL) and substitutions (SUBS) that are
		needed to transform the source sentences of the
		test set into manually normalized ones
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 gives the accuracy scores for Argument
		Classiﬁcation while using baseline features, and
		after incorporating dependency labels
	</Extractive Summary>
	<Extractive Summary> =
		Similarly, it seems that the reviews
		whose sentiment has been classiﬁed wrongly by
		the sentiment-only baseline and that don’t have
		any sarcastic / hate intent don’t get classiﬁed cor-
		rectly by our combined model either (Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		As Table 6
		shows, her best system reaches 45
	</Extractive Summary>
	<Extractive Summary> =
		To counter the argument that – mostly due to
		the introduced character-level noise and strong lan-
		guage modelling – MAGEC can only correct these
		“simple” errors, we evaluate it against test sets that
		contain either spelling and punctuation errors or
		all other error types; with the complement errors
		corrected (Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		9
		Table 7: GAP scores on different CEFR levels of target words in CEFR-LP
		6
		Results
		Table 6 shows the GAP scores for LS-SE, LS-CIC
		and CEFR-LS datasets
	</Extractive Summary>
	<Extractive Summary> =
		The last row of Table 6 shows
		the performance of our method with Sbest (i
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 reports some examples of the output of
		our method along with their translation before and
		after correction
	</Extractive Summary>
	<Extractive Summary> =
		From
		Table 6, we can see that each module affects posi-
		tively toward the performance of the pipeline
	</Extractive Summary>
</Paper ID=ument837>


<Paper ID=ument837> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Example paraphrases generated by several monolingual and bilingual methods11
		atis ﬂight no#
		atis airline
		Table 7: intents distribution (in percent) of the ATIS
		utterances used in this work
		274
		Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 275–285
		Hong Kong, Nov 4, 2019
		# Words
		TWE
		Src
		Tgt
		1
		5190
		124578
		122165
		2
		697441
		20692213
		20416466
		3
		853465
		26316110
		26002836
		SNS
		Src
		Tgt
		1
		8136
		108127
		110326
		2
		577281
		21120858
		21499465
		3
		835091
		70337827
		71257870
		SMS
		Src
		Tgt
		1
		7626
		111393
		113846
		2
		615195
		15356594
		15669683
		3
		766946
		22066532
		22651464
		Table 7: Parallel corpora data statistics for each exper-
		imental setup1
		Table 7:
		Comparing our systems to WNUT 2015
		shared task that allowed UGC resources
		6
		Discussion
		We now compare our system to previous works17
		Table 7: Results on on AKCES-GEC Test Set (Czech)4%
		Table 7:
		Distribution of the sarcasm/irony, negation,
		off-topic and Maltese language annotations
		The dataset has been published15 for general
		use under the Creative Commons Attribution-
		NonCommercial-ShareAlike
		49
		Table 7: GAP scores on different CEFR levels of target words in CEFR-LP
		6
		Results
		Table 6 shows the GAP scores for LS-SE, LS-CIC
		and CEFR-LS datasets
	</Abstractive Summary>
	<Extractive Summary> =
		The
		examples in Table 7 provides a ﬂavor of the outputs
		from each method and demonstrates some of the
		error cases
	</Extractive Summary>
	<Extractive Summary> =
		Table 7 shows the number of parallel training
		sentences and words in the original and target
		sides of each setup for each genre
	</Extractive Summary>
</Paper ID=ument837>


<Paper ID=ument837> <Table ID =8>
	<Abstractive Summary> =
		jongen tammy , ik u
		English
		boy tammy, me you
		Table 8: Examples of original (src), predicted (norm)
		and target (tgt) sentences using the NMT approach5
		lex15+5Mtweets
		-
		Table 8: Comparing our systems to the State-of-the-art
		system MoNoise on lexnorm15 test20
		Table 8: Results on on RULEC-GEC Test Set (Russian)
		Table 8: Example outputs of each method
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Error Analysis
		Table 8 shows some examples of sentences which
		were normalized using the proposed NMT ap-
		proach
	</Extractive Summary>
	<Extractive Summary> =
		For example, in
		the second sentence in Table 8, the system was
		unable to correctly normalize some of the words
	</Extractive Summary>
	<Extractive Summary> =
		As we see in Table 8, our non-UGC system is far
		from the State-of-the-Art model MoNoise (van der
		Goot and van Noord, 2017) in terms of F1 score
	</Extractive Summary>
	<Extractive Summary> =
		78 improvement) (Table 8)
	</Extractive Summary>
	<Extractive Summary> =
		We compare it in Table 8, demonstrating
		another practical interest for our approach
	</Extractive Summary>
	<Extractive Summary> =
		4
		Russian
		As Table 8 indicates, GEC in Russian currently
		seems to be the most challenging task
	</Extractive Summary>
	<Extractive Summary> =
		Table 8 lists the results where each row shows
		a ranking of substitution candidates by compared
		methods
	</Extractive Summary>
</Paper ID=ument837>


<Paper ID=ument837> <Table ID =9>
	<Abstractive Summary> =
		TWE
		INS
		DEL
		SUBS
		SUM
		Test
		1118
		934
		355
		2407
		Setup 0
		1101
		930
		351
		2382
		Setup 1
		1099
		924
		330
		2353
		Setup 2
		1105
		917
		326
		2348
		Setup 3
		1094
		903
		325
		2322
		SNS
		INS
		DEL
		SUBS
		SUM
		Test
		2483
		2238
		1021
		5742
		Setup 0
		2454
		2225
		992
		5671
		Setup 1
		2468
		2220
		995
		5683
		Setup 2
		2366
		2092
		968
		5426
		Setup 3
		2366
		2141
		971
		5478
		SMS
		INS
		DEL
		SUBS
		SUM
		Test
		4209
		508
		758
		5475
		Setup 0
		4107
		505
		727
		5339
		Setup 1
		4094
		483
		700
		5277
		Setup 2
		4099
		470
		709
		5278
		Setup 3
		4090
		468
		710
		5268
		Table 9: Number of solved operations at the character
		level after normalization for each genre
	</Abstractive Summary>
	<Extractive Summary> =
		As can be de-
		rived from the Table 9 many cases where correctly
		282
		normalized by the systems
	</Extractive Summary>
</Paper ID=ument837>


<Paper ID=ument839> <Table ID =1>
	<Abstractive Summary> =
		55
		Table 1: Label statistics for the SPOT datasets We explain this behavior with the
		statistics of Table 1: “Witness (Salient)” is higher
		in *-EDU datasets compared to *-SENT datasets
	</Abstractive Summary>
	<Extractive Summary> =
		For evalu-
		ating the segment-level classiﬁcation performance
		on Yelp’13 and IMDB, we use the SPOT-Yelp and
		SPOT-IMDB datasets, respectively (Angelidis and
		Lapata, 2018), annotated at two levels of gran-
		ularity, namely, sentences (SENT) and Elemen-
		tary Discourse Units (EDUs)1 (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		25, which
		is signiﬁcantly lower than the WR on sentiment
		classiﬁcation datasets (Table 1)
	</Extractive Summary>
</Paper ID=ument839>


<Paper ID=ument839> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: F1 score for segment-level sentiment classiﬁ-
		cation
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Experimental Results
		Sentiment Classiﬁcation:
		Table 2 reports the
		evaluation results on SPOT datasets for both
		sentence- and EDU-level classiﬁcation
	</Extractive Summary>
</Paper ID=ument839>


<Paper ID=ument839> <Table ID =3>
	<Abstractive Summary> =
		0
		Table 3: Review-level (left) and sentence-level (right)
		evaluation results for discovering foodborne illness
	</Abstractive Summary>
	<Extractive Summary> =
		Foodborne Illness Discovery:
		Table 3 reports
		the evaluation results for both review- and
		sentence-level foodborne classiﬁcation
	</Extractive Summary>
</Paper ID=ument839>


<Paper ID=ument84> <Table ID =1>
	<Abstractive Summary> =
		Language
		train
		dev
		test
		AR
		1687
		299
		315
		ZH
		4871
		596
		636
		Table 1: Num
	</Abstractive Summary>
</Paper ID=ument84>


<Paper ID=ument84> <Table ID =2>
	<Abstractive Summary> =
		14
		Table 2: Precision, Recall, and F1 on Chinese GALE
		test data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows
		that our model performs especially well in the
		NER setting, where we observe the largest improve-
		4Note that this step is necessary in order to make the pro-
		duced and reference alignments comparable; to evaluate align-
		ments, the sequences must be identical, or the evaluation is
		ill-posed
	</Extractive Summary>
	<Extractive Summary> =
		Analysis
		Taken together, Table 2 and Figure 2
		support the use of discriminative alignment models
		in settings where test data is presented in an on-
		line fashion as well as in cases where high-quality
		alignments matter
	</Extractive Summary>
	<Extractive Summary> =
		5
		NER Experiments
		In Table 2 we evaluated the quality of our model’s
		alignments intrinsically, showing that they dramat-
		ically outperform the baseline models, especially
		for NER spans
	</Extractive Summary>
</Paper ID=ument84>


<Paper ID=ument84> <Table ID =3>
	<Abstractive Summary> =
		71
		Table 3:
		Precision, Recall, and F1 on Arabic GALE
		test data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 further underscores these ﬁndings; we
		observe an even more dramatic increase in perfor-
		Figure 3: Chinese F1 performance with respect to the
		amount of unlabelled pretraining bitext (y axis) and the
		number of sentences annotated for alignment (x axis);
		increasing the number of annotated sentences has a
		far greater impact on the F1 score than increasing the
		amount of bitext
	</Extractive Summary>
</Paper ID=ument84>


<Paper ID=ument84> <Table ID =4>
	<Abstractive Summary> =
		57
		Table 4:
		F1 results on OntoNotes test for systems
		trained on data projected via FastAlign and DiscAlign
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results & Analysis
		Table 4 shows that while NER systems trained on
		projected data do categorically worse than an NER
		system trained on gold-standard data, the higher-
		quality alignments obtained from DiscAlign lead
		to a major improvement in F1 when compared to
		FastAlign
	</Extractive Summary>
	<Extractive Summary> =
		While mul-
		tilingual datasets directly annotated for a given task
		typically lead to the highest-performing systems
		(see Table 4) these datasets are task-speciﬁc and not
		robust to ontology changes
	</Extractive Summary>
</Paper ID=ument84>


<Paper ID=ument84> <Table ID =5>
	<Abstractive Summary> =
		77
		Table 5: OntoNotes test set performance when trained
		on subsamples of the Chinese gold NER data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows that projection out-
		performs a system trained on gold data when there
		is very little gold data available (500 sentences);
		as the amount of data increases, these gains disap-
		pear
	</Extractive Summary>
</Paper ID=ument84>


<Paper ID=ument84> <Table ID =6>
	<Abstractive Summary> =
		36
		Table 6: OntoNotes test performance when pre-trained
		on projected data and ﬁnetuned on varying amounts of
		Chinese gold NER data
	</Abstractive Summary>
	<Extractive Summary> =
		However, Table 6 indicates that projected data
		provides a useful pre-training objective for NER,
		making low-data NER models perform about as
		well as if they had been trained on twice as much
		gold data
	</Extractive Summary>
</Paper ID=ument84>


<Paper ID=ument84> <Table ID =7>
	<Abstractive Summary> =
		94
		Table 7:
		Sentences per minute and average scores
		against gold-labelled data for sentences annotated for
		alignment by human annotators (Hu
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results & Analysis
		Table 7 shows that alignment can be performed
		rapidly, at 4
	</Extractive Summary>
</Paper ID=ument84>


<Paper ID=ument840> <Table ID =1>
	<Abstractive Summary> =
		Hyper-parameter
		Value
		Encoder
		type
		LSTM
		rnn hidden size
		100
		layers
		1
		Decoder
		type
		LSTM
		rnn hidden size
		100
		layers
		1
		General
		word vec size
		200
		optimizer
		Adam
		learning rate
		1e−3
		train/validation split
		90/10
		vocabulary size
		30k for SSC, TV merged
		10k for single TV-shows
		Table 1: Hyperparameters
	</Abstractive Summary>
</Paper ID=ument840>


<Paper ID=ument840> <Table ID =2>
	<Abstractive Summary> =
		5)
		218k
		Table 2: Results of experiments on formality and sentence suitability
	</Abstractive Summary>
</Paper ID=ument840>


<Paper ID=ument841> <Table ID =1>
	<Abstractive Summary> =
		67
		4
		46
		Table 1: Key characteristics of the source sentences
	</Abstractive Summary>
</Paper ID=ument841>


<Paper ID=ument841> <Table ID =2>
	<Abstractive Summary> =
		72
		1
		68
		Table 2: Key characteristics of collected paraphrases
	</Abstractive Summary>
</Paper ID=ument841>


<Paper ID=ument841> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Priming questions used for human evaluation of paraphrase adequacy (ParA), paraphrase ﬂuency (ParF ),
		and translation adequacy (NMTA)
	</Abstractive Summary>
</Paper ID=ument841>


<Paper ID=ument841> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Results by paraphrasing method
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents empirical results organized by
		paraphrasing method, while Table 5 organizes by
		pivot languages used
	</Extractive Summary>
</Paper ID=ument841>


<Paper ID=ument841> <Table ID =5>
	<Abstractive Summary> =
		4
		Table 5: Results by pivot language
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents empirical results organized by
		paraphrasing method, while Table 5 organizes by
		pivot languages used
	</Extractive Summary>
</Paper ID=ument841>


<Paper ID=ument841> <Table ID =6>
	<Abstractive Summary> =
		45
		Table 6: Results for translation-based rewriting, ordered by decreasing average adequacy (ParA)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows results from our grounded para-
		phrasing experiment in which we compared how
		different translation methods affect monolingual
		rewriting quality
	</Extractive Summary>
</Paper ID=ument841>


<Paper ID=ument841> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Example paraphrases generated by several monolingual and bilingual methods
	</Abstractive Summary>
	<Extractive Summary> =
		The
		examples in Table 7 provides a ﬂavor of the outputs
		from each method and demonstrates some of the
		error cases
	</Extractive Summary>
</Paper ID=ument841>


<Paper ID=ument842> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1: Adaptation to Proﬁciency Level in F0
	</Abstractive Summary>
	<Extractive Summary> =
		The
		Spanish-A2 testset has the highest number of er-
		rors per 100 words among all the L1-Level test-
		sets, as shown in Table 1 in the Appendix
	</Extractive Summary>
</Paper ID=ument842>


<Paper ID=ument842> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Top: Adaptation to L1 Only
	</Abstractive Summary>
	<Extractive Summary> =
		The results in
		Table 2 (top) show that all L1-adapted models
		are better than the baseline, with improvements
		ranging from 1
	</Extractive Summary>
	<Extractive Summary> =
		All the models adapted to both
		Level and L1 outperform the models adapted to
		only one of these features, as shown in Table 2
		(bottom)
	</Extractive Summary>
</Paper ID=ument842>


<Paper ID=ument842> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Results on the CoNLL14 testsets for Chinese
		models
	</Abstractive Summary>
</Paper ID=ument842>


<Paper ID=ument842> <Table ID =4>
	<Abstractive Summary> =
		57
		Table 4: L1-Level breakdown by error type in relative improvements in F0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results
		for the systems adapted to both L1 and Level that
		improved the most in overall F0
	</Extractive Summary>
</Paper ID=ument842>


<Paper ID=ument843> <Table ID =1>
	<Abstractive Summary> =
		E2E-ABSA
		sentence
		aspect, aspect sentiment
		Table 1: Different problem settings in ABSA
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes three existing research
		problems related to ABSA
	</Extractive Summary>
</Paper ID=ument843>


<Paper ID=ument843> <Table ID =2>
	<Abstractive Summary> =
		06
		Table 2: Main results
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Main Results
		From Table 2, we surprisingly ﬁnd that only in-
		troducing a simple token-level classiﬁer, namely,
		BERT-Linear, already outperforms the existing
		38
		BERT-GRU
		BERT-TFM
		BERT-CRF
		0
		20
		40
		60
		80
		100
		F1 score
		46
	</Extractive Summary>
</Paper ID=ument843>


<Paper ID=ument843> <Table ID =3>
	<Abstractive Summary> =
		Dataset
		Train
		Dev
		Test
		Total
		LAPTOP
		# sent
		2741
		304
		800
		4245
		# aspect
		2041
		256
		634
		2931
		REST
		# sent
		3490
		387
		2158
		6035
		# aspect
		3893
		413
		2287
		6593
		Table 3: Statistics of datasets
	</Abstractive Summary>
</Paper ID=ument843>


<Paper ID=ument844> <Table ID =1>
	<Abstractive Summary> =
		whale → wahle
		Table 1: The synthetic noise types applied during training
	</Abstractive Summary>
	<Extractive Summary> =
		Synthetic Noise
		Table 1 describes the four types
		of synthetic noise we used during training
	</Extractive Summary>
</Paper ID=ument844>


<Paper ID=ument844> <Table ID =2>
	<Abstractive Summary> =
		9%
		Table 2: Performance on the IWSLT 2016 translation task with varying rates of natural noise in the test set
	</Abstractive Summary>
	<Extractive Summary> =
		Results
		Table 2 shows the model’s performance
		on data with varying amounts of natural errors
	</Extractive Summary>
</Paper ID=ument844>


<Paper ID=ument844> <Table ID =3>
	<Abstractive Summary> =
		27
		Table 3: Performance on IWSLT 2016 de-en test with
		maximal natural noise when training with one noise
		type (top) and three noise types (bottom)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the model’s performance on the
		German dataset when training with various mix-
		tures of noise
	</Extractive Summary>
</Paper ID=ument844>


<Paper ID=ument844> <Table ID =4>
	<Abstractive Summary> =
		4%
		Table 4:
		The proportion of natural errors caused
		by deleting/inserting/substituting a single character or
		swapping two adjacent characters
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the percentage
		of noised tokens that can be covered by a single
		noising operation
	</Extractive Summary>
</Paper ID=ument844>


<Paper ID=ument844> <Table ID =5>
	<Abstractive Summary> =
		1
		Table 5: The performance of a machine translation
		model on the MTNT task
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, noised training has min-
		imal impact on performance
	</Extractive Summary>
</Paper ID=ument844>


<Paper ID=ument845> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Results on dev and test
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Results
		Table 1 contains results without CNNs for the
		baselines, RNNs, and CRF
	</Extractive Summary>
</Paper ID=ument845>


<Paper ID=ument845> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Results of the CNN models
	</Abstractive Summary>
	<Extractive Summary> =
		For CNN results, Table 2 shows test set perfor-
		mance
	</Extractive Summary>
</Paper ID=ument845>


<Paper ID=ument845> <Table ID =3>
	<Abstractive Summary> =
		This for-
		55
		Table 3: Phone accuracy as a higher % of unicode sub-
		stitutions are made for lookalike ASCII characters
	</Abstractive Summary>
</Paper ID=ument845>


<Paper ID=ument845> <Table ID =4>
	<Abstractive Summary> =
		6%
		Table 4: Results of choosing text spans with the full
		phone number, or a partial match
	</Abstractive Summary>
	<Extractive Summary> =
		Can the models iden-
		tify the correct text span that contains a phone
		number? Table 4 shows these results for standard
		Text Span ID of Phone Numbers
		Full
		Full+Partial
		Zero pad
		70
	</Extractive Summary>
</Paper ID=ument845>


<Paper ID=ument846> <Table ID =1>
	<Abstractive Summary> =
		987
		Table 1: Afﬁnity and Speciﬁcity of terms found in
		r/motogp calculated on the word distributions of 10
		sample subreddits
	</Abstractive Summary>
</Paper ID=ument846>


<Paper ID=ument846> <Table ID =2>
	<Abstractive Summary> =
		000
		‘subreddit’, ‘order’,
		‘account’, ‘game’, ‘issue’
		Table 2: A sample presentation of high afﬁnity terms and low afﬁnity terms from subreddits with high high afﬁnity
		averages (top 1%), and low high afﬁnity averages (bottom 1%)
	</Abstractive Summary>
	<Extractive Summary> =
		Our results, as shown in Table 2,
		demonstrate that high afﬁnity terms have different
		characteristics across communities
	</Extractive Summary>
</Paper ID=ument846>


<Paper ID=ument846> <Table ID =3>
	<Abstractive Summary> =
		062
		Table 3: Coefﬁcient of determination values for linear models trained on community characteristics that predict
		semantic narrowing (left) and semantic broadening (right) of high afﬁnity terms
	</Abstractive Summary>
	<Extractive Summary> =
		Semantic Narrowing and Semantic Broaden-
		ing: Table 3 shows that number of comments has
		the strongest correlation to semantic narrowing
		and semantic broadening of high afﬁnity terms,
		achieving R2 values of 0
	</Extractive Summary>
	<Extractive Summary> =
		In contrast, while loyalty and dedication
		have similarly high R2 values when used for mod-
		eling semantic narrowing of high afﬁnity terms as
		shown in Table 3, it is more weakly linked to the
		semantic broadening of high afﬁnity terms
	</Extractive Summary>
</Paper ID=ument846>


<Paper ID=ument846> <Table ID =4>
	<Abstractive Summary> =
		001
		Table 4: Coefﬁcient of determination values for linear and multivariate models trained on community characteris-
		tics that predict rate of new users (δu(si))
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4 we present our results of regression
		and correlation testing
	</Extractive Summary>
</Paper ID=ument846>


<Paper ID=ument847> <Table ID =1>
	<Abstractive Summary> =
		6815
		Table 1:
		Impact of smoothing factor ϵ on the Quora
		validation set
	</Abstractive Summary>
</Paper ID=ument847>


<Paper ID=ument847> <Table ID =2>
	<Abstractive Summary> =
		6764
		Table 2:
		Impact of the batch size N on the Quora
		validation set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the effect
		of an increase in the number of negative examples
		in a batch
	</Extractive Summary>
</Paper ID=ument847>


<Paper ID=ument847> <Table ID =3>
	<Abstractive Summary> =
		6837
		Table 3:
		Comparison of different loss functions on
		Quora validation set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 and 4 compare the proposed loss with
		the triplet loss with random sampling, TL(Rand)
	</Extractive Summary>
</Paper ID=ument847>


<Paper ID=ument847> <Table ID =4>
	<Abstractive Summary> =
		6789
		Table 4:
		Comparison of different loss functions on
		Quora test set
	</Abstractive Summary>
</Paper ID=ument847>


<Paper ID=ument847> <Table ID =5>
	<Abstractive Summary> =
		7491
		Table 5: Comparison of different loss functions on
		open domain QA dataset validation set
	</Abstractive Summary>
</Paper ID=ument847>


<Paper ID=ument847> <Table ID =6>
	<Abstractive Summary> =
		7480
		Table 6:
		Comparison of different loss functions on
		open domain QA dataset test set
	</Abstractive Summary>
</Paper ID=ument847>


<Paper ID=ument848> <Table ID =1>
	<Abstractive Summary> =
		77
		Raw data
		After
		pre-processing
		# reviews
		53,273
		# words
		1,724,842
		529,035
		# unique words
		25,007
		10,102
		Table 1: Summary of dataset
	</Abstractive Summary>
</Paper ID=ument848>


<Paper ID=ument848> <Table ID =2>
	<Abstractive Summary> =
		They in-
		tegrate with my Samsung Smart Things Hub
		80
		Label
		Top keywords
		Price
		price, buy, gift, christmas, worth, black friday, money, sale, deal, item
		Integration
		light, control, command, system, integration, thermostat, room, ecosystem, connect
		Sound quality
		speaker, sound, quality, small, music, loud, great, room, bluetooth, volume
		Accuracy
		question, answer, time, response, quick, issue, problem, work, search, good
		Skills
		music, weather, news, alarm, timer, kitchen, morning, reminder, shopping list
		Fun
		fun, family, kid, useful, helpful, great, friend, game, information, question
		Ease of use
		easy, use, set, setup, simple, install, recommend, connect, quick, work
		Playing music
		music, play, song, playlist, favorite, pandora, prime, stream, subscription, beam
		Table 2: 8 merged dimensions and the keywords reveal how people use smart speakers and their perceptions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the respective top keywords
	</Extractive Summary>
</Paper ID=ument848>


<Paper ID=ument849> <Table ID =1>
	<Abstractive Summary> =
		84%
		Table 1: Dataset statistics for multiclass datasets
	</Abstractive Summary>
</Paper ID=ument849>


<Paper ID=ument849> <Table ID =2>
	<Abstractive Summary> =
		105
		808
		Table 2: Dataset statistics for multilabel datasets
	</Abstractive Summary>
</Paper ID=ument849>


<Paper ID=ument849> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: Classiﬁcation Accuracy for single label classi-
		ﬁcation
	</Abstractive Summary>
</Paper ID=ument849>


<Paper ID=ument849> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Classiﬁcation Accuracy for multi-label classiﬁcation
	</Abstractive Summary>
</Paper ID=ument849>


<Paper ID=ument849> <Table ID =5>
	<Abstractive Summary> =
		59
		Table 5: Performance on different categories of PWPs for different parts of the PWPs
	</Abstractive Summary>
</Paper ID=ument849>


<Paper ID=ument85> <Table ID =1>
	<Abstractive Summary> =
		The cake is delicious! Are you baked?
		Table 1: Examples of ZPs and translations where words
		in brackets are ZPs that are invisible in decoding and
		underlined words are antecedents of anaphoric ZPs
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, even
		a strong NMT model fails to recall the implicit
		information, which lead to problems like incom-
		pleteness and incorrectness
	</Extractive Summary>
	<Extractive Summary> =
		are sentence pair in a parallel cor-
		pus) in Table 1 for instance, the ZP “它 (it)” is
		dropped in the Chinese side while its equivalent
		“it” exists in the English side
	</Extractive Summary>
</Paper ID=ument85>


<Paper ID=ument85> <Table ID =2>
	<Abstractive Summary> =
		77
		Table 2:
		Evaluation of ZP translation and prediction on the Chinese–English data
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results on Chinese⇒English Task
		Table 2 lists the performance of ZP translation and
		prediction on Chinese⇒English data
	</Extractive Summary>
</Paper ID=ument85>


<Paper ID=ument85> <Table ID =3>
	<Abstractive Summary> =
		06
		Table 3: Translation quality on Japanese–English data
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results on Japanese⇒English Task
		Table 3 lists the results
	</Extractive Summary>
</Paper ID=ument85>


<Paper ID=ument85> <Table ID =4>
	<Abstractive Summary> =
		60
		Table 4: Translation results when no ZP-annotated in-
		put is used in decoding by removing the reconstructor
		component
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		lists the results
	</Extractive Summary>
</Paper ID=ument85>


<Paper ID=ument85> <Table ID =5>
	<Abstractive Summary> =
		38
		Table 5: Translation results when transforming the con-
		textual representation to decoder of different models
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, directly incorporating
		inter-sentential context into standard NMT model
		(one of document-level NMT architectures) can
		improve translation quality by +0
	</Extractive Summary>
</Paper ID=ument85>


<Paper ID=ument85> <Table ID =6>
	<Abstractive Summary> =
		Fixed
		70
		39
		38
		147
		New
		7
		9
		7
		23
		Total
		49
		11
		14
		74
		Table 6:
		Translation error statistics
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6, we count how many trans-
		lation errors caused by different types of ZPs
		(i
	</Extractive Summary>
</Paper ID=ument85>


<Paper ID=ument85> <Table ID =7>
	<Abstractive Summary> =
		Table 7:
		Example translations where pronouns in
		brackets are dropped in original inputs (“INP
	</Abstractive Summary>
	<Extractive Summary> =
		Case Study
		Table 7 shows two typical exam-
		ples, of which pronouns are mistakenly translated
		by the strong baseline (“External ZP Prediction”)
		model (Wang et al
	</Extractive Summary>
</Paper ID=ument85>


<Paper ID=ument850> <Table ID =1>
	<Abstractive Summary> =
		1
		Corpus
		Corpus
		Long-term
		Date
		Texts
		Tokens
		Types
		Texts
		Tokens
		Types
		Full
		10,696
		863,227
		32,020
		1,634
		127,644
		11,274
		Train+val
		1,464
		117,947
		9,973
		1,464
		115,227
		10,540
		Test
		150
		11,886
		2,383
		150
		11,738
		2,592
		Table 1: Descriptives of the dating proﬁle corpus
		A total sample of 12,310 dating proﬁles together
		with the indicated desired relationship goal was
		collected from a popular Dutch dating site (see
		Table 1)
	</Abstractive Summary>
	<Extractive Summary> =
		A total of 3,228 texts (1,614 texts for each re-
		lationship group; see Table 1) was used for train-
		ing and testing
	</Extractive Summary>
</Paper ID=ument850>


<Paper ID=ument850> <Table ID =2>
	<Abstractive Summary> =
		29**
		Table 2: Pearson’s r compared to humans
	</Abstractive Summary>
	<Extractive Summary> =
		05) was signiﬁcantly
		stronger than the correlation scores for the regres-
		sion model on these labels (see Table 2)
	</Extractive Summary>
</Paper ID=ument850>


<Paper ID=ument850> <Table ID =3>
	<Abstractive Summary> =
		62
		Table 3: Accuracy scores by humans and classiﬁca-
		tion models
	</Abstractive Summary>
	<Extractive Summary> =
		24), meaning that there was no method
		that performed signiﬁcantly better than any other
		method (see Table 3)
	</Extractive Summary>
</Paper ID=ument850>


<Paper ID=ument850> <Table ID =4>
	<Abstractive Summary> =
		rather, music, EVENT, my
		Nature and hobbies
		I, like, ﬁnd, nice, go, friend, eat, movie, cozy, time, watch, couch, delightful, walk, evening, day, good, make, enjoy, music
		Table 4: Translated topic models with assigned topic names
	</Abstractive Summary>
	<Extractive Summary> =
		01), and
		a nature-related topic model shows that a nature-
		focused label is important to distinguish relation-
		ship goals, which LIWC is currently lacking (see
		Table 4 and Table 5)
	</Extractive Summary>
</Paper ID=ument850>


<Paper ID=ument850> <Table ID =5>
	<Abstractive Summary> =
		Method
		Important features
		Human
		seek, date, nice, know, people, undertake, spontaneous,
		sweet, terrace, pretty, enjoy, child wish, family person
		Classiﬁer
		date, spontaneous, let, live, nature, sociable, send, build,
		exercise, nice, independent, again, friendship, sea, girl,
		terrace
		LLR
		quiet, sweet, nothing, nature, fetch, again, proﬁle,
		click, feel free, weekend, sea, people, visit, caring
		Table 5: Translated words not in LIWC’s lexicon or-
		dered by importance for relationship goal identiﬁca-
		tion
	</Abstractive Summary>
	<Extractive Summary> =
		01), and
		a nature-related topic model shows that a nature-
		focused label is important to distinguish relation-
		ship goals, which LIWC is currently lacking (see
		Table 4 and Table 5)
	</Extractive Summary>
</Paper ID=ument850>


<Paper ID=ument851> <Table ID =1>
	<Abstractive Summary> =
		80
		-
		Table 1: F1 scores over ﬁve runs on CONLL and ONTONOTES test set of ablation experiments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 reports results of 4 strong baselines
		that use popular embeddings (column X), further
		adding either the LS representation (Ghaddar and
		Langlais, 2018a) or ours
	</Extractive Summary>
</Paper ID=ument851>


<Paper ID=ument851> <Table ID =2>
	<Abstractive Summary> =
		84
		Table 2: Mention-level F1 scores
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Comparing Contextualized Embeddings
		Table 2 reports F1 scores on the test portion of
		the 7 datasets we considered, for models trained
		with different embedding combinations
	</Extractive Summary>
</Paper ID=ument851>


<Paper ID=ument851> <Table ID =3>
	<Abstractive Summary> =
		# entities
		Dataset
		Domain Types train
		dev
		test
		CONLL
		news
		4
		23499 5942
		5648
		ONTONOTES
		news
		18
		81828 11066 11257
		WNUT17
		tweet
		6
		1975
		836
		1079
		I2B2
		bio
		23
		11791 5453 11360
		FIN
		ﬁnance
		4
		460
		-
		120
		WIKIGOLD
		wikipedia
		4
		-
		-
		3558
		WEBPAGES
		web
		4
		-
		-
		783
		Table 3: Statistics on the datasets used in our ex-
		periments
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Dataset
		Table 3 list the dataset used in this study do-
		main, label size, and number of mentions in
		train/dev/test portions
	</Extractive Summary>
</Paper ID=ument851>


<Paper ID=ument852> <Table ID =1>
	<Abstractive Summary> =
		Paraphrase Questions
		我想知道卡片的开户行
		I would like to know the card’s bank
		您好,请帮我查询一下卡片的开户行
		Hi, please help me checkthe card’s bank
		卡片的开户行请帮我查询一下
		The card’s bank, please help me check it
		卡片的开户行能帮我查询一下吗？
		The card’s bank, can you help me check it?
		Table 1: Example of an question and its paraphrases
	</Abstractive Summary>
</Paper ID=ument852>


<Paper ID=ument852> <Table ID =2>
	<Abstractive Summary> =
		869
		1,063
		620
		873
		Table 2: Statistics of annotated datasets for question
		paraphrasing
	</Abstractive Summary>
</Paper ID=ument852>


<Paper ID=ument852> <Table ID =3>
	<Abstractive Summary> =
		690
		Table 3: The overall performance of different models on four datasets from two domains (bold number in each
		column is the best performance on that dataset)
	</Abstractive Summary>
</Paper ID=ument852>


<Paper ID=ument852> <Table ID =4>
	<Abstractive Summary> =
		747
		Table 4: Transfer learning performance of our pipeline
		model on the intersection datasets (bold number in
		each column is the best performance on that dataset)
	</Abstractive Summary>
</Paper ID=ument852>


<Paper ID=ument853> <Table ID =1>
	<Abstractive Summary> =
		26
		Table 1: Distribution of the replacement categories
		in the development data, ‘No norm
	</Abstractive Summary>
	<Extractive Summary> =
		Not
		surprisingly, there is a correlation visible with the
		frequencies (Table 1)
	</Extractive Summary>
</Paper ID=ument853>


<Paper ID=ument854> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Performances of the external bibliographic re-
		sources used for matching books on RFR via ISBN
	</Abstractive Summary>
	<Extractive Summary> =
		In order to retrieve the author’s
		name(s) associated with the books in the RFR dataset,
		we perform ISBN matching using public APIs on eight
		of them, listed in Table 1 along with the fraction of
		found ISBNs from this dataset
	</Extractive Summary>
</Paper ID=ument854>


<Paper ID=ument854> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Global system top-k accuracy at the book level
	</Abstractive Summary>
</Paper ID=ument854>


<Paper ID=ument855> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Targeted TV program
		# tweets
		3,745
		# chars per
	</Abstractive Summary>
</Paper ID=ument855>


<Paper ID=ument855> <Table ID =2>
	<Abstractive Summary> =
		)
		# vocab of chars
		1,693
		# vocab of words
		7,727
		Table 2: Statistics of Raw Data
		# B-REFERENCE
		7,871
		# I-REFERENCE
		5,558
		# O
		29,829
		Table 3: Statistics of Reference Tags
		1https://en
	</Abstractive Summary>
</Paper ID=ument855>


<Paper ID=ument855> <Table ID =3>
	<Abstractive Summary> =
		)
		# vocab of chars
		1,693
		# vocab of words
		7,727
		Table 2: Statistics of Raw Data
		# B-REFERENCE
		7,871
		# I-REFERENCE
		5,558
		# O
		29,829
		Table 3: Statistics of Reference Tags
		1https://en
	</Abstractive Summary>
</Paper ID=ument855>


<Paper ID=ument855> <Table ID =4>
	<Abstractive Summary> =
		5
		Table 4: Training Parameters
		5
	</Abstractive Summary>
</Paper ID=ument855>


<Paper ID=ument855> <Table ID =5>
	<Abstractive Summary> =
		95
		Table 5: Results
		as in CoNLL 2003
	</Abstractive Summary>
</Paper ID=ument855>


<Paper ID=ument856> <Table ID =1>
	<Abstractive Summary> =
		10
		Table 1: Accuracy of models (%) on Seq2seq task
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Comparing models on Seq2seq task
		In Table 1 we report the overall character level ac-
		curacy of the 4 best performing models for each
		conﬁguration and experiment: (1) LSTM-Token-
		seq: the model with the Token LSTM + Sequence
		encoder (yellow and blue parts of Figure 1) and
		Token decoder, (2) CNN-Token-seq: the model
		with the Token CNN + Sequence encoder (red
		and blue parts of Figure 1) and Token decoder
	</Extractive Summary>
</Paper ID=ument856>


<Paper ID=ument857> <Table ID =1>
	<Abstractive Summary> =
		23
		Table 1: The word error rates of the different models in relation to the test set
		source
		correct target
		prediction
		1
		joo
		joo
		joo
		2
		ettE
		ett¨a
		ett¨a
		3
		heet
		he
		heet
		4
		uskovah
		uskovat
		uskovat
		5
		n
		niin
		niin
		6 <ettE
		ett¨a
		ett¨a
		7
		sinn
		sinne
		sinne
		8 <ei
		ei
		ei
		9
		ole
		ole
		ole
		10
		,
		,
		,
		11
		kukhaan
		kukaan
		kukaan
		12
		ymm¨art¨anny
		ymm¨art¨anyt
		ymm¨art¨anyt
		13
		menn¨a
		menn¨a
		menn¨a
		14
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the WERs of the different meth-
		ods
	</Extractive Summary>
</Paper ID=ument857>


<Paper ID=ument857> <Table ID =2>
	<Abstractive Summary> =
		15
		Artj¨arveN
		Artj¨arven
		Artj¨arven
		16
		kirkolt
		kirkolta
		kirkolta
		17
		menn¨ah
		menn¨a¨an
		menn¨a¨an
		18
		sinneh
		sinne
		sinne
		19
		Hiitel¨ah
		Hiitel¨a¨an
		Hiitel¨ass¨a
		Table 2: Examples from input, output and prediction
		phonotactic accuracy makes selection of correct
		analysis from multiple predicted variants more dif-
		ﬁcult, as it is not possible to easily detect mor-
		phologically valid and invalid forms
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Error analysis
		Table 2 illustrates the general performance of the
		model, with errors marked in bold
	</Extractive Summary>
	<Extractive Summary> =
		Vast ma-
		jority of needed changes are individual insertions,
		replacements or deletions in the word end, as il-
		lustrated in Table 2 at lines 2, 4, 6, 7, 15, 16, 17
		and 18
	</Extractive Summary>
</Paper ID=ument857>


<Paper ID=ument858> <Table ID =1>
	<Abstractive Summary> =
		Relevance: irrelevant 
		Table 1: An example of relevance classification
	</Abstractive Summary>
	<Extractive Summary> =
		, topic and utterance1 in Table 1)
	</Extractive Summary>
</Paper ID=ument858>


<Paper ID=ument859> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Hyperparameters
		Parameter
		Value
		Number of encoder layers
		2
		Encoder forward cell size
		128
		Encoder backward cell size
		128
		Number of decoder layers
		1
		Decoder cell size
		512
		Input BPE vocab size
		40000
		BPE embedding size
		100
		UPOS embedding size
		100
		Language embedding size
		20
		Dropout rate
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows a full list of the hyperparameters used in
		the training procedure
	</Extractive Summary>
</Paper ID=ument859>


<Paper ID=ument859> <Table ID =2>
	<Abstractive Summary> =
		, all sen-
		tences belonging to the same group had the same
		156
		Table 2: Training Dataset Statistics
		English
		German
		Spanish
		Chinese
		Korean
		Dutch
		English
		-
		521
	</Abstractive Summary>
</Paper ID=ument859>


<Paper ID=ument859> <Table ID =3>
	<Abstractive Summary> =
		com/ccliu2/syn-emb
		Table 3: Syntactic Nearest-Neighbour Accuracy (%)
		ISO
		1-NN/5-NN
		Total/Groups
		English
		en
		97
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 also shows that the
		syntactic information of multiple languages was
		captured by a single embedding model
	</Extractive Summary>
	<Extractive Summary> =
		The nearest neighbours accuracy of our syn-
		tactic embeddings in Table 3 signiﬁcantly outper-
		forms the general purpose language models
	</Extractive Summary>
</Paper ID=ument859>


<Paper ID=ument859> <Table ID =4>
	<Abstractive Summary> =
		With this training method, no UPOS
		158
		Table 4: Syntactic Nearest-Neighbour for Language Models (%)
		English
		German
		Spanish
		Chinese
		Korean
		Dutch
		Model
		1-NN/5-NN
		1-NN/5-NN
		1-NN/5-NN
		1-NN/5-NN
		1-NN/5-NN
		1-NN/5-NN
		USE
		71
	</Abstractive Summary>
</Paper ID=ument859>


<Paper ID=ument859> <Table ID =5>
	<Abstractive Summary> =
		00
		Table 5: Functional Dissimilarity Scores (Lower is Better)
		Model
		English
		German
		Spanish
		Chinese
		Korean
		Dutch
		BERTavg
		0
	</Abstractive Summary>
</Paper ID=ument859>


<Paper ID=ument859> <Table ID =6>
	<Abstractive Summary> =
		We studied the quality of learned embeddings
		by examining nearest neighbours in the embed-
		Table 6: Syntactic Nearest-Neighbour on New lan-
		guages (%)
		Lang (ISO)
		1-NN/5-NN
		Total/Group
		French (fr)
		35
	</Abstractive Summary>
	<Extractive Summary> =
		The results are shown in Table 6 (top)
	</Extractive Summary>
	<Extractive Summary> =
		The results
		are shown in Table 6 (bottom)
	</Extractive Summary>
	<Extractive Summary> =
		Indonesian in
		Table 6, the above methods only improved nearest
		neighbours accuracy slightly
	</Extractive Summary>
</Paper ID=ument859>


<Paper ID=ument86> <Table ID =1>
	<Abstractive Summary> =
		84)
		Table 1: Experiment ressuts on NIST Zh-En task, including number of parameters (|θ|, excluding word embed-
		dings), training/testing speeds (vtrain/vtest), and translation results in case-insensitive BLEU
	</Abstractive Summary>
	<Extractive Summary> =
		1
		NIST Zh-En Translation
		We list the results of our experiments on NIST
		Zh-En task in Table 1 concerning two different ar-
		chitectures, i
	</Extractive Summary>
</Paper ID=ument86>


<Paper ID=ument86> <Table ID =2>
	<Abstractive Summary> =
		96
		Table 2: Case-sensitive BLEU on WMT14 En-De and
		WMT16 En-Ro tasks
	</Abstractive Summary>
</Paper ID=ument86>


<Paper ID=ument86> <Table ID =3>
	<Abstractive Summary> =
		80
		Table 3: Evaluation on translation quality and ade-
		quacy
	</Abstractive Summary>
	<Extractive Summary> =
		(2018), we also con-
		duct subjective evaluations to validate the bene-
		ﬁt of modeling PAST and FUTURE (the last three
		rows of Table 3)
	</Extractive Summary>
</Paper ID=ument86>


<Paper ID=ument860> <Table ID =1>
	<Abstractive Summary> =
		162
		Table 1:
		Examples on the left are considered valid
		usages in simplified Chinese (SC)
	</Abstractive Summary>
	<Extractive Summary> =
		The
		difference between simplified and tradi-
		tional Chinese shown in Table 1 is an
		example
	</Extractive Summary>
	<Extractive Summary> =
		It can also be applied to both
		simplified Chinese and traditional Chinese,
		despite the challenging issue that some er-
		roneous usages of characters in traditional
		texts are considered valid usages in simpli-
		fied texts (see Table 1)
	</Extractive Summary>
</Paper ID=ument860>


<Paper ID=ument860> <Table ID =2>
	<Abstractive Summary> =
		164
		Table 2: Examples of the computation of character similarities
	</Abstractive Summary>
	<Extractive Summary> =
		They help us to model the sub-
		tle nuances in different characters that are composed of
		identical strokes (see examples in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Secondly, if a pair
		of more complex characters have the same edit dis-
		tance as a pair of less complex characters, we want
		the similarity of the more complex characters to be
		slightly higher than that of the less complex char-
		acters (see examples in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Therefore, we utilize character pronunciations
		of all CJK languages (see examples in Table 2),
		which are provided by the Unihan Database
	</Extractive Summary>
</Paper ID=ument860>


<Paper ID=ument860> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Statistics of datasets
	</Abstractive Summary>
</Paper ID=ument860>


<Paper ID=ument860> <Table ID =4>
	<Abstractive Summary> =
		We
		use
		the
		pre-trained
		masked
		language
		166
		Table 4:
		Configurations of FASPell
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Exploring hyper-parameters
		First, we only change the number of candidates
		in Table 4 to see its effect on spell checking per-
		formance
	</Extractive Summary>
	<Extractive Summary> =
		The reason we set the number of candidates
		c = 4 in Table 4 and no larger is because there is a
		trade-off with time consumption
	</Extractive Summary>
</Paper ID=ument860>


<Paper ID=ument860> <Table ID =5>
	<Abstractive Summary> =
		Table 5:
		Speed comparison (ms/sent)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Filtering Speed10
		First, we measure the filtering speed of Chinese
		spell checking in terms of absolute time consump-
		tion per sentence (see Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 clearly
		shows that FASPell is much faster
	</Extractive Summary>
</Paper ID=ument860>


<Paper ID=ument860> <Table ID =6>
	<Abstractive Summary> =
		167
		Table 6:
		This table shows spell checking performances on both detection and correction level
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Performance
		As shown in Table 6, FASPell achieves state-of-
		the-art F1 performance on both detection level and
		correction level
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 also shows that all the components of
		FASPell contribute effectively to its good perfor-
		mance
	</Extractive Summary>
</Paper ID=ument860>


<Paper ID=ument861> <Table ID =1>
	<Abstractive Summary> =
		397
		Table 1: Model performance with threshold selection
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Baseline model performance
		Table 1 shows baseline model performance on the
		relationship disambiguation task and highlights
		model parameters
	</Extractive Summary>
	<Extractive Summary> =
		Overall, from the scores in Table 1 one can see
		that the vector similarity models improve over the
		return all, but that there is much work to be done
		to further improve precision and recall
	</Extractive Summary>
</Paper ID=ument861>


<Paper ID=ument861> <Table ID =2>
	<Abstractive Summary> =
		25
		Table 2: Average polysemy statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 report polysemy statistics
	</Extractive Summary>
</Paper ID=ument861>


<Paper ID=ument861> <Table ID =3>
	<Abstractive Summary> =
		73
		Table 3: Spearman’s rank correlation coefﬁcients on word similarity tasks
	</Abstractive Summary>
</Paper ID=ument861>


<Paper ID=ument861> <Table ID =4>
	<Abstractive Summary> =
		684
		Table 4:
		Spearman’s correlation on word similarity
		tasks
	</Abstractive Summary>
</Paper ID=ument861>


<Paper ID=ument861> <Table ID =5>
	<Abstractive Summary> =
		025
		Table 5: Correlations between F0
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Network Correlation Results
		Table 5 displays correlation values between graph-
		based semantic similarity metrics of F0
	</Extractive Summary>
</Paper ID=ument861>


<Paper ID=ument862> <Table ID =1>
	<Abstractive Summary> =
		(2011) explored the problem of identifying
		inﬂuenza epidemics using machine-learning based
		tweet classiﬁers along with search engine trends
		183
		Name
		Label
		Annotation
		Class
		Mention of Non-Tobacco Drugs
		OD
		-1
		Unrelated or Ambiguous Mention
		UM
		0
		Personal or Anecdotal Mention
		PM
		1
		Informative or Advisory Mention
		IM
		2
		Advertisements
		AD
		3
		Table 1: Label and ID associated with each class
	</Abstractive Summary>
</Paper ID=ument862>


<Paper ID=ument862> <Table ID =2>
	<Abstractive Summary> =
		”
		Table 2: Examples for each category represented by its label
	</Abstractive Summary>
</Paper ID=ument862>


<Paper ID=ument862> <Table ID =3>
	<Abstractive Summary> =
		67%
		Table 3: Binary Classiﬁcation accuracies for speciﬁc topic (Personal Health Mention) or general theme (Tobacco-
		related Mentions)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 illustrates the results
		for this experiment
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 illustrates the results for this
		experiment
	</Extractive Summary>
</Paper ID=ument862>


<Paper ID=ument862> <Table ID =4>
	<Abstractive Summary> =
		713
		Table 4: Evaluation scores for the Fine-grained classiﬁcation experiment
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 illustrates the results of
		the experiment
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 illustrates
		the results for this experiment
	</Extractive Summary>
</Paper ID=ument862>


<Paper ID=ument862> <Table ID =5>
	<Abstractive Summary> =
		868
		Table 5: Average retweets and favorites across classes
		the linguistic cues common among these tweets
		was not considered until now
	</Abstractive Summary>
	<Extractive Summary> =
		Similarly, Table 5 shows an interest-
		ing trends for the favorites
	</Extractive Summary>
</Paper ID=ument862>


<Paper ID=ument863> <Table ID =1>
	<Abstractive Summary> =
		Train
		Dev
		Test
		Total
		FA
		consistent
		3956
		470
		538
		4998
		inconsistent
		28
		4
		2
		GA
		consistent
		3887
		468
		495
		4878
		inconsistent
		16
		6
		6
		B
		consistent
		3138
		400
		416
		4843
		inconsistent
		702
		85
		102
		C
		consistent
		3036
		382
		381
		4523
		inconsistent
		570
		69
		85
		Start
		consistent
		3725
		451
		472
		4924
		inconsistent
		223
		28
		25
		Stub
		consistent
		3863
		470
		492
		4931
		inconsistent
		83
		12
		11
		Total
		—
		23227
		2845
		3025
		29097
		Table 1: A breakdown of our Wikipedia dataset
	</Abstractive Summary>
</Paper ID=ument863>


<Paper ID=ument863> <Table ID =2>
	<Abstractive Summary> =
		442
		Table 2:
		Intrinsic evaluation results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the average NLPD over
		10 runs on both test sets
	</Extractive Summary>
	<Extractive Summary> =
		Here we also see that
		GPs achieve signiﬁcantly better performance than
		RFs, although by a much lower margin compared
		to the results on the primary labels (Table 2)
	</Extractive Summary>
</Paper ID=ument863>


<Paper ID=ument863> <Table ID =3>
	<Abstractive Summary> =
		545
		Table 3: Point estimate comparison results
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 3,
		we see that while
		BILSTM+ outperform our methods in the consis-
		consistent
		inconsistent
		RMSE
		r
		RMSE
		r
		BILSTM+
		0
	</Extractive Summary>
</Paper ID=ument863>


<Paper ID=ument863> <Table ID =4>
	<Abstractive Summary> =
		152†
		Table 4: Results for prediction of secondary labels
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 summarises the results
	</Extractive Summary>
</Paper ID=ument863>


<Paper ID=ument864> <Table ID =1>
	<Abstractive Summary> =
		2%
		Table 1: Data set statistics, adverse drug event (AE) and sentiment scores
	</Abstractive Summary>
	<Extractive Summary> =
		Post were distributed over the 11 forums as
		shown in Table 1 in terms of number of posts
		and percentage of total (columns 2 and 3); also
		shown are median post length, percentage of posts
		with AEs, and percentages of posts with posi-
		tive/negative/neutral sentiment
	</Extractive Summary>
</Paper ID=ument864>


<Paper ID=ument864> <Table ID =2>
	<Abstractive Summary> =
		2,320
		216
		615
		51
		2,845
		1,037
		137
		360
		313
		169
		308
		Table 2: Occurrence counts for Phase 1 annotations (Que=Question, Neg=Negation, Gen=Generalisation, Spe=
		Speculation, Red=ReducedCertainty; see Section 4
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents occurrence counts for enti-
		ties and events from Phase 1
	</Extractive Summary>
</Paper ID=ument864>


<Paper ID=ument865> <Table ID =1>
	<Abstractive Summary> =
		(2016) explore
		constructions of verbal irony in social media texts,
		reporting that detection of contrasting polarities
		is a strong indicator and use sentiment analysis
		213
		Train
		All
		Ironic
		Non-Ironic
		Irony
		1
		2
		3
		# Tweets
		3817
		1901
		1383 (73%)
		316 (17%)
		202 (10%)
		1916
		# Tweets containing emoji
		406
		175
		162 (93%)
		7 (4%)
		6 (3%)
		231
		# Unique emojis
		158
		104
		122
		Test
		# Tweets
		784
		311
		164 (53%)
		85 (27%)
		62 (20%)
		473
		# Tweets containing emoji
		88
		33
		27 (82%)
		3 (9%)
		3 (9%)
		55
		# Unique emojis
		81
		23
		70
		Table 1: Dataset statistics
		Ironic
		Non-ironic
		Emoji
		Count
		Emoji
		Count
		42 (29
	</Abstractive Summary>
</Paper ID=ument865>


<Paper ID=ument865> <Table ID =2>
	<Abstractive Summary> =
		8%)
		Table 2: Top 10 most frequent emojis in ironic tweets and
		non-ironic tweets along with the count and percentage of each
		emoji
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows ten emojis that
		most frequently appear in ironic tweets and non-
		ironic tweets in the English dataset
	</Extractive Summary>
</Paper ID=ument865>


<Paper ID=ument865> <Table ID =3>
	<Abstractive Summary> =
		(Ironic)
		Case 3
		Another day in paradise haha (Ironic)
		Another day in paradise haha
		(Non-Ironic)
		Table 3: Examples of annotated tweets with respect to the different cases
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows some
		example tweets
	</Extractive Summary>
</Paper ID=ument865>


<Paper ID=ument865> <Table ID =4>
	<Abstractive Summary> =
		23
		52%
		Table 4: Fleiss’ κ and percent agreement scores for calculat-
		ing inter-annotator agreement
	</Abstractive Summary>
</Paper ID=ument865>


<Paper ID=ument865> <Table ID =5>
	<Abstractive Summary> =
		:)
		Love your new show @driverminnie
		+
		+
		Non-ironic (Yes)
		Table 5: Generated ironic examples
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 displays the generated ironic and
		non-ironic tweets
	</Extractive Summary>
</Paper ID=ument865>


<Paper ID=ument866> <Table ID =1>
	<Abstractive Summary> =
		54
		Table 1: Performance of prior work and proposed model
	</Abstractive Summary>
</Paper ID=ument866>


<Paper ID=ument867> <Table ID =1>
	<Abstractive Summary> =
		98∗∗
		0
		Table 1: Model performance and signiﬁcance levels with respect to text-only models: ∗ : p ≤ 0
	</Abstractive Summary>
</Paper ID=ument867>


<Paper ID=ument867> <Table ID =2>
	<Abstractive Summary> =
		03∗∗
		Table 2: Model performance and signiﬁcance levels with respect to text-only model AttCNN: ∗ : p ≤ 0
	</Abstractive Summary>
</Paper ID=ument867>


<Paper ID=ument868> <Table ID =1>
	<Abstractive Summary> =
		39
		W-NUT
		930
		–
		–
		Table 1: Number of labels and mean/median distance
		in km between instances and the cluster town center
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the
		resulting number of labels, and the mean distance
		in km between the new instance coordinates and
		the respective town center
	</Extractive Summary>
</Paper ID=ument868>


<Paper ID=ument868> <Table ID =2>
	<Abstractive Summary> =
		09∗
		Table 2: Performance of prior work and of the proposed model with W-NUT and P2C labels
	</Abstractive Summary>
</Paper ID=ument868>


<Paper ID=ument869> <Table ID =1>
	<Abstractive Summary> =
		Case-agnostic models discard orthographic infor-
		238
		Annotation
		O
		O
		O
		B-ORG
		I-ORG
		E-ORG
		(a)
		Original Sentence
		I
		live
		in
		New
		York
		City
		(b)
		Lower-cased Sentence
		i
		live
		in
		new
		york
		city
		(c)
		Upper-cased Sentence
		I
		LIVE
		IN
		NEW
		YORK
		CITY
		Table 1: Example of Data Augmentation
		mation (how the given text was capitalized), which
		is considered to be highly useful (Robinson et al
	</Abstractive Summary>
	<Extractive Summary> =
		See
		Table 1 (a) for an example annotated sentence
	</Extractive Summary>
	<Extractive Summary> =
		When a sentence is all-lowercased
		or all-uppercased as in Table 1 (b) and (c), each
		word would still correspond to the same en-
		tity
	</Extractive Summary>
</Paper ID=ument869>


<Paper ID=ument869> <Table ID =2>
	<Abstractive Summary> =
		2
		Table 2: F1 scores on original, lower-cased, and upper-cased test sets of English Datasets
	</Abstractive Summary>
	<Extractive Summary> =
		F1 scores are shown
		in Table 2 and 3
	</Extractive Summary>
	<Extractive Summary> =
		‘Transfer to Twitter’ col-
		umn of Table 2 reports results
	</Extractive Summary>
</Paper ID=ument869>


<Paper ID=ument869> <Table ID =3>
	<Abstractive Summary> =
		7
		Table 3: F1 scores on original, lower-cased, and upper-cased test sets of Non-English Datasets
		Question 2: How effective Caseless, True-
		casing, and Data Augmentation approaches are
		in improving robustness of models? All meth-
		ods show similar levels of performance on lower-
		cased or uppercased text
	</Abstractive Summary>
</Paper ID=ument869>


<Paper ID=ument87> <Table ID =1>
	<Abstractive Summary> =
		31
		Table 1: Corpus Statistics for TransErr, WMT17, AFRL and NRC
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Translation Error Analysis
		Table 1 shows the error distribution of TransErr
		and WMT 17 corpus
	</Extractive Summary>
	<Extractive Summary> =
		It is be-
		cause errors are scarcely distributed (see Table 1)
	</Extractive Summary>
</Paper ID=ument87>


<Paper ID=ument87> <Table ID =2>
	<Abstractive Summary> =
		35
		Table 2: F1 for each class annotated by the unsuper-
		vised method using our manual annotation as gold ref-
		erence
		QE and follow (Popovi´c and Ney, 2011) to de-
		rive source annotation through two stages of align-
		ments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 demonstrates
		its F1 score using the manual annotation as the
		gold standard
	</Extractive Summary>
</Paper ID=ument87>


<Paper ID=ument87> <Table ID =3>
	<Abstractive Summary> =
		378
		Table 3: F1 score for each class produced by different error detection models
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Error Detection Results
		Table 3 shows our error detection results on the
		developing set and test set
	</Extractive Summary>
</Paper ID=ument87>


<Paper ID=ument87> <Table ID =4>
	<Abstractive Summary> =
		OK
		W
		WT
		M
		MT
		OK
		12635
		203
		68
		106
		13
		W
		279
		83
		3
		40
		5
		WT
		109
		28
		54
		9
		6
		M
		121
		54
		12
		122
		11
		MT
		26
		12
		31
		16
		44
		Table 4: Confusion matrix on test set
	</Abstractive Summary>
</Paper ID=ument87>


<Paper ID=ument87> <Table ID =5>
	<Abstractive Summary> =
		662
		Table 5: Segment-level Pearson correlation of various
		metrics and their enhancements with our WAER mea-
		sure through (Eq
	</Abstractive Summary>
</Paper ID=ument87>


<Paper ID=ument870> <Table ID =1>
	<Abstractive Summary> =
		Label
		Train
		Dev
		Test
		Conﬂict
		+1
		14,749
		929
		509
		-1
		177,421
		13,130
		13,576
		Life
		+1
		17,434
		354
		154
		-1
		177,421
		13,130
		13,576
		Table 1: Numbers of the positive and negative samples
		of the LFK datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the staticstics of our generated datasets
	</Extractive Summary>
</Paper ID=ument870>


<Paper ID=ument870> <Table ID =2>
	<Abstractive Summary> =
		0 (4)
		Table 2: Model performance
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Evaluation
		Table 2 presents the performance of the models
		on the test performance for different datasets (i
	</Extractive Summary>
</Paper ID=ument870>


<Paper ID=ument871> <Table ID =1>
	<Abstractive Summary> =
		False
		Table 1: Examples of relations annotated by distant
		supervision
	</Abstractive Summary>
</Paper ID=ument871>


<Paper ID=ument871> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: Parameter Settings
		outperform the former over the entire range of re-
		call
	</Abstractive Summary>
</Paper ID=ument871>


<Paper ID=ument871> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: P@N for relation extraction in entity pairs with different number of sentences
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results on NYT dataset regard-
		ing P@100, P@200, P@300 and the mean of three
		settings for each model
	</Extractive Summary>
</Paper ID=ument871>


<Paper ID=ument871> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Some examples of Separate Head-Tail CNN
		corrections compared to PCNN
		4
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Case Study
		In Table 4, we show some of our SHTCNN
		model examples corrections compared to tradi-
		tional PCNN
	</Extractive Summary>
</Paper ID=ument871>


<Paper ID=ument872> <Table ID =1>
	<Abstractive Summary> =
		Phatic
		Could you send me your paper?
		Conative
		X:
		Sure, it’s here: <link>
		Referential
		Y:
		Thanks!
		Phatic
		Table 1: An example of a discussion where all func-
		tions of language are present
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1, we present
		an anecdotal example where all functions are ex-
		pressed in the conversation
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 in Zhang et al
	</Extractive Summary>
</Paper ID=ument872>


<Paper ID=ument872> <Table ID =2>
	<Abstractive Summary> =
		51%
		Table 2: Analysis per language function
	</Abstractive Summary>
	<Extractive Summary> =
		The
		percentage of controversial parent comments per
		function shown in Table 2 conﬁrms our intuition
		that conative comments are used to reply to con-
		troversial content more often than the other com-
		ments
	</Extractive Summary>
</Paper ID=ument872>


<Paper ID=ument872> <Table ID =3>
	<Abstractive Summary> =
		263
		referential
		phatic
		emotive
		poetic
		conative
		she has a credible claim
		absolutely agree
		shameful
		feeltheburn
		mind explaining why
		what time was that
		I’m sorry
		barely even human
		inverted triple bern
		keep fooling yourself
		it’s all marketing
		I upvoted you
		epic simply epic
		duality of man
		dude relax
		Table 3: Example of predictions of our semi-supervised approach
	</Abstractive Summary>
	<Extractive Summary> =
		We show in Table 3 ex-
		amples of the predictions for the unlabelled dataset
		made by our approach
	</Extractive Summary>
</Paper ID=ument872>


<Paper ID=ument872> <Table ID =4>
	<Abstractive Summary> =
		847
		Table 4: Precision, recall and F1 score per function
	</Abstractive Summary>
</Paper ID=ument872>


<Paper ID=ument873> <Table ID =1>
	<Abstractive Summary> =
		61
		Table 1: CO-MC F1-scores / intents accuracies of the ﬁrst 100 %, 75 %, 50 %, and 25 % of the tokens of the
		utterances of the test dataset of the cleaned human transcribed full utterances
		training dataset
		95 % conf
	</Abstractive Summary>
</Paper ID=ument873>


<Paper ID=ument873> <Table ID =2>
	<Abstractive Summary> =
		54
		Table 2: Intents accuracies / percentages of the used tokens for predicting the intents using the smallest partial
		utterance of the test dataset of the cleaned human transcribed incremental utterances for which the system has a
		conﬁdence of more or equal than 95 %, 90 %, 85 %, and 80 %, if the conﬁdence is not reached, the full utterance is
		used
		training dataset
		ﬁrst 100 %
		ﬁrst 75 %
		ﬁrst 50 %
		ﬁrst 25 %
		human, full
		90
	</Abstractive Summary>
</Paper ID=ument873>


<Paper ID=ument873> <Table ID =3>
	<Abstractive Summary> =
		17
		Table 3: CO-MC F1-scores / intents accuracies of the ﬁrst 100 %, 75 %, 50 %, and 25 % of the tokens of the
		utterances of the test dataset of the human transcribed full utterances
		training dataset
		95 % conf
	</Abstractive Summary>
</Paper ID=ument873>


<Paper ID=ument873> <Table ID =4>
	<Abstractive Summary> =
		33
		Table 4: Intents accuracies / percentages of the used tokens for predicting the intents using the smallest partial
		utterance of the test dataset of the human transcribed incremental utterances for which the system has a conﬁdence
		of more or equal than 95 %, 90 %, 85 %, and 80 %, if the conﬁdence is not reached, the full utterance is used
		training dataset
		ﬁrst 100 %
		ﬁrst 75 %
		ﬁrst 50 %
		ﬁrst 25 %
		human, full
		83
	</Abstractive Summary>
</Paper ID=ument873>


<Paper ID=ument873> <Table ID =5>
	<Abstractive Summary> =
		73
		Table 5: CO-MC F1-scores / intents accuracies of the ﬁrst partial automatically transcribed utterances that have
		equal or more than the ﬁrst 100 %, 75 %, 50 %, and 25 % of the tokens of the utterances of the test dataset of the
		automatically transcribed full utterances
		273
		training dataset
		95 % conf
	</Abstractive Summary>
</Paper ID=ument873>


<Paper ID=ument873> <Table ID =6>
	<Abstractive Summary> =
		54
		Table 6: Intents accuracies / percentages of the used tokens for predicting the intents using the ﬁrst partial utterance
		of the test dataset of the automatically transcribed incremental utterances for which the system has a conﬁdence of
		more or equal than 95 %, 90 %, 85 %, and 80 %, if the conﬁdence is not reached, the full utterance is used
		utterances
	</Abstractive Summary>
</Paper ID=ument873>


<Paper ID=ument873> <Table ID =7>
	<Abstractive Summary> =
		11
		atis ﬂight no#
		atis airline
		Table 7: intents distribution (in percent) of the ATIS
		utterances used in this work
	</Abstractive Summary>
</Paper ID=ument873>


<Paper ID=ument874> <Table ID =1>
	<Abstractive Summary> =
		xx
		@renskedemaessc dm me je
		gsmnummer eens ;-)
		<user> doormail me je gsm-
		nummer eens <emoji>
		<user> mail me your cell-
		phone number once <emoji>
		Table 1: Source and target pairs as parallel data for a machine translation approach
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists some examples of the noisy data
		we are dealing with
	</Extractive Summary>
</Paper ID=ument874>


<Paper ID=ument874> <Table ID =2>
	<Abstractive Summary> =
		117
		Table 2: Dutch parallel corpora data statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents the number of parallel sen-
		tences in each genre and the number of words be-
		fore and after normalization3
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 also reveals that the corpus
		amounts to only a few hundred parallel sentences
	</Extractive Summary>
	<Extractive Summary> =
		This has re-
		sulted in a different style of Twitter corpus which
		comprises more noisy and longer tweets (see CER
		in Table 2 vs
	</Extractive Summary>
	<Extractive Summary> =
		When comparing the results of Setups 2 and 3
		with the original WER values of our test set (see
		Table 2) we observe that the normalization task
		was not solved at all as the WER values are al-
		most always higher
	</Extractive Summary>
</Paper ID=ument874>


<Paper ID=ument874> <Table ID =3>
	<Abstractive Summary> =
		067
		Table 3: Parallel corpora data statistics after new anno-
		tations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		the size of the newly annotated data in terms of
		the number of sentences and tokens for each genre
	</Extractive Summary>
	<Extractive Summary> =
		As can be derived from Table 3, TWE remains
		the least noisy genre, whereas the SNS genre is
		now the noisiest one with higher WER and CER
		values
	</Extractive Summary>
	<Extractive Summary> =
		The new annotations as presented in Table 3
		were used for training (Setup 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 3)
	</Extractive Summary>
</Paper ID=ument874>


<Paper ID=ument874> <Table ID =4>
	<Abstractive Summary> =
		jaaa sws toch <emoji> hijzelf is wel tof
		jaaa sws toch <emoji> hij blijft echt leuk
		jaaa sws maar <emoji> hij is gewoon leuk
		jaaa sws dus <emoji> hij is inderdaad tof
		English
		yes anyway but/so <emoji> he/himself is
		really/just/indeed nice/cool
		Table 4: Data augmentation using pretrained embed-
		dings
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, we illustrate this
		augmentation technique, starting from the user-
		generated text jaaa sws toch :) hij is echt leuk
		(yes anyway <emoji> he is really nice)
	</Extractive Summary>
	<Extractive Summary> =
		In the upper part of Table 4 the standard words
		in the source and target sentences are placed
		in cursive
	</Extractive Summary>
</Paper ID=ument874>


<Paper ID=ument874> <Table ID =5>
	<Abstractive Summary> =
		jaaa sws maar <emoji> hij is gwn leuk
		jaaa sws dus <emoji> hij is idd tof
		English
		yes anyway but/so <emoji> he is really/
		just/indeed nice/cool
		Table 5: Data augmentation using dictionary of abbre-
		viations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 exempliﬁes this technique
	</Extractive Summary>
</Paper ID=ument874>


<Paper ID=ument874> <Table ID =6>
	<Abstractive Summary> =
		INS
		DEL
		SUBS
		SUM
		TWE
		1118
		934
		355
		2407
		SNS
		2483
		2238
		1021
		5742
		SMS
		4209
		508
		758
		5475
		Table 6: Operations needed at the character level to
		normalize the test set for each genre
	</Abstractive Summary>
	<Extractive Summary> =
		281
		Table 6 shows the number of insertions (INS),
		deletions (DEL) and substitutions (SUBS) that are
		needed to transform the source sentences of the
		test set into manually normalized ones
	</Extractive Summary>
</Paper ID=ument874>


<Paper ID=ument874> <Table ID =7>
	<Abstractive Summary> =
		# Words
		TWE
		Src
		Tgt
		1
		5190
		124578
		122165
		2
		697441
		20692213
		20416466
		3
		853465
		26316110
		26002836
		SNS
		Src
		Tgt
		1
		8136
		108127
		110326
		2
		577281
		21120858
		21499465
		3
		835091
		70337827
		71257870
		SMS
		Src
		Tgt
		1
		7626
		111393
		113846
		2
		615195
		15356594
		15669683
		3
		766946
		22066532
		22651464
		Table 7: Parallel corpora data statistics for each exper-
		imental setup
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the number of parallel training
		sentences and words in the original and target
		sides of each setup for each genre
	</Extractive Summary>
</Paper ID=ument874>


<Paper ID=ument874> <Table ID =8>
	<Abstractive Summary> =
		jongen tammy , ik u
		English
		boy tammy, me you
		Table 8: Examples of original (src), predicted (norm)
		and target (tgt) sentences using the NMT approach
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Error Analysis
		Table 8 shows some examples of sentences which
		were normalized using the proposed NMT ap-
		proach
	</Extractive Summary>
	<Extractive Summary> =
		For example, in
		the second sentence in Table 8, the system was
		unable to correctly normalize some of the words
	</Extractive Summary>
</Paper ID=ument874>


<Paper ID=ument874> <Table ID =9>
	<Abstractive Summary> =
		TWE
		INS
		DEL
		SUBS
		SUM
		Test
		1118
		934
		355
		2407
		Setup 0
		1101
		930
		351
		2382
		Setup 1
		1099
		924
		330
		2353
		Setup 2
		1105
		917
		326
		2348
		Setup 3
		1094
		903
		325
		2322
		SNS
		INS
		DEL
		SUBS
		SUM
		Test
		2483
		2238
		1021
		5742
		Setup 0
		2454
		2225
		992
		5671
		Setup 1
		2468
		2220
		995
		5683
		Setup 2
		2366
		2092
		968
		5426
		Setup 3
		2366
		2141
		971
		5478
		SMS
		INS
		DEL
		SUBS
		SUM
		Test
		4209
		508
		758
		5475
		Setup 0
		4107
		505
		727
		5339
		Setup 1
		4094
		483
		700
		5277
		Setup 2
		4099
		470
		709
		5278
		Setup 3
		4090
		468
		710
		5268
		Table 9: Number of solved operations at the character
		level after normalization for each genre
	</Abstractive Summary>
	<Extractive Summary> =
		As can be de-
		rived from the Table 9 many cases where correctly
		283
		normalized by the systems
	</Extractive Summary>
</Paper ID=ument874>


<Paper ID=ument875> <Table ID =1>
	<Abstractive Summary> =
		Given a word, the
		287
		Method
		Input Text
		Google Translate
		Clean Input
		there is a fat duck swimming in the lake
		湖里 有一只胖鸭子在游泳
		Noisy Input
		there is a fat dack swimming in the leake
		在 leake 里游泳时有一个 胖子
		Spell-Checker
		there is a fat sack swimming in the leak
		在 泄露处 有一个肥胖 袋在游泳
		Grammaly2
		there is a fat dack swimming in the lake
		湖里 游泳很胖
		Ours
		there is a fat duck swimming in the lake
		湖里 有一只胖鸭子在游泳
		Table 1: Illustrative example of spell-checker and contextual denoising
	</Abstractive Summary>
	<Extractive Summary> =
		There are several beneﬁts of the proposed method:
		• Our method can make accurate corrections
		based on the context and semantic meaning
		of the whole sentence as Table 1 shows
	</Extractive Summary>
</Paper ID=ument875>


<Paper ID=ument875> <Table ID =2>
	<Abstractive Summary> =
		of [MASK] (n)
		Top k
		Size
		1
		3000
		3000
		2
		5
		25
		3
		3
		27
		4
		2
		16
		Total:
		3068
		Table 2: Size of the candidate list
		3
	</Abstractive Summary>
</Paper ID=ument875>


<Paper ID=ument875> <Table ID =3>
	<Abstractive Summary> =
		40
		Table 3: BLEU scores of EN-to-DE tranlsation
		As can be seen, both fairseq model and Google
		Translate suffer from a signiﬁcant performance
		drop on the noisy texts with both natural and syn-
		thetic noise
	</Abstractive Summary>
	<Extractive Summary> =
		All experiment results is summarized in
		Table 3, where we use BLEU score (Papineni
		et al
	</Extractive Summary>
</Paper ID=ument875>


<Paper ID=ument875> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: SNLI classiﬁcation accuracy with artiﬁcial
		noise and natural noise
	</Abstractive Summary>
</Paper ID=ument875>


<Paper ID=ument875> <Table ID =5>
	<Abstractive Summary> =
		4
		Table 5: Classiﬁcation F1 score on MRPC
		4
		Conclusion and Future Work
		In this paper, we present a novel text denois-
		ing algorithm using ready-to-use masked language
		model
	</Abstractive Summary>
</Paper ID=ument875>


<Paper ID=ument876> <Table ID =1>
	<Abstractive Summary> =
		Label
		Description
		ARGA
		Causer
		ARG0
		Agent or Experiencer or Doer
		ARG1
		Theme or Patient
		ARG2
		Beniﬁciary
		ARG2 ATTR
		Attribute or Quality
		ARG2 LOC
		Physical Location
		ARG2 GOL
		Destination or Goal
		ARG2 SOU
		Source
		ARG3
		Instrument
		ARGM DIR
		Direction
		ARGM LOC
		Location
		ARGM MNR
		Manner
		ARGM EXT
		Extent or Comparison
		ARGM TMP
		Temporal
		ARGM REC
		Reciprocal
		ARGM PRP
		Purpose
		ARGM CAU
		Cause or Reason
		ARGM DIS
		Discourse
		ARGM ADV
		Adverb
		ARGM NEG
		Negative
		ARGM PRX
		Complex Predicate
		Table 1: PropBank Tagset
		Social media data doesn’t conform to the rules
		of spelling, grammar or punctuation
	</Abstractive Summary>
</Paper ID=ument876>


<Paper ID=ument876> <Table ID =2>
	<Abstractive Summary> =
		07
		used as a light verb
		Table 2:
		Rolesets and meanings for the Hindi verb
		xeKa
	</Abstractive Summary>
</Paper ID=ument876>


<Paper ID=ument876> <Table ID =3>
	<Abstractive Summary> =
		Feature
		Argument Identiﬁcation
		P
		R
		f-score
		Predicate
		33
		50
		40
		Headword (HW)
		52
		47
		49
		HeadwordPOS
		33
		50
		40
		Phrasetype (PT)
		41
		34
		37
		Predicate-PT
		42
		65
		51
		Predicate-HW
		55
		49
		51
		Dependency
		78
		78
		78
		Named Entity
		57
		50
		65
		HeadwordPOS-PT
		41
		34
		37
		Headword-PT
		57
		49
		53
		HeadwordPOS(UD)
		32
		50
		39
		UD dependency
		64
		65
		64
		Predicate-language
		43
		65
		52
		Headword-language
		55
		47
		51
		Table 3: Individual feature performance for Argument
		Identiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the precision, recall and
		F1 scores of the features for Argument Identiﬁca-
		tion
	</Extractive Summary>
	<Extractive Summary> =
		We also see a signiﬁcant increase in accu-
		racy when we use the combinational feature of
		predicate and its language, as compared to using
		only predicate as a feature (Table 3)
	</Extractive Summary>
</Paper ID=ument876>


<Paper ID=ument876> <Table ID =4>
	<Abstractive Summary> =
		Feature
		Argument Identiﬁcation
		P
		R
		f-score
		Baseline
		56
		53
		55
		with predicate-lang
		57
		54
		55
		+dependency
		81
		76
		78
		Table 4: Accuracy scores for Argument Identiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 gives the accuracy scores for the system
		using baseline features
	</Extractive Summary>
</Paper ID=ument876>


<Paper ID=ument876> <Table ID =5>
	<Abstractive Summary> =
		Feature
		Argument Classiﬁcation
		P
		R
		f-score
		Predicate
		06
		09
		06
		Headword (HW)
		18
		10
		13
		HeadwordPOS
		05
		07
		06
		Phrasetype (PT)
		08
		10
		08
		Predicate-PT
		05
		08
		06
		Predicate-HW
		05
		06
		06
		Dependency
		81
		86
		83
		Named Entity
		20
		14
		16
		HeadwordPOS-PT
		07
		09
		08
		Headword-PT
		12
		09
		10
		HeadwordPOS(UD)
		08
		11
		09
		UD dependency
		77
		83
		80
		Predicate-language
		06
		10
		07
		Headword-language
		18
		11
		14
		Table 5: Individual feature performance for Argument
		Classiﬁcation
	</Abstractive Summary>
</Paper ID=ument876>


<Paper ID=ument876> <Table ID =6>
	<Abstractive Summary> =
		Feature
		Argument Classiﬁcation
		P
		R
		f-score
		Baseline
		27
		15
		19
		+dependency
		84
		84
		84
		Table 6: Accuracy scores for Argument Classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 gives the accuracy scores for Argument
		Classiﬁcation while using baseline features, and
		after incorporating dependency labels
	</Extractive Summary>
</Paper ID=ument876>


<Paper ID=ument877> <Table ID =1>
	<Abstractive Summary> =
		@beautifulloser8 i’m about
		to type it up !
		Table 1: Noisy UGC example and its canonical form
		(Gold)
		We make a few comments on this deﬁnition
	</Abstractive Summary>
	<Extractive Summary> =
		We illustrate
		it with the following example (Table 1)
	</Extractive Summary>
</Paper ID=ument877>


<Paper ID=ument877> <Table ID =2>
	<Abstractive Summary> =
		We will
		see in section 5 that despite its simplicity, such
		301
		noisy
		canonical
		ye
		yeah
		##a
		[SPACE]
		im
		i
		�
		MASK
		�
		’
		�
		MASK
		�
		m
		already
		already
		knowing
		knowing
		wa
		what
		##t
		[SPACE]
		Table 2: Independent Alignment of yea im already
		knowing wat u sayin normalized as yeah i’m already
		knowing what you saying
		an alignment allows our model to reach good
		performances
	</Abstractive Summary>
</Paper ID=ument877>


<Paper ID=ument877> <Table ID =3>
	<Abstractive Summary> =
		variance) along i dimension
		302
		Noisy
		Canonical
		ye
		ye
		##a
		##ah
		im
		i
		�
		MASK
		�
		’
		�
		MASK
		�
		m
		already
		already
		knowing
		knowing
		wa
		wh
		##t
		##at
		Table 3: Parallel Alignment of yea im already knowing
		wat u sayin normalized as yeah i’m already knowing
		what you saying
		Noisy
		Gold
		#next mask
		ye
		ye
		0
		##a
		##ah
		0
		im
		i
		2
		�
		MASK
		�
		’
		-
		�
		MASK
		�
		m
		-
		already
		already
		0
		knowing
		knowing
		0
		wa
		wh
		0
		##t
		##at
		0
		Table 4: Parallel Alignment of yea im already knowing
		wat u sayin normalized as yeah i’m already knowing
		what you saying with gold number of next masks for
		each source token
		the overall architecture
	</Abstractive Summary>
</Paper ID=ument877>


<Paper ID=ument877> <Table ID =4>
	<Abstractive Summary> =
		variance) along i dimension
		302
		Noisy
		Canonical
		ye
		ye
		##a
		##ah
		im
		i
		�
		MASK
		�
		’
		�
		MASK
		�
		m
		already
		already
		knowing
		knowing
		wa
		wh
		##t
		##at
		Table 3: Parallel Alignment of yea im already knowing
		wat u sayin normalized as yeah i’m already knowing
		what you saying
		Noisy
		Gold
		#next mask
		ye
		ye
		0
		##a
		##ah
		0
		im
		i
		2
		�
		MASK
		�
		’
		-
		�
		MASK
		�
		m
		-
		already
		already
		0
		knowing
		knowing
		0
		wa
		wh
		0
		##t
		##at
		0
		Table 4: Parallel Alignment of yea im already knowing
		wat u sayin normalized as yeah i’m already knowing
		what you saying with gold number of next masks for
		each source token
		the overall architecture
	</Abstractive Summary>
</Paper ID=ument877>


<Paper ID=ument877> <Table ID =5>
	<Abstractive Summary> =
		18
		Table 5: Impact of our noise-speciﬁc strategy on the F1
		score (development set) reported with best alignment
		setting
		(78
	</Abstractive Summary>
</Paper ID=ument877>


<Paper ID=ument877> <Table ID =6>
	<Abstractive Summary> =
		6
		Table 6: Comparing our systems to the State-of-the-
		art system MoNoise (we report on same development
		dataset reported in MoNoise original paper (last 950
		sentences))
		Model
		F1
		Supranovich and Patsepnia, 2015
		82
	</Abstractive Summary>
</Paper ID=ument877>


<Paper ID=ument877> <Table ID =7>
	<Abstractive Summary> =
		1
		Table 7:
		Comparing our systems to WNUT 2015
		shared task that allowed UGC resources
		6
		Discussion
		We now compare our system to previous works
	</Abstractive Summary>
</Paper ID=ument877>


<Paper ID=ument877> <Table ID =8>
	<Abstractive Summary> =
		5
		lex15+5Mtweets
		-
		Table 8: Comparing our systems to the State-of-the-art
		system MoNoise on lexnorm15 test
	</Abstractive Summary>
	<Extractive Summary> =
		As we see in Table 8, our non-UGC system is far
		from the State-of-the-Art model MoNoise (van der
		Goot and van Noord, 2017) in terms of F1 score
	</Extractive Summary>
	<Extractive Summary> =
		78 improvement) (Table 8)
	</Extractive Summary>
	<Extractive Summary> =
		We compare it in Table 8, demonstrating
		another practical interest for our approach
	</Extractive Summary>
</Paper ID=ument877>


<Paper ID=ument878> <Table ID =1>
	<Abstractive Summary> =
		Dog sitting disaster
		Table 1: Each unshaded box shows the titles of a randomly chosen post and the 5 posts most similar to it
		counted the proportion of them that are clustered
		into the same cluster
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Qualitative Evaluation
		Table 1, shows the titles of 3 randomly chosen
		posts and the ﬁve most similar posts to them
	</Extractive Summary>
</Paper ID=ument878>


<Paper ID=ument879> <Table ID =1>
	<Abstractive Summary> =
		Many of
		Snippet
		haaapppyyyy birthday best friend!!
		Love you lots #love
		Word
		‘haaapppyyyy’, ‘birthday’, ‘best’,
		‘friend’, ‘!’, ‘!’, ‘Love’, ‘you’,
		‘lots’, ‘#’, ‘love’
		Wordpiece ‘ha’, ‘##aa’, ‘##pp’, ‘##py’, ‘##y’,
		‘##y’,
		‘##y’,
		‘birthday’,
		‘best’,
		‘friend’, ‘!’, ‘!’, ‘Love’, ‘you’,
		‘lots’, ‘#’, ‘love’
		Domain-
		speciﬁc
		‘happy’,
		‘<elongated>’,
		‘birth-
		day’, ‘best’, ‘friend’, ‘!’, ‘<re-
		peated>’,
		‘Love’,
		‘you’,
		‘lots’,
		‘</hashtag>’, ‘love’, ‘<hashtag>’
		Table 1: Example of an real-world irregular expression
		preprocessed by methods at different levels
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows an example of an irregular expres-
		sion and how it can be preprocessed at word level,
		wordpiece level and at domain level
	</Extractive Summary>
</Paper ID=ument879>


<Paper ID=ument879> <Table ID =2>
	<Abstractive Summary> =
		Original
		Processed
		’:)’, ’:-)’
		<happy>
		’REAL’
		<allcaps>real </allcaps>
		’gooooood’
		good <elongated>
		October 8th
		<date>
		@jeremy
		<user>
		#Christmas
		<hashtag>Christmas </hashtag>
		Table 2: Examples of typical Twitter-speciﬁc expres-
		sions and their preprocessed versions with annotation
		marks
		3
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 give a few examples of preprocessed
		words with annotation, where <*>is a designated
		annotation mark
	</Extractive Summary>
</Paper ID=ument879>


<Paper ID=ument879> <Table ID =3>
	<Abstractive Summary> =
		3†
		Table 3: Result of multi-label emotion classiﬁcation on
		SemEval-2018
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Evaluation
		Table 3 lists the results of multi-label emotion
		classiﬁcation on SemEval-2018
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows that BERT integrating with
		Twitter-speciﬁc features outperforms both general
		and domain-speciﬁc component model
	</Extractive Summary>
</Paper ID=ument879>


<Paper ID=ument879> <Table ID =4>
	<Abstractive Summary> =
		53)
		Table 4: F1 on binary classiﬁcation for each emotion
		class
	</Abstractive Summary>
	<Extractive Summary> =
		For more detailed investigation of the effect of
		domain knowledge, Table 4 shows the result of
		binary classiﬁcation for each emotion class mea-
		sured by F1 score
	</Extractive Summary>
</Paper ID=ument879>


<Paper ID=ument88> <Table ID =1>
	<Abstractive Summary> =
		342
		Table 1: F1 accuracy of detecting under-translation er-
		rors with the estimated word importance
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 lists the accuracy of detecting under-
		Type
		Zh⇒En
		En⇒Fr
		En⇒Ja
		POS Tags
		Noun
		21
	</Extractive Summary>
</Paper ID=ument88>


<Paper ID=ument88> <Table ID =2>
	<Abstractive Summary> =
		1%
		Table 2: Correlation between Attribution word impor-
		tance with POS tags, Fertility, and Syntactic Depth
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists the correlations, where a higher
		value indicates a stronger correlation
	</Extractive Summary>
</Paper ID=ument88>


<Paper ID=ument88> <Table ID =3>
	<Abstractive Summary> =
		69%
		Table 3:
		Distribution of syntactic categories (e
	</Abstractive Summary>
	<Extractive Summary> =
		Therefore, in the follow-
		ing analyses, we mainly focus on the POS tags
		(Table 3) and fertility properties (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 3, content
		words are more important on Chinese⇒English
		but content-free words are more important on
		English⇒Japanese
	</Extractive Summary>
</Paper ID=ument88>


<Paper ID=ument88> <Table ID =4>
	<Abstractive Summary> =
		69%
		Table 4: Distributions of word fertility and their relative change based on Attribution importance and word count
	</Abstractive Summary>
	<Extractive Summary> =
		Therefore, in the follow-
		ing analyses, we mainly focus on the POS tags
		(Table 3) and fertility properties (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 lists the results
	</Extractive Summary>
</Paper ID=ument88>


<Paper ID=ument880> <Table ID =1>
	<Abstractive Summary> =
		0)
		Table 1:
		Number of users in SMHD dataset per con-
		dition and the average number of posts per user (with
		std
	</Abstractive Summary>
	<Extractive Summary> =
		As the average number of posts per user is roughly
		160 (Table 1), it is reasonable to expect of a model
		to perform well with similar amounts of data avail-
		able
	</Extractive Summary>
</Paper ID=ument880>


<Paper ID=ument880> <Table ID =2>
	<Abstractive Summary> =
		94
		Table 2: F1 measure averaged over ﬁve runs with different control groups
	</Abstractive Summary>
</Paper ID=ument880>


<Paper ID=ument880> <Table ID =3>
	<Abstractive Summary> =
		pronouns
		I, my, her, your, they
		I’ve never, your thoughts
		Affective
		like, nice, love, bad
		I love
		Social
		friend, boyfriend, girl, guy
		my dad, my girlfriend, my ex
		Biological
		pain, sex, skin, sleep, porn
		your pain, a doctor, a therapist
		Informal
		omg, lol, shit, fuck, cool
		tl dr, holy shit
		Other
		advice, please, reddit
		thank you, your advice
		Table 3:
		Unigrams and bigrams most often given the
		highest weight by attention mechanism in depression
		classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		Some of the top 100 most common unigrams and
		bigrams are presented in Table 3, aggregated un-
		der the most common LIWC categories
	</Extractive Summary>
</Paper ID=ument880>


<Paper ID=ument881> <Table ID =1>
	<Abstractive Summary> =
		27M
		Table 1: Size of parallel and monolingual data
	</Abstractive Summary>
</Paper ID=ument881>


<Paper ID=ument881> <Table ID =2>
	<Abstractive Summary> =
		54%
		Table 2: Noise level of ASR generated data ﬁltered
		with different length ratio thresholds
	</Abstractive Summary>
</Paper ID=ument881>


<Paper ID=ument881> <Table ID =3>
	<Abstractive Summary> =
		07
		Table 3: BLEU scores of models ﬁne-tuned on different data in the Fr→En direction
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Data Augmentation
		As it is common in the ﬁeld, we experimented
		with back translation (third row in Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		5
		External Data
		To explore the effect of other types of noise, we
		ﬁne-tuned our baseline model on different external
		datasets (see the “Unconstrained" rows in Table 3
		and 4)
	</Extractive Summary>
	<Extractive Summary> =
		In the Fr→En direction, we
		submitted the model ﬁne-tuned on merged MTNT
		data, forward translation and fuzzy match data
		(row 5 in Table 3)
	</Extractive Summary>
</Paper ID=ument881>


<Paper ID=ument881> <Table ID =4>
	<Abstractive Summary> =
		05
		Table 4: BLEU scores of models ﬁne-tuned on different data in the En→Fr direction
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in the last row in Table 4, mixed training
		could improve the performance over baseline on
		noisy texts
	</Extractive Summary>
	<Extractive Summary> =
		In the En→Fr direction, the
		double-tuned model with punctuation ﬁxed was
		submitted (row 6 in Table 4)
	</Extractive Summary>
</Paper ID=ument881>


<Paper ID=ument881> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: WMT19 Robustness Leaderboard on Fr→En
	</Abstractive Summary>
	<Extractive Summary> =
		6
		WMT19 Robustness Leaderboard
		We submitted our best constrained systems to
		WMT19 Robustness Leaderboard10, as shown in
		Table 5 and Table 6
	</Extractive Summary>
</Paper ID=ument881>


<Paper ID=ument881> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6: WMT19 Robustness Leaderboard on En→Fr
	</Abstractive Summary>
</Paper ID=ument881>


<Paper ID=ument882> <Table ID =1>
	<Abstractive Summary> =
		Hate Speech
		Table 1:
		Examples where our combined model is able to predict correct label whereas the baseline sentiment
		model fails
	</Abstractive Summary>
	<Extractive Summary> =
		On looking at some reviews in the test dataset
		that our combined model (Se + Sc + Hu + Ha)
		got right and that the baseline sentiment model
		got wrong, we observe that our model deﬁnitively
		helps with identifying the right sentiment for sar-
		castic reviews (Table 1) and some hate reviews, al-
		though we couldn’t ﬁnd many humorous reviews
		in this context
	</Extractive Summary>
</Paper ID=ument882>


<Paper ID=ument882> <Table ID =2>
	<Abstractive Summary> =
		22
		Table 2: Testing our hypothesis: Mi refers to the re-
		spective model, P and L indicate using predicted prob-
		abilities and labels respectively
	</Abstractive Summary>
</Paper ID=ument882>


<Paper ID=ument882> <Table ID =3>
	<Abstractive Summary> =
		18
		Table 3:
		Model Performances for various combina-
		tions of Sentiment Se (E1), Sarcasm Sc (E2), Humor
		Hu (E3) and Hate Speech Ha (E4)
	</Abstractive Summary>
</Paper ID=ument882>


<Paper ID=ument882> <Table ID =4>
	<Abstractive Summary> =
		40
		Table 4: Model performances during several runs of the baseline and the combined (Se + Sc + Hu + Ha) models
	</Abstractive Summary>
</Paper ID=ument882>


<Paper ID=ument882> <Table ID =5>
	<Abstractive Summary> =
		Extended story line: the ﬁrst half of
		the review is negative, and the model
		likely misses the turning point towards
		positive halfway through the review
		Table 5: Examples where the combined model goes wrong and the baseline sentiment model predicts the correct
		sentiment
	</Abstractive Summary>
</Paper ID=ument882>


<Paper ID=ument882> <Table ID =6>
	<Abstractive Summary> =
		Flip in sentiment
		Table 6: Examples where the combined and the baseline sentiment models both fail to predict the correct senti-
		ment
	</Abstractive Summary>
	<Extractive Summary> =
		Similarly, it seems that the reviews
		whose sentiment has been classiﬁed wrongly by
		the sentiment-only baseline and that don’t have
		any sarcastic / hate intent don’t get classiﬁed cor-
		rectly by our combined model either (Table 6)
	</Extractive Summary>
</Paper ID=ument882>


<Paper ID=ument883> <Table ID =1>
	<Abstractive Summary> =
		Error type Description
		Example
		Occ
		incorInﬂ
		incorrect inﬂection
		[pracovají → pracují] v továrnˇe
		8 986
		incorBase incorrect word base
		musíš to [posvˇetlit → posvˇetit]
		20 334
		fwFab
		non-emendable, „fabricated“ word
		pokud nechceš slyšet [smášky]
		78
		fwNC
		foreign word
		váza je na [Tisch → stole]
		166
		ﬂex
		supplementary ﬂag used with fwFab and
		jdu do [shopa → obchodu]
		34
		fwNC marking the presence of inﬂection
		wbdPre
		preﬁx separated by a space or preposition w/o space musím to [pˇri pravit → pˇripravit]
		817
		wbdComp wrongly separated compound
		[ˇceský anglický → ˇcesko-anglický] slovník
		92
		wbdOther other word boundary error
		[mocdobˇre → moc dobˇre]; [atak → a tak]
		1326
		stylColl
		colloquial form
		[dobrej → dobrý] ﬁlm
		3 533
		stylOther
		bookish, dialectal, slang, hyper-correct form
		holka s [hnˇedými oˇcimi → hnˇedýma oˇcima]
		156
		agr
		violated agreement rules
		to jsou [hezké → hezcí] chlapci; Jana [ˇctu → ˇcte]
		5 162
		dep
		error in valency
		bojí se [pes → psa]; otázka [ˇcas → ˇcasu]
		6 733
		ref
		error in pronominal reference
		dal jsem to jemu i [jejího → jeho] bratrovi
		344
		vbx
		error in analytical verb form or compound predicate musíš [pˇrijdeš → pˇrijít]; kluci [jsou] bˇehali
		864
		rﬂx
		error in reﬂexive expression
		dívá [∅ → se] na televizi; Pavel [si → se] raduje
		915
		neg
		error in negation
		[p˚ujdu ne → nep˚ujdu] do školy
		111
		lex
		error in lexicon or phraseology
		dopadlo to [pˇrírodnˇe → pˇrirozenˇe]
		3 967
		use
		error in the use of a grammar category
		pošta je [nejvíc blízko → nejblíže]
		1 458
		sec
		secondary error (supplementary ﬂag)
		stará se o [našich holˇciˇckách → naše holˇciˇcky]
		866
		stylColl
		colloquial expression
		vidˇeli jsme [hezký → hezké] holky
		3 533
		stylOther
		bookish, dialectal, slang, hyper-correct expression
		rozbil se mi [hadr]
		156
		stylMark
		redundant discourse marker
		[no]; [teda]; [jo]
		15
		disr
		disrupted construction
		známe [hodné spoustu → spoustu hodných] lidí
		64
		problem
		supplementary label for problematic cases
		175
		unspec
		unspeciﬁed error type
		69 123
		Table 1: Error types used in CzeSL corpus taken from (Jelínek et al
	</Abstractive Summary>
</Paper ID=ument883>


<Paper ID=ument883> <Table ID =2>
	<Abstractive Summary> =
		1 %
		Table 2: Statistics of the AKCES-GEC dataset – number of documents, sentences, words and error rates
	</Abstractive Summary>
	<Extractive Summary> =
		This fact is supported by Table 2, which
		shows that the average error rate of Romani de-
		velopment set is 21
	</Extractive Summary>
</Paper ID=ument883>


<Paper ID=ument883> <Table ID =3>
	<Abstractive Summary> =
		4%
		Table 3: Statistics of available corpora for Grammatical
		Error Correction
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Corpora Statistics
		Table 3 indicates that there is a variety of English
		datasets for GEC
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 also presents an average error rate of
		each corpus
	</Extractive Summary>
</Paper ID=ument883>


<Paper ID=ument883> <Table ID =4>
	<Abstractive Summary> =
		25
		0
		Table 4: Language speciﬁc constants for token- and character-level noising operations
	</Abstractive Summary>
</Paper ID=ument883>


<Paper ID=ument883> <Table ID =5>
	<Abstractive Summary> =
		40
		Table 5: Comparison of systems on two English GEC datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows an evident performance
		boost (3
	</Extractive Summary>
</Paper ID=ument883>


<Paper ID=ument883> <Table ID =6>
	<Abstractive Summary> =
		71
		Table 6: Results on on Falko-Merlin Test Set (German)
	</Abstractive Summary>
	<Extractive Summary> =
		As Table 6
		shows, her best system reaches 45
	</Extractive Summary>
</Paper ID=ument883>


<Paper ID=ument883> <Table ID =7>
	<Abstractive Summary> =
		17
		Table 7: Results on on AKCES-GEC Test Set (Czech)
	</Abstractive Summary>
</Paper ID=ument883>


<Paper ID=ument883> <Table ID =8>
	<Abstractive Summary> =
		20
		Table 8: Results on on RULEC-GEC Test Set (Russian)
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Russian
		As Table 8 indicates, GEC in Russian currently
		seems to be the most challenging task
	</Extractive Summary>
</Paper ID=ument883>


<Paper ID=ument884> <Table ID =1>
	<Abstractive Summary> =
		html
		358
		Word
		Confusion set
		had
		hard head hand gad has ha ad hat
		night
		knight naught nought nights bight might nightie
		then
		them the hen ten than thin thee thew
		haben
		habend halben gaben habe habet haken
		Nacht
		Nachts Nascht Macht Naht Acht Nach Jacht Pacht
		dann
		sann dank denn dünn kann wann bannen kannst
		имел
		им ел им-ел имела имели имело мел умел
		ночь
		ночью ночи дочь мочь ноль новь точь
		затем
		за тем за-тем затеем затеям зятем затеями
		Table 1: Examples of spell-broken confusion sets for
		English, German and Russian
	</Abstractive Summary>
	<Extractive Summary> =
		Most context-free spell-checkers
		combine a weighted edit distance and phonetic al-
		gorithms to order suggestions, which produces reli-
		able confusion sets (Table 1)
	</Extractive Summary>
</Paper ID=ument884>


<Paper ID=ument884> <Table ID =2>
	<Abstractive Summary> =
		Corpus
		Dev
		Test
		Train
		EN
		W&I+LOCNESS
		4,384
		4,477
		34,308
		DE
		Falco+MERLIN
		2,503
		2,337
		18,7544
		RU
		RULEC-GEC
		2,500
		5,000
		4,980
		Table 2: Sizes of labelled corpora in no
	</Abstractive Summary>
	<Extractive Summary> =
		As primary development and test data, we use
		the following learner corpora (Table 2):
		• English: the new W&I+LOCNESS corpus
		(Bryant et al
	</Extractive Summary>
</Paper ID=ument884>


<Paper ID=ument884> <Table ID =3>
	<Abstractive Summary> =
		99
		Table 3: Performance for different confusion sets and
		edit weighting techniques on W&I+LOCNESS Dev
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results and analysis
		Confusion sets
		On English data, all proposed
		confusion set generation methods perform better
		than random word substitution (Table 3)
	</Extractive Summary>
</Paper ID=ument884>


<Paper ID=ument884> <Table ID =4>
	<Abstractive Summary> =
		46
		(c) Russian (RULEC-GEC)
		Table 4: Unsupervised and ﬁne-tuned MAGEC sys-
		tems for English, German and Russian, contrasted with
		systems from related work and spell-checking base-
		lines
	</Abstractive Summary>
	<Extractive Summary> =
		Main results
		We ﬁrst compare the GEC systems
		with simple baselines using a greedy and context
		spell-checking (Table 4); the latter selects the best
		correction suggestion based on the sentence per-
		plexity from a Transformer language model
	</Extractive Summary>
	<Extractive Summary> =
		Roth (2019) for their systems that use authentic
		error-annotated data for training (Table 4b and 4c)
	</Extractive Summary>
	<Extractive Summary> =
		, 2019) from the
		BEA shared task trained on publicly available error-
		annotated corpora (Table 4a)
	</Extractive Summary>
</Paper ID=ument884>


<Paper ID=ument884> <Table ID =5>
	<Abstractive Summary> =
		01
		Table 5:
		Comparison with LM-based GEC on the
		CoNLL (M2) and JFLEG (GLEU) test sets for unsuper-
		vised (⋆) and supervised systems trained or ﬁne-tuned
		on different amounts of labelled data
	</Abstractive Summary>
	<Extractive Summary> =
		, 2017)
		(Table 5)
	</Extractive Summary>
</Paper ID=ument884>


<Paper ID=ument884> <Table ID =6>
	<Abstractive Summary> =
		37
		Table 6: Performance of single MAGEC w/ LM models
		on two groups of errors on respective development sets
	</Abstractive Summary>
	<Extractive Summary> =
		To counter the argument that – mostly due to
		the introduced character-level noise and strong lan-
		guage modelling – MAGEC can only correct these
		“simple” errors, we evaluate it against test sets that
		contain either spelling and punctuation errors or
		all other error types; with the complement errors
		corrected (Table 6)
	</Extractive Summary>
</Paper ID=ument884>


<Paper ID=ument885> <Table ID =1>
	<Abstractive Summary> =
		com/topsites/countries/MT
		Source Type & Name
		Query strings/
		Articles
		Online
		Posts
		SNS - Twitter
		1
		38
		SNS - Facebook
		1
		28
		SNS
		-
		Twitter-The
		Malta Independent
		1
		12
		News - Times of Malta
		4
		249
		News - MaltaToday
		4
		175
		News - The Malta Inde-
		pendent
		4
		45
		Table 1: Data sources used for the consolidated dataset
		terms of content published– were selected for each
		newswire mentioned:
		• Overview of the upcoming budget, published
		on the budget day;
		• Near to real-time live updates in commentary
		format, on the budget measures being pre-
		sented for the upcoming year;
		• Overview of the presented budget, published
		after the budget ﬁnishes, on the same day
		and/or the following day
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents details about the social dataset
		collected on the Malta Budget 2018
	</Extractive Summary>
</Paper ID=ument885>


<Paper ID=ument885> <Table ID =2>
	<Abstractive Summary> =
		9669
		Table 2: Inter-rater reliability measures for each anno-
		tation type
		A third expert in the domain consolidated the
		annotations to create a ﬁnal dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2, shows the inter-rater reli-
		ability agreement scores for each annotation type
	</Extractive Summary>
</Paper ID=ument885>


<Paper ID=ument885> <Table ID =3>
	<Abstractive Summary> =
		2%
		Table 3: Distribution of sentiment polarity annotations
		Polarity Intensity
		Online Posts
		Percentage
		Very Positive
		37
		6
	</Abstractive Summary>
	<Extractive Summary> =
		The distribution of
		the dataset annotations are represented as follows:
		sentiment polarity in Table 3, sentiment polarity
		intensity in Table 4 and emotion in Tables 5 and 6,
		respectively
	</Extractive Summary>
</Paper ID=ument885>


<Paper ID=ument885> <Table ID =4>
	<Abstractive Summary> =
		9%
		Table 4:
		Distribution of sentiment polarity intensity
		annotations
		Emotion
		Online Posts
		Percentage
		Anger
		131
		23
	</Abstractive Summary>
	<Extractive Summary> =
		The distribution of
		the dataset annotations are represented as follows:
		sentiment polarity in Table 3, sentiment polarity
		intensity in Table 4 and emotion in Tables 5 and 6,
		respectively
	</Extractive Summary>
</Paper ID=ument885>


<Paper ID=ument885> <Table ID =5>
	<Abstractive Summary> =
		3%
		Table 5: Distribution of emotion (6-levels) annotations
		Emotion
		Online Posts
		Percentage
		Anger
		121
		22
	</Abstractive Summary>
</Paper ID=ument885>


<Paper ID=ument885> <Table ID =6>
	<Abstractive Summary> =
		1%
		Trust
		60
		11%
		Table 6: Distribution of emotion (8-levels) annotations
		The dataset annotation results displayed do not
		fully reﬂect the opinions portrayed by the writers,
		368
		since a large amount of online posts were off-topic
		to the budget (34
	</Abstractive Summary>
</Paper ID=ument885>


<Paper ID=ument885> <Table ID =7>
	<Abstractive Summary> =
		4%
		Table 7:
		Distribution of the sarcasm/irony, negation,
		off-topic and Maltese language annotations
		The dataset has been published15 for general
		use under the Creative Commons Attribution-
		NonCommercial-ShareAlike
		4
	</Abstractive Summary>
</Paper ID=ument885>


<Paper ID=ument886> <Table ID =1>
	<Abstractive Summary> =
		3571
		Table 1: Performance of Article Bias Classiﬁer
		When comment labels were utilized for train-
		ing, existing language models could success-
		fully detect the slant of news comments, as dis-
		373
		Figure 1: The proportion of liberal and conservative news comments within the progressive and conser-
		vative media’s comment sections
		played in Table 2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summa-
		rizes the results, which shows the BODY-TEXT
		representation of the validation set showed the best
		result
	</Extractive Summary>
</Paper ID=ument886>


<Paper ID=ument886> <Table ID =2>
	<Abstractive Summary> =
		8322
		Table 2: Performance of Comment Bias Classiﬁer
		5
	</Abstractive Summary>
</Paper ID=ument886>


<Paper ID=ument887> <Table ID =1>
	<Abstractive Summary> =
		Singular
		Table 1: Examples from our two domains
	</Abstractive Summary>
	<Extractive Summary> =
		See the ﬁrst two tweets in Table 1,
		for an example of these two uses by the same
		user
	</Extractive Summary>
</Paper ID=ument887>


<Paper ID=ument887> <Table ID =2>
	<Abstractive Summary> =
		In-
		stead, we employ different techniques to obtain
		distantly supervised labels in two domains, as
		377
		Twitter
		Europarl
		Train
		58963
		11249
		Dev
		7370
		1405
		Test
		7370
		1405
		Total
		73703
		14059
		Table 2: Number of instances in our two corpora
	</Abstractive Summary>
	<Extractive Summary> =
		See Table 2 for details about each
		of these datasets, which we make publicly avail-
		able
	</Extractive Summary>
</Paper ID=ument887>


<Paper ID=ument887> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Accuracy (percent of correct predictions) of
		our ﬁne-tuned BERT model, tested both in- and out-of-
		domain
	</Abstractive Summary>
</Paper ID=ument887>


<Paper ID=ument888> <Table ID =1>
	<Abstractive Summary> =
		23
		Table 1: Impact of the parameters on validation perfor-
		mance for the WWII article history
	</Abstractive Summary>
	<Extractive Summary> =
		As seen on Table 1, when compared against
		the regular encoder, utilizing our edit-sentence ap-
		proach with token-level labels leads to a higher
		F1-Score and accuracy, showing the effectiveness
		of our proposed edit encoder
	</Extractive Summary>
</Paper ID=ument888>


<Paper ID=ument888> <Table ID =2>
	<Abstractive Summary> =
		28
		Table 2: Summary of our results
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 summarizes our best results on each se-
		lected article
	</Extractive Summary>
</Paper ID=ument888>


<Paper ID=ument889> <Table ID =1>
	<Abstractive Summary> =
		The approximated global optimum point
		of SGNS is shifted-PMI (Levy and Goldberg,
		Symbol
		Description
		V
		Vocabulary
		Gi
		Set of sub-words in word i
		i, j
		Index of a word in vocabulary
		D
		Set of co-occurring word pairs
		σ
		Sigmoid function
		⃗ui,⃗vj
		Embedding vector of word i, j
		⃗gi,⃗hj
		Embedding vector of sub-word i, j
		k
		Regularization parameter
		s
		Statistic sub-sampling parameter
		Sij
		Regularization coefﬁcient
		of word pair (i, j)
		Table 1: Summary of the symbols
		2014b)
	</Abstractive Summary>
</Paper ID=ument889>


<Paper ID=ument889> <Table ID =2>
	<Abstractive Summary> =
		18
		Table 2: Spearman’s rank correlation coefﬁcient of word similarity task on 2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results for the
		same experiment but with words that occur ﬁve or
		more times in the corpus for a vocabulary size of
		2
	</Extractive Summary>
</Paper ID=ument889>


<Paper ID=ument889> <Table ID =3>
	<Abstractive Summary> =
		81
		Table 3: Spearman’s rank correlation coefﬁcient of word similarity task on 6
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the results of the word similar-
		ity experiment with words that occur two or more
		times in the corpus for a vocabulary size of 6
	</Extractive Summary>
</Paper ID=ument889>


<Paper ID=ument889> <Table ID =4>
	<Abstractive Summary> =
		48
		Table 4: Spearman/Pearson correlation of sentence similarity on typo/omitted-word setting
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 show that OLIVE outperforms in each
		sub-word and word embedding group on both
		typo/omitted-word settings
	</Extractive Summary>
</Paper ID=ument889>


<Paper ID=ument89> <Table ID =1>
	<Abstractive Summary> =
		13
		Table 1: BLEU score of 23 languages→English with multilingual models based on different methods of language clustering:
		Random, Family (Language Family) and Embedding (Language Embedding)
	</Abstractive Summary>
</Paper ID=ument89>


<Paper ID=ument89> <Table ID =2>
	<Abstractive Summary> =
		13
		Table 2: BLEU score of 23 languages→English with different number of clusters: Universal (all the languages share one
		model), Individual (each language with separate models, totally 23 models)), Embedding (Language Embedding with 7 mod-
		els)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in
		Table 2, we have several observations:
		• First, Universal in general performs the worst
		due to the diverse languages and limited
		13Embedding performs worse than Family on only 2 lan-
		guages
	</Extractive Summary>
	<Extractive Summary> =
		An extreme case is that Sl
		(slovene) training together with all the lan-
		guages gets the best accuracy as shown in
		Table 2, since Sl is extremely low-resource,
		which beneﬁts a lot from more training data
	</Extractive Summary>
</Paper ID=ument89>


<Paper ID=ument89> <Table ID =3>
	<Abstractive Summary> =
		52
		Table 3: BLEU score of English→23 languages with multilingual models based on different methods of language
		clustering: Universal (all the languages share one model), Individual (each language with separate model), Family
		(Language Family), Embedding (Language Embedding)
	</Abstractive Summary>
	<Extractive Summary> =
		Figure 6 shows the clustering results based
		on language embeddings, and Table 3 shows the
		BLEU score of multilingual models clustered by
		different methods
	</Extractive Summary>
	<Extractive Summary> =
		Our
		experiment results shown in Table 3 demonstrate
		that the language embedding can provide more
		reasonable clustering than prior knowledge, and
		thus result in higher BLEU score for most of the
		languages
	</Extractive Summary>
	<Extractive Summary> =
		We also ﬁnd from Table 3 that each language
		with separate model (Individual) perform best on
		nearly 10 languages, due to the abundant model
		capacity
	</Extractive Summary>
</Paper ID=ument89>


<Paper ID=ument89> <Table ID =4>
	<Abstractive Summary> =
		We will also study
		971
		Language
		Ar
		Bg
		Cs
		De
		El
		Es
		Fa
		Fr
		He
		Hu
		It
		Ja
		Data size
		180K
		140K
		110K
		180K
		180K
		180K
		70K
		180K
		150K
		90K
		180K
		90K
		Language
		Nl
		Pl
		Pt
		Ro
		Ru
		Sk
		Sl
		Th
		Tr
		Vi
		Zh
		Data size
		140K
		140K
		180K
		180K
		160K
		70K
		14K
		80K
		130K
		130K
		180K
		Table 4: The size of training data for 23 language↔English in our experiments
	</Abstractive Summary>
	<Extractive Summary> =
		The training data sizes of each lan-
		guages are shown in Table 4 and we use the default
		validation and test set for each language pairs
	</Extractive Summary>
</Paper ID=ument89>


<Paper ID=ument89> <Table ID =5>
	<Abstractive Summary> =
		Language
		Arabic
		Bulgarian
		Czech
		German
		Greek
		Spanish
		Persian
		French
		Hebrew
		ISO code
		Ar
		Bg
		Cs
		De
		El
		Es
		Pt
		Fr
		He
		Language
		Hungarian
		Italian
		Japanese
		Dutch
		Polish
		Portuguese
		Romanian
		Russian
		Slovak
		ISO code
		Hu
		It
		Ja
		Nl
		Pl
		Pt
		Ro
		Ru
		Sk
		Language
		Slovenian
		Thai
		Turkish
		Vietnamese
		Chinese
		English
		ISO code
		Sl
		Th
		Tr
		Vi
		Zh
		En
		Table 5: The ISO 639-1 code of each language in our experiments
	</Abstractive Summary>
</Paper ID=ument89>


<Paper ID=ument890> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example of the lexical substitution tasks
		weights
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1
		shows an example of the lexical substitution task
		with a sentence,1 the target word to replace, and
		words of substitution candidates
	</Extractive Summary>
	<Extractive Summary> =
		Ranking Method
		As shown in Table 1, lexical
		substitution ranks substitution candidates of the
		target word based on their paraphrasabilities under
		a given context
	</Extractive Summary>
</Paper ID=ument890>


<Paper ID=ument890> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Examples of CEFR-LP
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows examples sampled from CEFR-
		LP
	</Extractive Summary>
</Paper ID=ument890>


<Paper ID=ument890> <Table ID =3>
	<Abstractive Summary> =
		65
		Table 3: Basic statistics in CEFR-LP compared to LS-SE and LS-CIC
		CEFR level
		target
		candidate
		all
		863
		14, 259
		A1
		300
		2, 090
		A2
		190
		2, 856
		B1
		110
		4, 513
		B2
		186
		3, 201
		C1
		30
		648
		C2
		47
		951
		Table 4: Distribution of CEFR levels in CEFR-LP
		a certain candidate
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Analysis of CEFR-LP
		Table 3 shows the basic statistics for CEFR-LP
		compared to those in LS-SE and LS-CIC
	</Extractive Summary>
</Paper ID=ument890>


<Paper ID=ument890> <Table ID =4>
	<Abstractive Summary> =
		65
		Table 3: Basic statistics in CEFR-LP compared to LS-SE and LS-CIC
		CEFR level
		target
		candidate
		all
		863
		14, 259
		A1
		300
		2, 090
		A2
		190
		2, 856
		B1
		110
		4, 513
		B2
		186
		3, 201
		C1
		30
		648
		C2
		47
		951
		Table 4: Distribution of CEFR levels in CEFR-LP
		a certain candidate
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the distribution of CEFR levels
		context embedding units
		300
		LSTM hidden/output units
		600
		MLP input units
		1200
		MLP hidden units
		1200
		sentential context units
		600
		target word units
		600
		number of negative samples
		10
		negative sampling rate
		0
	</Extractive Summary>
</Paper ID=ument890>


<Paper ID=ument890> <Table ID =5>
	<Abstractive Summary> =
		75
		number of epochs
		10
		Table 5: Context2vec hyper-parameters that show the
		best performance in (Melamud et al
	</Abstractive Summary>
	<Extractive Summary> =
		Pre-training used the same hyper-parameter set-
		tings of context2vec (Table 5)
	</Extractive Summary>
</Paper ID=ument890>


<Paper ID=ument890> <Table ID =6>
	<Abstractive Summary> =
		2
		Table 6: GAP scores on LS-SE, LS-CIC and CEFR-LP datasets, where bold denotes the highest scores
	</Abstractive Summary>
	<Extractive Summary> =
		9
		Table 7: GAP scores on different CEFR levels of target words in CEFR-LP
		6
		Results
		Table 6 shows the GAP scores for LS-SE, LS-CIC
		and CEFR-LS datasets
	</Extractive Summary>
	<Extractive Summary> =
		The last row of Table 6 shows
		the performance of our method with Sbest (i
	</Extractive Summary>
</Paper ID=ument890>


<Paper ID=ument890> <Table ID =7>
	<Abstractive Summary> =
		9
		Table 7: GAP scores on different CEFR levels of target words in CEFR-LP
		6
		Results
		Table 6 shows the GAP scores for LS-SE, LS-CIC
		and CEFR-LS datasets
	</Abstractive Summary>
</Paper ID=ument890>


<Paper ID=ument890> <Table ID =8>
	<Abstractive Summary> =
		Table 8: Example outputs of each method
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 lists the results where each row shows
		a ranking of substitution candidates by compared
		methods
	</Extractive Summary>
</Paper ID=ument890>


<Paper ID=ument891> <Table ID =1>
	<Abstractive Summary> =
		38
		Table 1: Statistics on the French side of the corpora used in our experiments
	</Abstractive Summary>
	<Extractive Summary> =
		OpenTest, for which Table 1 reports some statis-
		tics
	</Extractive Summary>
</Paper ID=ument891>


<Paper ID=ument891> <Table ID =2>
	<Abstractive Summary> =
		5†
		Table 2: BLEU score results for our two benchmark models for the different train-test combinations
	</Abstractive Summary>
</Paper ID=ument891>


<Paper ID=ument891> <Table ID =3>
	<Abstractive Summary> =
		Table 3: BLEU score results for our three benchmark models on normalized test sets
	</Abstractive Summary>
	<Extractive Summary> =
		BLEU scores for our normalized test sets are
		reported in Table 3a and Table 3b, for the G2P
		and Espeak phonetizers
	</Extractive Summary>
	<Extractive Summary> =
		Additionally,
		these results could be regarded as evidence support-
		ing that our proposed method performs generally
		better for short sentences, as observed in Table 3
		results’ discussion
	</Extractive Summary>
</Paper ID=ument891>


<Paper ID=ument891> <Table ID =4>
	<Abstractive Summary> =
		4
		Table 4: BLEU score results comparison on the
		MTNT and Cr#pbank blind test sets
	</Abstractive Summary>
	<Extractive Summary> =
		These results are displayed in Table 4,
		we also show the performance of the (Michel and
		Neubig, 2018)’s baseline system on such test sets
	</Extractive Summary>
</Paper ID=ument891>


<Paper ID=ument891> <Table ID =5>
	<Abstractive Summary> =
		87
		16
		15
		13
		12
		11
		8
		7
		6
		6
		Table 5: Most frequent normalization replacements on the Cr#pbank test corpus
	</Abstractive Summary>
</Paper ID=ument891>


<Paper ID=ument891> <Table ID =6>
	<Abstractive Summary> =
		we talk about it or not WWW44
		Table 6: Examples from our noisy UGC corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 reports some examples of the output of
		our method along with their translation before and
		after correction
	</Extractive Summary>
</Paper ID=ument891>


<Paper ID=ument892> <Table ID =1>
	<Abstractive Summary> =
		919
		Table 1: Data Set Detail
		Dhar et al
	</Abstractive Summary>
	<Extractive Summary> =
		37, means that the level of
		mixing between languages in the data is quite high
		(see Table 1 for detail)
		2https://en
	</Extractive Summary>
</Paper ID=ument892>


<Paper ID=ument892> <Table ID =2>
	<Abstractive Summary> =
		”jobnya” (in English: ”the job”)
		Table 2: Type of OOV Tokens
		3
	</Abstractive Summary>
</Paper ID=ument892>


<Paper ID=ument892> <Table ID =3>
	<Abstractive Summary> =
		14
		Table 3: Language Identiﬁcation Experiment Result
		For evaluation of lexical normalization, we
		conduct a number of scenario
	</Abstractive Summary>
	<Extractive Summary> =
		44 (the detail is in Table 3)
		Language
		prec
		recall
		F1-score
		en
		89
	</Extractive Summary>
</Paper ID=ument892>


<Paper ID=ument892> <Table ID =4>
	<Abstractive Summary> =
		50
		Table 4: Lexical Normalization Experimental Result
		Moreover, we investigate the errors by draw-
		ing sample of misclassiﬁed cases
	</Abstractive Summary>
	<Extractive Summary> =
		See Table 4) for detail
	</Extractive Summary>
</Paper ID=ument892>


<Paper ID=ument892> <Table ID =5>
	<Abstractive Summary> =
		50
		Table 5: Translation Experimental Result
		(BLEU: higher is better, WER: lower is better)
		As integration of aforementioned modules, we
		evaluate the pipeline model by conducting four
		experiments, 1) comparing raw tweets with ﬁnal
		tweets, 2) comparing raw tweets which have been
		translated (without MLF model) into Indonesian
		with ﬁnal tweets, 3) comparing raw tweets which
		423
		have been translated (with MLF Model) into In-
		donesian with ﬁnal tweets, and 4) comparing raw
		tweets which have been normalized and trans-
		lated (with MLF Model) into Indonesian with ﬁnal
		tweets
	</Abstractive Summary>
</Paper ID=ument892>


<Paper ID=ument892> <Table ID =6>
	<Abstractive Summary> =
		89
		Table 6: Pipeline Experiment Result
		(BLEU: higher is better, WER: lower is better)
		6
		Conclusion and Future Work
		In this paper, we have proposed a pipeline model
		comprising of four modules, i
	</Abstractive Summary>
	<Extractive Summary> =
		From
		Table 6, we can see that each module affects posi-
		tively toward the performance of the pipeline
	</Extractive Summary>
</Paper ID=ument892>


<Paper ID=ument893> <Table ID =1>
	<Abstractive Summary> =
		428
		Sample of detected neologisms
		politics
		pizzagate, drumpf, trumpster, shillary, killary
		news
		antifa, brexit, drumpf, Libruls, redpilling, neonazi
		worldnews
		burkini, brexit, pizzagate, edgelord, petrodollar
		sports
		deﬂategate, handegg, ballboy, skurﬁng, playstyle
		movies
		plothole, stuckmannized, jumpscare, MetaHuman
		gaming
		playerbase, pokestop, jumpscare, hitscan
		Table 1: Subreddit-level detected neologisms
		Accuracy
		BLEU
		Baseline
		55
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows samples of the top detected ne-
		ologisms for each subreddit
	</Extractive Summary>
</Paper ID=ument893>


<Paper ID=ument893> <Table ID =2>
	<Abstractive Summary> =
		7
		Table 2: Evaluation of the normalization systems
		least two of the three annotators agree upon were
		selected as normalizations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the results
	</Extractive Summary>
</Paper ID=ument893>


<Paper ID=ument893> <Table ID =3>
	<Abstractive Summary> =
		Sentence
		Raw
		republicans who don’t want drumpf are voting for hilldawg
		Best system
		republicans who don’t want trump are voting for hillary
		Reference
		republicans who don’t want trump are voting for hillary
		Raw
		this is one of the biggest clickbate news outlets
		Best system
		this is one of the biggest click bait news outlets
		Reference
		this is one of the biggest click bait news outlets
		Raw
		the hillbots have gone full insanity
		Best system
		the hill bots have gone full insanity
		Reference
		the hillary bots have gone full insanity
		Table 3: Normalization examples
		neologisms/canonical-equivalents
		level)
		along
		with using BLEU score (Papineni et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents three normalization examples,
		with the raw, gold reference, and the output of our
		system
	</Extractive Summary>
</Paper ID=ument893>


<Paper ID=ument894> <Table ID =1>
	<Abstractive Summary> =
		433
		Datasets
		DemB
		IMDBs
		AphB
		Task nature
		Structured
		X
		X
		Partially structured
		X
		Language type
		Verbal
		X
		X
		Written
		X
		Lexics
		Complex
		X
		Medium
		X
		Simple
		X
		Syntax
		Complex
		X
		Medium
		X
		Simple
		X
		Table 1: Comparison of the datasets in terms of task
		nature, type of language used to collect the data, lexical
		and syntactic complexity
	</Abstractive Summary>
</Paper ID=ument894>


<Paper ID=ument894> <Table ID =2>
	<Abstractive Summary> =
		19
		Table 2: Lexical complexity and richness, and syntactic
		complexity of the three datasets
	</Abstractive Summary>
	<Extractive Summary> =
		IMDB is the most
		lexically rich dataset (see Table 2), with the high-
		est ratio of uni-, bi-, and trigrams occuring only
		once
	</Extractive Summary>
	<Extractive Summary> =
		IMDB is the most complex according to various
		measures of syntactic complexity: it has the high-
		est scores with metrics associated with length of
		production unit, amount of subordination, coordi-
		nation, and particular structures, and it also has the
		highest amount of complex sentences (sentences
		of D-level 5-7, as shown in Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		AphB has the lowest
		level of syntactic complexity, containing the high-
		est amount of the simplest sentences (D-level 0),
		and lowest scores in other subgroups of syntactic
		features (see Table 2)
	</Extractive Summary>
</Paper ID=ument894>


<Paper ID=ument894> <Table ID =3>
	<Abstractive Summary> =
		31
		Table 3: Change of feature values, per dataset and per
		level of text alterations
	</Abstractive Summary>
	<Extractive Summary> =
		Such a difference is observed in
		all three datasets individually (see Table 3)
	</Extractive Summary>
</Paper ID=ument894>


<Paper ID=ument894> <Table ID =4>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 4
		show that syntactic features have more predictive
		power than lexical features
	</Extractive Summary>
</Paper ID=ument894>


<Paper ID=ument894> <Table ID =5>
	<Abstractive Summary> =
		91
		Incorrect (AD)
		DemB
		Table 5: Examples of two features, cond entropy 3gram and C/S, their value change when text samples are mod-
		iﬁed on the level of 20%, 40% and 60%, and associated classiﬁer’s predictions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 provides examples of two
		features, one lexical and one syntactic, their value
		changes when text samples are modiﬁed, and the
		associated change of the classiﬁer’s predictions
	</Extractive Summary>
</Paper ID=ument894>


<Paper ID=ument897> <Table ID =1>
	<Abstractive Summary> =
		# tokens (De)
		320
		324
		325
		Vocabulary size (En)
		4163
		-
		-
		Vocabulary size (De)
		5425
		-
		-
		Table 1: Data statistics of RotoWire English-German
		Dataset05
		Table 1:
		Evaluation on MultiWOZ with the greedy
		sampling procedure7M
		En-Vi
		tst2012
		1,553
		34K
		tst2013
		1,268
		34K
		Table 1: Sizes of the experimental datasets08%
		Human
		Table 1: Human evaluation of ﬂuency, relevancy, and answerability
		Table 1: Story continuations conditioned on various
		control attributes generated from our framework85
		Table 1: Translation accuracy (BLEU; Papineni et al’
		Table 1:
		Example of an overly abstractive summary
		with zero bigram overlap with the document from a
		CNN/DM training sample89
		Table 1: Performance of the Transformer and RNN
		model trained synchronously and asynchronously,
		across different learning rates To
		93
		domain
		subset
		carriers
		signatures
		slots
		words
		Movies
		train
		1,382
		179
		21
		353
		dev
		622
		109
		15
		292
		test
		520
		69
		10
		254
		Live Entertainment
		train
		4269
		588
		120
		332
		dev
		1236
		244
		77
		194
		test
		1335
		271
		74
		217
		All
		train
		5,651
		767
		141
		685
		dev
		1,858
		353
		92
		486
		test
		1,855
		340
		84
		471
		Table 1: Data distribution and splits Since the initial multilingual model
		was trained on both these directions (Z→X and
		101
		Method
		Back-Translated Data
		Training Regime
		pivot from scratch
		BT-pivot
		train from scratch
		pivot ﬁne-tune
		BT-pivot
		ﬁne-tune initial model
		pivot-parallel combined
		BT-pivot + BT-parallel
		ﬁne-tune initial model
		Table 1: Summary of the proposed methods for zero-shot NMT using pivot-language monolingual data5M
		104M
		Wiki
		72M
		2086M
		News
		210M
		3657M
		Table 1: Monolingual (English) training data
		after having being trained on the source corpora66
		Table 1: �C-VAELSTM performance with C = {3, 15, 100} on the test sets of CBT, WIKI, and WebText This observation is further supported by the
		increasing pattern of active units in Table 1: Given
		that AU increases with increase of C one would
		expect that activation pattern of a latent variable
		becomes more complex as it comprises more in-
		formation
		To understand this behavior further, we need two
		additional results from Table 1: LogDetCov and
		||µ||2
		2
		Table 1: Several random input lines alongside with human written reformulation and the reformulation generated
		by the baseline81
		the host that walked through to the ta-
		ble and are quite perfect !
		Table 1: Examples showing why Acc is insufﬁcient
		Table 1:
		An example of box-score (top), line-score (middle) and the corresponding summary (bottom) from
		ROTOWIRE dataset65
		Table 1: Training and test performance
		We observe that all latent models outperform
		their deterministic counterparts (crossed curves)
		in terms of both, generalization and overall test
		performance
		Corpus
		Sentences
		Reviews
		Words (FR)
		4SQ-PE
		12 080
		8 004
		141 958
		4SQ-HT
		2 784
		1 625
		29 075
		4SQ-valid
		1 243
		765
		13 976
		4SQ-test
		1 838
		1 157
		21 525
		Table 1: 4SQ corpora5
		Table 1: Results for three language pairs3M
		Table 1: Results of the evaluation of models in translating languages with different morphological typology using
		the IWSLT data sets6
		Table 1: WMT19 English→German Document-Level
		Translation5
		Table 1: Experimental results92
		Table 1: Evaluation results on WAT17 English⇔Japanese translation task5445
		Table 1: Examples of GPT-2 generated paraphrased sentences with scores for each pair
		Peters et al
		Number of tokens (+EOS)
		139465
		Percentage of function words
		68%
		Percentage of content words
		32%
		Table 1: Percentage of function and content words in
		the generated translation2
		Random Search
		As an alternative to grid-based searches, random
		hyperparameter search has been demonstrated to
		be a strong baseline for neural network architec-
		ture searches as it can search between grid points
		to increase the size of the search space (Bergstra
		233
		Dataset
		Size
		Ara–Eng
		234k
		Fra–Eng
		235k
		Hau–Eng
		45k
		Tir–Eng
		15k
		Table 1: Number of parallel sentences in training bi-
		texts12
		Table 1: Evaluation results (BLEU) on IWSLT 2014
		De–En task, sn) with n
		words, the task is to generate an alternative output
		249
		S: What are the dumbest questions ever asked
		on Quora?
		G: what is the stupidest question on quora?
		R: What is the most stupid question asked on
		Quora?
		S: How can I lose fat without doing any aero-
		bic physical activity
		G: how can i lose weight without exercise?
		R: How can I lose weight in a month without
		doing exercise?
		S: How did Donald Trump won the 2016 USA
		presidential election?
		G: how did donald trump win the 2016 presi-
		dential
		R: How did Donald Trump become presi-
		dent?
		Table 1:
		Examples of our generated paraphrases on
		the QUORA sampled test set, where S, G, R repre-
		sents Source, Generated and Reference sentences re-
		spectively46M
		Rotowire
		3247
		Table 1: Sentence-parallel training corpora statistics
		Table 1: Example of data-records (left) and document summary (right) from the ROTOWIRE dataset
		270
		Train
		Dev
		Test
		English
		3398
		727
		728
		German
		242
		240
		241
		Table 1:
		Count of examples in Training, Devel-
		opment and Test sections of English and German
		dataset1M
		534M
		Table 1: Statistics of the allowed resources3
		Table 1: Effects of noisy backward-forward translation
		(NBFT) and Multi-Agent Dual Learning on teacher-
		student training (newstest2014)
		It seems unusual to feed our student with de-
		graded training data, but the goal is to closely
		mimic the teacher09
		–
		Table 1: Machine translation results measured with
		sacre-BLEU and task-speciﬁc tokenization10
		Table 1: Comparison of BLEU scores and model sizes on newstest2014 and newstest2015
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the statistics of the obtained dataset
	</Extractive Summary>
	<Extractive Summary> =
		Interestingly, each GPU system by the
		Marian team shares almost the same amount of
		GPU memory as shown in Table 12 and Figure
		2(b)
	</Extractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
	<Extractive Summary> =
		320
		11
		Table 10: Time consumption and MT evaluation metrics (GPU systems)
	</Extractive Summary>
	<Extractive Summary> =
		971
		12
		Table 11: Peak memory consumption (CPU systems)
	</Extractive Summary>
	<Extractive Summary> =
		09
		13
		Table 12: Peak memory consumption (GPU systems)
	</Extractive Summary>
	<Extractive Summary> =
		The results are summarized in Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows several examples of con-
		trol attributes and generated continuations corre-
		sponding to them from our model
	</Extractive Summary>
	<Extractive Summary> =
		Table 10 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		Human Preference
		system 1
		system 2
		1
		2
		neither
		BS
		TS
		43
		16
		29
		BS
		reranking
		18
		30
		15
		BS
		predicted
		38
		29
		19
		TS
		reranking
		18
		38
		16
		TS
		predicted
		18
		27
		34
		reranking
		predicted
		27
		24
		21
		Table 10: Human preferences when given three contin-
		uations from each pair of systems
	</Extractive Summary>
	<Extractive Summary> =
		Figure 3, Table 11, and Table
		12 show the frequency (%) of matching the target
		control variable for length, predicates, and frames,
		respectively
	</Extractive Summary>
	<Extractive Summary> =
		05
		Table 11: Frequency (%) of the generated continuations containing the desired predicate
	</Extractive Summary>
	<Extractive Summary> =
		3
		Table 12: Frequency (%) of the generated continuations containing the desired frame
	</Extractive Summary>
	<Extractive Summary> =
		1
		Adapting Between Domains
		The ﬁrst 6 result columns of Table 1 show the ex-
		perimental results on the OPUS dataset
	</Extractive Summary>
	<Extractive Summary> =
		2
		Adapting from a General Domain to a
		Speciﬁc Domain
		The last two result columns of Table 1 show the
		experimental results in the WMT-TED setting
	</Extractive Summary>
	<Extractive Summary> =
		Results in Table 1 conﬁrm that asynchronous
		SGD generally yields lower-quality systems than
		synchronous SGD
	</Extractive Summary>
	<Extractive Summary> =
		These were near the top in both
		asynchronous and synchronous settings (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		With staleness 3,
		the model stayed at 0 BLEU for both synchronous
		or asynchronous SGD, consistent with our earlier
		result (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Generalization Across Learning Rates
		Earlier in Table 1 we show that asynchronous
		Transformer learning is very sensitive towards the
		learning rate
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the data splits for the movie, live
		entertainment and ‘all’ datasets, the latter contain-
		ing both movies and live entertainment data, in-
		cluding the number of signatures, slot types and
		unique non-slot words in each set
	</Extractive Summary>
	<Extractive Summary> =
		We use the same data as for the data
		generation experiments (see Table 1), and group
		our class labels into intents (as opposed to signa-
		tures), which leads to classifying 136 intents in the
		combined movies and entertainment data (‘all’)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the three proposed methods
		for zero-shot NMT
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes
		the statistics of these three monolingual corpora
	</Extractive Summary>
	<Extractive Summary> =
		Addi-
		tionally, as reported in Table 1, encouraging higher
		rates (via larger C) encourages more active units
		5We attribute the difference in performance across our
		models to the non-optimal selection of training hyperparam-
		eters, and corpus speciﬁc factors such as sentence length
	</Extractive Summary>
	<Extractive Summary> =
		The above observation is better pronounced in
		Table 1, where we also report the mean (||µ||2
		2)
		of unbiased samples of z, highlighting the diver-
		gence from the mean of the prior distribution as
		rate increases
	</Extractive Summary>
	<Extractive Summary> =
		We generated synthetic corpora using trained
		models from Table 1 with different C and decod-
		ing schemes and using the same exact z samples
		for all corpora
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 illustrates this tendency
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows ex-
		amples of transferred sentences at several points
		in training the model of Shen et al
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 illus-
		trates an example of data-to-text NLG, with statis-
		tics of a NBA basketball game (top) and the corre-
		sponding game summary (bottom)
	</Extractive Summary>
	<Extractive Summary> =
		(2017) investigates
		different data-to-text generation approaches and
		introduces a new corpus (ROTOWIRE, see Table 1)
		for the data-to-text generation task along with
		a series of automatic measures for the content-
		oriented evaluation
	</Extractive Summary>
	<Extractive Summary> =
		Input of the model consists of a table of records
		(see Table 1, top and middle)
	</Extractive Summary>
	<Extractive Summary> =
		The output t (see Table 1, bottom) is a text
		document which is a descriptive summary for the
		record set s
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 il-
		lustrates an example of the dataset
	</Extractive Summary>
	<Extractive Summary> =
		5k French reviews from 4SQ,
		mostly in the food category,3 split them into sen-
		tences (18k), and grouped them into train, valid
		and test sets (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		20
		Table 10 shows the accuracy of our models when
		translating these words
	</Extractive Summary>
	<Extractive Summary> =
		174
		Model
		cadre
		cuisine
		carte
		Total
		Total (source)
		23
		32
		29
		100%
		WMT
		13
		17
		14
		52%
		UGC (Inline case)
		22
		27
		18
		80%
		UGC ⊕ PE + tags
		23
		31
		29
		99%
		Table 10: Number of correct translations for difficult
		polysemous words in 4SQ-test by different models
	</Extractive Summary>
	<Extractive Summary> =
		3
		Table 11: Indirect evaluation with Aspect-Based Senti-
		ment Analysis (accuracy in %)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 11, translations obtained with
		domain-adapted models lead to significantly better
		scores on the ABSA task than the generic models
	</Extractive Summary>
	<Extractive Summary> =
		We got 12 an-
		Pairs
		Win
		Tie
		Loss
		Tags ≈ Tags + noise
		82
		453
		63
		Tags ≫ Baseline
		187
		337
		74
		Tags ≫ GT
		226
		302
		70
		Tags + noise ≫ Baseline
		178
		232
		97
		Tags + noise ≫ GT
		218
		315
		65
		Baseline ≫ GT
		173
		302
		123
		Table 12: In-house human evaluation (“≫” means bet-
		ter with p ≤ 0
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results and Analysis
		Table 1 reports the results for baselines and the
		proposed method4
	</Extractive Summary>
	<Extractive Summary> =
		6
		Results
		The results of the experiments given in Table 1 show that
		the hierarchical decoder can reach performance comparable
		to or better than the NMT model based on subword units in
		all languages while using almost three times less number of
		parameters
	</Extractive Summary>
	<Extractive Summary> =
		5
		BLEU) and statistically signiﬁcant gains over
		vMF on both datasets, although there is no con-
		sistent winner among the two syn-margin vari-
		ants (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results
		In Table 1 and Table 2, we present the experi-
		ment results measured by BLEU on WAT17 and
		IWSLT14
	</Extractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 1 shows the percentage of function and con-
		tent words generated by the trained model
	</Extractive Summary>
	<Extractive Summary> =
		4,5
		Table 1 summarises the number of sentences of
		each corpus in the pre-processed sentence-parallel
		dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 illus-
		trates an example of data-to-text NLG, with statis-
		tics of a NBA basketball game (top) and the corre-
		sponding game summary (bottom)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 illustrates an ex-
		ample of the dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 il-
		lustrates an example of the dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 describes the al-
		lowed parallel and monolingual corpora
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 contains BLEU scores of the teacher
		ensemble (T) and a student model distilled from
		this teacher (Student ← T)
	</Extractive Summary>
	<Extractive Summary> =
		9 BLEU (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Results
		Table 1 shows the results of our systems for both
		directions measured with sacre-BLEU
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =2>
	<Abstractive Summary> =
		31
		C
		Table 2: Results on the NLG: (Data → En) track of DGT task55
		Table 2: Evaluation on MultiWOZ with the nucleus
		sampling procedure0 (ﬁxed)
		Table 2: Hyperparameter settings3
		Table 2: Our proposed model MSQGsharedh,rmrep sig-
		niﬁcantly outperforms baselines, based on the auto-
		mated retrieval statistics
		Table 2: Generated continuations from our framework
		with different control attribute values79
		Table 2: Performance of DDA-Deep when fusing dif-
		ferent parts of models on the law and medical datasets4
		Table 2: ROUGE-F1 (%) scores of manually crafted
		extractive trainers for producing sentence-level extrac-
		tive labels for CNN/DM000
		Table 2: The Adam optimizer slows down when gradients have larger variance even if they have the same average,
		in this case 1 An example
		appears in Table 2: updating with 1 moves faster
		than individually applying -1 and 2 ‘All’ contains the combined Movie and Entertainment live datasets
		domain
		subset
		carriers
		intents
		All
		train
		5651
		136
		dev
		1858
		101
		train+dev
		7509
		136
		test
		1855
		94
		Movies
		test
		520
		37
		Table 2: Data distribution and splits for the extrinsic
		task The
		data comes from the WMT16 news translation
		102
		Corpus
		Sentences
		EN↔DE
		4 497 878
		EN↔RU
		2 500 502
		EN monolingual
		1 000 000
		Table 2: Number of sentences in each training corpus
		for the DE↔RU experiments5
		Table 2: In/Out of Domain test sets
		Table 2: Homotopy (CBT corpus) - The three blocks correspond to C = {3, 15, 100} values used for training
		�C-VAELSTM
		Table 2: Reformulations generated by the baseline with additional discriminator, shifted autoencoder and shifted
		autoencoder with additional discriminator corresponding to the inputs in Table 18
		Table 2: Yelp results with various systems and auto-
		matic metrics at a nearly-ﬁxed Acc, with best scores in
		boldface22
		Table 2: Automatic evaluation on ROTOWIRE devel-
		opment set using relation generation (RG) count (#)
		and precision (P%), content selection (CS) precision
		(P%) and recall (R%), content ordering (CO) in nor-
		malized Damerau-Levenshtein distance (DLD%), and
		BLEU
		Uppercase
		Lowercase
		Input
		UNE HONTE !
		une honte !
		Pre-proc
		UN E _H ON TE _!
		une _honte _!
		MT output
		A _H ON E Y !
		A _dis gra ce !
		Post-proc
		A HONEY!
		A disgrace!
		Table 2: Capital letters break NMT17
		Table 2: Results of the evaluation of models in capturing morphological variations in the output using the Morphe-
		val English-German test set
		Table 2: An example where the Insertion Transformer gets confused with sentence alignment: it maps one sentence
		from the source into two sentences in the translation and loses semantic accuracy6
		Table 2: Error margins and accuracies41††
		Table 2: Evaluation results on IWSLT14 De-En5762
		Table 2: Average USE, ROUGE-L, BLEU Scores of
		the datasets
		4
		Experiments
		We implemented the system described above us-
		ing GPT-2 and trained it on the different datasets
		for various lengths of training5762
		Table 2: Average USE, ROUGE-L, BLEU Scores of
		the datasets
		4
		Experiments
		We implemented the system described above us-
		ing GPT-2 and trained it on the different datasets
		for various lengths of training
		Method
		% for FWs
		% for CWs
		1
		RandomPermute
		33%
		6%
		2
		Uniform
		53%
		11%
		3
		ZeroOutMax
		52%
		15%
		4
		Aggregate(1+2+3)
		68%
		21%
		5
		ZeroOut
		9%
		0%
		6
		LastEncoderState
		20%
		2%
		7
		OnlyMax
		71%
		83%
		8
		KeepMaxUniformOthers
		86%
		86%
		Table 2: Percentage of the preserved function and content
		words in the proposed attention methods: Trying out all the
		methods to ﬁnd a counterfactual attention vector maximizes
		the chance of success5k
		Table 2: Comparison of BLEU scores, model size, and training time on Tigrinya-English, Hausa-English, and
		French-English61
		Table 2: Evaluation results (BLEU) on WMT 2017 Tr–
		En task
		Table 2:
		Examples of our generated paraphrases on
		the MSCOCO sampled test set, where S, G, R repre-
		sents Source, Generated and Reference sentences re-
		spectively71
		Table 2: BLEU scores for the Transformer vs22
		Table 2: Automatic evaluation on ROTOWIRE devel-
		opment set using relation generation (RG) count (#)
		and precision (P%), content selection (CS) precision
		(P%) and recall (R%), content ordering (CO) in nor-
		malized Damerau-Levenshtein distance (DLD%), and
		BLEU95
		Table 2:
		Automatic evaluation for track 1/2 on
		the ROTOWIRE test set using record generation
		(RG) precision, content selection (CS) precision
		and recall, content ordering (CO) in normalized
		Damerau-Levenshtein distance, and BLEU
		Table 2: Metadata: our metadata encoding84)
		Table 2: Relative speed-up for new CPU-bound op-
		timizations compared to ﬂoat32 MKL baseline and
		WNMT2018 mixed precision inference (in parenthe-
		ses) for same SSRU-Tied student model1
		Table 2: Content evaluation of the German-to-English translation models on test and dev-sets from parallel Ro-
		towire using the IE models of (Puduppully et al
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		We show the results for each track in Table 2
		through 7
	</Extractive Summary>
	<Extractive Summary> =
		Hyperparameters:
		Table 2 summarizes the hy-
		perparameter settings
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows
		examples of generated continuations for a single
		story context with several values for the control at-
		tributes described below
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2, we
		show examples where noise causes Adam to slow
		down
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the generated sentences via dif-
		ferent decoding schemes for each channel capac-
		ity
	</Extractive Summary>
	<Extractive Summary> =
		Depending on which model is being trained (see
		Table 2), the λi’s for the unused losses will be
		zero
	</Extractive Summary>
	<Extractive Summary> =
		Depending on which model is being
		trained (see Table 2), the λi’s for the unused losses
		will be zero
	</Extractive Summary>
	<Extractive Summary> =
		1
		Analyzing Metric Relationships
		Table 2 shows results for the Yelp dataset and Fig-
		ure 1 plots learning trajectories of those models
	</Extractive Summary>
	<Extractive Summary> =
		2
		Table 4: Manual evaluation results (%) using models from Table 2 (i
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 and Figure 1 show that under
		similar Acc, M2 has much better semantic simi-
		larity for both Yelp and Literature
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows that the model
		with paraphrase loss (M1) slightly improves Sim
		over M0 on both datasets under similar Acc
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2,
		we can see that the RG and CS precisions are both
		improved by 2
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, as expected,
		the model after ﬁne-tuning generates more rela-
		tions in the output summaries
	</Extractive Summary>
	<Extractive Summary> =
		As shown in
		Table 2, all content-oriented evaluation metrics
		are improved compared to DATA-TRANS but not
		as much as each single of the data augmentation
		method
	</Extractive Summary>
	<Extractive Summary> =
		2
		Capital letters
		As shown in Table 2, capital letters are another
		source of confusion
	</Extractive Summary>
	<Extractive Summary> =
		17The “LC to cased” and “Noised case” models are not able
		to preserve capital letters for emphasis (as in Table 2), and the
		“Cased” model often breaks on such examples
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 lists the per-
		formance of different NMT models implementing decoding
		at the level of subwords, characters, or hierarchical word-
		character units in capturing variances in each individual mor-
		phological paradigm and preserving the agreement between
		inﬂected words and their dependent lexical items
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows such
		an example where Big BIRD captures alignment
		196
		Source:
		(
	</Extractive Summary>
	<Extractive Summary> =
		This is accompanied by increases
		in accuracy @2, @5 and @10 (Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results
		In Table 1 and Table 2, we present the experi-
		ment results measured by BLEU on WAT17 and
		IWSLT14
	</Extractive Summary>
	<Extractive Summary> =
		By calculating USE, ROUGE-L and BLEU
		scores for each dataset we are able to quantify the
		quality of human-generated paraphrases and then
		use that as a comparison for the models generated
		sentences (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		By calculating USE, ROUGE-L and BLEU
		scores for each dataset we are able to quantify the
		quality of human-generated paraphrases and then
		use that as a comparison for the models generated
		sentences (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Effectiveness of the proposed
		counterfactual attention methods
		Table 2 shows the percentage of function and
		content words for which counterfactual attention
		weights were found using the proposed attention
		methods
	</Extractive Summary>
	<Extractive Summary> =
		As evident from
		Table 2, this approach greatly increases the chance
		of ﬁnding a counterfactual attention
	</Extractive Summary>
	<Extractive Summary> =
		2
		Function words are more easily
		generated compared to content words
		An important observation in Table 2 is that the
		proposed methods are considerably more effective
		in preserving function words compared to con-
		tent words
	</Extractive Summary>
	<Extractive Summary> =
		Row 6 in Table 2 shows that there is a signiﬁ-
		cant gap between the LastEncoderState and
		the counterfactual methods proposed in Table 2
	</Extractive Summary>
	<Extractive Summary> =
		Random Search
		Table 2 compares the performance of random
		search with auto-sizing across, BLEU scores,
		model size, and training times
	</Extractive Summary>
	<Extractive Summary> =
		2
		English→German
		It can be seen from Table 2 that for all runs, the
		document-level models outperform the sentence-
		level baseline trained with 4 times the data
	</Extractive Summary>
	<Extractive Summary> =
		From Table 2,
		we can see that the RG and CS precisions are both
		improved by 2
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, as expected,
		the model after ﬁne-tuning generates more rela-
		tions in the output summaries
	</Extractive Summary>
	<Extractive Summary> =
		As shown in
		Table 2, all content-oriented evaluation metrics
		are improved compared to DATA-TRANS but not
		as much as each single of the data augmentation
		method
	</Extractive Summary>
	<Extractive Summary> =
		7
		Results
		Table 2 shows our results for English and German
		datasets on the Test set as provided by the shared
		task organizers
	</Extractive Summary>
	<Extractive Summary> =
		4
		Qualitative evaluation
		As shown in Table 2,
		the NLG model (3-
		player) has several good properties besides coher-
		ent document-level generation and the ability to
		“copy” metadata
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2 we see a respectable speed-up against
		a pure MKL ﬂoat32 version and our mixed 32/16-
		bit inference (in parentheses)
	</Extractive Summary>
	<Extractive Summary> =
		The combined
		speed-up (Table 2) from these optimizations further
		improves on top of the “fancier” methods
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the
		values for development and test sets
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =3>
	<Abstractive Summary> =
		84
		C
		Table 3: Results on the NLG: (Data → De) track of DGT task This
		Model 1
		vs
		Model 2
		GPT
		59 %
		41%
		Baseline
		GPT
		46 %
		54 %
		Target
		GPT2
		46 %
		54 %
		Target
		GPT2
		45 %
		55 %
		Baseline
		Baseline
		43 %
		57 %
		Target
		GPT2
		51 %
		49 %
		GPT
		Table 3: Human ranking of responses between all pairs
		of four analyzed models and the original responses0 × 10−4
		N/A because training did not converge
		Table 3: Results of the system comparison3
		Table 3: Full results, comparing models constructed
		with M256, M512, and Mattn
		256 2
		Table 3: Sentiment match percentages of generated
		continuations and target sentiment values We ﬁrst deﬁne
		Source
		warum wurde Ab- ili- fy zugelassen ?
		Reference
		why has Ab- ili- fy been approved ?
		Baseline
		reasons was received why a reminder
		was accepted ?
		DDA-Shallow
		why has been approved?
		Copy
		why ,
		DDA-Deep
		why was Ab- ili- fy authorised ?
		Table 3: Translation examples under the law to medi-
		cal adaptation setting4
		Table 3: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various extractive models on the CNN/DM
		test set5
		Table 3: Quality and convergence of asynchronous SGD with accumulated gradients on English to German dataset00
		Table 3: Best performance per metric for each model when applied to ‘all’ domains1
		Table 3:
		BLEU scores for the initial multilingual models and zero-resource models without monolingual data,
		for the baselines with pivot monolingual data, and for our proposed zero-resource models with pivot monolingual
		data
		Table 3: Example of a poor translation when adding unknown token to source sentences (translation done with a
		baseline transformer model)
		news14
		news18
		iwslt15
		wiki
		kde
		OpenSub
		Baseline
		271
		-
		Table 3: Forward Cross Entropy (FCE) To
		understand this further, we report additional statis-
		tics in Table 3: percentage of 〈unk〉 symbols, self-
		BLEU and average sentence length in the corpus8
		Table 3: Literature results with various systems and au-
		tomatic metrics at a nearly-ﬁxed Acc, with best scores
		in boldface14
		Table 3: Automatic evaluation on ROTOWIRE test set39M
		1 125M
		1 041M
		Table 3: Size of the WMT and UGC training corpora
		(after filtering)19
		Table 3: Experiment results in the English-to-German
		direction with WMT data sets97
		Table 3: Results on IWSLT14 De-En and WAT17 Ja-En for effectiveness of learning word order824
		Table 3: Showing Candidates Selection and Scoring - *Selected Sentence
		and phrase selection to the original sentence to be
		useful paraphrases824
		Table 3: Showing Candidates Selection and Scoring - *Selected Sentence
		and phrase selection to the original sentence to be
		useful paraphrases
		In Table 4, we sort such tokens based on their
		coverage, which is the percentage of their total
		Token
		# preserved
		Coverage
		going
		310
		70%
		people
		237
		46%
		know
		219
		62%
		world
		215
		67%
		like
		189
		47%
		think
		176
		50%
		way
		162
		68%
		get
		160
		53%
		thing
		147
		79%
		things
		142
		56%
		time
		139
		54%
		see
		137
		51%
		years
		136
		64%
		make
		126
		49%
		little
		113
		55%
		just
		109
		29%
		really
		93
		37%
		bit
		92
		88%
		said
		89
		59%
		got
		86
		59%
		Table 3: Top 20 content words preserved by the aggre-
		gate method sorted by the number of times they were
		preserved1k
		Table 3: Overall training times in seconds on a Nvidia
		GeForce GTX 1080Ti GPU for small regularization
		values98
		Table 3: Training efﬁciency results on IWSLT 2014
		De–En dataset54
		Table 3: Performance of our model against various models on the QUORA dataset with 50k,100k,150k training
		examples62
		Table 3: BLEU scores for the Transformer vs14
		Table 3: Automatic evaluation on ROTOWIRE test set47
		Table 3:
		Size (number of parallel training sen-
		tences) in 100,000 of the EN-DE training data2
		Table 3: Doc-level BLEU scores on the DGT valid and
		test sets of our submitted models in all tracks5
		Table 3: Conﬁguration of student models and submissions70
		Table 3: Evaluation of Content Selection and Plan-
		ning (CSP) module, with and without joint training
		with the Text Generator (TG)
	</Abstractive Summary>
	<Extractive Summary> =
		The results are summarized in Table 3, while
		some example dialogues with responses are pro-
		vided in Figure 3
	</Extractive Summary>
	<Extractive Summary> =
		Note that zero epochs of decoder training
		(the top line) mean that ﬁne-tuning was directly
		applied without decoder training (the same as in
		Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		Considering the
		score of newstest2015 in the experiment in
		Table 3 was +1
	</Extractive Summary>
	<Extractive Summary> =
		6
		Results
		Table 3 shows the mean retrieval statistics and
		their proportion of unique generated questions
		from 55,065 10-passage instances
	</Extractive Summary>
	<Extractive Summary> =
		Supplementary Material
		A
		Full Experiment Results
		Table 3 shows the retrieval results on a larger set
		of baselines and MSQG models
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the sentiment distribution
		for each label
	</Extractive Summary>
	<Extractive Summary> =
		We also we sample some translation results and
		show them in Table 3 to qualitatively demonstrate
		the differences between the methods
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 and 5 presents extractive and ab-
		stractive results on the CNN/DM dataset respec-
		tively, while Tables 4 and 6 for the Newsroom
		dataset
	</Extractive Summary>
	<Extractive Summary> =
		Those
		applicable are shown in Table 3, where ‘para’, ‘re-
		con’, ‘prior’ and ‘post’ denote paraphrasing, recon-
		struction, prior and posterior respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents the intrinsic evaluation results,
		where generators are trained and tested on ‘all’ data,
		for the best performing model per case, tuned on
		the dev set
	</Extractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 3 shows translation performance (as es-
		timated by BLEU score) for our main experi-
		ments
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 gives an ex-
		ample of such translations for baseline model7
	</Extractive Summary>
	<Extractive Summary> =
		In Table 3 we
		report the average FCE (NLL) for the generated
		corpora
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows results for the Literature dataset
	</Extractive Summary>
	<Extractive Summary> =
		1
		Training data
		After some initial work with the WMT 2014 data,
		we built a new training corpus named UGC (User
		Generated Content), closer to our domain, by
		combining: Multi UN, OpenSubtitles, Wikipedia,
		Books, Tatoeba, TED talks, ParaCrawl11 and
		Gourmet12 (See Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		5 (see
		Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		When we further compared the results of unﬁl-
		tered examples generated by the model (Table 3)
		we observe that when the USE score is below 0
	</Extractive Summary>
	<Extractive Summary> =
		When we further compared the results of unﬁl-
		tered examples generated by the model (Table 3)
		we observe that when the USE score is below 0
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 contains the top 20 content words sorted
		by the number of times they were preserved
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the total training time for
		both ℓ2,1 = 0
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows that CoNMT and PCoNMT can
		process 28 times faster than NPMT, and converge
		six times faster, i
	</Extractive Summary>
	<Extractive Summary> =
		3
		German→English
		From Table 3, we see that the document-level
		models again outperform the sentence-level Trans-
		former baseline for all runs by a wider margin
		than for English→German
	</Extractive Summary>
	<Extractive Summary> =
		1
		In-Domain Parallel Data
		Table 3 highlights the extremely limited amount of
		in-domain parallel training data used; ROTOWIRE
		English-German makes up only 0
	</Extractive Summary>
	<Extractive Summary> =
		The scores are shown in Table 3, and
		a description of the submitted models is given in
		Table 4
	</Extractive Summary>
	<Extractive Summary> =
		Until this moment, we kept model dimensions
		and decoder depth constant while optimizing a
		conﬁguration that corresponds to the Microsoft in-
		production models (bold row in Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		The ﬁle sizes reported in Table 3 refer to pre-
		packed 8-bit models with 32-bit embeddings for
		the CPU, and to models stored in FP16 for the GPU
	</Extractive Summary>
	<Extractive Summary> =
		Re-
		sults in Table 3 show that our model outperforms
		the baseline (Puduppully et al
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =4>
	<Abstractive Summary> =
		91
		C
		Table 4: DGT results on the MT track (De → En)89
		Decoder training converged
		Baseline for statistical testing
		Table 4: Changes in the perplexity of the development set (Dev9
		Table 4: Frequency (%) of the generated continuations
		in the range of dif = |l−lp| where l is the continuation
		length and lp is the desired length22
		Table 4: Performance of ensembling different LMs on
		the law and medical datasets96
		Table 4: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various extractive models on the Newsroom
		released test set41
		Table 4: The effect of global accumulation on translation quality for different language pairs on development and
		test set, measured with BLEU score1%
		Table 4: Relative change when adding more training data for generator training (movies only vs ‘all’) across
		evaluation metrics on the movies test set
		02
		Table 4: BLEU scores for the proposed models on the
		test set (newstest2015)8
		Table 4: FT: initialize NMT encoder with BERT and ﬁnetune; Freeze: ﬁx NMT encoder parameters to BERT
		parameters; Emb: ﬁx encoder embeddding layer with BERT contextual word embeddings02
		Table 4: p1: p(x�|z+) < p(x+|z+) and p2: p(x�|z�) < p(x+|z�); ¯p1: p(x�|¯z+) < p(x+|¯z+) and ¯p2:
		p(x�|¯z�) < p(x+|¯z�); �C=3-VAELSTM (D:103, R:3); �C=100-VAELSTM (D:39, R:101)2
		Table 4: Manual evaluation results (%) using models from Table 2 (i24
		Table 4: Ablation results on ROTOWIRE dev set63
		Table 4: Robustness to capital letters (see Section 4
		Table 4: Example translations with different approaches in Turkish
		the training data, where the subword-based model obtains the
		best accuracy, followed by the hierarchical model, and the
		character-level NMT model obtains signiﬁcantly lower accu-
		racy compared to both approaches95
		Table 4: Results of ablation experiments on IWSLT14
		De-En
		Token
		Coverage
		Total
		bit
		88%
		105
		course
		87%
		91
		thank
		83%
		89
		thing
		79%
		186
		fact
		78%
		74
		half
		78%
		27
		own
		75%
		75
		ones
		73%
		30
		states
		73%
		30
		difference
		71%
		21
		going
		70%
		444
		turns
		69%
		26
		way
		68%
		237
		able
		67%
		85
		world
		67%
		323
		doing
		66%
		103
		planet
		65%
		37
		years
		64%
		212
		know
		62%
		353
		united
		62%
		21
		Table 4: Top 20 content words preserved by the aggre-
		gate method sorted by percentage of their total occur-
		rences that are preserved (coverage)0
		100%
		Table 4: BLEU scores and percentage of parameter rows deleted by auto-sizing on various sub-components of the
		model, across varying strengths of ℓ2,1 regularization13
		Table 4: The Precision, Recall, and F1 evaluation re-
		sults on the fertility prediction of Fertility23
		Table 4: Performance of our model against various models on the MSCOCO dataset39
		Table 4: BLEU scores for submitted systems48
		Table 4: Track 3-6: ROTOWIRE dev set results,
		showing BLEU without monolingual data Parallel
		and with monolingual data Monolingual7
		BLEU improvement on Rotowire-test by our En-
		glish NLG model compared to the previous state
		276
		Track
		N best players
		Details
		NLG (EN)
		4
		Rotowire BT + DGT-train + tags
		NLG (DE)
		6
		Rotowire BT + DGT-train + tags
		MT (DE-EN)
		N/A
		Unconstrained: Rotowire BT +
		DGT-train + tags + ensemble
		Constrained: DGT-train only +
		ensemble
		MT (EN-DE)
		N/A
		DGT-train only + ensemble
		MT+NLG (EN)
		3
		Rotowire BT + DGT-train + 20%
		text masking + tags + ensemble
		MT+NLG (DE)
		3
		Rotowire BT + DGT-train +
		tags + ensemble
		Table 4: Description of our submissions27
		Table 4: Generation results of our submitted systems as reported by the shared task organizers (Hayashi et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results after ﬁne-tuning when
		decoder training was stopped after various num-
		bers of epochs
	</Extractive Summary>
	<Extractive Summary> =
		The results are
		shown in Table 4 and demonstrate that our model
		can generate continuations with the desired length
		with only small differences
	</Extractive Summary>
	<Extractive Summary> =
		The results shown in Table 4 empirically con-
		ﬁrm that accumulating the gradient to obtain a
		85
		Transformer
		Communication
		accumulation
		batch
		avg
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4, we show intrinsic results on the
		movies test set
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the performance on the test
		dataset (newstest2015) when a second iteration of
		back-translation and training is performed
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 4 presents the results of our experiments
	</Extractive Summary>
	<Extractive Summary> =
		Then we
		condition the decoder on the z+ and try to deter-
		mine whether the decoder assigns higher probabil-
		ity to the grammatical sentence (denoted by x+):
		p(x�|z+) < p(x+|z+) (denoted by p1 in Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		We repeat the same experiment but this time try to
		determine whether the decoder, when conditioned
		on the ungrammatical code (z�), still prefers to
		assign higher probability to the grammatical sen-
		tence: p(x�|z�) < p(x+|z�) (denoted by p2 in
		Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the p1 and p2 for the �C-
		VAELSTM model trained with C = {3, 100}
	</Extractive Summary>
	<Extractive Summary> =
		The eval-
		uation results are shown in Table 4 (-CS OBJ)
	</Extractive Summary>
	<Extractive Summary> =
		From the re-
		sults in Table 4 (-REC EMB), we can see that with-
		out record embedding all content-oriented evalu-
		ation results are degraded, especially the RG pre-
		cision and CS recall
	</Extractive Summary>
	<Extractive Summary> =
		5
		BLEU evaluation
		Capital letters
		Table 4 compares the case han-
		dling techniques presented in Section 4
	</Extractive Summary>
	<Extractive Summary> =
		In order to better illustrate the differences in the outputs of
		each NMT model, we also present some sample translations
		in Table 4, obtained by translating English into Turkish using
		the NMT models trained on the TED Talks corpus
	</Extractive Summary>
	<Extractive Summary> =
		In Table 4, we sort such tokens based on their
		coverage, which is the percentage of their total
		Token
		# preserved
		Coverage
		going
		310
		70%
		people
		237
		46%
		know
		219
		62%
		world
		215
		67%
		like
		189
		47%
		think
		176
		50%
		way
		162
		68%
		get
		160
		53%
		thing
		147
		79%
		things
		142
		56%
		time
		139
		54%
		see
		137
		51%
		years
		136
		64%
		make
		126
		49%
		little
		113
		55%
		just
		109
		29%
		really
		93
		37%
		bit
		92
		88%
		said
		89
		59%
		got
		86
		59%
		Table 3: Top 20 content words preserved by the aggre-
		gate method sorted by the number of times they were
		preserved
	</Extractive Summary>
	<Extractive Summary> =
		The comparison be-
		tween Table 5 and Table 4 shows that the Fertility2
		has slighly higher F-1 score than Fertility4 in both
		datasets
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the scores as provided by
		the WNGT organisers
	</Extractive Summary>
	<Extractive Summary> =
		3
		Results
		Results on the development set in Table 4 show
		that the inclusion of monolingual data leads to
		a signiﬁcant increase in bleu (between 5 and 7
		points)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		We ﬁnd that the template rewriting approach is
		competitive with the end-to-end trained models in
		terms of content metrics (Table 4), and subjec-
		tively appears to create natural sounding genera-
		tions
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =5>
	<Abstractive Summary> =
		26
		C
		Table 5: DGT results on the MT track (En → De)
		457 (+)
		Table 5: Results of the IWSLT-2015 data85
		said
		100
		bought
		100
		Table 5: Match percentages (M%) showing fraction of
		stories for which generated continuations contain the
		desired predicate17
		Table 5: BLEU points of models after continued training on the IT development dataset with different values of
		coverage penalty β92
		Table 5: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various abstractive models on the CNN/DM
		test set20
		Table 5: Performance of the asynchronous Transformer
		on English to German with 4x Global accumulations
		(GA) across different learning rates on development set
		measured with BLEU score2
		Table 5: Robustness tests67
		Table 5: Human sentence-level validation of metrics;
		100 examples for each dataset for validating Acc; 150
		each for Sim and PP; see text for validation of GM
		Table 5:
		Example output from DATA-TRANS (right) and ablation model -REC EMB (left)69
		Table 5: Baseline model with or without natural noise
		(see Section 476
		Table 5:
		Results of different attention scope on
		IWSLT14 De-En
		3947
		60%
		of
		3003
		87%
		to
		2923
		86%
		and
		2639
		67%
		a
		2187
		65%
		that
		1936
		69%
		i
		1737
		76%
		&apos;s
		1732
		95%
		you
		1501
		72%
		it
		1497
		72%
		is
		1496
		88%
		in
		1364
		64%
		we
		1246
		64%
		they
		624
		69%
		&quot;
		620
		81%
		have
		613
		70%
		be
		582
		91%
		&apos;t
		580
		96%
		&apos;re
		542
		86%
		this
		541
		42%
		so
		531
		57%
		are
		526
		77%
		was
		514
		66%
		do
		433
		77%
		about
		417
		65%
		what
		415
		61%
		can
		400
		54%
		Table 5: Top 30 function words preserved by the aggre-
		gate method sorted by the number of times they were
		preserved3
		100%
		Table 5: BLEU scores and percentage of model deleted using auto-sizing with various l∞,1 regularization strengths15
		0%
		0
		0
		0
		Table 5: The Precision, Recall, and F1 evaluation re-
		sults on the fertility prediction of Fertility415
		Table 5:
		Automatic evaluation for track 3-6 on
		the ROTOWIRE test set using record generation
		(RG) precision, content selection (CS) precision
		and recall, content ordering (CO) in normalized
		Damerau-Levenshtein distance, and BLEU3
		Table 5: BLEU scores of the MT models at different
		stages of training, and comparison with the state of the
		art
	</Abstractive Summary>
	<Extractive Summary> =
		61), as shown in Table 5 (β = 0)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the coverage penalty
		can improve the deep adaptation method by more
		than 5 BLEU points while the baseline model can
		only be improved by 2 BLEU points
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 5, the model is now capable to
		learn on higher learning rate and yield compara-
		ble results compared to its synchronous variant
	</Extractive Summary>
	<Extractive Summary> =
		5
		Robustness analysis
		Table 5 reports BLEU scores for the noisy test sets
		(described in section 4
	</Extractive Summary>
	<Extractive Summary> =
		From Table 5, all
		validations show strong correlations on the Yelp
		dataset and reasonable correlations on Literature
	</Extractive Summary>
	<Extractive Summary> =
		An example output of -REC EMB system is
		shown in Table 5 (left)
	</Extractive Summary>
	<Extractive Summary> =
		Another problem we have observed, not only
		in Table 5 but also in other output summaries, is
		repetition and self-contradictory
	</Extractive Summary>
	<Extractive Summary> =
		In the left ex-
		ample of Table 5, it contains two sentences (in
		orange color) which are completely contradictory
		with each other
	</Extractive Summary>
	<Extractive Summary> =
		Natural noise
		Table 5 compares the baseline
		“inline case” model with the same model aug-
		mented with natural noise (Section 4
	</Extractive Summary>
	<Extractive Summary> =
		We repeat the same
		process for function words (Table 5 and Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		As evident from Table 5, we have successfully
		yielded the same token in 94% of the occurrences
		of the EOS token but with a contradictory expla-
		nation
	</Extractive Summary>
	<Extractive Summary> =
		We evaluate the prediction results on the test set
		245
		with the gold label obtained from the word align-
		ment model in Table 5 and Table 4
	</Extractive Summary>
	<Extractive Summary> =
		The comparison be-
		tween Table 5 and Table 4 shows that the Fertility2
		has slighly higher F-1 score than Fertility4 in both
		datasets
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows test set results for both En-
		glish and German target languages
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the BLEU scores
		of our MT models at different stages of training
		(sent-level, doc-level, fine-tuned), and compares
		them against one of the top contestants of the
		WMT19 news translation task (Ng et al
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =6>
	<Abstractive Summary> =
		83
		C
		Table 6: Results on the MT+NLG: (Data+En → De) track of DGT task2
		Table 6: Match percentages (M%) showing fraction of
		stories for which generated continuations contain the
		desired frame17
		Table 6: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various abstractive models on the Newsroom
		released test set9
		Table 6: IWSLT dataset results901
		Table 6: Results on Yelp sentiment transfer, where
		BLEU is between 1000 transferred sentences and hu-
		man references, and Acc is restricted to the same 1000
		sentences84
		Table 6:
		Comparison of different back-translation
		schemes (see Section 5
		Row 8 shows
		that when other source hidden states are uniformly
		combined, although the ratio of unaffected content
		words has increased by 3%, ratio of unaffected
		226
		Token
		Coverage
		Total
		&apos;t
		96%
		602
		&apos;s
		95%
		1819
		EOS
		94%
		6748
		be
		91%
		641
		is
		88%
		1707
		of
		87%
		3450
		to
		86%
		3383
		&apos;re
		86%
		631
		,
		85%
		8582
		&apos;m
		84%
		311
		been
		82%
		233
		lot
		82%
		148
		the
		82%
		6386
		&quot;
		81%
		770
		are
		77%
		679
		do
		77%
		565
		i
		76%
		2290
		who
		73%
		300
		it
		72%
		2089
		you
		72%
		2099
		have
		70%
		876
		up
		70%
		235
		they
		69%
		904
		that
		69%
		2812
		well
		67%
		153
		and
		67%
		3922
		was
		66%
		774
		were
		65%
		240
		same
		65%
		154
		a
		65%
		3369
		Table 6: Top 30 function words preserved by the aggre-
		gate method sorted by coverage4
		Table 6: Test BLEU scores for the models with the best
		dev perplexity found using random search over num-
		ber of layers and size of layers50
		Table 6: Percentages of categories of randomly sam-
		pled 100 phrases generated by PCoNMT on IWSLT
		2014 De–En test set and the accuracy of PCoNMT and
		CoNMT phrase translations, respectively2
		Table 6: English NLG comparison against state-of-the-
		art on Rotowire-test
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 6 reports the results of different sets of the
		experiments on IWSLT data
	</Extractive Summary>
	<Extractive Summary> =
		Domain adaptation
		Table 6 shows the results
		of the back-translation (BT) techniques
	</Extractive Summary>
	<Extractive Summary> =
		We repeat the same
		process for function words (Table 5 and Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		The results in Table 6 show that the most fre-
		quently generated phrases are collocations (56%)
		followed by verb phrases (28%) and compound
		nouns (16%)
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows a 5
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =7>
	<Abstractive Summary> =
		47
		C
		Table 7: Results on the MT+NLG: (Data+De → En) track of DGT task3
		Table 7: Cluster match percentages (%) for each value
		of the cluster control variable
		Table 7:
		Examples of some of our generated para-
		phrases from the CNN/DM dataset, where O, G, R
		represents Originating document sentence, our model’s
		Generated paraphrase and Reference sentences from
		the ground-truth summary respectively00
		Table 7: Domain adaptation with 4SQ-PE fine-tuning
		(FT) or corpus tags
		98%
		6526
		?
		88%
		589
		&apos;t
		86%
		602
		!
		68%
		22
		&quot;
		60%
		770
		&apos;s
		24%
		1819
		;
		24%
		63
		are
		18%
		679
		is
		18%
		1707
		Table 7:
		Top 10 unaffected function words in the
		LastEncoderState method
		PCoNMT
		I ’m full of contradictions
		Table 7: Translation output examples from CoNMT and PCoNMT systems7
		Table 7: English NLG ablation study, starting from a
		3 best player baseline (the submitted NLG model has 4
		players)
	</Abstractive Summary>
	<Extractive Summary> =
		From the results in
		Table 7, we still see controllability for most clus-
		ters; however, for target cluster 3, which is rather
		generic based on our observations, the generated
		output seems ﬂat
	</Extractive Summary>
	<Extractive Summary> =
		Table 7 show that our
		paraphrases are well formed, abstractive (e
	</Extractive Summary>
	<Extractive Summary> =
		Table 7 compares the domain adaptation tech-
		niques presented in Section 5
	</Extractive Summary>
	<Extractive Summary> =
		If we look at Table 7, we can see that the most
		covered functions words, are the words that usu-
		ally appear at the end of the sentence (e
	</Extractive Summary>
	<Extractive Summary> =
		From Table 7, we see that sorting players helps,
		but only slightly
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =8>
	<Abstractive Summary> =
		10
		Table 8: Image ﬁle sizes of submitted systems7
		Table 8: Automatic metrics for baseline system and when using oracle values for control attributes60
		Table 8: Combination of several robustness or domain
		adaptation techniques
		Season-level
		player stats (-)
		NLG: It was a season-high in points for Thomas , who ’s now averaging 17 points per game
		on the season
		Table 8: Correctly predicted information that is not explicitly in the metadata (+), or hallucinations (-)
	</Abstractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Evaluation with Oracle Attributes
		Table 8 shows automatic metric scores with oracle
		attribute values, i
	</Extractive Summary>
	<Extractive Summary> =
		19
		As shown in Table 8, these techniques can be
		combined to achieve the best results
	</Extractive Summary>
	<Extractive Summary> =
		As such, it can
		generate relevant information which is absent from
		metadata (see Table 8)
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =9>
	<Abstractive Summary> =
		10
		Table 9: Time consumption and MT evaluation metrics (CPU systems)5
		Table 9: Metric scores to evaluate the potential of a list of continuations
		French word
		Meanings
		Cadre
		setting, frame, executive
		Cuisine
		food, kitchen
		Carte
		menu, card, map
		Table 9: French polysemous words found in 4SQ, and
		translation candidates in English
	</Abstractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =10>
	<Abstractive Summary> =
		320
		11
		Table 10: Time consumption and MT evaluation metrics (GPU systems)
		Human Preference
		system 1
		system 2
		1
		2
		neither
		BS
		TS
		43
		16
		29
		BS
		reranking
		18
		30
		15
		BS
		predicted
		38
		29
		19
		TS
		reranking
		18
		38
		16
		TS
		predicted
		18
		27
		34
		reranking
		predicted
		27
		24
		21
		Table 10: Human preferences when given three contin-
		uations from each pair of systems
		174
		Model
		cadre
		cuisine
		carte
		Total
		Total (source)
		23
		32
		29
		100%
		WMT
		13
		17
		14
		52%
		UGC (Inline case)
		22
		27
		18
		80%
		UGC ⊕ PE + tags
		23
		31
		29
		99%
		Table 10: Number of correct translations for difficult
		polysemous words in 4SQ-test by different models
	</Abstractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
	<Extractive Summary> =
		Table 10 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		20
		Table 10 shows the accuracy of our models when
		translating these words
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =11>
	<Abstractive Summary> =
		971
		12
		Table 11: Peak memory consumption (CPU systems)05
		Table 11: Frequency (%) of the generated continuations containing the desired predicate3
		Table 11: Indirect evaluation with Aspect-Based Senti-
		ment Analysis (accuracy in %)
	</Abstractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
	<Extractive Summary> =
		Figure 3, Table 11, and Table
		12 show the frequency (%) of matching the target
		control variable for length, predicates, and frames,
		respectively
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 11, translations obtained with
		domain-adapted models lead to significantly better
		scores on the ABSA task than the generic models
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument897> <Table ID =12>
	<Abstractive Summary> =
		09
		13
		Table 12: Peak memory consumption (GPU systems)3
		Table 12: Frequency (%) of the generated continuations containing the desired frame
		We got 12 an-
		Pairs
		Win
		Tie
		Loss
		Tags ≈ Tags + noise
		82
		453
		63
		Tags ≫ Baseline
		187
		337
		74
		Tags ≫ GT
		226
		302
		70
		Tags + noise ≫ Baseline
		178
		232
		97
		Tags + noise ≫ GT
		218
		315
		65
		Baseline ≫ GT
		173
		302
		123
		Table 12: In-house human evaluation (“≫” means bet-
		ter with p ≤ 0
	</Abstractive Summary>
	<Extractive Summary> =
		Interestingly, each GPU system by the
		Marian team shares almost the same amount of
		GPU memory as shown in Table 12 and Figure
		2(b)
	</Extractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
</Paper ID=ument897>


<Paper ID=ument899> <Table ID =1>
	<Abstractive Summary> =
		# tokens (De)
		320
		324
		325
		Vocabulary size (En)
		4163
		-
		-
		Vocabulary size (De)
		5425
		-
		-
		Table 1: Data statistics of RotoWire English-German
		Dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows
		the statistics of the obtained dataset
	</Extractive Summary>
	<Extractive Summary> =
		Interestingly, each GPU system by the
		Marian team shares almost the same amount of
		GPU memory as shown in Table 12 and Figure
		2(b)
	</Extractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
	<Extractive Summary> =
		320
		12
		Table 10: Time consumption and MT evaluation metrics (GPU systems)
	</Extractive Summary>
	<Extractive Summary> =
		971
		13
		Table 11: Peak memory consumption (CPU systems)
	</Extractive Summary>
	<Extractive Summary> =
		09
		14
		Table 12: Peak memory consumption (GPU systems)
	</Extractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =2>
	<Abstractive Summary> =
		31
		C
		Table 2: Results on the NLG: (Data → En) track of DGT task
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		We show the results for each track in Table 2
		through 7
	</Extractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =3>
	<Abstractive Summary> =
		84
		C
		Table 3: Results on the NLG: (Data → De) track of DGT task
	</Abstractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =4>
	<Abstractive Summary> =
		91
		C
		Table 4: DGT results on the MT track (De → En)
	</Abstractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =5>
	<Abstractive Summary> =
		26
		C
		Table 5: DGT results on the MT track (En → De)
		4
	</Abstractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =6>
	<Abstractive Summary> =
		83
		C
		Table 6: Results on the MT+NLG: (Data+En → De) track of DGT task
	</Abstractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =7>
	<Abstractive Summary> =
		47
		C
		Table 7: Results on the MT+NLG: (Data+De → En) track of DGT task
	</Abstractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =8>
	<Abstractive Summary> =
		11
		Table 8: Image ﬁle sizes of submitted systems
	</Abstractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =9>
	<Abstractive Summary> =
		10
		Table 9: Time consumption and MT evaluation metrics (CPU systems)
	</Abstractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =10>
	<Abstractive Summary> =
		320
		12
		Table 10: Time consumption and MT evaluation metrics (GPU systems)
	</Abstractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =11>
	<Abstractive Summary> =
		971
		13
		Table 11: Peak memory consumption (CPU systems)
	</Abstractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
</Paper ID=ument899>


<Paper ID=ument899> <Table ID =12>
	<Abstractive Summary> =
		09
		14
		Table 12: Peak memory consumption (GPU systems)
	</Abstractive Summary>
	<Extractive Summary> =
		Interestingly, each GPU system by the
		Marian team shares almost the same amount of
		GPU memory as shown in Table 12 and Figure
		2(b)
	</Extractive Summary>
	<Extractive Summary> =
		These include the full
		size of the image ﬁle for the translation system
		(Table 8), the comparison between compute time
		and evaluation scores on CPU (Table 9) and GPU
		(Table 10), and the comparison between memory
		and evaluation scores on CPU (Table 11) and GPU
		(Table 12)
	</Extractive Summary>
</Paper ID=ument899>


<Paper ID=ument9> <Table ID =1>
	<Abstractive Summary> =
		2
		-
		-
		-
		-
		-
		Table 1: Comparison with state-of-the-art algorithms: unsupervised or knowledge-based (unsup
	</Abstractive Summary>
</Paper ID=ument9>


<Paper ID=ument9> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Comparison using different priors
	</Abstractive Summary>
</Paper ID=ument9>


<Paper ID=ument9> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Performance on the WiC dataset
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3 the disambiguation
		step has a big impact on the results
	</Extractive Summary>
</Paper ID=ument9>


<Paper ID=ument9> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: WSDGα results on the SICK dataset
	</Abstractive Summary>
	<Extractive Summary> =
		The results of this experiment are reported
		in Table 4 as Pearson and Spearman correlation
		and Mean Squared Error (MSE)
	</Extractive Summary>
</Paper ID=ument9>


<Paper ID=ument90> <Table ID =1>
	<Abstractive Summary> =
		Source
		Word
		Target
		Word
		Source
		Lemma
		Target
		Lemma
		Tag
		morza
		moˇre
		morze
		moˇre
		N;NOM;PL
		morzu
		moˇri
		morze
		moˇre
		N;DAT;SG
		morze
		moˇre
		morze
		moˇre
		N;NOM;SG
		morzami
		moˇri
		morze
		moˇre
		N;INS;PL
		m´orz
		moˇr
		morze
		moˇre
		N;GEN;PL
		morzu
		moˇri
		morze
		moˇre
		N;ESS;SG
		morzom
		moˇr´ım
		morze
		moˇre
		N;DAT;PL
		Table 1: An example extract from our morphologically
		complete Polish–Czech dictionary
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 presents an example extract from
		our resource; every source–target pair is followed
		by their corresponding lemmata and a shared tag
	</Extractive Summary>
</Paper ID=ument90>


<Paper ID=ument90> <Table ID =2>
	<Abstractive Summary> =
		2
		Our Dictionaries
		To address the shortcomings of the existing evalu-
		ation, we built 40 new morphologically complete
		976
		Slavic
		Czech
		Russian
		Slovak
		Ukrainian
		Romance
		Spanish
		Catalan
		Portuguese
		Italian
		Polish
		53,353
		128,638
		14,517
		12,361
		French
		686,139
		381,825
		486,575
		705,800
		Czech
		-
		65,123
		10,817
		8,194
		Spanish
		-
		343,780
		476,543
		619,174
		Russian
		-
		128,638
		10,554
		Catalan
		-
		261,016
		351,609
		Slovak
		-
		3,434
		Portuguese
		-
		468,945
		Table 2: The sizes of our morphologically complete dictionaries for Slavic and Romance language families
	</Abstractive Summary>
</Paper ID=ument90>


<Paper ID=ument90> <Table ID =3>
	<Abstractive Summary> =
		1
		0%
		0%
		0%
		14%
		0%
		7%
		14%
		7%
		3%
		55%
		Table 3: BLI results for word pairs that have a speciﬁc morphosyntactic category (left) and a distribution of those
		forms across different frequency bins (right)
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3 we present our ﬁndings for a represen-
		tative sample of morphosyntactic categories for one
		Slavic and one Romance language pair (we present
		the results for all models and all language pairs
		in the supplementary material)
	</Extractive Summary>
	<Extractive Summary> =
		emes; all of which are common morphosyntactic
		categories (see Table 3)
	</Extractive Summary>
</Paper ID=ument90>


<Paper ID=ument90> <Table ID =4>
	<Abstractive Summary> =
		8
		4270
		6095
		Table 4: The results on the standard BLI task and BLI controlled for lexeme for the original Ruder et al
	</Abstractive Summary>
	<Extractive Summary> =
		We present results on 8 representative language
		pairs in Table 4 (column Lexeme)
	</Extractive Summary>
	<Extractive Summary> =
		The results of our experiments are presented
		in the last two rows of Table 4 and, for Polish–
		Spanish, also in Figure 4
	</Extractive Summary>
	<Extractive Summary> =
		16 The
		results, a subset of which we present in Table 4,
		show that the constraint, despite its simplicity and
		being trained on less data, leads to performance im-
		provements for every Romance language pair and
		many of the Slavic language pairs
	</Extractive Summary>
</Paper ID=ument90>


<Paper ID=ument900> <Table ID =1>
	<Abstractive Summary> =
		05
		Table 1:
		Evaluation on MultiWOZ with the greedy
		sampling procedure
	</Abstractive Summary>
	<Extractive Summary> =
		The results are summarized in Table 1)
	</Extractive Summary>
</Paper ID=ument900>


<Paper ID=ument900> <Table ID =2>
	<Abstractive Summary> =
		55
		Table 2: Evaluation on MultiWOZ with the nucleus
		sampling procedure
	</Abstractive Summary>
</Paper ID=ument900>


<Paper ID=ument900> <Table ID =3>
	<Abstractive Summary> =
		This
		Model 1
		vs
		Model 2
		GPT
		59 %
		41%
		Baseline
		GPT
		46 %
		54 %
		Target
		GPT2
		46 %
		54 %
		Target
		GPT2
		45 %
		55 %
		Baseline
		Baseline
		43 %
		57 %
		Target
		GPT2
		51 %
		49 %
		GPT
		Table 3: Human ranking of responses between all pairs
		of four analyzed models and the original responses
	</Abstractive Summary>
	<Extractive Summary> =
		The results are summarized in Table 3, while
		some example dialogues with responses are pro-
		vided in Figure 3
	</Extractive Summary>
</Paper ID=ument900>


<Paper ID=ument901> <Table ID =1>
	<Abstractive Summary> =
		7M
		En-Vi
		tst2012
		1,553
		34K
		tst2013
		1,268
		34K
		Table 1: Sizes of the experimental datasets
	</Abstractive Summary>
</Paper ID=ument901>


<Paper ID=ument901> <Table ID =2>
	<Abstractive Summary> =
		0 (ﬁxed)
		Table 2: Hyperparameter settings
	</Abstractive Summary>
	<Extractive Summary> =
		Hyperparameters:
		Table 2 summarizes the hy-
		perparameter settings
	</Extractive Summary>
</Paper ID=ument901>


<Paper ID=ument901> <Table ID =3>
	<Abstractive Summary> =
		0 × 10−4
		N/A because training did not converge
		Table 3: Results of the system comparison
	</Abstractive Summary>
	<Extractive Summary> =
		Note that zero epochs of decoder training
		(the top line) mean that ﬁne-tuning was directly
		applied without decoder training (the same as in
		Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		Considering the
		score of newstest2015 in the experiment in
		Table 3 was +1
	</Extractive Summary>
</Paper ID=ument901>


<Paper ID=ument901> <Table ID =4>
	<Abstractive Summary> =
		89
		Decoder training converged
		Baseline for statistical testing
		Table 4: Changes in the perplexity of the development set (Dev
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results after ﬁne-tuning when
		decoder training was stopped after various num-
		bers of epochs
	</Extractive Summary>
</Paper ID=ument901>


<Paper ID=ument901> <Table ID =5>
	<Abstractive Summary> =
		57 (+)
		Table 5: Results of the IWSLT-2015 data
	</Abstractive Summary>
</Paper ID=ument901>


<Paper ID=ument902> <Table ID =1>
	<Abstractive Summary> =
		08%
		Human
		Table 1: Human evaluation of ﬂuency, relevancy, and answerability
	</Abstractive Summary>
</Paper ID=ument902>


<Paper ID=ument902> <Table ID =2>
	<Abstractive Summary> =
		3
		Table 2: Our proposed model MSQGsharedh,rmrep sig-
		niﬁcantly outperforms baselines, based on the auto-
		mated retrieval statistics
	</Abstractive Summary>
</Paper ID=ument902>


<Paper ID=ument902> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: Full results, comparing models constructed
		with M256, M512, and Mattn
		256
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results
		Table 3 shows the mean retrieval statistics and
		their proportion of unique generated questions
		from 55,065 10-passage instances
	</Extractive Summary>
	<Extractive Summary> =
		Supplementary Material
		A
		Full Experiment Results
		Table 3 shows the retrieval results on a larger set
		of baselines and MSQG models
	</Extractive Summary>
</Paper ID=ument902>


<Paper ID=ument903> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Story continuations conditioned on various
		control attributes generated from our framework
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows several examples of con-
		trol attributes and generated continuations corre-
		sponding to them from our model
	</Extractive Summary>
	<Extractive Summary> =
		Table 10 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		Human Preference
		system 1
		system 2
		1
		2
		neither
		BS
		TS
		43
		16
		29
		BS
		reranking
		18
		30
		15
		BS
		predicted
		38
		29
		19
		TS
		reranking
		18
		38
		16
		TS
		predicted
		18
		27
		34
		reranking
		predicted
		27
		24
		21
		Table 10: Human preferences when given three contin-
		uations from each pair of systems
	</Extractive Summary>
	<Extractive Summary> =
		Figure 3, Table 11, and Table
		12 show the frequency (%) of matching the target
		control variable for length, predicates, and frames,
		respectively
	</Extractive Summary>
	<Extractive Summary> =
		05
		Table 11: Frequency (%) of the generated continuations containing the desired predicate
	</Extractive Summary>
	<Extractive Summary> =
		3
		Table 12: Frequency (%) of the generated continuations containing the desired frame
	</Extractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Generated continuations from our framework
		with different control attribute values
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows
		examples of generated continuations for a single
		story context with several values for the control at-
		tributes described below
	</Extractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Sentiment match percentages of generated
		continuations and target sentiment values
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the sentiment distribution
		for each label
	</Extractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: Frequency (%) of the generated continuations
		in the range of dif = |l−lp| where l is the continuation
		length and lp is the desired length
	</Abstractive Summary>
	<Extractive Summary> =
		The results are
		shown in Table 4 and demonstrate that our model
		can generate continuations with the desired length
		with only small differences
	</Extractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =5>
	<Abstractive Summary> =
		85
		said
		100
		bought
		100
		Table 5: Match percentages (M%) showing fraction of
		stories for which generated continuations contain the
		desired predicate
	</Abstractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =6>
	<Abstractive Summary> =
		2
		Table 6: Match percentages (M%) showing fraction of
		stories for which generated continuations contain the
		desired frame
	</Abstractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =7>
	<Abstractive Summary> =
		3
		Table 7: Cluster match percentages (%) for each value
		of the cluster control variable
	</Abstractive Summary>
	<Extractive Summary> =
		From the results in
		Table 7, we still see controllability for most clus-
		ters; however, for target cluster 3, which is rather
		generic based on our observations, the generated
		output seems ﬂat
	</Extractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =8>
	<Abstractive Summary> =
		7
		Table 8: Automatic metrics for baseline system and when using oracle values for control attributes
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Evaluation with Oracle Attributes
		Table 8 shows automatic metric scores with oracle
		attribute values, i
	</Extractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =9>
	<Abstractive Summary> =
		5
		Table 9: Metric scores to evaluate the potential of a list of continuations
	</Abstractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =10>
	<Abstractive Summary> =
		Human Preference
		system 1
		system 2
		1
		2
		neither
		BS
		TS
		43
		16
		29
		BS
		reranking
		18
		30
		15
		BS
		predicted
		38
		29
		19
		TS
		reranking
		18
		38
		16
		TS
		predicted
		18
		27
		34
		reranking
		predicted
		27
		24
		21
		Table 10: Human preferences when given three contin-
		uations from each pair of systems
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10 shows the results
	</Extractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =11>
	<Abstractive Summary> =
		05
		Table 11: Frequency (%) of the generated continuations containing the desired predicate
	</Abstractive Summary>
	<Extractive Summary> =
		Figure 3, Table 11, and Table
		12 show the frequency (%) of matching the target
		control variable for length, predicates, and frames,
		respectively
	</Extractive Summary>
</Paper ID=ument903>


<Paper ID=ument903> <Table ID =12>
	<Abstractive Summary> =
		3
		Table 12: Frequency (%) of the generated continuations containing the desired frame
	</Abstractive Summary>
</Paper ID=ument903>


<Paper ID=ument904> <Table ID =1>
	<Abstractive Summary> =
		85
		Table 1: Translation accuracy (BLEU; Papineni et al
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Adapting Between Domains
		The ﬁrst 6 result columns of Table 1 show the ex-
		perimental results on the OPUS dataset
	</Extractive Summary>
	<Extractive Summary> =
		2
		Adapting from a General Domain to a
		Speciﬁc Domain
		The last two result columns of Table 1 show the
		experimental results in the WMT-TED setting
	</Extractive Summary>
</Paper ID=ument904>


<Paper ID=ument904> <Table ID =2>
	<Abstractive Summary> =
		79
		Table 2: Performance of DDA-Deep when fusing dif-
		ferent parts of models on the law and medical datasets
	</Abstractive Summary>
</Paper ID=ument904>


<Paper ID=ument904> <Table ID =3>
	<Abstractive Summary> =
		We ﬁrst deﬁne
		Source
		warum wurde Ab- ili- fy zugelassen ?
		Reference
		why has Ab- ili- fy been approved ?
		Baseline
		reasons was received why a reminder
		was accepted ?
		DDA-Shallow
		why has been approved?
		Copy
		why ,
		DDA-Deep
		why was Ab- ili- fy authorised ?
		Table 3: Translation examples under the law to medi-
		cal adaptation setting
	</Abstractive Summary>
	<Extractive Summary> =
		We also we sample some translation results and
		show them in Table 3 to qualitatively demonstrate
		the differences between the methods
	</Extractive Summary>
</Paper ID=ument904>


<Paper ID=ument904> <Table ID =4>
	<Abstractive Summary> =
		22
		Table 4: Performance of ensembling different LMs on
		the law and medical datasets
	</Abstractive Summary>
</Paper ID=ument904>


<Paper ID=ument904> <Table ID =5>
	<Abstractive Summary> =
		17
		Table 5: BLEU points of models after continued training on the IT development dataset with different values of
		coverage penalty β
	</Abstractive Summary>
	<Extractive Summary> =
		61), as shown in Table 5 (β = 0)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the coverage penalty
		can improve the deep adaptation method by more
		than 5 BLEU points while the baseline model can
		only be improved by 2 BLEU points
	</Extractive Summary>
</Paper ID=ument904>


<Paper ID=ument905> <Table ID =1>
	<Abstractive Summary> =
		’
		Table 1:
		Example of an overly abstractive summary
		with zero bigram overlap with the document from a
		CNN/DM training sample
	</Abstractive Summary>
</Paper ID=ument905>


<Paper ID=ument905> <Table ID =2>
	<Abstractive Summary> =
		4
		Table 2: ROUGE-F1 (%) scores of manually crafted
		extractive trainers for producing sentence-level extrac-
		tive labels for CNN/DM
	</Abstractive Summary>
</Paper ID=ument905>


<Paper ID=ument905> <Table ID =3>
	<Abstractive Summary> =
		4
		Table 3: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various extractive models on the CNN/DM
		test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 and 5 presents extractive and ab-
		stractive results on the CNN/DM dataset respec-
		tively, while Tables 4 and 6 for the Newsroom
		dataset
	</Extractive Summary>
</Paper ID=ument905>


<Paper ID=ument905> <Table ID =4>
	<Abstractive Summary> =
		96
		Table 4: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various extractive models on the Newsroom
		released test set
	</Abstractive Summary>
</Paper ID=ument905>


<Paper ID=ument905> <Table ID =5>
	<Abstractive Summary> =
		92
		Table 5: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various abstractive models on the CNN/DM
		test set
	</Abstractive Summary>
</Paper ID=ument905>


<Paper ID=ument905> <Table ID =6>
	<Abstractive Summary> =
		17
		Table 6: ROUGE-F1 (%) scores (with 95% conﬁdence interval) of various abstractive models on the Newsroom
		released test set
	</Abstractive Summary>
</Paper ID=ument905>


<Paper ID=ument905> <Table ID =7>
	<Abstractive Summary> =
		Table 7:
		Examples of some of our generated para-
		phrases from the CNN/DM dataset, where O, G, R
		represents Originating document sentence, our model’s
		Generated paraphrase and Reference sentences from
		the ground-truth summary respectively
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 show that our
		paraphrases are well formed, abstractive (e
	</Extractive Summary>
</Paper ID=ument905>


<Paper ID=ument906> <Table ID =1>
	<Abstractive Summary> =
		89
		Table 1: Performance of the Transformer and RNN
		model trained synchronously and asynchronously,
		across different learning rates
	</Abstractive Summary>
	<Extractive Summary> =
		Results in Table 1 conﬁrm that asynchronous
		SGD generally yields lower-quality systems than
		synchronous SGD
	</Extractive Summary>
	<Extractive Summary> =
		These were near the top in both
		asynchronous and synchronous settings (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		With staleness 3,
		the model stayed at 0 BLEU for both synchronous
		or asynchronous SGD, consistent with our earlier
		result (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		2
		Generalization Across Learning Rates
		Earlier in Table 1 we show that asynchronous
		Transformer learning is very sensitive towards the
		learning rate
	</Extractive Summary>
</Paper ID=ument906>


<Paper ID=ument906> <Table ID =2>
	<Abstractive Summary> =
		000
		Table 2: The Adam optimizer slows down when gradients have larger variance even if they have the same average,
		in this case 1 An example
		appears in Table 2: updating with 1 moves faster
		than individually applying -1 and 2
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2, we
		show examples where noise causes Adam to slow
		down
	</Extractive Summary>
</Paper ID=ument906>


<Paper ID=ument906> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Quality and convergence of asynchronous SGD with accumulated gradients on English to German dataset
	</Abstractive Summary>
</Paper ID=ument906>


<Paper ID=ument906> <Table ID =4>
	<Abstractive Summary> =
		41
		Table 4: The effect of global accumulation on translation quality for different language pairs on development and
		test set, measured with BLEU score
	</Abstractive Summary>
	<Extractive Summary> =
		The results shown in Table 4 empirically con-
		ﬁrm that accumulating the gradient to obtain a
		86
		Transformer
		Communication
		accumulation
		batch
		avg
	</Extractive Summary>
</Paper ID=ument906>


<Paper ID=ument906> <Table ID =5>
	<Abstractive Summary> =
		20
		Table 5: Performance of the asynchronous Transformer
		on English to German with 4x Global accumulations
		(GA) across different learning rates on development set
		measured with BLEU score
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 5, the model is now capable to
		learn on higher learning rate and yield compara-
		ble results compared to its synchronous variant
	</Extractive Summary>
</Paper ID=ument906>


<Paper ID=ument907> <Table ID =1>
	<Abstractive Summary> =
		To
		94
		domain
		subset
		carriers
		signatures
		slots
		words
		Movies
		train
		1,382
		179
		21
		353
		dev
		622
		109
		15
		292
		test
		520
		69
		10
		254
		Live Entertainment
		train
		4269
		588
		120
		332
		dev
		1236
		244
		77
		194
		test
		1335
		271
		74
		217
		All
		train
		5,651
		767
		141
		685
		dev
		1,858
		353
		92
		486
		test
		1,855
		340
		84
		471
		Table 1: Data distribution and splits
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the data splits for the movie, live
		entertainment and ‘all’ datasets, the latter contain-
		ing both movies and live entertainment data, in-
		cluding the number of signatures, slot types and
		unique non-slot words in each set
	</Extractive Summary>
	<Extractive Summary> =
		We use the same data as for the data
		generation experiments (see Table 1), and group
		our class labels into intents (as opposed to signa-
		tures), which leads to classifying 136 intents in the
		combined movies and entertainment data (‘all’)
	</Extractive Summary>
</Paper ID=ument907>


<Paper ID=ument907> <Table ID =2>
	<Abstractive Summary> =
		‘All’ contains the combined Movie and Entertainment live datasets
		domain
		subset
		carriers
		intents
		All
		train
		5651
		136
		dev
		1858
		101
		train+dev
		7509
		136
		test
		1855
		94
		Movies
		test
		520
		37
		Table 2: Data distribution and splits for the extrinsic
		task
	</Abstractive Summary>
</Paper ID=ument907>


<Paper ID=ument907> <Table ID =3>
	<Abstractive Summary> =
		00
		Table 3: Best performance per metric for each model when applied to ‘all’ domains
	</Abstractive Summary>
	<Extractive Summary> =
		Those
		applicable are shown in Table 3, where ‘para’, ‘re-
		con’, ‘prior’ and ‘post’ denote paraphrasing, recon-
		struction, prior and posterior respectively
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents the intrinsic evaluation results,
		where generators are trained and tested on ‘all’ data,
		for the best performing model per case, tuned on
		the dev set
	</Extractive Summary>
</Paper ID=ument907>


<Paper ID=ument907> <Table ID =4>
	<Abstractive Summary> =
		1%
		Table 4: Relative change when adding more training data for generator training (movies only vs ‘all’) across
		evaluation metrics on the movies test set
		0
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, we show intrinsic results on the
		movies test set
	</Extractive Summary>
</Paper ID=ument907>


<Paper ID=ument908> <Table ID =1>
	<Abstractive Summary> =
		Since the initial multilingual model
		was trained on both these directions (Z→X and
		102
		Method
		Back-Translated Data
		Training Regime
		pivot from scratch
		BT-pivot
		train from scratch
		pivot ﬁne-tune
		BT-pivot
		ﬁne-tune initial model
		pivot-parallel combined
		BT-pivot + BT-parallel
		ﬁne-tune initial model
		Table 1: Summary of the proposed methods for zero-shot NMT using pivot-language monolingual data
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes the three proposed methods
		for zero-shot NMT
	</Extractive Summary>
</Paper ID=ument908>


<Paper ID=ument908> <Table ID =2>
	<Abstractive Summary> =
		The
		data comes from the WMT16 news translation
		103
		Corpus
		Sentences
		EN↔DE
		4 497 878
		EN↔RU
		2 500 502
		EN monolingual
		1 000 000
		Table 2: Number of sentences in each training corpus
		for the DE↔RU experiments
	</Abstractive Summary>
</Paper ID=ument908>


<Paper ID=ument908> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3:
		BLEU scores for the initial multilingual models and zero-resource models without monolingual data,
		for the baselines with pivot monolingual data, and for our proposed zero-resource models with pivot monolingual
		data
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 3 shows translation performance (as es-
		timated by BLEU score) for our main experi-
		ments
	</Extractive Summary>
</Paper ID=ument908>


<Paper ID=ument908> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: BLEU scores for the proposed models on the
		test set (newstest2015)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the performance on the test
		dataset (newstest2015) when a second iteration of
		back-translation and training is performed
	</Extractive Summary>
</Paper ID=ument908>


<Paper ID=ument909> <Table ID =1>
	<Abstractive Summary> =
		5M
		104M
		Wiki
		72M
		2086M
		News
		210M
		3657M
		Table 1: Monolingual (English) training data
		after having being trained on the source corpora
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes
		the statistics of these three monolingual corpora
	</Extractive Summary>
</Paper ID=ument909>


<Paper ID=ument909> <Table ID =2>
	<Abstractive Summary> =
		5
		Table 2: In/Out of Domain test sets
	</Abstractive Summary>
</Paper ID=ument909>


<Paper ID=ument909> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Example of a poor translation when adding unknown token to source sentences (translation done with a
		baseline transformer model)
		news14
		news18
		iwslt15
		wiki
		kde
		OpenSub
		Baseline
		27
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 gives an ex-
		ample of such translations for baseline model7
	</Extractive Summary>
</Paper ID=ument909>


<Paper ID=ument909> <Table ID =4>
	<Abstractive Summary> =
		8
		Table 4: FT: initialize NMT encoder with BERT and ﬁnetune; Freeze: ﬁx NMT encoder parameters to BERT
		parameters; Emb: ﬁx encoder embeddding layer with BERT contextual word embeddings
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Table 4 presents the results of our experiments
	</Extractive Summary>
</Paper ID=ument909>


<Paper ID=ument909> <Table ID =5>
	<Abstractive Summary> =
		2
		Table 5: Robustness tests
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Robustness analysis
		Table 5 reports BLEU scores for the noisy test sets
		(described in section 4
	</Extractive Summary>
</Paper ID=ument909>


<Paper ID=ument909> <Table ID =6>
	<Abstractive Summary> =
		9
		Table 6: IWSLT dataset results
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 6 reports the results of different sets of the
		experiments on IWSLT data
	</Extractive Summary>
</Paper ID=ument909>


<Paper ID=ument91> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Macro-averaged accuracy over 100 language
		test pairs
	</Abstractive Summary>
	<Extractive Summary> =
		3, is better than our models, but this diﬀer-
		ence is not reﬂected on the test set (see Table 1)
		where our model performs slightly better
	</Extractive Summary>
</Paper ID=ument91>


<Paper ID=ument91> <Table ID =2>
	<Abstractive Summary> =
		6†
		Table 2: Our proposed two-step attention architecture
		outperforms almost all baselines on the development
		set (but with higher gains on the test set)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 presents the macro-averaged accu-
		racy for the baseline (Wu and Cotterell, 2019) and
		the diﬀerent versions of our model
	</Extractive Summary>
	<Extractive Summary> =
		Another advantage is the two-step
		6Table 2 includes unpublished results kindly shared by
		Wu and Cotterell, the authors of the baseline
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 also reports the development set
		accuracy when using hallucinated data
	</Extractive Summary>
</Paper ID=ument91>


<Paper ID=ument91> <Table ID =3>
	<Abstractive Summary> =
		55
		Table 3: Results with a single transfer language
	</Abstractive Summary>
</Paper ID=ument91>


<Paper ID=ument91> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: Results with multiple transfer languages (sam-
		ple)
	</Abstractive Summary>
	<Extractive Summary> =
		We present extensive ablations on
		such cases in Table 4 with results on the rest of the
		SIGMORPHON language pairs
	</Extractive Summary>
</Paper ID=ument91>


<Paper ID=ument91> <Table ID =5>
	<Abstractive Summary> =
		55
		Table 5: Results with a single transfer language
	</Abstractive Summary>
	<Extractive Summary> =
		B
		Complete Result Tables
		Table 5 lists all results for test languages with a single candidate transfer language
	</Extractive Summary>
</Paper ID=ument91>


<Paper ID=ument91> <Table ID =6>
	<Abstractive Summary> =
		1
		estonian
		livonian
		27
		27
		35
		34
		33±1
		hungarian
		28
		30
		35
		33
		ﬁnnish
		30
		26
		34
		35
		all
		26
		25
		36
		36
		Table 6: Multiple transfer language results (part 1)
	</Abstractive Summary>
</Paper ID=ument91>


<Paper ID=ument91> <Table ID =7>
	<Abstractive Summary> =
		9
		Table 7: Multiple transfer language results (part 2)
	</Abstractive Summary>
</Paper ID=ument91>


<Paper ID=ument910> <Table ID =1>
	<Abstractive Summary> =
		66
		Table 1: �C-VAELSTM performance with C = {3, 15, 100} on the test sets of CBT, WIKI, and WebText This observation is further supported by the
		increasing pattern of active units in Table 1: Given
		that AU increases with increase of C one would
		expect that activation pattern of a latent variable
		becomes more complex as it comprises more in-
		formation
		To understand this behavior further, we need two
		additional results from Table 1: LogDetCov and
		||µ||2
		2
	</Abstractive Summary>
	<Extractive Summary> =
		Addi-
		tionally, as reported in Table 1, encouraging higher
		rates (via larger C) encourages more active units
		5We attribute the difference in performance across our
		models to the non-optimal selection of training hyperparam-
		eters, and corpus speciﬁc factors such as sentence length
	</Extractive Summary>
	<Extractive Summary> =
		The above observation is better pronounced in
		Table 1, where we also report the mean (||µ||2
		2)
		of unbiased samples of z, highlighting the diver-
		gence from the mean of the prior distribution as
		rate increases
	</Extractive Summary>
	<Extractive Summary> =
		We generated synthetic corpora using trained
		models from Table 1 with different C and decod-
		ing schemes and using the same exact z samples
		for all corpora
	</Extractive Summary>
</Paper ID=ument910>


<Paper ID=ument910> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Homotopy (CBT corpus) - The three blocks correspond to C = {3, 15, 100} values used for training
		�C-VAELSTM
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the generated sentences via dif-
		ferent decoding schemes for each channel capac-
		ity
	</Extractive Summary>
</Paper ID=ument910>


<Paper ID=ument910> <Table ID =3>
	<Abstractive Summary> =
		1
		-
		Table 3: Forward Cross Entropy (FCE) To
		understand this further, we report additional statis-
		tics in Table 3: percentage of 〈unk〉 symbols, self-
		BLEU and average sentence length in the corpus
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 3 we
		report the average FCE (NLL) for the generated
		corpora
	</Extractive Summary>
</Paper ID=ument910>


<Paper ID=ument910> <Table ID =4>
	<Abstractive Summary> =
		02
		Table 4: p1: p(x�|z+) < p(x+|z+) and p2: p(x�|z�) < p(x+|z�); ¯p1: p(x�|¯z+) < p(x+|¯z+) and ¯p2:
		p(x�|¯z�) < p(x+|¯z�); �C=3-VAELSTM (D:103, R:3); �C=100-VAELSTM (D:39, R:101)
	</Abstractive Summary>
	<Extractive Summary> =
		Then we
		condition the decoder on the z+ and try to deter-
		mine whether the decoder assigns higher probabil-
		ity to the grammatical sentence (denoted by x+):
		p(x�|z+) < p(x+|z+) (denoted by p1 in Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		We repeat the same experiment but this time try to
		determine whether the decoder, when conditioned
		on the ungrammatical code (z�), still prefers to
		assign higher probability to the grammatical sen-
		tence: p(x�|z�) < p(x+|z�) (denoted by p2 in
		Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the p1 and p2 for the �C-
		VAELSTM model trained with C = {3, 100}
	</Extractive Summary>
</Paper ID=ument910>


<Paper ID=ument911> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Several random input lines alongside with human written reformulation and the reformulation generated
		by the baseline
	</Abstractive Summary>
</Paper ID=ument911>


<Paper ID=ument911> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Reformulations generated by the baseline with additional discriminator, shifted autoencoder and shifted
		autoencoder with additional discriminator corresponding to the inputs in Table 1
	</Abstractive Summary>
</Paper ID=ument911>


<Paper ID=ument912> <Table ID =1>
	<Abstractive Summary> =
		81
		the host that walked through to the ta-
		ble and are quite perfect !
		Table 1: Examples showing why Acc is insufﬁcient
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 illustrates this tendency
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows ex-
		amples of transferred sentences at several points
		in training the model of Shen et al
	</Extractive Summary>
</Paper ID=ument912>


<Paper ID=ument912> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: Yelp results with various systems and auto-
		matic metrics at a nearly-ﬁxed Acc, with best scores in
		boldface
	</Abstractive Summary>
	<Extractive Summary> =
		Depending on which model is being trained (see
		Table 2), the λi’s for the unused losses will be
		zero
	</Extractive Summary>
	<Extractive Summary> =
		Depending on which model is being
		trained (see Table 2), the λi’s for the unused losses
		will be zero
	</Extractive Summary>
	<Extractive Summary> =
		1
		Analyzing Metric Relationships
		Table 2 shows results for the Yelp dataset and Fig-
		ure 1 plots learning trajectories of those models
	</Extractive Summary>
	<Extractive Summary> =
		2
		Table 4: Manual evaluation results (%) using models from Table 2 (i
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 and Figure 1 show that under
		similar Acc, M2 has much better semantic simi-
		larity for both Yelp and Literature
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows that the model
		with paraphrase loss (M1) slightly improves Sim
		over M0 on both datasets under similar Acc
	</Extractive Summary>
</Paper ID=ument912>


<Paper ID=ument912> <Table ID =3>
	<Abstractive Summary> =
		8
		Table 3: Literature results with various systems and au-
		tomatic metrics at a nearly-ﬁxed Acc, with best scores
		in boldface
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows results for the Literature dataset
	</Extractive Summary>
</Paper ID=ument912>


<Paper ID=ument912> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Manual evaluation results (%) using models from Table 2 (i
	</Abstractive Summary>
</Paper ID=ument912>


<Paper ID=ument912> <Table ID =5>
	<Abstractive Summary> =
		67
		Table 5: Human sentence-level validation of metrics;
		100 examples for each dataset for validating Acc; 150
		each for Sim and PP; see text for validation of GM
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 5, all
		validations show strong correlations on the Yelp
		dataset and reasonable correlations on Literature
	</Extractive Summary>
</Paper ID=ument912>


<Paper ID=ument912> <Table ID =6>
	<Abstractive Summary> =
		901
		Table 6: Results on Yelp sentiment transfer, where
		BLEU is between 1000 transferred sentences and hu-
		man references, and Acc is restricted to the same 1000
		sentences
	</Abstractive Summary>
</Paper ID=ument912>


<Paper ID=ument913> <Table ID =1>
	<Abstractive Summary> =
		Table 1:
		An example of box-score (top), line-score (middle) and the corresponding summary (bottom) from
		ROTOWIRE dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 illus-
		trates an example of data-to-text NLG, with statis-
		tics of a NBA basketball game (top) and the corre-
		sponding game summary (bottom)
	</Extractive Summary>
	<Extractive Summary> =
		(2017) investigates
		different data-to-text generation approaches and
		introduces a new corpus (ROTOWIRE, see Table 1)
		for the data-to-text generation task along with
		a series of automatic measures for the content-
		oriented evaluation
	</Extractive Summary>
	<Extractive Summary> =
		Input of the model consists of a table of records
		(see Table 1, top and middle)
	</Extractive Summary>
	<Extractive Summary> =
		The output t (see Table 1, bottom) is a text
		document which is a descriptive summary for the
		record set s
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 il-
		lustrates an example of the dataset
	</Extractive Summary>
</Paper ID=ument913>


<Paper ID=ument913> <Table ID =2>
	<Abstractive Summary> =
		22
		Table 2: Automatic evaluation on ROTOWIRE devel-
		opment set using relation generation (RG) count (#)
		and precision (P%), content selection (CS) precision
		(P%) and recall (R%), content ordering (CO) in nor-
		malized Damerau-Levenshtein distance (DLD%), and
		BLEU
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2,
		we can see that the RG and CS precisions are both
		improved by 2
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, as expected,
		the model after ﬁne-tuning generates more rela-
		tions in the output summaries
	</Extractive Summary>
	<Extractive Summary> =
		As shown in
		Table 2, all content-oriented evaluation metrics
		are improved compared to DATA-TRANS but not
		as much as each single of the data augmentation
		method
	</Extractive Summary>
</Paper ID=ument913>


<Paper ID=ument913> <Table ID =3>
	<Abstractive Summary> =
		14
		Table 3: Automatic evaluation on ROTOWIRE test set
	</Abstractive Summary>
</Paper ID=ument913>


<Paper ID=ument913> <Table ID =4>
	<Abstractive Summary> =
		24
		Table 4: Ablation results on ROTOWIRE dev set
	</Abstractive Summary>
	<Extractive Summary> =
		The eval-
		uation results are shown in Table 4 (-CS OBJ)
	</Extractive Summary>
	<Extractive Summary> =
		From the re-
		sults in Table 4 (-REC EMB), we can see that with-
		out record embedding all content-oriented evalu-
		ation results are degraded, especially the RG pre-
		cision and CS recall
	</Extractive Summary>
</Paper ID=ument913>


<Paper ID=ument913> <Table ID =5>
	<Abstractive Summary> =
		Table 5:
		Example output from DATA-TRANS (right) and ablation model -REC EMB (left)
	</Abstractive Summary>
	<Extractive Summary> =
		An example output of -REC EMB system is
		shown in Table 5 (left)
	</Extractive Summary>
	<Extractive Summary> =
		Another problem we have observed, not only
		in Table 5 but also in other output summaries, is
		repetition and self-contradictory
	</Extractive Summary>
	<Extractive Summary> =
		In the left ex-
		ample of Table 5, it contains two sentences (in
		orange color) which are completely contradictory
		with each other
	</Extractive Summary>
</Paper ID=ument913>


<Paper ID=ument914> <Table ID =1>
	<Abstractive Summary> =
		65
		Table 1: Training and test performance
		We observe that all latent models outperform
		their deterministic counterparts (crossed curves)
		in terms of both, generalization and overall test
		performance
	</Abstractive Summary>
</Paper ID=ument914>


<Paper ID=ument915> <Table ID =1>
	<Abstractive Summary> =
		Corpus
		Sentences
		Reviews
		Words (FR)
		4SQ-PE
		12 080
		8 004
		141 958
		4SQ-HT
		2 784
		1 625
		29 075
		4SQ-valid
		1 243
		765
		13 976
		4SQ-test
		1 838
		1 157
		21 525
		Table 1: 4SQ corpora
	</Abstractive Summary>
	<Extractive Summary> =
		5k French reviews from 4SQ,
		mostly in the food category,3 split them into sen-
		tences (18k), and grouped them into train, valid
		and test sets (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		20
		Table 10 shows the accuracy of our models when
		translating these words
	</Extractive Summary>
	<Extractive Summary> =
		175
		Model
		cadre
		cuisine
		carte
		Total
		Total (source)
		23
		32
		29
		100%
		WMT
		13
		17
		14
		52%
		UGC (Inline case)
		22
		27
		18
		80%
		UGC ⊕ PE + tags
		23
		31
		29
		99%
		Table 10: Number of correct translations for difficult
		polysemous words in 4SQ-test by different models
	</Extractive Summary>
	<Extractive Summary> =
		3
		Table 11: Indirect evaluation with Aspect-Based Senti-
		ment Analysis (accuracy in %)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 11, translations obtained with
		domain-adapted models lead to significantly better
		scores on the ABSA task than the generic models
	</Extractive Summary>
	<Extractive Summary> =
		We got 12 an-
		Pairs
		Win
		Tie
		Loss
		Tags ≈ Tags + noise
		82
		453
		63
		Tags ≫ Baseline
		187
		337
		74
		Tags ≫ GT
		226
		302
		70
		Tags + noise ≫ Baseline
		178
		232
		97
		Tags + noise ≫ GT
		218
		315
		65
		Baseline ≫ GT
		173
		302
		123
		Table 12: In-house human evaluation (“≫” means bet-
		ter with p ≤ 0
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =2>
	<Abstractive Summary> =
		Uppercase
		Lowercase
		Input
		UNE HONTE !
		une honte !
		Pre-proc
		UN E _H ON TE _!
		une _honte _!
		MT output
		A _H ON E Y !
		A _dis gra ce !
		Post-proc
		A HONEY!
		A disgrace!
		Table 2: Capital letters break NMT
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Capital letters
		As shown in Table 2, capital letters are another
		source of confusion
	</Extractive Summary>
	<Extractive Summary> =
		17The “LC to cased” and “Noised case” models are not able
		to preserve capital letters for emphasis (as in Table 2), and the
		“Cased” model often breaks on such examples
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =3>
	<Abstractive Summary> =
		39M
		1 125M
		1 041M
		Table 3: Size of the WMT and UGC training corpora
		(after filtering)
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Training data
		After some initial work with the WMT 2014 data,
		we built a new training corpus named UGC (User
		Generated Content), closer to our domain, by
		combining: Multi UN, OpenSubtitles, Wikipedia,
		Books, Tatoeba, TED talks, ParaCrawl11 and
		Gourmet12 (See Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		5 (see
		Table 3)
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =4>
	<Abstractive Summary> =
		63
		Table 4: Robustness to capital letters (see Section 4
	</Abstractive Summary>
	<Extractive Summary> =
		5
		BLEU evaluation
		Capital letters
		Table 4 compares the case han-
		dling techniques presented in Section 4
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =5>
	<Abstractive Summary> =
		69
		Table 5: Baseline model with or without natural noise
		(see Section 4
	</Abstractive Summary>
	<Extractive Summary> =
		Natural noise
		Table 5 compares the baseline
		“inline case” model with the same model aug-
		mented with natural noise (Section 4
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =6>
	<Abstractive Summary> =
		84
		Table 6:
		Comparison of different back-translation
		schemes (see Section 5
	</Abstractive Summary>
	<Extractive Summary> =
		Domain adaptation
		Table 6 shows the results
		of the back-translation (BT) techniques
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =7>
	<Abstractive Summary> =
		00
		Table 7: Domain adaptation with 4SQ-PE fine-tuning
		(FT) or corpus tags
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 compares the domain adaptation tech-
		niques presented in Section 5
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =8>
	<Abstractive Summary> =
		60
		Table 8: Combination of several robustness or domain
		adaptation techniques
	</Abstractive Summary>
	<Extractive Summary> =
		19
		As shown in Table 8, these techniques can be
		combined to achieve the best results
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =9>
	<Abstractive Summary> =
		French word
		Meanings
		Cadre
		setting, frame, executive
		Cuisine
		food, kitchen
		Carte
		menu, card, map
		Table 9: French polysemous words found in 4SQ, and
		translation candidates in English
	</Abstractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =10>
	<Abstractive Summary> =
		175
		Model
		cadre
		cuisine
		carte
		Total
		Total (source)
		23
		32
		29
		100%
		WMT
		13
		17
		14
		52%
		UGC (Inline case)
		22
		27
		18
		80%
		UGC ⊕ PE + tags
		23
		31
		29
		99%
		Table 10: Number of correct translations for difficult
		polysemous words in 4SQ-test by different models
	</Abstractive Summary>
	<Extractive Summary> =
		20
		Table 10 shows the accuracy of our models when
		translating these words
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =11>
	<Abstractive Summary> =
		3
		Table 11: Indirect evaluation with Aspect-Based Senti-
		ment Analysis (accuracy in %)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 11, translations obtained with
		domain-adapted models lead to significantly better
		scores on the ABSA task than the generic models
	</Extractive Summary>
</Paper ID=ument915>


<Paper ID=ument915> <Table ID =12>
	<Abstractive Summary> =
		We got 12 an-
		Pairs
		Win
		Tie
		Loss
		Tags ≈ Tags + noise
		82
		453
		63
		Tags ≫ Baseline
		187
		337
		74
		Tags ≫ GT
		226
		302
		70
		Tags + noise ≫ Baseline
		178
		232
		97
		Tags + noise ≫ GT
		218
		315
		65
		Baseline ≫ GT
		173
		302
		123
		Table 12: In-house human evaluation (“≫” means bet-
		ter with p ≤ 0
	</Abstractive Summary>
</Paper ID=ument915>


<Paper ID=ument916> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Results for three language pairs
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results and Analysis
		Table 1 reports the results for baselines and the
		proposed method4
	</Extractive Summary>
</Paper ID=ument916>


<Paper ID=ument917> <Table ID =1>
	<Abstractive Summary> =
		3M
		Table 1: Results of the evaluation of models in translating languages with different morphological typology using
		the IWSLT data sets
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results
		The results of the experiments given in Table 1 show that
		the hierarchical decoder can reach performance comparable
		to or better than the NMT model based on subword units in
		all languages while using almost three times less number of
		parameters
	</Extractive Summary>
</Paper ID=ument917>


<Paper ID=ument917> <Table ID =2>
	<Abstractive Summary> =
		17
		Table 2: Results of the evaluation of models in capturing morphological variations in the output using the Morphe-
		val English-German test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists the per-
		formance of different NMT models implementing decoding
		at the level of subwords, characters, or hierarchical word-
		character units in capturing variances in each individual mor-
		phological paradigm and preserving the agreement between
		inﬂected words and their dependent lexical items
	</Extractive Summary>
</Paper ID=ument917>


<Paper ID=ument917> <Table ID =3>
	<Abstractive Summary> =
		19
		Table 3: Experiment results in the English-to-German
		direction with WMT data sets
	</Abstractive Summary>
</Paper ID=ument917>


<Paper ID=ument917> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Example translations with different approaches in Turkish
		the training data, where the subword-based model obtains the
		best accuracy, followed by the hierarchical model, and the
		character-level NMT model obtains signiﬁcantly lower accu-
		racy compared to both approaches
	</Abstractive Summary>
	<Extractive Summary> =
		In order to better illustrate the differences in the outputs of
		each NMT model, we also present some sample translations
		in Table 4, obtained by translating English into Turkish using
		the NMT models trained on the TED Talks corpus
	</Extractive Summary>
</Paper ID=ument917>


<Paper ID=ument918> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: WMT19 English→German Document-Level
		Translation
	</Abstractive Summary>
</Paper ID=ument918>


<Paper ID=ument918> <Table ID =2>
	<Abstractive Summary> =
		Table 2: An example where the Insertion Transformer gets confused with sentence alignment: it maps one sentence
		from the source into two sentences in the translation and loses semantic accuracy
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows such
		an example where Big BIRD captures alignment
		197
		Source:
		(
	</Extractive Summary>
</Paper ID=ument918>


<Paper ID=ument919> <Table ID =1>
	<Abstractive Summary> =
		5
		Table 1: Experimental results
	</Abstractive Summary>
	<Extractive Summary> =
		5
		BLEU) and statistically signiﬁcant gains over
		vMF on both datasets, although there is no con-
		sistent winner among the two syn-margin vari-
		ants (Table 1)
	</Extractive Summary>
</Paper ID=ument919>


<Paper ID=ument919> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: Error margins and accuracies
	</Abstractive Summary>
	<Extractive Summary> =
		This is accompanied by increases
		in accuracy @2, @5 and @10 (Table 2)
	</Extractive Summary>
</Paper ID=ument919>


<Paper ID=ument92> <Table ID =1>
	<Abstractive Summary> =
		73
		Table 1: Experiments of corpus mixing
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the results
	</Extractive Summary>
	<Extractive Summary> =
		If both
		are removed, the performance is only comparable
		with the baseline src model (see Table 1)
	</Extractive Summary>
</Paper ID=ument92>


<Paper ID=ument92> <Table ID =2>
	<Abstractive Summary> =
		59
		Table 2: Ablation experiments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the experimental results
	</Extractive Summary>
</Paper ID=ument92>


<Paper ID=ument92> <Table ID =3>
	<Abstractive Summary> =
		36
		Table 3: Final results
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, the PartProj model gives
		only comparable performance with Src
	</Extractive Summary>
</Paper ID=ument92>


<Paper ID=ument92> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: Comparison with previous work (UAS)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results, where
		the UAS values are reported
	</Extractive Summary>
</Paper ID=ument92>


<Paper ID=ument920> <Table ID =1>
	<Abstractive Summary> =
		92
		Table 1: Evaluation results on WAT17 English⇔Japanese translation task
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		In Table 1 and Table 2, we present the experi-
		ment results measured by BLEU on WAT17 and
		IWSLT14
	</Extractive Summary>
</Paper ID=ument920>


<Paper ID=ument920> <Table ID =2>
	<Abstractive Summary> =
		41††
		Table 2: Evaluation results on IWSLT14 De-En
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		In Table 1 and Table 2, we present the experi-
		ment results measured by BLEU on WAT17 and
		IWSLT14
	</Extractive Summary>
</Paper ID=ument920>


<Paper ID=ument920> <Table ID =3>
	<Abstractive Summary> =
		97
		Table 3: Results on IWSLT14 De-En and WAT17 Ja-En for effectiveness of learning word order
	</Abstractive Summary>
</Paper ID=ument920>


<Paper ID=ument920> <Table ID =4>
	<Abstractive Summary> =
		95
		Table 4: Results of ablation experiments on IWSLT14
		De-En
	</Abstractive Summary>
</Paper ID=ument920>


<Paper ID=ument920> <Table ID =5>
	<Abstractive Summary> =
		76
		Table 5:
		Results of different attention scope on
		IWSLT14 De-En
	</Abstractive Summary>
</Paper ID=ument920>


<Paper ID=ument921> <Table ID =1>
	<Abstractive Summary> =
		5445
		Table 1: Examples of GPT-2 generated paraphrased sentences with scores for each pair
		Peters et al
	</Abstractive Summary>
</Paper ID=ument921>


<Paper ID=ument921> <Table ID =2>
	<Abstractive Summary> =
		5762
		Table 2: Average USE, ROUGE-L, BLEU Scores of
		the datasets
		4
		Experiments
		We implemented the system described above us-
		ing GPT-2 and trained it on the different datasets
		for various lengths of training5762
		Table 2: Average USE, ROUGE-L, BLEU Scores of
		the datasets
		4
		Experiments
		We implemented the system described above us-
		ing GPT-2 and trained it on the different datasets
		for various lengths of training
	</Abstractive Summary>
	<Extractive Summary> =
		By calculating USE, ROUGE-L and BLEU
		scores for each dataset we are able to quantify the
		quality of human-generated paraphrases and then
		use that as a comparison for the models generated
		sentences (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		By calculating USE, ROUGE-L and BLEU
		scores for each dataset we are able to quantify the
		quality of human-generated paraphrases and then
		use that as a comparison for the models generated
		sentences (see Table 2)
	</Extractive Summary>
</Paper ID=ument921>


<Paper ID=ument921> <Table ID =3>
	<Abstractive Summary> =
		824
		Table 3: Showing Candidates Selection and Scoring - *Selected Sentence
		and phrase selection to the original sentence to be
		useful paraphrases824
		Table 3: Showing Candidates Selection and Scoring - *Selected Sentence
		and phrase selection to the original sentence to be
		useful paraphrases
	</Abstractive Summary>
	<Extractive Summary> =
		When we further compared the results of unﬁl-
		tered examples generated by the model (Table 3)
		we observe that when the USE score is below 0
	</Extractive Summary>
	<Extractive Summary> =
		When we further compared the results of unﬁl-
		tered examples generated by the model (Table 3)
		we observe that when the USE score is below 0
	</Extractive Summary>
</Paper ID=ument921>


<Paper ID=ument922> <Table ID =1>
	<Abstractive Summary> =
		Number of tokens (+EOS)
		139465
		Percentage of function words
		68%
		Percentage of content words
		32%
		Table 1: Percentage of function and content words in
		the generated translation
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Table 1 shows the percentage of function and con-
		tent words generated by the trained model
	</Extractive Summary>
</Paper ID=ument922>


<Paper ID=ument922> <Table ID =2>
	<Abstractive Summary> =
		Method
		% for FWs
		% for CWs
		1
		RandomPermute
		33%
		6%
		2
		Uniform
		53%
		11%
		3
		ZeroOutMax
		52%
		15%
		4
		Aggregate(1+2+3)
		68%
		21%
		5
		ZeroOut
		9%
		0%
		6
		LastEncoderState
		20%
		2%
		7
		OnlyMax
		71%
		83%
		8
		KeepMaxUniformOthers
		86%
		86%
		Table 2: Percentage of the preserved function and content
		words in the proposed attention methods: Trying out all the
		methods to ﬁnd a counterfactual attention vector maximizes
		the chance of success
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Effectiveness of the proposed
		counterfactual attention methods
		Table 2 shows the percentage of function and
		content words for which counterfactual attention
		weights were found using the proposed attention
		methods
	</Extractive Summary>
	<Extractive Summary> =
		As evident from
		Table 2, this approach greatly increases the chance
		of ﬁnding a counterfactual attention
	</Extractive Summary>
	<Extractive Summary> =
		2
		Function words are more easily
		generated compared to content words
		An important observation in Table 2 is that the
		proposed methods are considerably more effective
		in preserving function words compared to con-
		tent words
	</Extractive Summary>
	<Extractive Summary> =
		Row 6 in Table 2 shows that there is a signiﬁ-
		cant gap between the LastEncoderState and
		the counterfactual methods proposed in Table 2
	</Extractive Summary>
</Paper ID=ument922>


<Paper ID=ument922> <Table ID =3>
	<Abstractive Summary> =
		In Table 4, we sort such tokens based on their
		coverage, which is the percentage of their total
		Token
		# preserved
		Coverage
		going
		310
		70%
		people
		237
		46%
		know
		219
		62%
		world
		215
		67%
		like
		189
		47%
		think
		176
		50%
		way
		162
		68%
		get
		160
		53%
		thing
		147
		79%
		things
		142
		56%
		time
		139
		54%
		see
		137
		51%
		years
		136
		64%
		make
		126
		49%
		little
		113
		55%
		just
		109
		29%
		really
		93
		37%
		bit
		92
		88%
		said
		89
		59%
		got
		86
		59%
		Table 3: Top 20 content words preserved by the aggre-
		gate method sorted by the number of times they were
		preserved
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 contains the top 20 content words sorted
		by the number of times they were preserved
	</Extractive Summary>
</Paper ID=ument922>


<Paper ID=ument922> <Table ID =4>
	<Abstractive Summary> =
		Token
		Coverage
		Total
		bit
		88%
		105
		course
		87%
		91
		thank
		83%
		89
		thing
		79%
		186
		fact
		78%
		74
		half
		78%
		27
		own
		75%
		75
		ones
		73%
		30
		states
		73%
		30
		difference
		71%
		21
		going
		70%
		444
		turns
		69%
		26
		way
		68%
		237
		able
		67%
		85
		world
		67%
		323
		doing
		66%
		103
		planet
		65%
		37
		years
		64%
		212
		know
		62%
		353
		united
		62%
		21
		Table 4: Top 20 content words preserved by the aggre-
		gate method sorted by percentage of their total occur-
		rences that are preserved (coverage)
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, we sort such tokens based on their
		coverage, which is the percentage of their total
		Token
		# preserved
		Coverage
		going
		310
		70%
		people
		237
		46%
		know
		219
		62%
		world
		215
		67%
		like
		189
		47%
		think
		176
		50%
		way
		162
		68%
		get
		160
		53%
		thing
		147
		79%
		things
		142
		56%
		time
		139
		54%
		see
		137
		51%
		years
		136
		64%
		make
		126
		49%
		little
		113
		55%
		just
		109
		29%
		really
		93
		37%
		bit
		92
		88%
		said
		89
		59%
		got
		86
		59%
		Table 3: Top 20 content words preserved by the aggre-
		gate method sorted by the number of times they were
		preserved
	</Extractive Summary>
</Paper ID=ument922>


<Paper ID=ument922> <Table ID =5>
	<Abstractive Summary> =
		3947
		60%
		of
		3003
		87%
		to
		2923
		86%
		and
		2639
		67%
		a
		2187
		65%
		that
		1936
		69%
		i
		1737
		76%
		&apos;s
		1732
		95%
		you
		1501
		72%
		it
		1497
		72%
		is
		1496
		88%
		in
		1364
		64%
		we
		1246
		64%
		they
		624
		69%
		&quot;
		620
		81%
		have
		613
		70%
		be
		582
		91%
		&apos;t
		580
		96%
		&apos;re
		542
		86%
		this
		541
		42%
		so
		531
		57%
		are
		526
		77%
		was
		514
		66%
		do
		433
		77%
		about
		417
		65%
		what
		415
		61%
		can
		400
		54%
		Table 5: Top 30 function words preserved by the aggre-
		gate method sorted by the number of times they were
		preserved
	</Abstractive Summary>
	<Extractive Summary> =
		We repeat the same
		process for function words (Table 5 and Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		As evident from Table 5, we have successfully
		yielded the same token in 94% of the occurrences
		of the EOS token but with a contradictory expla-
		nation
	</Extractive Summary>
</Paper ID=ument922>


<Paper ID=ument922> <Table ID =6>
	<Abstractive Summary> =
		Row 8 shows
		that when other source hidden states are uniformly
		combined, although the ratio of unaffected content
		words has increased by 3%, ratio of unaffected
		227
		Token
		Coverage
		Total
		&apos;t
		96%
		602
		&apos;s
		95%
		1819
		EOS
		94%
		6748
		be
		91%
		641
		is
		88%
		1707
		of
		87%
		3450
		to
		86%
		3383
		&apos;re
		86%
		631
		,
		85%
		8582
		&apos;m
		84%
		311
		been
		82%
		233
		lot
		82%
		148
		the
		82%
		6386
		&quot;
		81%
		770
		are
		77%
		679
		do
		77%
		565
		i
		76%
		2290
		who
		73%
		300
		it
		72%
		2089
		you
		72%
		2099
		have
		70%
		876
		up
		70%
		235
		they
		69%
		904
		that
		69%
		2812
		well
		67%
		153
		and
		67%
		3922
		was
		66%
		774
		were
		65%
		240
		same
		65%
		154
		a
		65%
		3369
		Table 6: Top 30 function words preserved by the aggre-
		gate method sorted by coverage
	</Abstractive Summary>
	<Extractive Summary> =
		We repeat the same
		process for function words (Table 5 and Table 6)
	</Extractive Summary>
</Paper ID=ument922>


<Paper ID=ument922> <Table ID =7>
	<Abstractive Summary> =
		98%
		6526
		?
		88%
		589
		&apos;t
		86%
		602
		!
		68%
		22
		&quot;
		60%
		770
		&apos;s
		24%
		1819
		;
		24%
		63
		are
		18%
		679
		is
		18%
		1707
		Table 7:
		Top 10 unaffected function words in the
		LastEncoderState method
	</Abstractive Summary>
	<Extractive Summary> =
		If we look at Table 7, we can see that the most
		covered functions words, are the words that usu-
		ally appear at the end of the sentence (e
	</Extractive Summary>
</Paper ID=ument922>


<Paper ID=ument923> <Table ID =1>
	<Abstractive Summary> =
		2
		Random Search
		As an alternative to grid-based searches, random
		hyperparameter search has been demonstrated to
		be a strong baseline for neural network architec-
		ture searches as it can search between grid points
		to increase the size of the search space (Bergstra
		234
		Dataset
		Size
		Ara–Eng
		234k
		Fra–Eng
		235k
		Hau–Eng
		45k
		Tir–Eng
		15k
		Table 1: Number of parallel sentences in training bi-
		texts
	</Abstractive Summary>
</Paper ID=ument923>


<Paper ID=ument923> <Table ID =2>
	<Abstractive Summary> =
		5k
		Table 2: Comparison of BLEU scores, model size, and training time on Tigrinya-English, Hausa-English, and
		French-English
	</Abstractive Summary>
	<Extractive Summary> =
		Random Search
		Table 2 compares the performance of random
		search with auto-sizing across, BLEU scores,
		model size, and training times
	</Extractive Summary>
</Paper ID=ument923>


<Paper ID=ument923> <Table ID =3>
	<Abstractive Summary> =
		1k
		Table 3: Overall training times in seconds on a Nvidia
		GeForce GTX 1080Ti GPU for small regularization
		values
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the total training time for
		both ℓ2,1 = 0
	</Extractive Summary>
</Paper ID=ument923>


<Paper ID=ument923> <Table ID =4>
	<Abstractive Summary> =
		0
		100%
		Table 4: BLEU scores and percentage of parameter rows deleted by auto-sizing on various sub-components of the
		model, across varying strengths of ℓ2,1 regularization
	</Abstractive Summary>
</Paper ID=ument923>


<Paper ID=ument923> <Table ID =5>
	<Abstractive Summary> =
		3
		100%
		Table 5: BLEU scores and percentage of model deleted using auto-sizing with various l∞,1 regularization strengths
	</Abstractive Summary>
</Paper ID=ument923>


<Paper ID=ument923> <Table ID =6>
	<Abstractive Summary> =
		4
		Table 6: Test BLEU scores for the models with the best
		dev perplexity found using random search over num-
		ber of layers and size of layers
	</Abstractive Summary>
</Paper ID=ument923>


<Paper ID=ument924> <Table ID =1>
	<Abstractive Summary> =
		12
		Table 1: Evaluation results (BLEU) on IWSLT 2014
		De–En task
	</Abstractive Summary>
</Paper ID=ument924>


<Paper ID=ument924> <Table ID =2>
	<Abstractive Summary> =
		61
		Table 2: Evaluation results (BLEU) on WMT 2017 Tr–
		En task
	</Abstractive Summary>
</Paper ID=ument924>


<Paper ID=ument924> <Table ID =3>
	<Abstractive Summary> =
		98
		Table 3: Training efﬁciency results on IWSLT 2014
		De–En dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows that CoNMT and PCoNMT can
		process 28 times faster than NPMT, and converge
		six times faster, i
	</Extractive Summary>
</Paper ID=ument924>


<Paper ID=ument924> <Table ID =4>
	<Abstractive Summary> =
		13
		Table 4: The Precision, Recall, and F1 evaluation re-
		sults on the fertility prediction of Fertility2
	</Abstractive Summary>
	<Extractive Summary> =
		The comparison be-
		tween Table 5 and Table 4 shows that the Fertility2
		has slighly higher F-1 score than Fertility4 in both
		datasets
	</Extractive Summary>
</Paper ID=ument924>


<Paper ID=ument924> <Table ID =5>
	<Abstractive Summary> =
		15
		0%
		0
		0
		0
		Table 5: The Precision, Recall, and F1 evaluation re-
		sults on the fertility prediction of Fertility4
	</Abstractive Summary>
	<Extractive Summary> =
		We evaluate the prediction results on the test set
		246
		with the gold label obtained from the word align-
		ment model in Table 5 and Table 4
	</Extractive Summary>
	<Extractive Summary> =
		The comparison be-
		tween Table 5 and Table 4 shows that the Fertility2
		has slighly higher F-1 score than Fertility4 in both
		datasets
	</Extractive Summary>
</Paper ID=ument924>


<Paper ID=ument924> <Table ID =6>
	<Abstractive Summary> =
		50
		Table 6: Percentages of categories of randomly sam-
		pled 100 phrases generated by PCoNMT on IWSLT
		2014 De–En test set and the accuracy of PCoNMT and
		CoNMT phrase translations, respectively
	</Abstractive Summary>
	<Extractive Summary> =
		The results in Table 6 show that the most fre-
		quently generated phrases are collocations (56%)
		followed by verb phrases (28%) and compound
		nouns (16%)
	</Extractive Summary>
</Paper ID=ument924>


<Paper ID=ument924> <Table ID =7>
	<Abstractive Summary> =
		PCoNMT
		I ’m full of contradictions
		Table 7: Translation output examples from CoNMT and PCoNMT systems
	</Abstractive Summary>
</Paper ID=ument924>


<Paper ID=ument925> <Table ID =1>
	<Abstractive Summary> =
		, sn) with n
		words, the task is to generate an alternative output
		250
		S: What are the dumbest questions ever asked
		on Quora?
		G: what is the stupidest question on quora?
		R: What is the most stupid question asked on
		Quora?
		S: How can I lose fat without doing any aero-
		bic physical activity
		G: how can i lose weight without exercise?
		R: How can I lose weight in a month without
		doing exercise?
		S: How did Donald Trump won the 2016 USA
		presidential election?
		G: how did donald trump win the 2016 presi-
		dential
		R: How did Donald Trump become presi-
		dent?
		Table 1:
		Examples of our generated paraphrases on
		the QUORA sampled test set, where S, G, R repre-
		sents Source, Generated and Reference sentences re-
		spectively
	</Abstractive Summary>
</Paper ID=ument925>


<Paper ID=ument925> <Table ID =2>
	<Abstractive Summary> =
		Table 2:
		Examples of our generated paraphrases on
		the MSCOCO sampled test set, where S, G, R repre-
		sents Source, Generated and Reference sentences re-
		spectively
	</Abstractive Summary>
</Paper ID=ument925>


<Paper ID=ument925> <Table ID =3>
	<Abstractive Summary> =
		54
		Table 3: Performance of our model against various models on the QUORA dataset with 50k,100k,150k training
		examples
	</Abstractive Summary>
</Paper ID=ument925>


<Paper ID=ument925> <Table ID =4>
	<Abstractive Summary> =
		3
		Table 4: Performance of our model against various models on the MSCOCO dataset
	</Abstractive Summary>
</Paper ID=ument925>


<Paper ID=ument926> <Table ID =1>
	<Abstractive Summary> =
		46M
		Rotowire
		3247
		Table 1: Sentence-parallel training corpora statistics
	</Abstractive Summary>
	<Extractive Summary> =
		4,5
		Table 1 summarises the number of sentences of
		each corpus in the pre-processed sentence-parallel
		dataset
	</Extractive Summary>
</Paper ID=ument926>


<Paper ID=ument926> <Table ID =2>
	<Abstractive Summary> =
		71
		Table 2: BLEU scores for the Transformer vs
	</Abstractive Summary>
	<Extractive Summary> =
		2
		English→German
		It can be seen from Table 2 that for all runs, the
		document-level models outperform the sentence-
		level baseline trained with 4 times the data
	</Extractive Summary>
</Paper ID=ument926>


<Paper ID=ument926> <Table ID =3>
	<Abstractive Summary> =
		62
		Table 3: BLEU scores for the Transformer vs
	</Abstractive Summary>
	<Extractive Summary> =
		3
		German→English
		From Table 3, we see that the document-level
		models again outperform the sentence-level Trans-
		former baseline for all runs by a wider margin
		than for English→German
	</Extractive Summary>
</Paper ID=ument926>


<Paper ID=ument926> <Table ID =4>
	<Abstractive Summary> =
		39
		Table 4: BLEU scores for submitted systems
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the scores as provided by
		the WNGT organisers
	</Extractive Summary>
</Paper ID=ument926>


<Paper ID=ument927> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example of data-records (left) and document summary (right) from the ROTOWIRE dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 illus-
		trates an example of data-to-text NLG, with statis-
		tics of a NBA basketball game (top) and the corre-
		sponding game summary (bottom)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 illustrates an ex-
		ample of the dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 il-
		lustrates an example of the dataset
	</Extractive Summary>
</Paper ID=ument927>


<Paper ID=ument927> <Table ID =2>
	<Abstractive Summary> =
		22
		Table 2: Automatic evaluation on ROTOWIRE devel-
		opment set using relation generation (RG) count (#)
		and precision (P%), content selection (CS) precision
		(P%) and recall (R%), content ordering (CO) in nor-
		malized Damerau-Levenshtein distance (DLD%), and
		BLEU
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 2,
		we can see that the RG and CS precisions are both
		improved by 2
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, as expected,
		the model after ﬁne-tuning generates more rela-
		tions in the output summaries
	</Extractive Summary>
	<Extractive Summary> =
		As shown in
		Table 2, all content-oriented evaluation metrics
		are improved compared to DATA-TRANS but not
		as much as each single of the data augmentation
		method
	</Extractive Summary>
</Paper ID=ument927>


<Paper ID=ument927> <Table ID =3>
	<Abstractive Summary> =
		14
		Table 3: Automatic evaluation on ROTOWIRE test set
	</Abstractive Summary>
</Paper ID=ument927>


<Paper ID=ument928> <Table ID =1>
	<Abstractive Summary> =
		271
		Train
		Dev
		Test
		English
		3398
		727
		728
		German
		242
		240
		241
		Table 1:
		Count of examples in Training, Devel-
		opment and Test sections of English and German
		dataset
	</Abstractive Summary>
</Paper ID=ument928>


<Paper ID=ument928> <Table ID =2>
	<Abstractive Summary> =
		95
		Table 2:
		Automatic evaluation for track 1/2 on
		the ROTOWIRE test set using record generation
		(RG) precision, content selection (CS) precision
		and recall, content ordering (CO) in normalized
		Damerau-Levenshtein distance, and BLEU
	</Abstractive Summary>
	<Extractive Summary> =
		7
		Results
		Table 2 shows our results for English and German
		datasets on the Test set as provided by the shared
		task organizers
	</Extractive Summary>
</Paper ID=ument928>


<Paper ID=ument928> <Table ID =3>
	<Abstractive Summary> =
		47
		Table 3:
		Size (number of parallel training sen-
		tences) in 100,000 of the EN-DE training data
	</Abstractive Summary>
	<Extractive Summary> =
		1
		In-Domain Parallel Data
		Table 3 highlights the extremely limited amount of
		in-domain parallel training data used; ROTOWIRE
		English-German makes up only 0
	</Extractive Summary>
</Paper ID=ument928>


<Paper ID=ument928> <Table ID =4>
	<Abstractive Summary> =
		48
		Table 4: Track 3-6: ROTOWIRE dev set results,
		showing BLEU without monolingual data Parallel
		and with monolingual data Monolingual
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Results on the development set in Table 4 show
		that the inclusion of monolingual data leads to
		a signiﬁcant increase in bleu (between 5 and 7
		points)
	</Extractive Summary>
</Paper ID=ument928>


<Paper ID=ument928> <Table ID =5>
	<Abstractive Summary> =
		15
		Table 5:
		Automatic evaluation for track 3-6 on
		the ROTOWIRE test set using record generation
		(RG) precision, content selection (CS) precision
		and recall, content ordering (CO) in normalized
		Damerau-Levenshtein distance, and BLEU
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows test set results for both En-
		glish and German target languages
	</Extractive Summary>
</Paper ID=ument928>


<Paper ID=ument929> <Table ID =1>
	<Abstractive Summary> =
		1M
		534M
		Table 1: Statistics of the allowed resources
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 describes the al-
		lowed parallel and monolingual corpora
	</Extractive Summary>
</Paper ID=ument929>


<Paper ID=ument929> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Metadata: our metadata encoding
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Qualitative evaluation
		As shown in Table 2,
		the NLG model (3-
		player) has several good properties besides coher-
		ent document-level generation and the ability to
		“copy” metadata
	</Extractive Summary>
</Paper ID=ument929>


<Paper ID=ument929> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Doc-level BLEU scores on the DGT valid and
		test sets of our submitted models in all tracks
	</Abstractive Summary>
	<Extractive Summary> =
		The scores are shown in Table 3, and
		a description of the submitted models is given in
		Table 4
	</Extractive Summary>
</Paper ID=ument929>


<Paper ID=ument929> <Table ID =4>
	<Abstractive Summary> =
		7
		BLEU improvement on Rotowire-test by our En-
		glish NLG model compared to the previous state
		277
		Track
		N best players
		Details
		NLG (EN)
		4
		Rotowire BT + DGT-train + tags
		NLG (DE)
		6
		Rotowire BT + DGT-train + tags
		MT (DE-EN)
		N/A
		Unconstrained: Rotowire BT +
		DGT-train + tags + ensemble
		Constrained: DGT-train only +
		ensemble
		MT (EN-DE)
		N/A
		DGT-train only + ensemble
		MT+NLG (EN)
		3
		Rotowire BT + DGT-train + 20%
		text masking + tags + ensemble
		MT+NLG (DE)
		3
		Rotowire BT + DGT-train +
		tags + ensemble
		Table 4: Description of our submissions
	</Abstractive Summary>
</Paper ID=ument929>


<Paper ID=ument929> <Table ID =5>
	<Abstractive Summary> =
		3
		Table 5: BLEU scores of the MT models at different
		stages of training, and comparison with the state of the
		art
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the BLEU scores
		of our MT models at different stages of training
		(sent-level, doc-level, fine-tuned), and compares
		them against one of the top contestants of the
		WMT19 news translation task (Ng et al
	</Extractive Summary>
</Paper ID=ument929>


<Paper ID=ument929> <Table ID =6>
	<Abstractive Summary> =
		2
		Table 6: English NLG comparison against state-of-the-
		art on Rotowire-test
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows a 5
	</Extractive Summary>
</Paper ID=ument929>


<Paper ID=ument929> <Table ID =7>
	<Abstractive Summary> =
		7
		Table 7: English NLG ablation study, starting from a
		3 best player baseline (the submitted NLG model has 4
		players)
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 7, we see that sorting players helps,
		but only slightly
	</Extractive Summary>
</Paper ID=ument929>


<Paper ID=ument929> <Table ID =8>
	<Abstractive Summary> =
		Season-level
		player stats (-)
		NLG: It was a season-high in points for Thomas , who ’s now averaging 17 points per game
		on the season
		Table 8: Correctly predicted information that is not explicitly in the metadata (+), or hallucinations (-)
	</Abstractive Summary>
	<Extractive Summary> =
		As such, it can
		generate relevant information which is absent from
		metadata (see Table 8)
	</Extractive Summary>
</Paper ID=ument929>


<Paper ID=ument93> <Table ID =1>
	<Abstractive Summary> =
		10
		Table 1: Dependency parsing results on 7 UD Treebanks
	</Abstractive Summary>
</Paper ID=ument93>


<Paper ID=ument93> <Table ID =2>
	<Abstractive Summary> =
		03
		Table 2: Dependency parsing results on English Penn
		Treebank v3
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2
		presents the results on English Penn Treebank
	</Extractive Summary>
</Paper ID=ument93>


<Paper ID=ument93> <Table ID =3>
	<Abstractive Summary> =
		2
		Table 3: Discourse parsing results with gold segmen-
		tation
	</Abstractive Summary>
</Paper ID=ument93>


<Paper ID=ument930> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Effects of noisy backward-forward translation
		(NBFT) and Multi-Agent Dual Learning on teacher-
		student training (newstest2014)
		It seems unusual to feed our student with de-
		graded training data, but the goal is to closely
		mimic the teacher
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 contains BLEU scores of the teacher
		ensemble (T) and a student model distilled from
		this teacher (Student ← T)
	</Extractive Summary>
	<Extractive Summary> =
		9 BLEU (Table 1)
	</Extractive Summary>
</Paper ID=ument930>


<Paper ID=ument930> <Table ID =2>
	<Abstractive Summary> =
		84)
		Table 2: Relative speed-up for new CPU-bound op-
		timizations compared to ﬂoat32 MKL baseline and
		WNMT2018 mixed precision inference (in parenthe-
		ses) for same SSRU-Tied student model
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 2 we see a respectable speed-up against
		a pure MKL ﬂoat32 version and our mixed 32/16-
		bit inference (in parentheses)
	</Extractive Summary>
	<Extractive Summary> =
		The combined
		speed-up (Table 2) from these optimizations further
		improves on top of the “fancier” methods
	</Extractive Summary>
</Paper ID=ument930>


<Paper ID=ument930> <Table ID =3>
	<Abstractive Summary> =
		5
		Table 3: Conﬁguration of student models and submissions
	</Abstractive Summary>
	<Extractive Summary> =
		Until this moment, we kept model dimensions
		and decoder depth constant while optimizing a
		conﬁguration that corresponds to the Microsoft in-
		production models (bold row in Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		The ﬁle sizes reported in Table 3 refer to pre-
		packed 8-bit models with 32-bit embeddings for
		the CPU, and to models stored in FP16 for the GPU
	</Extractive Summary>
</Paper ID=ument930>


<Paper ID=ument931> <Table ID =1>
	<Abstractive Summary> =
		09
		–
		Table 1: Machine translation results measured with
		sacre-BLEU and task-speciﬁc tokenization1
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Results
		Table 1 shows the results of our systems for both
		directions measured with sacre-BLEU
	</Extractive Summary>
</Paper ID=ument931>


<Paper ID=ument931> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: Content evaluation of the German-to-English translation models on test and dev-sets from parallel Ro-
		towire using the IE models of (Puduppully et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the
		values for development and test sets
	</Extractive Summary>
</Paper ID=ument931>


<Paper ID=ument931> <Table ID =3>
	<Abstractive Summary> =
		70
		Table 3: Evaluation of Content Selection and Plan-
		ning (CSP) module, with and without joint training
		with the Text Generator (TG)
	</Abstractive Summary>
	<Extractive Summary> =
		Re-
		sults in Table 3 show that our model outperforms
		the baseline (Puduppully et al
	</Extractive Summary>
</Paper ID=ument931>


<Paper ID=ument931> <Table ID =4>
	<Abstractive Summary> =
		27
		Table 4: Generation results of our submitted systems as reported by the shared task organizers (Hayashi et al
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		We ﬁnd that the template rewriting approach is
		competitive with the end-to-end trained models in
		terms of content metrics (Table 4), and subjec-
		tively appears to create natural sounding genera-
		tions
	</Extractive Summary>
</Paper ID=ument931>


<Paper ID=ument932> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1: Comparison of BLEU scores and model sizes on newstest2014 and newstest2015
	</Abstractive Summary>
</Paper ID=ument932>


<Paper ID=ument933> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Distribution of labels in the PharmaCoNER
		datasets30
		Table 1: All scores in %
		Table 1: The parameters of the neural model and
		their values used for the PharmaCoNER results81
		Table 1: The results we have obtained for NER task6
		Table 1: Precision (P), Recall (R) and F1 for Task 1io/
		39
		Collection\Corpus
		IBECS
		SciELO
		MedlineNLM
		MedlinePlus
		UFAL
		Documents
		168,198
		161,710
		330,928
		1,063
		265,410
		Words
		23,648,768
		26,169,655
		4,710,191
		217,515
		41,604,517
		Unique Words
		184,936
		159,997
		20,942
		5,099
		198,424
		Table 1: Biomedical Spanish corpus details22
		Table 1: Performance of NER and CI on the test set
		Label
		P(%)
		R(%)
		F(%)
		Prediction
		Annotation
		Correct
		NORMALIZABLES
		89
		We participate in the PharmaCoNER task using
		a collection of tools developed for English as well
		Item
		Train
		Devel
		Documents
		500
		250
		Tokens
		177 022
		85 148
		Annotations
		3 822
		1 926
		Protein
		1 405
		745
		Chemical(+)
		2 304
		1 121
		Chemical(-)
		24
		16
		Other
		89
		44
		Table 1: Data statistics
		Table 1: Statistics of annotations in total, training and test sets
		Total
		Training set
		Test set
		# of Abstracts
		500
		250
		250
		# of Sentences
		5,080
		2,534
		2,546
		# of Named entities
		5,741
		3,317
		2,424
		�Bio-concept Named Entities
		2,274
		1,428
		846
		Var (Variation)
		1,304
		735
		569
		MPA (Molecular Physiological Activity)
		618
		418
		200
		Interaction
		35
		28
		7
		Pathway
		38
		24
		14
		CPA (Cell Physiological Activity)
		279
		223
		56
		�Regulatory Named Entities
		1,514
		905
		609
		Regulation
		613
		215
		398
		Positive Regulation
		406
		323
		83
		Negative Regulation
		495
		367
		128
		�Other Entities
		1,953
		984
		969
		Disease
		751
		336
		415
		Gene
		1,004
		529
		475
		Protein
		150
		90
		60
		Enzyme
		48
		29
		19
		# of Thematic roles
		4,677
		2,729
		1,948
		ThemeOf
		2,986
		1,698
		1,288
		ThemeOf (Intra/inter sentential)
		(2910/76)
		(1657/41)
		(1253/35)
		CauseOf
		1,691
		1,031
		660
		CauseOf (Intra/inter sentential)
		(1581/110)
		(961/70)
		(620/40)
		64
		4
		300
		301
		302
		303
		304
		305
		306
		307
		308
		309
		310
		311
		312
		313
		314
		315
		316
		317
		318
		319
		320
		321
		322
		323
		324
		325
		326
		327
		328
		329
		330
		331
		332
		333
		334
		335
		336
		337
		338
		339
		340
		341
		342
		343
		344
		345
		346
		347
		348
		349
		350
		351
		352
		353
		354
		355
		356
		357
		358
		359
		360
		361
		362
		363
		364
		365
		366
		367
		368
		369
		370
		371
		372
		373
		374
		375
		376
		377
		378
		379
		380
		381
		382
		383
		384
		385
		386
		387
		388
		389
		390
		391
		392
		393
		394
		395
		396
		397
		398
		399
		Conﬁdential Review Copy For
		Dataset
		#Train
		#Dev
		#Test
		BC5CDR
		4,559
		4,580
		4,796
		NCBI disease
		5,423
		922
		939
		BC2GM
		12,573
		2,518
		5,037
		2010 i2b2/VA
		16,315
		-
		27,626
		Table 1: Datasets for joint learning in recognizing the
		trigger words Based on the 
		value of roleRatio, we split all the entities into 3 
		subgroups, each containing 4 entity types:  
		Subgroup 
		Entities 
		A 
		PosReg (positive regulation), NegReg 
		(negative regulation), Reg (regulation), 
		Interaction 
		B 
		Gene, Pathway, Protein, Disease 
		C 
		Enzyme, Var (variation), CPA (cell 
		physiological activity), MPA 
		(Molecular physiological activity) 
		Table 1: Subgroups of 12 Entities for Task 
		Decomposition  
		In subgroup A, the roleRatio values of all the 
		entities are all less than 1 indicating they are more 
		likely to be the tail entity of a relation070
		Table 1: Model results on the four pre-deﬁned classes,
		as well as ‘No rel’ (the negative class) when the macro-
		averaged F1-score (over the positive labels only) is
		used as our early stopping criterion379841E-6
		Table 1: SNOMED CT word features for labels PROTEINAS and NORMALIZABLES
		word receiving the best label by the softmax
		output, accumulating labels as the window passes
		by1
		Table 1:
		Detailed Hyper-parameter Settings in the
		PharmaCoNER task
		Table 1: Input Features of Our Model and Their Description
		as:
		EyS =
		N
		�
		i=1
		Vi
		(2)
		T represents transition score which can be deﬁned
		as:
		TyS =
		N
		�
		i=1
		TMyS
		i−1,yS
		i
		(3)
		where TMyi−1,yi means the transition probability
		from tag yi−1 to yi60
		Table 1: The F1 score of CNN and CNN-LSTM on the
		dev data set for SeeDev-binary task
		2
		118
		Resource
		Input text
		Snomed Term
		Snomed Code
		Wikidata
		adriamicina
		doxorrubicina
		372817009
		Chemical symbols
		Na
		sodio
		39972003
		AbreMES-DB
		Hb
		hemoglobina
		38082009
		Hunspell Library
		6-Metil-Prednisolona
		metilprednisolona
		116593003
		Table 1: Examples of Snomed concept indexing723
		Table 1:
		Inter-annotator agreement metrics (SER
		stands for Slot Error Rate) Egg Pulp, 97 encoded as Ccc Ccccp nn
		tri-gram
		tri-gram as features
		ﬁve-gram
		ﬁve-gram as features
		length
		length of the word
		sdp-rel
		dependency relation tag
		alpha-features
		detect if certain linguistic pattern occurred
		in the current word or the next word
		Table 1: Word-level features for NER
		153
		Table 1: Distributed vs Smart Matching for relation ex-
		traction
		The fea-
		Train
		Dev
		Lives In
		715
		395
		Exhibits
		281
		138
		Total relatonships
		996
		533
		Intra-sentence relationships
		885
		467
		Inter-sentence relationships
		111
		66
		Table 1: BB-rel data statistics on the training and de-
		velopment set8536 
		Table 1:  The BB task results comparison0
		Table 1:
		Results showing the average score over all
		test documents for each metric from the structural an-
		notation (dependency parse construction) task for all
		participating teams7714
		*ontology pretraining disabled
		Table 1: F-Score results of our experiments using the CRAFT corpus
		14,679
		1,623
		7,185
		Table 1: Characteristics of the CRAFT corpus While these efforts have
		covered a wide range of languages, genres and text
		domains, and introduced end-to-end parsing from
		plain text as the objective, they have not speciﬁ-
		Train
		Test
		Documents
		67
		30
		Sentences
		21 731
		9 099
		Tokens
		561 032
		232 619
		Table 1: CRAFT Structural Annotation statistics
		Train
		Devel
		Eval
		Documents
		47
		10
		10
		Sentences
		15 007
		3 421
		3 303
		Tokens
		387 473
		91 306
		82 253
		Table 2: CRAFT Train data split for development
		cally involved scientiﬁc articles or biomedical do-
		main texts
		Table 1: Subset of RDoC constructs used for this task
		and their domain
		Table 1: RDoC construct - This table shows two PubMed abstracts labelled with two different RDoC construct
		and PubMed ID (PMID)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Evaluation
		Results
		Table 1 compares the scores of our sys-
		tems
	</Extractive Summary>
	<Extractive Summary> =
		1
		Experimental Results
		In Table 1, we report the results on the Pharma-
		CoNER development and test sets using the ofﬁ-
		cial shared task evaluation metrics
	</Extractive Summary>
	<Extractive Summary> =
		The statistics 
		of the corpus, including the number of 
		documents, chemical & drug mentions in 
		different types are listed in Table 1, where “UNK” 
		denotes unknown
	</Extractive Summary>
	<Extractive Summary> =
		Table 10 shows our result for valid and test dataset
		43
		Detail
		W2V-
		SBWC
		FastText-
		SBWC
		FastText-
		SBC
		SNOMED-
		SBC
		Reddit
		Type
		Word
		Word
		Word
		Concept
		Sense
		Corpus size
		1
	</Extractive Summary>
	<Extractive Summary> =
		64
		Table 10: Results for PharmaCoNER track 2 on valid and test dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the ﬁve submitted results of NER
		and CI in terms of F-score on the test set
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 brieﬂy summarizes data statis-
		tics
	</Extractive Summary>
	<Extractive Summary> =
		i) Imbalanced Data: The statistics in Table 1
		clearly shows that the entity distribution is
		imbalanced over the entity types, e
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the exter-
		nal data sets used under the joint learning method
	</Extractive Summary>
	<Extractive Summary> =
		Results show that each feature listed in Table 1
		plays a key role
	</Extractive Summary>
	<Extractive Summary> =
		In Table 1 we can see some examples of how
		the resources and tools applied in the archi-
		tecture can contribute to the achievement of
		Snomed concept mapping
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes
		the inter-annotator agreement for named entities,
		normalization and relations
	</Extractive Summary>
	<Extractive Summary> =
		3
		BB-kb and BB-kb+ner
		BLAIR GMU is the only team to submit to the
		BB-kb and BB-kb+ner tasks, their results are
		shown in Table 10
	</Extractive Summary>
	<Extractive Summary> =
		259
		Table 10: Results for the BB-kb and BB-kb+ner sub-
		tasks
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the detailed data statistics 
		provided by the task
	</Extractive Summary>
	<Extractive Summary> =
		While there is slight
		increase in the precision, the recall is observed
		to decrease considerably for the smart matching
		method (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, while the performance of 
		our system is average compared to those of other 
		teams in the BB-rel subtask, we ranked second 
		among all participants in the BB-rel+ner, BB-norm 
		and BB-norm+ner subtasks
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		Two teams submitted ﬁve runs in total for the
		CRAFT-SA task (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the
		statistics numbers of these three subsets
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summa-
		rizes the key statistics of the data
	</Extractive Summary>
	<Extractive Summary> =
		Similarly, they per-
		formed well for abstracts with more gold-standard
		sentences (see Table 10)
	</Extractive Summary>
	<Extractive Summary> =
		We would
		like to thank Robell Basset, Lenin Lewis, Ninoo
		Table 10: Variation of Accuracy over the number of
		most relevant (gold-standard) sentences in abstracts
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the association be-
		tween PubMed abstracts and RDoC constructs de-
		pending on the semantic knowledge of the high-
		lighted content words
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument933> <Table ID =2>
	<Abstractive Summary> =
		com/PlanTL-SANIDAD/
		PharmaCoNER-CODALAB-Evaluation-Script
		4
		Table 2: Overview of Team Participation in the PharmaCoNER trackXHPEFYUo85k
		Table 2: Oﬃcial results of the neural Model for the
		two tasks of the PharmaCoNER34
		Table 2: Results for Concept Indexing task00
		Ghz,
		8
		Cores, 16 Logi-
		cal Processors
		RAM
		32 Gb
		32 Gb
		Corpus Size
		1Gb
		1Gb
		Training Time
		4 hours
		8 hours
		Table 2: Training parameters for embeddings models built in this work76
		1, 836
		1, 876
		1, 600
		Table 2: Sub-task 1: Categorical performance on the test set
		dient clipping to 5, the dropout rate to 047
		Table 2: NERsuite development set results for various boundary matching criteria (precision/recall/F-score)
		Table 2: Participants Performance of Task 1
		Participants
		Precision
		Recall
		F-score
		Main NLP techniques
		1st
		DX-HITSZ
		0847
		Table 2: Model comparision in development set with
		different pre-trained models
		development set results34 
		Table 2:  Official Submission Results in AGAC 
		 
		Entity Name 
		Precision 
		Recall 
		F1 
		Var 
		0034
		Table 2: Model results on the four pre-deﬁned classes,
		as well as ‘No rel’ (the negative class) when the macro-
		averaged F1-score (over all labels) is used as our early
		stopping criterion8318
		Table 2: Development results of RACAI’s NER sys-
		tems
		System
		P
		R
		F1
		Baseline
		002
		Table 2:
		The Experimental Results of Multilingual-
		BERT and BioBERT
		107
		Table 2: The Statistics of The Dataset: Number of
		Files, Relations and Entities
		File
		Entity
		Relation
		train
		133
		2266
		1127
		development
		66
		1271
		608
		test
		32
		Unknown
		Unknown
		Table 3: Performance for Each Relation Type
		SER
		precision
		recall
		All-types
		033
		Table 2: The F1, recall and precision of cluster on the
		test data set for SeeDev-binary task
		Team
		F1
		Recall
		Precision
		MIC-CIS-1
		076965
		Table 2: Results of Track 1
		Documents
		392
		Words
		60,402
		Unique words
		12,566
		Sentences
		2,646
		Entity mentions
		7,232
		Unique entity mentions
		3,300
		Concepts
		1,072
		Relations
		3,578
		Unique relations between concepts
		1,931
		Table 2: Global statistics of the corpus
		In the following, we present more detailed
		statistics and highlight corpus characteristics that
		may be challenging for the participants word tokens between
		entity-type
		type of the entity mentions
		target entities) including three
		words to the target entities
		bow-lemma
		bow representation of the lemmatized
		dist-entities-cat
		distance between target entities
		tokens in the between context
		as categorical
		pos-tags
		part-of-speech tags
		dist-entities
		distance between target entities
		sdp
		shortest dependency path as bow
		entity-count
		count of entities in between context
		sdp-len
		length of shortest dependency path
		entity-count-cat
		count of entities in between context
		as scalar
		as categorical
		sdp-rel
		dependency relation tag
		e1 type = e2 type
		if type of e1 and e2 is same
		emb-sdp
		average embeddings of sdp
		sdp-entity
		sdp with entity as bow
		keyword-vec
		if current word is part of feature
		entity-patterns
		check if certain linguistic patterns
		list of relations
		occur in the vicinity of target entities
		Table 2: General and Entity features used in Relation Extraction
		Tokens
		Models
		Voting
		Post-
		M1
		M2
		M3
		processing
		r1
		Presence
		O
		O
		O
		O
		O
		r2
		of
		O
		O
		B-H
		O
		O
		r3
		ﬁsh
		I-H
		B-H
		I-H
		I-H
		B-H
		r4
		pathogen
		I-H
		I-P
		I-P
		I-P
		B-P
		r5
		Vibrio
		B-M
		B-M
		B-M
		B-M
		B-M
		r6
		salmonicida
		I-M
		O
		I-M
		I-M
		I-M
		r7
		in
		B-H
		O
		O
		O
		O
		r8
		ﬁsh
		B-H
		O
		B-H
		B-H
		B-H
		r9
		farm
		I-H
		O
		I-M
		I-H
		I-H
		r10
		
		Table 2: Soft Filter vs Hard Filter for relation extrac-
		tion5
		Optimizer
		sgd
		MLP layer
		1
		Table 2: Hyper-parameter setting mentions
		4,485
		1,845
		Table 2: Descriptive statistics of the coreference reso-
		lution annotations in the CRAFT training and test sets3384
		*ontology pretraining disabled
		Table 2: SER results of our experiments using the CRAFT corpus93
		Table 2: The most frequent patterns of mentions in the
		training set While these efforts have
		covered a wide range of languages, genres and text
		domains, and introduced end-to-end parsing from
		plain text as the objective, they have not speciﬁ-
		Train
		Test
		Documents
		67
		30
		Sentences
		21 731
		9 099
		Tokens
		561 032
		232 619
		Table 1: CRAFT Structural Annotation statistics
		Train
		Devel
		Eval
		Documents
		47
		10
		10
		Sentences
		15 007
		3 421
		3 303
		Tokens
		387 473
		91 306
		82 253
		Table 2: CRAFT Train data split for development
		cally involved scientiﬁc articles or biomedical do-
		main texts
		Table 2: Inter-annotator agreement of Task 1 and Task
		2 Here, rsup
		siamese refers to a relevance
		232
		L1
		L2
		L3
		L4
		L5
		L6
		L7
		L8
		Total
		All data
		39
		38
		47
		21
		28
		27
		48
		18
		266
		Train set
		31
		30
		37
		16
		22
		21
		38
		14
		209
		Dev set
		8
		8
		10
		5
		6
		6
		10
		4
		57
		Test set (Task1)
		79
		108
		123
		144
		138
		139
		122
		146
		999
		Test set (Task2)
		19
		26
		30
		35
		34
		34
		30
		36
		244
		Table 2: Data statistics - # of PubMed abstracts belong-
		ing to each RDoC construct in different data partitions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the categorical performances us-
		ing ensemble learning of NER on the test set
	</Extractive Summary>
	<Extractive Summary> =
		The results of Task 1,
		2, and 3 are presented in Table 2, 3, 4, respectively
	</Extractive Summary>
	<Extractive Summary> =
		3
		Main Results
		Table 2 compares the results of the two tasks of
		the pre-trained model in trigger words NER and
		thematic roles identiﬁcation
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 2 presents the runs of the RACAI baseline
		system, RPCN and of four ensemble methods ap-
		plied to the baseline (ﬁrst input) and RPCN (sec-
		ond input)
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the F1, recall and precision of
		cluster on the test data sets, and Table 3 shows the
		result of all types on the test data sets
	</Extractive Summary>
	<Extractive Summary> =
		3
		Descriptive Statistics
		Table 2 gives the size of the corpus, in terms of
		documents, words, sentences and annotated ele-
		7http://2016
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 lists computed gen-
		eral and entity features
	</Extractive Summary>
	<Extractive Summary> =
		Performance of different models on develop-
		ment dataset (Precision) 
			
		As shown in Table 2, non-supervised baseline 
		model yielded a precision score of 0
	</Extractive Summary>
	<Extractive Summary> =
		The performance
		of each approach is tested on the development data
		set (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		1
		Patterns of Gold Mentions
		Table 2 reports some patterns4 with the highest
		frequencies in the training set
	</Extractive Summary>
	<Extractive Summary> =
		The statistics of this split are shown in Table 2
		The original CRAFT corpus syntactic annota-
		tion uses a modiﬁed Penn Treebank (PTB) con-
		stituency formalism (Verspoor et al
	</Extractive Summary>
	<Extractive Summary> =
		Number of abstracts for each RDoC construct is
		described in Table 2, where ﬁrst row describes
		the statistics for all abstracts and second & third
		row shows the split of those abstracts into training
		and development sets maintaining a 80-20 ratio for
		each RDoC construct
	</Extractive Summary>
	<Extractive Summary> =
		(Refer to Table 2 for header notations) (mAP: “Mean Average Precision”; MAA:
		Macro-Average Accuracy)
		PubMed Abstract
		(PMID: “23386529”; RDoC construct: “Loss”)
		Most Relevant Sentence
		(using reRank(BM25-Extra))
		Sentence
		ID
		Gold
		Label
		Nurses are expected to care for
		grieving women and families
		suffering from perinatal loss
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument933> <Table ID =3>
	<Abstractive Summary> =
		Table 4 shows the results for sub-track 2 (Con-
		cept Indexing), ordered by team performance (ﬁrst
		5
		Table 3: Results for sub-track 1: NER offset and entity type classiﬁcation
		In addition, other complex concept indexing
		rules could be applied to chose the best nor-
		19
		Table 3: Performance of the neural model for each category in the Named Entity Recognition Task of
		the PharmaCoNER
		63
		64
		O
		O
		Table 3: Tokens annotated with BMEWO-V encoding in the ConLL-2003 format07
		Table 3: Performance of NER and CI on the development set
		Label
		P(%)
		R(%)
		F(%)
		Prediction
		Annotation
		Correct
		NORMALIZABLES
		9177
		Table 3: BERT development set results for various boundary matching criteria (precision/recall/F-score)
		Table 3: Participants Performance of Task 2
		Participants
		Precision
		Recall
		F-score
		Main NLP techniques
		1st
		Zheng-UMASS
		060
		Table 3: Precision (P), Recall (R) and F1 scores in test
		set of Task 145 
		Table 3:  Entity-level NER Performance of Run1 
		 
		 
		Relation 
		Precision 
		Recall 
		F1 
		CauseOf 
		0012
		Table 3:
		Baseline results on the four pre-deﬁned
		classes, as well as ‘No rel’ (the negative class)81284
		Table 3:
		Ofﬁcial PharmaCoNER 2019 results of
		RACAI’s NER systems
		on these evaluations, we decided to submit the out-
		put of the following systems: RPCN (best preci-
		sion), LARGER (C4, best F1 score) and Baseline
		(ofﬁcial reference system)
		107
		Table 2: The Statistics of The Dataset: Number of
		Files, Relations and Entities
		File
		Entity
		Relation
		train
		133
		2266
		1127
		development
		66
		1271
		608
		test
		32
		Unknown
		Unknown
		Table 3: Performance for Each Relation Type
		SER
		precision
		recall
		All-types
		0011
		Table 3: The result of all types on the test data set for
		SeeDev-binary task
		Team
		F1
		Recall
		Precision
		MIC-CIS-1
		070833
		Table 3: Results of Track 25
		N/A
		Table 3: Statistics for each entity type
		Intra-sent
		O
		O
		O
		O
		O
		Table 3: NER: Ensembling and Post-processing cor-
		recting individual models mistakes
		Table 3: Comparison with the participant systems for
		the normalization task of bacteria biotopes3
		Table 3: The ofﬁcial results of the BB-rel task362
		Table 3: Results for the coreference resolution task06
		Table 3: System performance for unseen concepts: precision (P) and recall (R) calculated over the subset of
		annotations and predictions of IDs that were absent from the training data5182
		Table 3: Results on the test set22
		Table 3: Development set results with different word vectors
		Table 3: The number of training examples (positively
		labeled abstracts) provided for Tasks 1 and 2 across
		constructs994
		Table 3:
		RDoC Task-1 results (on development
		set): Classiﬁcation accuracy and mean Average Pre-
		cision (mAP) of a-supDocNADE and SVM mod-
		els
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 3 shows the results for sub-track 1 (NER off-
		set and entity type classiﬁcation), ordered by team
		performance (ﬁrst column), then system perfor-
		mance (second column)
	</Extractive Summary>
	<Extractive Summary> =
		34%
		Table 3 presents the results of the NER task
		for each entity type independently
	</Extractive Summary>
	<Extractive Summary> =
		1
		Ablation Study
		We show the performances of different NER mod-
		els for Sub-tasks 1 and 2 on the development set
		in Table 3 to compare the possible scenarios of the
		52
		Sub-task 1: NER
		Sub-task 2: CI
		Span representation
		P(%)
		R(%)
		F(%)
		P(%)
		R(%)
		F(%)
		Ensemble
		90
	</Extractive Summary>
	<Extractive Summary> =
		The Sub-tasks 1 and 2
		results in Table 3 shows that almost all the results
		in different approaches are close to each other to
		solve the Sub-tasks 1 and 2
	</Extractive Summary>
	<Extractive Summary> =
		As for the single NER
		model, the results on Sub-tasks 1 and 2 in Table 3
		show that attention performs better than averaging
		when the other settings are same
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 summarises the
		results, which have been averaged over 1,000 ran-
		dom sampling experiments
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 presents the
		ofﬁcial results that were communicated to us by
		the task organizers
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows
		our results of different relation type
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the F1, recall and precision of
		cluster on the test data sets, and Table 3 shows the
		result of all types on the test data sets
	</Extractive Summary>
	<Extractive Summary> =
		1
		Entities and Concepts
		Table 3 shows the number of mentions, unique
		(lemmatized) mentions, concepts and average
		number of mentions per concept for each entity
		type
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the
		ensemble correcting individual model’s erroneous
		predictions
	</Extractive Summary>
	<Extractive Summary> =
		7% preci-
		sion increase in habitat and phenotype entity link-
		ing tasks respectively compared to the chal-
		lenged-provided baseline model (Table 3), where 
		case-insensitive string matching was applied for 
																																																										
		7 https://sites
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		One team submitted three runs for evaluation in
		the CRAFT-CR task (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		As can be seen
		in Table 3, ontology pretraining led to prediction
		of IDs outside the training data in four entity types
		(PR[ EXT], UBERON[ EXT])
	</Extractive Summary>
	<Extractive Summary> =
		provided in the Table 3 and the distribution of the
		number of most relevant sentences per construct is
		shown in Table 4
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results: RDoC Task-1
		Table 3 shows the performance of supervised Doc-
		ument Ranker models i
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument933> <Table ID =4>
	<Abstractive Summary> =
		24160
		6
		Table 4: Results for sub-track 2: Concept Indexing
		Dataset
		Subset
		Documents
		Sentences
		Entities
		PharmaCoNER
		Train
		500
		8036
		3822
		Valid
		250
		3759
		1926
		Test
		3751
		62000
		Table 4: PharmaCoNER subsets details28
		1, 862
		1, 926
		1, 691
		Table 4: Sub-task 1: Categorical performances on the development set
		given solutions and to report the best system sub-
		missions for NER and CI78
		Table 4: Recall of the BERT model on development
		set with and without pretraining on all entities, entity
		spans which are also present in the training data (seen)
		and entity spans which do not appear in the training
		data (unseen)03
		SVM
		Table 4: Participants Performance of Task 3
		Participants
		Precision
		Recall
		F-score
		Main NLP techniques
		*
		Baseline
		025
		Table 4: Precision (P), Recall (R) and F1 scores in test
		set of Task 272% 
		Overall 
		3321 
		100% 
		Table 4: Entity Statistics of Training Data 
		 
		 
		81
		Confidential Review Copy, 2018) ﬁrstly, then perform relation extrac-
		Table 4: Performance indicates statistically signiﬁcant
		difference from our model, NTS, MHS, Copy RE and
		Pipeline043
		Table 4: The result of ignoring types on the test data
		set for SeeDev-binary task
		112
		Binary relation type
		F1
		Recall
		Precision
		Binds To
		05%) 3,578
		Table 4: Statistics for each relation type
		the PubMed references
		136
		Task
		Train
		Dev
		Test
		Sentence Counts
		PharmaCo
		8068
		3748
		3930
		SeeDev
		644
		308
		466
		BB-norm+ner
		822
		413
		735
		PharmaCoNER Entities
		NORMALIZABLES
		2304
		1121
		859
		PROTEINAS
		1405
		745
		973
		UNCLEAR
		89
		44
		34
		NO NORMALIZABLES
		24
		16
		10
		BB-norm+NER Entities
		Habitat
		1118
		610
		-
		Microorganism
		739
		402
		-
		Phenotype
		369
		161
		-
		Table 4: Dataset statistics for NER
		4
		Experiments and Results
		4
		Table 4: Comparison with the participant systems for
		the normalization task considering only Phenotype en-
		tities7
		Table 4: Effects of ensemble training and inference3460928
		180
		Ontology
		Training
		Test
		Ontology
		Training
		Test
		CHEBI
		4,548 (18)
		2,200 (14)
		CHEBI EXT
		11,915 (38)
		5,248 (19)
		CL
		4,043 (244)
		1,749 (175)
		CL EXT
		6,275 (249)
		2,872 (175)
		GO BP
		9,280 (493)
		3,681 (272)
		GO BP EXT
		13,954 (526)
		5,847 (287)
		GO CC
		4,075 (80)
		1,184 (14)
		GO CC EXT
		8,495 (150)
		3,217 (30)
		GO MF
		375 (0)
		94 (0)
		GO MF EXT
		4,070 (28)
		1,822 (20)
		MOP
		240 (0)
		101 (0)
		MOP EXT
		386 (0)
		111 (0)
		NCBITaxon
		7,362 (2)
		3,101(0)
		NCBITaxon EXT
		7,592 (2)
		3,219 (0)
		PR
		17,038 (84)
		6,409 (44)
		PR EXT
		19,862 (110)
		7,932 (44)
		SO
		8,797 (108)
		3,446 (45)
		SO EXT
		24,955 (182)
		9,136 (72)
		UBERON
		12,269 (235)
		6,551 (118)
		UBERON EXT
		14,910 (255)
		7,416 (133)
		Table 4: Total and discontinuous (in parentheses) concept annotation counts by ontology for both the 67 article
		training and 30 article test sets5673
		Table 4: Results on the development set41
		Table 4: Development set results with extra training data
		Bio, word2vec/CBOW (window 2) and CRAFT
		tokenized, word2vec/CBOW (default parameters)edu/rdoc-task/
		220
		Table 4: Distribution of the number of most relevant (gold-standard) sentences in abstracts for each construct in
		the training data41
		PTA
		28316567
		Loss
		Table 4:
		RDoC Task-1 analysis:
		Ranking of
		PubMed abstracts within “Potential Threat Anxiety
		(PTA)” cluster using supervised prediction probabil-
		ities (p(q|v))
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results for sub-track 2 (Con-
		cept Indexing), ordered by team performance (ﬁrst
		5
		Table 3: Results for sub-track 1: NER offset and entity type classiﬁcation
	</Extractive Summary>
	<Extractive Summary> =
		1 
		Ablation Study 
		Table 4 provided additional ablation study 
		results analyzing the contribution of individual 
		features 
		on 
		track 
		1 
		and 
		reporting 
		the 
		performance of each standard terminology 
		selection method (STS) on track 2
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the categorical performances us-
		ing ensemble learning of NER on the develop-
		ment set
	</Extractive Summary>
	<Extractive Summary> =
		To obtain a rough understanding of how well
		the model performs on the entities unseen dur-
		ing training, we measure the recall of the model
		separately for entity spans seen and not seen dur-
		ing training (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 4, our model achieves im-
		provements on BB dataset comparing with the
		other four models
	</Extractive Summary>
	<Extractive Summary> =
		Table 4
		shows the result of ignoring types on the test da-
		ta sets and Table 5 shows detailed results of our
		method on the test data set
	</Extractive Summary>
	<Extractive Summary> =
		2
		Relations
		Table 4 shows the number of relations for both
		Lives in and Exhibits types,
		including intra-
		sentence and inter-sentence relations
	</Extractive Summary>
	<Extractive Summary> =
		514
		As the results in Table 4 demonstrate, our sys-
		tem performs signiﬁcantly better than the other
		systems for the normalization of new Phenotype
		entities in the test set (Precision: 70
	</Extractive Summary>
	<Extractive Summary> =
		3
		Analysis: RDoC Task-1
		Table 4 shows an impure “Potential Threat Anxi-
		ety” cluster of abstracts containing an intruder ab-
		stract with label (RDoC construct) “Loss”
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument933> <Table ID =5>
	<Abstractive Summary> =
		49207
		Table 5: Results by category for sub-track 15 billion
		6 trillion
		6 trillion
		2 billion
		Vocab size
		1 million
		1 million
		2 million
		2 million
		1 million
		Array size
		300
		300
		300
		300
		128
		Algorithm
		Word2Vec
		Skip-gram
		BOW
		FastText
		Skip-gram
		BOW
		FastText
		Skip-gram
		BOW
		FastText
		Skip-gram
		BOW
		Sense2Vec
		Table 5: Embedding models details75
		Table 5: Development set results for BERT model with
		and without pretraining499
		Table 5: Ablation study of Task 1 in development set35 
		Table 5:  Relation-level RE Performance of Run2  
		 
		 Entity Name  
		Count 
		Percentage 
		Var 
		733 
		22
		Table 5: Ablation Study
		Model
		SER
		P
		R
		Our Model
		044
		Table 5: Detailed results of our method on the test data set for SeeDev-binary task
		velopment, 2019)
		Wuhan University
		Yuhang Wu
		Yunnan University
		Table 5: Participating teams and their afﬁliations0
		Table 5: Hyper parameter settings for NER and RE The aim of
		the ﬁrst system is the normalization of the entity
		mentions in a biomedical text through the corre-
		sponding ontology, whereas the goal of the second
		Table 5: Comparison with the participant systems for
		the relation extraction task of bacteria biotopes7
		Table 5:
		Results of recognizing inter- and intra-
		sentence relations75
		Table 5: Aggregate concept annotation results evaluated per ontology against the 30 CRAFT test documents
		204
		Tag
		Description
		NP
		noun phrase
		NN
		noun, singular or mass
		NML
		sub-NP nominal substrings
		PRP$
		possessive pronoun
		LS
		list item marker
		Table 5: The deﬁnition of relevant Penn Treebank labels536
		Table 5: Final submission results on test data
		Metrics
		Run 1
		Run 2
		Run 3
		Tokens
		997
		Table 5: The number of abstracts in test set for task
		1737
		Table 5: RDoC Task-2 results (on development set):
		Performance of unsupervised and supervised sentence
		rankers (Figure 2) under different conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Ablation Study
		As shown in Table 5, we found that adding a layer
		of BiLSTM behind the BERT encoder did not im-
		prove the performance of the model, resulting in a
		0
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 shows the detailed performance of the 
		best-performing run of our system on the relation 
		extraction task
	</Extractive Summary>
	<Extractive Summary> =
		To investigate the in-
		ﬂuence of these two factors, we conduct ablation
		study and list results on Table 5
	</Extractive Summary>
	<Extractive Summary> =
		“No” preﬁx in Table 5 means that we train and
		evaluate our model without the corresponding fea-
		ture
	</Extractive Summary>
	<Extractive Summary> =
		Table 4
		shows the result of ignoring types on the test da-
		ta sets and Table 5 shows detailed results of our
		method on the test data set
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 details team
		afﬁliations
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 lists the best conﬁguration of hyper-
		parameters for all the tasks
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 5, we obtained an F1-score of
		65
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		One team submitted three runs to the CRAFT-CA
		task (Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		3
		Test set results
		The properties of the three runs we submitted to
		the task are summarized in Table 5 together with
		their development and test set LAS scores
	</Extractive Summary>
	<Extractive Summary> =
		Although included in Table 7, we
		excluded the two constructs, Circadian Rhythms
		and Sleep and Wakefulness, from the ﬁnal anal-
		ysis since these constructs contain one and zero
		negative articles, respectively, leading to perfect
		performance (see Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results: RDoC Task-2
		Table 5 shows results for Task-2 using three unsu-
		pervised and two supervised sentence ranker mod-
		els
	</Extractive Summary>
	<Extractive Summary> =
		#6
		Relevant
		Table 7: RDoC Task-2 analysis: This table shows
		that the most relevant sentence predicted using
		reRank(BM25-Extra) is actually not a relevant
		sentence, but Ensemble {#1, #2, #4} (Table 5)
		predicts the correct sentence as the most relevant
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument933> <Table ID =6>
	<Abstractive Summary> =
		Surprisingly, the
		Table 6: Statistics by track005
		Epochs
		100
		100
		100
		100
		Table 6: System hyperparameters for each run450
		Table 6: Results for the BB-norm sub-task788
		-
		Table 6: Scores on dev set using different features on
		PharmaCoNER and BB-norm+NER tasks7
		Table 6: Effects of lexical chains4053
		Table 6: Non-coreference results for the BLANC metric on the testing set544
		Table 6: Final submission test results for all metrics
		limiting the performance of the parser, and that the
		remaining challenges for substantially advancing
		the performance of the system lie speciﬁcally in
		more accurately recovering the dependency struc-
		ture of the sentences We used the def-
		Table 6: The number of abstracts and their percentages
		in test set for task 246
		Table 6: RDoC Tasks ofﬁcial results - performance on test set of different competing systems
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 describes our different
		experiments conﬁgurations
	</Extractive Summary>
	<Extractive Summary> =
		NER: Table 6 shows the score on dev set for
		PharmaCoNER and BB-norm+NER
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows the performance
		comparison of the “lexical chains” method and the
		“root nodes” method on the development set
	</Extractive Summary>
	<Extractive Summary> =
		4
		Analysis of ﬁnal results
		Table 6 provides a detailed look at the perfor-
		mance of our three ﬁnal submissions using all met-
		rics implemented in the CoNLL 2018 shared task
		evaluation script (see Section 4
	</Extractive Summary>
	<Extractive Summary> =
		The
		only metrics showing notable remaining room for
		improvement are dependency-based (last ﬁve rows
		in Table 6)
	</Extractive Summary>
	<Extractive Summary> =
		The distribution of the test set for task 2 across
		constructs is shown in Table 6 and the distribution
		of the number of most relevant sentences per con-
		struct is provided in Table 4
	</Extractive Summary>
	<Extractive Summary> =
		6
		Results: RDoC Task 1 & 2 on Test set
		Table 6 shows the ﬁnal evaluation scores of differ-
		ent competing systems for both the RDoC Task-1
		& Task-2 on ﬁnal test set
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument933> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Combining systems using a voting scheme10
		Table 7: Results for valid dataset entities597
		Table 7: Results for the BB-norm+ner sub-task301
		Table 7: Scores on dev set using different features on
		SeeDev task36
		Total Gold
		74,219
		Table 7: Results of models on each distance group; TP: True Positive; G
		223
		Table 7: Performance of retrieving PubMed Abstracts related to the corresponding RDoC construct (Task 1)
		#6
		Relevant
		Table 7: RDoC Task-2 analysis: This table shows
		that the most relevant sentence predicted using
		reRank(BM25-Extra) is actually not a relevant
		sentence, but Ensemble {#1, #2, #4} (Table 5)
		predicts the correct sentence as the most relevant
	</Abstractive Summary>
	<Extractive Summary> =
		Further-
		more, Table 7 shows the classiﬁcation results ob-
		3http://temu
	</Extractive Summary>
	<Extractive Summary> =
		RE: Table 7 shows the score on dev set for
		SeeDev10
	</Extractive Summary>
	<Extractive Summary> =
		Although included in Table 7, we
		excluded the two constructs, Circadian Rhythms
		and Sleep and Wakefulness, from the ﬁnal anal-
		ysis since these constructs contain one and zero
		negative articles, respectively, leading to perfect
		performance (see Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		The last column in Table 7 reports the aver-
		age score for the corresponding construct
	</Extractive Summary>
	<Extractive Summary> =
		5
		Analysis: RDoC Task-2
		Table 7 shows that the most relevant sentence
		predicted by reRank(BM25-Extra) is actually a
		non-relevant sentence
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument933> <Table ID =8>
	<Abstractive Summary> =
		75
		Table 8: Embeddings model results for track 1 on valid dataset507
		Table 8: Results for the BB-rel sub-task823
		Table 8: Comparison of our system (MIC-CIS) with
		top-5 participants: Scores on Test set for SeeDev and
		BB-norm+NER
		achieves the best score among all participating
		systems with F1 score of 086
		–
		–
		Table 8: Performance of extracting the most relevant sentence from each abstract related to the corresponding
		RDoC construct (Task 2)
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 8, we compare the different pre-trained
		models in Spanish on the validation dataset
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 8 speciﬁc domain word embed-
		dings outperform general domain models by al-
		most 5 points
	</Extractive Summary>
	<Extractive Summary> =
		For the test dataset, we applied our
		best system conﬁguration FastText-SBC + Reddit
		(see Table 8) obtaining an f-score of 85
	</Extractive Summary>
	<Extractive Summary> =
		4
		Comparison with Participating Systems
		SeeDev: Table 8 (left) is the ofﬁcial result of
		SeeDev Shared Task
	</Extractive Summary>
	<Extractive Summary> =
		BB-norm+NER: Table 8 (right) shows the
		comparison of performance among participating
		teams on BB-norm+NER test set
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument933> <Table ID =9>
	<Abstractive Summary> =
		75
		Table 9: Baseline comparison for track 1 on valid dataset249
		Table 9: Results for the BB-rel+ner sub-task (Prec58
		–
		–
		Table 9: Variation of Accuracy over various size of ab-
		stract
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 shows that our ex-
		tended system outperforms the baseline system,
		which has proven that POS and BMEWO-V for-
		mat to be an additional source of information that
		can be leveraged by neural networks and keep our
		model domain agnostic
	</Extractive Summary>
	<Extractive Summary> =
		Typically, participating teams performed rela-
		tively better on shorter abstracts (see Table 9),
		which is intuitive due to that fact the models have
		a higher chance of ﬁnding the most similar sen-
		tences for shorter abstracts
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument933> <Table ID =10>
	<Abstractive Summary> =
		64
		Table 10: Results for PharmaCoNER track 2 on valid and test dataset259
		Table 10: Results for the BB-kb and BB-kb+ner sub-
		tasks We would
		like to thank Robell Basset, Lenin Lewis, Ninoo
		Table 10: Variation of Accuracy over the number of
		most relevant (gold-standard) sentences in abstracts
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10 shows our result for valid and test dataset
		43
		Detail
		W2V-
		SBWC
		FastText-
		SBWC
		FastText-
		SBC
		SNOMED-
		SBC
		Reddit
		Type
		Word
		Word
		Word
		Concept
		Sense
		Corpus size
		1
	</Extractive Summary>
	<Extractive Summary> =
		Similarly, they per-
		formed well for abstracts with more gold-standard
		sentences (see Table 10)
	</Extractive Summary>
</Paper ID=ument933>


<Paper ID=ument935> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Distribution of labels in the PharmaCoNER
		datasets
	</Abstractive Summary>
</Paper ID=ument935>


<Paper ID=ument935> <Table ID =2>
	<Abstractive Summary> =
		com/PlanTL-SANIDAD/
		PharmaCoNER-CODALAB-Evaluation-Script
		5
		Table 2: Overview of Team Participation in the PharmaCoNER track
	</Abstractive Summary>
</Paper ID=ument935>


<Paper ID=ument935> <Table ID =3>
	<Abstractive Summary> =
		Table 4 shows the results for sub-track 2 (Con-
		cept Indexing), ordered by team performance (ﬁrst
		6
		Table 3: Results for sub-track 1: NER offset and entity type classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Results
		Table 3 shows the results for sub-track 1 (NER off-
		set and entity type classiﬁcation), ordered by team
		performance (ﬁrst column), then system perfor-
		mance (second column)
	</Extractive Summary>
</Paper ID=ument935>


<Paper ID=ument935> <Table ID =4>
	<Abstractive Summary> =
		24160
		7
		Table 4: Results for sub-track 2: Concept Indexing
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the results for sub-track 2 (Con-
		cept Indexing), ordered by team performance (ﬁrst
		6
		Table 3: Results for sub-track 1: NER offset and entity type classiﬁcation
	</Extractive Summary>
</Paper ID=ument935>


<Paper ID=ument935> <Table ID =5>
	<Abstractive Summary> =
		49207
		Table 5: Results by category for sub-track 1
	</Abstractive Summary>
</Paper ID=ument935>


<Paper ID=ument935> <Table ID =6>
	<Abstractive Summary> =
		Surprisingly, the
		Table 6: Statistics by track
	</Abstractive Summary>
</Paper ID=ument935>


<Paper ID=ument935> <Table ID =7>
	<Abstractive Summary> =
		Table 7: Combining systems using a voting scheme
	</Abstractive Summary>
</Paper ID=ument935>


<Paper ID=ument936> <Table ID =1>
	<Abstractive Summary> =
		30
		Table 1: All scores in %
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Evaluation
		Results
		Table 1 compares the scores of our sys-
		tems
	</Extractive Summary>
</Paper ID=ument936>


<Paper ID=ument937> <Table ID =1>
	<Abstractive Summary> =
		Table 1: The parameters of the neural model and
		their values used for the PharmaCoNER results
	</Abstractive Summary>
</Paper ID=ument937>


<Paper ID=ument937> <Table ID =2>
	<Abstractive Summary> =
		XHPEFYUo85k
		Table 2: Oﬃcial results of the neural Model for the
		two tasks of the PharmaCoNER
	</Abstractive Summary>
</Paper ID=ument937>


<Paper ID=ument937> <Table ID =3>
	<Abstractive Summary> =
		In addition, other complex concept indexing
		rules could be applied to chose the best nor-
		20
		Table 3: Performance of the neural model for each category in the Named Entity Recognition Task of
		the PharmaCoNER
	</Abstractive Summary>
	<Extractive Summary> =
		34%
		Table 3 presents the results of the NER task
		for each entity type independently
	</Extractive Summary>
</Paper ID=ument937>


<Paper ID=ument938> <Table ID =1>
	<Abstractive Summary> =
		81
		Table 1: The results we have obtained for NER task
	</Abstractive Summary>
</Paper ID=ument938>


<Paper ID=ument938> <Table ID =2>
	<Abstractive Summary> =
		34
		Table 2: Results for Concept Indexing task
	</Abstractive Summary>
</Paper ID=ument938>


<Paper ID=ument939> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Precision (P), Recall (R) and F1 for Task 1
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Experimental Results
		In Table 1, we report the results on the Pharma-
		CoNER development and test sets using the ofﬁ-
		cial shared task evaluation metrics
	</Extractive Summary>
</Paper ID=ument939>


<Paper ID=ument94> <Table ID =1>
	<Abstractive Summary> =
		001
		Table 1: Hyperparameter values
	</Abstractive Summary>
</Paper ID=ument94>


<Paper ID=ument94> <Table ID =2>
	<Abstractive Summary> =
		1
		Table 2: English results on CoNLL-2009 in-domain
		(WSJ) test set
	</Abstractive Summary>
</Paper ID=ument94>


<Paper ID=ument94> <Table ID =3>
	<Abstractive Summary> =
		9
		Table 3: CoNLL-2009 out-of domain results (English;
		Brown test set)
	</Abstractive Summary>
</Paper ID=ument94>


<Paper ID=ument94> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4: CoNLL-2009 results on Chinese, German, and
		Spanish (test sets)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents the results of our experiments
		(without ELMo) on Chinese, German, and Span-
		ish
	</Extractive Summary>
</Paper ID=ument94>


<Paper ID=ument94> <Table ID =5>
	<Abstractive Summary> =
		6
		Table 5: Ablation results on the CoNLL-2009 English
		development set
	</Abstractive Summary>
	<Extractive Summary> =
		Both kinds of modules improve per-
		formance (over a supervised model without CVT,
		see second row in Table 5); future and past mod-
		ules are slightly better corroborating the results of
		Clark et al
	</Extractive Summary>
	<Extractive Summary> =
		8 improvement in F1 over a supervised model
		without CVT (second row in Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		6 over
		a supervised model without CVT (see Table 5);
		the model concentrates on a few predicates with
		very high scores (these tend to be common verbs
		such as say, is, and have), while ignoring nominal
		predicates and less frequent verbs
	</Extractive Summary>
</Paper ID=ument94>


<Paper ID=ument94> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6: CVT with different auxiliary modules for SRL
		(CoNLL-2009 English development set)
	</Abstractive Summary>
	<Extractive Summary> =
		3
		CVT Analysis
		In Table 6, we brieﬂy explore which auxiliary pre-
		diction modules are more important for CVT when
		applied to SRL
	</Extractive Summary>
	<Extractive Summary> =
		Overall,
		the results in Table 6 suggest that more restricted
		views of the input are beneﬁcial
	</Extractive Summary>
</Paper ID=ument94>


<Paper ID=ument94> <Table ID =7>
	<Abstractive Summary> =
		5
		Table 7: CVT with different predicate selection strate-
		gies for SRL (CoNLL-2009 English development set)
	</Abstractive Summary>
	<Extractive Summary> =
		The experiments in Table 7 conﬁrm
		that the adopted strategy works best delivering a
		0
	</Extractive Summary>
</Paper ID=ument94>


<Paper ID=ument940> <Table ID =1>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		The statistics 
		of the corpus, including the number of 
		documents, chemical & drug mentions in 
		different types are listed in Table 1, where “UNK” 
		denotes unknown
	</Extractive Summary>
</Paper ID=ument940>


<Paper ID=ument940> <Table ID =2>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument940>


<Paper ID=ument940> <Table ID =3>
	<Abstractive Summary> =
		
	</Abstractive Summary>
</Paper ID=ument940>


<Paper ID=ument940> <Table ID =4>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		1 
		Ablation Study 
		Table 4 provided additional ablation study 
		results analyzing the contribution of individual 
		features 
		on 
		track 
		1 
		and 
		reporting 
		the 
		performance of each standard terminology 
		selection method (STS) on track 2
	</Extractive Summary>
</Paper ID=ument940>


<Paper ID=ument941> <Table ID =1>
	<Abstractive Summary> =
		io/
		40
		Collection\Corpus
		IBECS
		SciELO
		MedlineNLM
		MedlinePlus
		UFAL
		Documents
		168,198
		161,710
		330,928
		1,063
		265,410
		Words
		23,648,768
		26,169,655
		4,710,191
		217,515
		41,604,517
		Unique Words
		184,936
		159,997
		20,942
		5,099
		198,424
		Table 1: Biomedical Spanish corpus details
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10 shows our result for valid and test dataset
		44
		Detail
		W2V-
		SBWC
		FastText-
		SBWC
		FastText-
		SBC
		SNOMED-
		SBC
		Reddit
		Type
		Word
		Word
		Word
		Concept
		Sense
		Corpus size
		1
	</Extractive Summary>
	<Extractive Summary> =
		64
		Table 10: Results for PharmaCoNER track 2 on valid and test dataset
	</Extractive Summary>
</Paper ID=ument941>


<Paper ID=ument941> <Table ID =2>
	<Abstractive Summary> =
		00
		Ghz,
		8
		Cores, 16 Logi-
		cal Processors
		RAM
		32 Gb
		32 Gb
		Corpus Size
		1Gb
		1Gb
		Training Time
		4 hours
		8 hours
		Table 2: Training parameters for embeddings models built in this work
	</Abstractive Summary>
</Paper ID=ument941>


<Paper ID=ument941> <Table ID =3>
	<Abstractive Summary> =
		63
		64
		O
		O
		Table 3: Tokens annotated with BMEWO-V encoding in the ConLL-2003 format
	</Abstractive Summary>
</Paper ID=ument941>


<Paper ID=ument941> <Table ID =4>
	<Abstractive Summary> =
		Dataset
		Subset
		Documents
		Sentences
		Entities
		PharmaCoNER
		Train
		500
		8036
		3822
		Valid
		250
		3759
		1926
		Test
		3751
		62000
		Table 4: PharmaCoNER subsets details
	</Abstractive Summary>
</Paper ID=ument941>


<Paper ID=ument941> <Table ID =5>
	<Abstractive Summary> =
		5 billion
		6 trillion
		6 trillion
		2 billion
		Vocab size
		1 million
		1 million
		2 million
		2 million
		1 million
		Array size
		300
		300
		300
		300
		128
		Algorithm
		Word2Vec
		Skip-gram
		BOW
		FastText
		Skip-gram
		BOW
		FastText
		Skip-gram
		BOW
		FastText
		Skip-gram
		BOW
		Sense2Vec
		Table 5: Embedding models details
	</Abstractive Summary>
</Paper ID=ument941>


<Paper ID=ument941> <Table ID =6>
	<Abstractive Summary> =
		005
		Epochs
		100
		100
		100
		100
		Table 6: System hyperparameters for each run
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 describes our different
		experiments conﬁgurations
	</Extractive Summary>
</Paper ID=ument941>


<Paper ID=ument941> <Table ID =7>
	<Abstractive Summary> =
		10
		Table 7: Results for valid dataset entities
	</Abstractive Summary>
	<Extractive Summary> =
		Further-
		more, Table 7 shows the classiﬁcation results ob-
		3http://temu
	</Extractive Summary>
</Paper ID=ument941>


<Paper ID=ument941> <Table ID =8>
	<Abstractive Summary> =
		75
		Table 8: Embeddings model results for track 1 on valid dataset
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 8, we compare the different pre-trained
		models in Spanish on the validation dataset
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 8 speciﬁc domain word embed-
		dings outperform general domain models by al-
		most 5 points
	</Extractive Summary>
	<Extractive Summary> =
		For the test dataset, we applied our
		best system conﬁguration FastText-SBC + Reddit
		(see Table 8) obtaining an f-score of 85
	</Extractive Summary>
</Paper ID=ument941>


<Paper ID=ument941> <Table ID =9>
	<Abstractive Summary> =
		75
		Table 9: Baseline comparison for track 1 on valid dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 9 shows that our ex-
		tended system outperforms the baseline system,
		which has proven that POS and BMEWO-V for-
		mat to be an additional source of information that
		can be leveraged by neural networks and keep our
		model domain agnostic
	</Extractive Summary>
</Paper ID=ument941>


<Paper ID=ument941> <Table ID =10>
	<Abstractive Summary> =
		64
		Table 10: Results for PharmaCoNER track 2 on valid and test dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 10 shows our result for valid and test dataset
		44
		Detail
		W2V-
		SBWC
		FastText-
		SBWC
		FastText-
		SBC
		SNOMED-
		SBC
		Reddit
		Type
		Word
		Word
		Word
		Concept
		Sense
		Corpus size
		1
	</Extractive Summary>
</Paper ID=ument941>


<Paper ID=ument942> <Table ID =1>
	<Abstractive Summary> =
		22
		Table 1: Performance of NER and CI on the test set
		Label
		P(%)
		R(%)
		F(%)
		Prediction
		Annotation
		Correct
		NORMALIZABLES
		89
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the ﬁve submitted results of NER
		and CI in terms of F-score on the test set
	</Extractive Summary>
</Paper ID=ument942>


<Paper ID=ument942> <Table ID =2>
	<Abstractive Summary> =
		76
		1, 836
		1, 876
		1, 600
		Table 2: Sub-task 1: Categorical performance on the test set
		dient clipping to 5, the dropout rate to 0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the categorical performances us-
		ing ensemble learning of NER on the test set
	</Extractive Summary>
</Paper ID=ument942>


<Paper ID=ument942> <Table ID =3>
	<Abstractive Summary> =
		07
		Table 3: Performance of NER and CI on the development set
		Label
		P(%)
		R(%)
		F(%)
		Prediction
		Annotation
		Correct
		NORMALIZABLES
		91
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Ablation Study
		We show the performances of different NER mod-
		els for Sub-tasks 1 and 2 on the development set
		in Table 3 to compare the possible scenarios of the
		53
		Sub-task 1: NER
		Sub-task 2: CI
		Span representation
		P(%)
		R(%)
		F(%)
		P(%)
		R(%)
		F(%)
		Ensemble
		90
	</Extractive Summary>
	<Extractive Summary> =
		The Sub-tasks 1 and 2
		results in Table 3 shows that almost all the results
		in different approaches are close to each other to
		solve the Sub-tasks 1 and 2
	</Extractive Summary>
	<Extractive Summary> =
		As for the single NER
		model, the results on Sub-tasks 1 and 2 in Table 3
		show that attention performs better than averaging
		when the other settings are same
	</Extractive Summary>
</Paper ID=ument942>


<Paper ID=ument942> <Table ID =4>
	<Abstractive Summary> =
		28
		1, 862
		1, 926
		1, 691
		Table 4: Sub-task 1: Categorical performances on the development set
		given solutions and to report the best system sub-
		missions for NER and CI
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the categorical performances us-
		ing ensemble learning of NER on the develop-
		ment set
	</Extractive Summary>
</Paper ID=ument942>


<Paper ID=ument943> <Table ID =1>
	<Abstractive Summary> =
		We participate in the PharmaCoNER task using
		a collection of tools developed for English as well
		Item
		Train
		Devel
		Documents
		500
		250
		Tokens
		177 022
		85 148
		Annotations
		3 822
		1 926
		Protein
		1 405
		745
		Chemical(+)
		2 304
		1 121
		Chemical(-)
		24
		16
		Other
		89
		44
		Table 1: Data statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 brieﬂy summarizes data statis-
		tics
	</Extractive Summary>
</Paper ID=ument943>


<Paper ID=ument943> <Table ID =2>
	<Abstractive Summary> =
		47
		Table 2: NERsuite development set results for various boundary matching criteria (precision/recall/F-score)
	</Abstractive Summary>
</Paper ID=ument943>


<Paper ID=ument943> <Table ID =3>
	<Abstractive Summary> =
		77
		Table 3: BERT development set results for various boundary matching criteria (precision/recall/F-score)
	</Abstractive Summary>
</Paper ID=ument943>


<Paper ID=ument943> <Table ID =4>
	<Abstractive Summary> =
		78
		Table 4: Recall of the BERT model on development
		set with and without pretraining on all entities, entity
		spans which are also present in the training data (seen)
		and entity spans which do not appear in the training
		data (unseen)
	</Abstractive Summary>
	<Extractive Summary> =
		To obtain a rough understanding of how well
		the model performs on the entities unseen dur-
		ing training, we measure the recall of the model
		separately for entity spans seen and not seen dur-
		ing training (Table 4)
	</Extractive Summary>
</Paper ID=ument943>


<Paper ID=ument943> <Table ID =5>
	<Abstractive Summary> =
		75
		Table 5: Development set results for BERT model with
		and without pretraining
	</Abstractive Summary>
</Paper ID=ument943>


<Paper ID=ument944> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Statistics of annotations in total, training and test sets
		Total
		Training set
		Test set
		# of Abstracts
		500
		250
		250
		# of Sentences
		5,080
		2,534
		2,546
		# of Named entities
		5,741
		3,317
		2,424
		�Bio-concept Named Entities
		2,274
		1,428
		846
		Var (Variation)
		1,304
		735
		569
		MPA (Molecular Physiological Activity)
		618
		418
		200
		Interaction
		35
		28
		7
		Pathway
		38
		24
		14
		CPA (Cell Physiological Activity)
		279
		223
		56
		�Regulatory Named Entities
		1,514
		905
		609
		Regulation
		613
		215
		398
		Positive Regulation
		406
		323
		83
		Negative Regulation
		495
		367
		128
		�Other Entities
		1,953
		984
		969
		Disease
		751
		336
		415
		Gene
		1,004
		529
		475
		Protein
		150
		90
		60
		Enzyme
		48
		29
		19
		# of Thematic roles
		4,677
		2,729
		1,948
		ThemeOf
		2,986
		1,698
		1,288
		ThemeOf (Intra/inter sentential)
		(2910/76)
		(1657/41)
		(1253/35)
		CauseOf
		1,691
		1,031
		660
		CauseOf (Intra/inter sentential)
		(1581/110)
		(961/70)
		(620/40)
		65
		4
		300
		301
		302
		303
		304
		305
		306
		307
		308
		309
		310
		311
		312
		313
		314
		315
		316
		317
		318
		319
		320
		321
		322
		323
		324
		325
		326
		327
		328
		329
		330
		331
		332
		333
		334
		335
		336
		337
		338
		339
		340
		341
		342
		343
		344
		345
		346
		347
		348
		349
		350
		351
		352
		353
		354
		355
		356
		357
		358
		359
		360
		361
		362
		363
		364
		365
		366
		367
		368
		369
		370
		371
		372
		373
		374
		375
		376
		377
		378
		379
		380
		381
		382
		383
		384
		385
		386
		387
		388
		389
		390
		391
		392
		393
		394
		395
		396
		397
		398
		399
		Conﬁdential Review Copy
	</Abstractive Summary>
	<Extractive Summary> =
		i) Imbalanced Data: The statistics in Table 1
		clearly shows that the entity distribution is
		imbalanced over the entity types, e
	</Extractive Summary>
</Paper ID=ument944>


<Paper ID=ument944> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Participants Performance of Task 1
		Participants
		Precision
		Recall
		F-score
		Main NLP techniques
		1st
		DX-HITSZ
		0
	</Abstractive Summary>
	<Extractive Summary> =
		The results of Task 1,
		2, and 3 are presented in Table 2, 3, 4, respectively
	</Extractive Summary>
</Paper ID=ument944>


<Paper ID=ument944> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Participants Performance of Task 2
		Participants
		Precision
		Recall
		F-score
		Main NLP techniques
		1st
		Zheng-UMASS
		0
	</Abstractive Summary>
</Paper ID=ument944>


<Paper ID=ument944> <Table ID =4>
	<Abstractive Summary> =
		03
		SVM
		Table 4: Participants Performance of Task 3
		Participants
		Precision
		Recall
		F-score
		Main NLP techniques
		*
		Baseline
		0
	</Abstractive Summary>
</Paper ID=ument944>


<Paper ID=ument945> <Table ID =1>
	<Abstractive Summary> =
		For
		Dataset
		#Train
		#Dev
		#Test
		BC5CDR
		4,559
		4,580
		4,796
		NCBI disease
		5,423
		922
		939
		BC2GM
		12,573
		2,518
		5,037
		2010 i2b2/VA
		16,315
		-
		27,626
		Table 1: Datasets for joint learning in recognizing the
		trigger words
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the exter-
		nal data sets used under the joint learning method
	</Extractive Summary>
</Paper ID=ument945>


<Paper ID=ument945> <Table ID =2>
	<Abstractive Summary> =
		847
		Table 2: Model comparision in development set with
		different pre-trained models
		development set results
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Main Results
		Table 2 compares the results of the two tasks of
		the pre-trained model in trigger words NER and
		thematic roles identiﬁcation
	</Extractive Summary>
</Paper ID=ument945>


<Paper ID=ument945> <Table ID =3>
	<Abstractive Summary> =
		60
		Table 3: Precision (P), Recall (R) and F1 scores in test
		set of Task 1
	</Abstractive Summary>
</Paper ID=ument945>


<Paper ID=ument945> <Table ID =4>
	<Abstractive Summary> =
		25
		Table 4: Precision (P), Recall (R) and F1 scores in test
		set of Task 2
	</Abstractive Summary>
</Paper ID=ument945>


<Paper ID=ument945> <Table ID =5>
	<Abstractive Summary> =
		499
		Table 5: Ablation study of Task 1 in development set
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Ablation Study
		As shown in Table 5, we found that adding a layer
		of BiLSTM behind the BERT encoder did not im-
		prove the performance of the model, resulting in a
		0
	</Extractive Summary>
</Paper ID=ument945>


<Paper ID=ument946> <Table ID =1>
	<Abstractive Summary> =
		Based on the 
		value of roleRatio, we split all the entities into 3 
		subgroups, each containing 4 entity types:  
		Subgroup 
		Entities 
		A 
		PosReg (positive regulation), NegReg 
		(negative regulation), Reg (regulation), 
		Interaction 
		B 
		Gene, Pathway, Protein, Disease 
		C 
		Enzyme, Var (variation), CPA (cell 
		physiological activity), MPA 
		(Molecular physiological activity) 
		Table 1: Subgroups of 12 Entities for Task 
		Decomposition  
		In subgroup A, the roleRatio values of all the 
		entities are all less than 1 indicating they are more 
		likely to be the tail entity of a relation
	</Abstractive Summary>
</Paper ID=ument946>


<Paper ID=ument946> <Table ID =2>
	<Abstractive Summary> =
		34 
		Table 2:  Official Submission Results in AGAC 
		 
		Entity Name 
		Precision 
		Recall 
		F1 
		Var 
		0
	</Abstractive Summary>
</Paper ID=ument946>


<Paper ID=ument946> <Table ID =3>
	<Abstractive Summary> =
		45 
		Table 3:  Entity-level NER Performance of Run1 
		 
		 
		Relation 
		Precision 
		Recall 
		F1 
		CauseOf 
		0
	</Abstractive Summary>
</Paper ID=ument946>


<Paper ID=ument946> <Table ID =4>
	<Abstractive Summary> =
		72% 
		Overall 
		3321 
		100% 
		Table 4: Entity Statistics of Training Data 
		 
		 
		82
		Confidential Review Copy
	</Abstractive Summary>
</Paper ID=ument946>


<Paper ID=ument946> <Table ID =5>
	<Abstractive Summary> =
		35 
		Table 5:  Relation-level RE Performance of Run2  
		 
		 Entity Name  
		Count 
		Percentage 
		Var 
		733 
		22
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the detailed performance of the 
		best-performing run of our system on the relation 
		extraction task
	</Extractive Summary>
</Paper ID=ument946>


<Paper ID=ument947> <Table ID =1>
	<Abstractive Summary> =
		070
		Table 1: Model results on the four pre-deﬁned classes,
		as well as ‘No rel’ (the negative class) when the macro-
		averaged F1-score (over the positive labels only) is
		used as our early stopping criterion
	</Abstractive Summary>
</Paper ID=ument947>


<Paper ID=ument947> <Table ID =2>
	<Abstractive Summary> =
		034
		Table 2: Model results on the four pre-deﬁned classes,
		as well as ‘No rel’ (the negative class) when the macro-
		averaged F1-score (over all labels) is used as our early
		stopping criterion
	</Abstractive Summary>
</Paper ID=ument947>


<Paper ID=ument947> <Table ID =3>
	<Abstractive Summary> =
		012
		Table 3:
		Baseline results on the four pre-deﬁned
		classes, as well as ‘No rel’ (the negative class)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 summarises the
		results, which have been averaged over 1,000 ran-
		dom sampling experiments
	</Extractive Summary>
</Paper ID=ument947>


<Paper ID=ument948> <Table ID =1>
	<Abstractive Summary> =
		379841E-6
		Table 1: SNOMED CT word features for labels PROTEINAS and NORMALIZABLES
		word receiving the best label by the softmax
		output, accumulating labels as the window passes
		by
	</Abstractive Summary>
</Paper ID=ument948>


<Paper ID=ument948> <Table ID =2>
	<Abstractive Summary> =
		8318
		Table 2: Development results of RACAI’s NER sys-
		tems
		System
		P
		R
		F1
		Baseline
		0
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 2 presents the runs of the RACAI baseline
		system, RPCN and of four ensemble methods ap-
		plied to the baseline (ﬁrst input) and RPCN (sec-
		ond input)
	</Extractive Summary>
</Paper ID=ument948>


<Paper ID=ument948> <Table ID =3>
	<Abstractive Summary> =
		81284
		Table 3:
		Ofﬁcial PharmaCoNER 2019 results of
		RACAI’s NER systems
		on these evaluations, we decided to submit the out-
		put of the following systems: RPCN (best preci-
		sion), LARGER (C4, best F1 score) and Baseline
		(ofﬁcial reference system)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 presents the
		ofﬁcial results that were communicated to us by
		the task organizers
	</Extractive Summary>
</Paper ID=ument948>


<Paper ID=ument949> <Table ID =1>
	<Abstractive Summary> =
		1
		Table 1:
		Detailed Hyper-parameter Settings in the
		PharmaCoNER task
	</Abstractive Summary>
</Paper ID=ument949>


<Paper ID=ument949> <Table ID =2>
	<Abstractive Summary> =
		02
		Table 2:
		The Experimental Results of Multilingual-
		BERT and BioBERT
	</Abstractive Summary>
</Paper ID=ument949>


<Paper ID=ument95> <Table ID =1>
	<Abstractive Summary> =
		35
		None
		Table 1: NER F1 scores on test sets of European languages
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, the mean/variance align-
		ment strategy (MLMA-Mv) is competitive with
		previous work which utilizes extra bilingual re-
		sources (Section 5
	</Extractive Summary>
	<Extractive Summary> =
		The results of MLMA-Avl (init) shown in
		Table 1 indicate that the CLWEs lead to a better
		initialization and improved performance
	</Extractive Summary>
	<Extractive Summary> =
		As shown in
		Table 1, our MLMA-Avl achieves a better perfor-
		mance on Spanish and Dutch
	</Extractive Summary>
	<Extractive Summary> =
		Fully-Weighted Sum
		As shown in Table 1, 2 and 3, we observe that
		Self-Weighted Sum (SWS) generally outperforms
		Fully-Weighted Sum (FWS) in NER tasks, while
		the opposite is true for POS tasks
	</Extractive Summary>
</Paper ID=ument95>


<Paper ID=ument95> <Table ID =2>
	<Abstractive Summary> =
		22
		Table 2: NER F1 scores on test sets for Chinese
	</Abstractive Summary>
</Paper ID=ument95>


<Paper ID=ument95> <Table ID =3>
	<Abstractive Summary> =
		87
		Table 3: POS accuracy on test sets of European languages
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 3, our mod-
		els with deep semantic alignment outperform pre-
		vious lexicon-based cross-lingual clustering by a
		large margin
	</Extractive Summary>
</Paper ID=ument95>


<Paper ID=ument95> <Table ID =4>
	<Abstractive Summary> =
		)
		Table 4: English words and their nearest Spanish words according to MUSE and MLMA-Avl
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, MUSE successfully groups
		the English word brown with Spanish words that
		are related to colors
	</Extractive Summary>
</Paper ID=ument95>


<Paper ID=ument950> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Input Features of Our Model and Their Description
		as:
		EyS =
		N
		�
		i=1
		Vi
		(2)
		T represents transition score which can be deﬁned
		as:
		TyS =
		N
		�
		i=1
		TMyS
		i−1,yS
		i
		(3)
		where TMyi−1,yi means the transition probability
		from tag yi−1 to yi
	</Abstractive Summary>
	<Extractive Summary> =
		Results show that each feature listed in Table 1
		plays a key role
	</Extractive Summary>
</Paper ID=ument950>


<Paper ID=ument950> <Table ID =2>
	<Abstractive Summary> =
		108
		Table 2: The Statistics of The Dataset: Number of
		Files, Relations and Entities
		File
		Entity
		Relation
		train
		133
		2266
		1127
		development
		66
		1271
		608
		test
		32
		Unknown
		Unknown
		Table 3: Performance for Each Relation Type
		SER
		precision
		recall
		All-types
		0
	</Abstractive Summary>
</Paper ID=ument950>


<Paper ID=ument950> <Table ID =3>
	<Abstractive Summary> =
		108
		Table 2: The Statistics of The Dataset: Number of
		Files, Relations and Entities
		File
		Entity
		Relation
		train
		133
		2266
		1127
		development
		66
		1271
		608
		test
		32
		Unknown
		Unknown
		Table 3: Performance for Each Relation Type
		SER
		precision
		recall
		All-types
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		our results of different relation type
	</Extractive Summary>
</Paper ID=ument950>


<Paper ID=ument950> <Table ID =4>
	<Abstractive Summary> =
		, 2018) ﬁrstly, then perform relation extrac-
		Table 4: Performance indicates statistically signiﬁcant
		difference from our model, NTS, MHS, Copy RE and
		Pipeline
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 4, our model achieves im-
		provements on BB dataset comparing with the
		other four models
	</Extractive Summary>
</Paper ID=ument950>


<Paper ID=ument950> <Table ID =5>
	<Abstractive Summary> =
		Table 5: Ablation Study
		Model
		SER
		P
		R
		Our Model
		0
	</Abstractive Summary>
	<Extractive Summary> =
		To investigate the in-
		ﬂuence of these two factors, we conduct ablation
		study and list results on Table 5
	</Extractive Summary>
	<Extractive Summary> =
		“No” preﬁx in Table 5 means that we train and
		evaluate our model without the corresponding fea-
		ture
	</Extractive Summary>
</Paper ID=ument950>


<Paper ID=ument951> <Table ID =1>
	<Abstractive Summary> =
		60
		Table 1: The F1 score of CNN and CNN-LSTM on the
		dev data set for SeeDev-binary task
		2
	</Abstractive Summary>
</Paper ID=ument951>


<Paper ID=ument951> <Table ID =2>
	<Abstractive Summary> =
		33
		Table 2: The F1, recall and precision of cluster on the
		test data set for SeeDev-binary task
		Team
		F1
		Recall
		Precision
		MIC-CIS-1
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the F1, recall and precision of
		cluster on the test data sets, and Table 3 shows the
		result of all types on the test data sets
	</Extractive Summary>
</Paper ID=ument951>


<Paper ID=ument951> <Table ID =3>
	<Abstractive Summary> =
		011
		Table 3: The result of all types on the test data set for
		SeeDev-binary task
		Team
		F1
		Recall
		Precision
		MIC-CIS-1
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the F1, recall and precision of
		cluster on the test data sets, and Table 3 shows the
		result of all types on the test data sets
	</Extractive Summary>
</Paper ID=ument951>


<Paper ID=ument951> <Table ID =4>
	<Abstractive Summary> =
		043
		Table 4: The result of ignoring types on the test data
		set for SeeDev-binary task
		113
		Binary relation type
		F1
		Recall
		Precision
		Binds To
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		shows the result of ignoring types on the test da-
		ta sets and Table 5 shows detailed results of our
		method on the test data set
	</Extractive Summary>
</Paper ID=ument951>


<Paper ID=ument951> <Table ID =5>
	<Abstractive Summary> =
		44
		Table 5: Detailed results of our method on the test data set for SeeDev-binary task
		velopment
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		shows the result of ignoring types on the test da-
		ta sets and Table 5 shows detailed results of our
		method on the test data set
	</Extractive Summary>
</Paper ID=ument951>


<Paper ID=ument952> <Table ID =1>
	<Abstractive Summary> =
		119
		Resource
		Input text
		Snomed Term
		Snomed Code
		Wikidata
		adriamicina
		doxorrubicina
		372817009
		Chemical symbols
		Na
		sodio
		39972003
		AbreMES-DB
		Hb
		hemoglobina
		38082009
		Hunspell Library
		6-Metil-Prednisolona
		metilprednisolona
		116593003
		Table 1: Examples of Snomed concept indexing
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1 we can see some examples of how
		the resources and tools applied in the archi-
		tecture can contribute to the achievement of
		Snomed concept mapping
	</Extractive Summary>
</Paper ID=ument952>


<Paper ID=ument952> <Table ID =2>
	<Abstractive Summary> =
		76965
		Table 2: Results of Track 1
	</Abstractive Summary>
</Paper ID=ument952>


<Paper ID=ument952> <Table ID =3>
	<Abstractive Summary> =
		70833
		Table 3: Results of Track 2
	</Abstractive Summary>
</Paper ID=ument952>


<Paper ID=ument953> <Table ID =1>
	<Abstractive Summary> =
		723
		Table 1:
		Inter-annotator agreement metrics (SER
		stands for Slot Error Rate)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes
		the inter-annotator agreement for named entities,
		normalization and relations
	</Extractive Summary>
	<Extractive Summary> =
		3
		BB-kb and BB-kb+ner
		BLAIR GMU is the only team to submit to the
		BB-kb and BB-kb+ner tasks, their results are
		shown in Table 10
	</Extractive Summary>
	<Extractive Summary> =
		259
		Table 10: Results for the BB-kb and BB-kb+ner sub-
		tasks
	</Extractive Summary>
</Paper ID=ument953>


<Paper ID=ument953> <Table ID =2>
	<Abstractive Summary> =
		Documents
		392
		Words
		60,402
		Unique words
		12,566
		Sentences
		2,646
		Entity mentions
		7,232
		Unique entity mentions
		3,300
		Concepts
		1,072
		Relations
		3,578
		Unique relations between concepts
		1,931
		Table 2: Global statistics of the corpus
		In the following, we present more detailed
		statistics and highlight corpus characteristics that
		may be challenging for the participants
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Descriptive Statistics
		Table 2 gives the size of the corpus, in terms of
		documents, words, sentences and annotated ele-
		7http://2016
	</Extractive Summary>
</Paper ID=ument953>


<Paper ID=ument953> <Table ID =3>
	<Abstractive Summary> =
		5
		N/A
		Table 3: Statistics for each entity type
		Intra-sent
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Entities and Concepts
		Table 3 shows the number of mentions, unique
		(lemmatized) mentions, concepts and average
		number of mentions per concept for each entity
		type
	</Extractive Summary>
</Paper ID=ument953>


<Paper ID=ument953> <Table ID =4>
	<Abstractive Summary> =
		5%) 3,578
		Table 4: Statistics for each relation type
		the PubMed references
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Relations
		Table 4 shows the number of relations for both
		Lives in and Exhibits types,
		including intra-
		sentence and inter-sentence relations
	</Extractive Summary>
</Paper ID=ument953>


<Paper ID=ument953> <Table ID =5>
	<Abstractive Summary> =
		, 2019)
		Wuhan University
		Yuhang Wu
		Yunnan University
		Table 5: Participating teams and their afﬁliations
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 details team
		afﬁliations
	</Extractive Summary>
</Paper ID=ument953>


<Paper ID=ument953> <Table ID =6>
	<Abstractive Summary> =
		450
		Table 6: Results for the BB-norm sub-task
	</Abstractive Summary>
</Paper ID=ument953>


<Paper ID=ument953> <Table ID =7>
	<Abstractive Summary> =
		597
		Table 7: Results for the BB-norm+ner sub-task
	</Abstractive Summary>
</Paper ID=ument953>


<Paper ID=ument953> <Table ID =8>
	<Abstractive Summary> =
		507
		Table 8: Results for the BB-rel sub-task
	</Abstractive Summary>
</Paper ID=ument953>


<Paper ID=ument953> <Table ID =9>
	<Abstractive Summary> =
		249
		Table 9: Results for the BB-rel+ner sub-task (Prec
	</Abstractive Summary>
</Paper ID=ument953>


<Paper ID=ument953> <Table ID =10>
	<Abstractive Summary> =
		259
		Table 10: Results for the BB-kb and BB-kb+ner sub-
		tasks
	</Abstractive Summary>
</Paper ID=ument953>


<Paper ID=ument954> <Table ID =1>
	<Abstractive Summary> =
		Egg Pulp, 97 encoded as Ccc Ccccp nn
		tri-gram
		tri-gram as features
		ﬁve-gram
		ﬁve-gram as features
		length
		length of the word
		sdp-rel
		dependency relation tag
		alpha-features
		detect if certain linguistic pattern occurred
		in the current word or the next word
		Table 1: Word-level features for NER
	</Abstractive Summary>
</Paper ID=ument954>


<Paper ID=ument954> <Table ID =2>
	<Abstractive Summary> =
		word tokens between
		entity-type
		type of the entity mentions
		target entities) including three
		words to the target entities
		bow-lemma
		bow representation of the lemmatized
		dist-entities-cat
		distance between target entities
		tokens in the between context
		as categorical
		pos-tags
		part-of-speech tags
		dist-entities
		distance between target entities
		sdp
		shortest dependency path as bow
		entity-count
		count of entities in between context
		sdp-len
		length of shortest dependency path
		entity-count-cat
		count of entities in between context
		as scalar
		as categorical
		sdp-rel
		dependency relation tag
		e1 type = e2 type
		if type of e1 and e2 is same
		emb-sdp
		average embeddings of sdp
		sdp-entity
		sdp with entity as bow
		keyword-vec
		if current word is part of feature
		entity-patterns
		check if certain linguistic patterns
		list of relations
		occur in the vicinity of target entities
		Table 2: General and Entity features used in Relation Extraction
		Tokens
		Models
		Voting
		Post-
		M1
		M2
		M3
		processing
		r1
		Presence
		O
		O
		O
		O
		O
		r2
		of
		O
		O
		B-H
		O
		O
		r3
		ﬁsh
		I-H
		B-H
		I-H
		I-H
		B-H
		r4
		pathogen
		I-H
		I-P
		I-P
		I-P
		B-P
		r5
		Vibrio
		B-M
		B-M
		B-M
		B-M
		B-M
		r6
		salmonicida
		I-M
		O
		I-M
		I-M
		I-M
		r7
		in
		B-H
		O
		O
		O
		O
		r8
		ﬁsh
		B-H
		O
		B-H
		B-H
		B-H
		r9
		farm
		I-H
		O
		I-M
		I-H
		I-H
		r10
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists computed gen-
		eral and entity features
	</Extractive Summary>
</Paper ID=ument954>


<Paper ID=ument954> <Table ID =3>
	<Abstractive Summary> =
		O
		O
		O
		O
		O
		Table 3: NER: Ensembling and Post-processing cor-
		recting individual models mistakes
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the
		ensemble correcting individual model’s erroneous
		predictions
	</Extractive Summary>
</Paper ID=ument954>


<Paper ID=ument954> <Table ID =4>
	<Abstractive Summary> =
		137
		Task
		Train
		Dev
		Test
		Sentence Counts
		PharmaCo
		8068
		3748
		3930
		SeeDev
		644
		308
		466
		BB-norm+ner
		822
		413
		735
		PharmaCoNER Entities
		NORMALIZABLES
		2304
		1121
		859
		PROTEINAS
		1405
		745
		973
		UNCLEAR
		89
		44
		34
		NO NORMALIZABLES
		24
		16
		10
		BB-norm+NER Entities
		Habitat
		1118
		610
		-
		Microorganism
		739
		402
		-
		Phenotype
		369
		161
		-
		Table 4: Dataset statistics for NER
		4
		Experiments and Results
		4
	</Abstractive Summary>
</Paper ID=ument954>


<Paper ID=ument954> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5: Hyper parameter settings for NER and RE
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 lists the best conﬁguration of hyper-
		parameters for all the tasks
	</Extractive Summary>
</Paper ID=ument954>


<Paper ID=ument954> <Table ID =6>
	<Abstractive Summary> =
		788
		-
		Table 6: Scores on dev set using different features on
		PharmaCoNER and BB-norm+NER tasks
	</Abstractive Summary>
	<Extractive Summary> =
		NER: Table 6 shows the score on dev set for
		PharmaCoNER and BB-norm+NER
	</Extractive Summary>
</Paper ID=ument954>


<Paper ID=ument954> <Table ID =7>
	<Abstractive Summary> =
		301
		Table 7: Scores on dev set using different features on
		SeeDev task
	</Abstractive Summary>
	<Extractive Summary> =
		RE: Table 7 shows the score on dev set for
		SeeDev10
	</Extractive Summary>
</Paper ID=ument954>


<Paper ID=ument954> <Table ID =8>
	<Abstractive Summary> =
		823
		Table 8: Comparison of our system (MIC-CIS) with
		top-5 participants: Scores on Test set for SeeDev and
		BB-norm+NER
		achieves the best score among all participating
		systems with F1 score of 0
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Comparison with Participating Systems
		SeeDev: Table 8 (left) is the ofﬁcial result of
		SeeDev Shared Task
	</Extractive Summary>
	<Extractive Summary> =
		BB-norm+NER: Table 8 (right) shows the
		comparison of performance among participating
		teams on BB-norm+NER test set
	</Extractive Summary>
</Paper ID=ument954>


<Paper ID=ument955> <Table ID =1>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the detailed data statistics 
		provided by the task
	</Extractive Summary>
</Paper ID=ument955>


<Paper ID=ument955> <Table ID =2>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Performance of different models on develop-
		ment dataset (Precision) 
			
		As shown in Table 2, non-supervised baseline 
		model yielded a precision score of 0
	</Extractive Summary>
</Paper ID=ument955>


<Paper ID=ument955> <Table ID =3>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		7% preci-
		sion increase in habitat and phenotype entity link-
		ing tasks respectively compared to the chal-
		lenged-provided baseline model (Table 3), where 
		case-insensitive string matching was applied for 
																																																										
		7 https://sites
	</Extractive Summary>
</Paper ID=ument955>


<Paper ID=ument956> <Table ID =1>
	<Abstractive Summary> =
		154
		Table 1: Distributed vs Smart Matching for relation ex-
		traction
	</Abstractive Summary>
	<Extractive Summary> =
		While there is slight
		increase in the precision, the recall is observed
		to decrease considerably for the smart matching
		method (see Table 1)
	</Extractive Summary>
</Paper ID=ument956>


<Paper ID=ument956> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Soft Filter vs Hard Filter for relation extrac-
		tion
	</Abstractive Summary>
	<Extractive Summary> =
		The performance
		of each approach is tested on the development data
		set (see Table 2)
	</Extractive Summary>
</Paper ID=ument956>


<Paper ID=ument956> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Comparison with the participant systems for
		the normalization task of bacteria biotopes
	</Abstractive Summary>
</Paper ID=ument956>


<Paper ID=ument956> <Table ID =4>
	<Abstractive Summary> =
		Table 4: Comparison with the participant systems for
		the normalization task considering only Phenotype en-
		tities
	</Abstractive Summary>
	<Extractive Summary> =
		514
		As the results in Table 4 demonstrate, our sys-
		tem performs signiﬁcantly better than the other
		systems for the normalization of new Phenotype
		entities in the test set (Precision: 70
	</Extractive Summary>
</Paper ID=ument956>


<Paper ID=ument956> <Table ID =5>
	<Abstractive Summary> =
		The aim of
		the ﬁrst system is the normalization of the entity
		mentions in a biomedical text through the corre-
		sponding ontology, whereas the goal of the second
		Table 5: Comparison with the participant systems for
		the relation extraction task of bacteria biotopes
	</Abstractive Summary>
</Paper ID=ument956>


<Paper ID=ument957> <Table ID =1>
	<Abstractive Summary> =
		The fea-
		Train
		Dev
		Lives In
		715
		395
		Exhibits
		281
		138
		Total relatonships
		996
		533
		Intra-sentence relationships
		885
		467
		Inter-sentence relationships
		111
		66
		Table 1: BB-rel data statistics on the training and de-
		velopment set
	</Abstractive Summary>
</Paper ID=ument957>


<Paper ID=ument957> <Table ID =2>
	<Abstractive Summary> =
		5
		Optimizer
		sgd
		MLP layer
		1
		Table 2: Hyper-parameter setting
	</Abstractive Summary>
</Paper ID=ument957>


<Paper ID=ument957> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: The ofﬁcial results of the BB-rel task
	</Abstractive Summary>
</Paper ID=ument957>


<Paper ID=ument957> <Table ID =4>
	<Abstractive Summary> =
		7
		Table 4: Effects of ensemble training and inference
	</Abstractive Summary>
</Paper ID=ument957>


<Paper ID=ument957> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5:
		Results of recognizing inter- and intra-
		sentence relations
	</Abstractive Summary>
	<Extractive Summary> =
		As
		shown in Table 5, we obtained an F1-score of
		65
	</Extractive Summary>
</Paper ID=ument957>


<Paper ID=ument957> <Table ID =6>
	<Abstractive Summary> =
		7
		Table 6: Effects of lexical chains
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the performance
		comparison of the “lexical chains” method and the
		“root nodes” method on the development set
	</Extractive Summary>
</Paper ID=ument957>


<Paper ID=ument958> <Table ID =1>
	<Abstractive Summary> =
		8536 
		Table 1:  The BB task results comparison
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, while the performance of 
		our system is average compared to those of other 
		teams in the BB-rel subtask, we ranked second 
		among all participants in the BB-rel+ner, BB-norm 
		and BB-norm+ner subtasks
	</Extractive Summary>
</Paper ID=ument958>


<Paper ID=ument959> <Table ID =1>
	<Abstractive Summary> =
		0
		Table 1:
		Results showing the average score over all
		test documents for each metric from the structural an-
		notation (dependency parse construction) task for all
		participating teams
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		Two teams submitted ﬁve runs in total for the
		CRAFT-SA task (Table 1)
	</Extractive Summary>
</Paper ID=ument959>


<Paper ID=ument959> <Table ID =2>
	<Abstractive Summary> =
		mentions
		4,485
		1,845
		Table 2: Descriptive statistics of the coreference reso-
		lution annotations in the CRAFT training and test sets
	</Abstractive Summary>
</Paper ID=ument959>


<Paper ID=ument959> <Table ID =3>
	<Abstractive Summary> =
		362
		Table 3: Results for the coreference resolution task
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		One team submitted three runs for evaluation in
		the CRAFT-CR task (Table 3)
	</Extractive Summary>
</Paper ID=ument959>


<Paper ID=ument959> <Table ID =4>
	<Abstractive Summary> =
		3460928
		181
		Ontology
		Training
		Test
		Ontology
		Training
		Test
		CHEBI
		4,548 (18)
		2,200 (14)
		CHEBI EXT
		11,915 (38)
		5,248 (19)
		CL
		4,043 (244)
		1,749 (175)
		CL EXT
		6,275 (249)
		2,872 (175)
		GO BP
		9,280 (493)
		3,681 (272)
		GO BP EXT
		13,954 (526)
		5,847 (287)
		GO CC
		4,075 (80)
		1,184 (14)
		GO CC EXT
		8,495 (150)
		3,217 (30)
		GO MF
		375 (0)
		94 (0)
		GO MF EXT
		4,070 (28)
		1,822 (20)
		MOP
		240 (0)
		101 (0)
		MOP EXT
		386 (0)
		111 (0)
		NCBITaxon
		7,362 (2)
		3,101(0)
		NCBITaxon EXT
		7,592 (2)
		3,219 (0)
		PR
		17,038 (84)
		6,409 (44)
		PR EXT
		19,862 (110)
		7,932 (44)
		SO
		8,797 (108)
		3,446 (45)
		SO EXT
		24,955 (182)
		9,136 (72)
		UBERON
		12,269 (235)
		6,551 (118)
		UBERON EXT
		14,910 (255)
		7,416 (133)
		Table 4: Total and discontinuous (in parentheses) concept annotation counts by ontology for both the 67 article
		training and 30 article test sets
	</Abstractive Summary>
</Paper ID=ument959>


<Paper ID=ument959> <Table ID =5>
	<Abstractive Summary> =
		75
		Table 5: Aggregate concept annotation results evaluated per ontology against the 30 CRAFT test documents
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		One team submitted three runs to the CRAFT-CA
		task (Table 5)
	</Extractive Summary>
</Paper ID=ument959>


<Paper ID=ument96> <Table ID =1>
	<Abstractive Summary> =
		1k
		Table 1: Statistics of datasets
	</Abstractive Summary>
</Paper ID=ument96>


<Paper ID=ument96> <Table ID =2>
	<Abstractive Summary> =
		89
		Table 2: Main results on OntoNotes
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Main Results
		OntoNotes Table 27 shows the results of word-
		level and character-level methods on OntoNotes
		with various settings
	</Extractive Summary>
	<Extractive Summary> =
		, 2016), cross-domain, and semi-
		7In Table 2, 3, 4 and 5, the models with * use external
		labeled data for semi-supervised learning
	</Extractive Summary>
</Paper ID=ument96>


<Paper ID=ument96> <Table ID =3>
	<Abstractive Summary> =
		46
		Table 3: Main results on MSRA
	</Abstractive Summary>
</Paper ID=ument96>


<Paper ID=ument96> <Table ID =4>
	<Abstractive Summary> =
		21
		Table 4: Main results on Weibo
	</Abstractive Summary>
</Paper ID=ument96>


<Paper ID=ument96> <Table ID =5>
	<Abstractive Summary> =
		37
		Table 5: Main results on Resume
	</Abstractive Summary>
</Paper ID=ument96>


<Paper ID=ument96> <Table ID =6>
	<Abstractive Summary> =
		31
		Table 6:
		An ablation study of LGN
	</Abstractive Summary>
</Paper ID=ument96>


<Paper ID=ument96> <Table ID =7>
	<Abstractive Summary> =
		Table 7:
		An example with overlapping ambiguity
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Case Study
		Table 7 illustrates an example that probes the
		ability of LGN to tackle the word ambiguity
		problems
	</Extractive Summary>
</Paper ID=ument96>


<Paper ID=ument960> <Table ID =1>
	<Abstractive Summary> =
		7714
		*ontology pretraining disabled
		Table 1: F-Score results of our experiments using the CRAFT corpus
	</Abstractive Summary>
</Paper ID=ument960>


<Paper ID=ument960> <Table ID =2>
	<Abstractive Summary> =
		3384
		*ontology pretraining disabled
		Table 2: SER results of our experiments using the CRAFT corpus
	</Abstractive Summary>
</Paper ID=ument960>


<Paper ID=ument960> <Table ID =3>
	<Abstractive Summary> =
		06
		Table 3: System performance for unseen concepts: precision (P) and recall (R) calculated over the subset of
		annotations and predictions of IDs that were absent from the training data
	</Abstractive Summary>
	<Extractive Summary> =
		As can be seen
		in Table 3, ontology pretraining led to prediction
		of IDs outside the training data in four entity types
		(PR[ EXT], UBERON[ EXT])
	</Extractive Summary>
</Paper ID=ument960>


<Paper ID=ument961> <Table ID =1>
	<Abstractive Summary> =
		14,679
		1,623
		7,185
		Table 1: Characteristics of the CRAFT corpus
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the
		statistics numbers of these three subsets
	</Extractive Summary>
</Paper ID=ument961>


<Paper ID=ument961> <Table ID =2>
	<Abstractive Summary> =
		93
		Table 2: The most frequent patterns of mentions in the
		training set
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Patterns of Gold Mentions
		Table 2 reports some patterns4 with the highest
		frequencies in the training set
	</Extractive Summary>
</Paper ID=ument961>


<Paper ID=ument961> <Table ID =3>
	<Abstractive Summary> =
		5182
		Table 3: Results on the test set
	</Abstractive Summary>
</Paper ID=ument961>


<Paper ID=ument961> <Table ID =4>
	<Abstractive Summary> =
		5673
		Table 4: Results on the development set
	</Abstractive Summary>
</Paper ID=ument961>


<Paper ID=ument961> <Table ID =5>
	<Abstractive Summary> =
		205
		Tag
		Description
		NP
		noun phrase
		NN
		noun, singular or mass
		NML
		sub-NP nominal substrings
		PRP$
		possessive pronoun
		LS
		list item marker
		Table 5: The deﬁnition of relevant Penn Treebank labels
	</Abstractive Summary>
</Paper ID=ument961>


<Paper ID=ument961> <Table ID =6>
	<Abstractive Summary> =
		4053
		Table 6: Non-coreference results for the BLANC metric on the testing set
	</Abstractive Summary>
</Paper ID=ument961>


<Paper ID=ument961> <Table ID =7>
	<Abstractive Summary> =
		36
		Total Gold
		74,219
		Table 7: Results of models on each distance group; TP: True Positive; G
	</Abstractive Summary>
</Paper ID=ument961>


<Paper ID=ument962> <Table ID =1>
	<Abstractive Summary> =
		While these efforts have
		covered a wide range of languages, genres and text
		domains, and introduced end-to-end parsing from
		plain text as the objective, they have not speciﬁ-
		Train
		Test
		Documents
		67
		30
		Sentences
		21 731
		9 099
		Tokens
		561 032
		232 619
		Table 1: CRAFT Structural Annotation statistics
		Train
		Devel
		Eval
		Documents
		47
		10
		10
		Sentences
		15 007
		3 421
		3 303
		Tokens
		387 473
		91 306
		82 253
		Table 2: CRAFT Train data split for development
		cally involved scientiﬁc articles or biomedical do-
		main texts
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summa-
		rizes the key statistics of the data
	</Extractive Summary>
</Paper ID=ument962>


<Paper ID=ument962> <Table ID =2>
	<Abstractive Summary> =
		While these efforts have
		covered a wide range of languages, genres and text
		domains, and introduced end-to-end parsing from
		plain text as the objective, they have not speciﬁ-
		Train
		Test
		Documents
		67
		30
		Sentences
		21 731
		9 099
		Tokens
		561 032
		232 619
		Table 1: CRAFT Structural Annotation statistics
		Train
		Devel
		Eval
		Documents
		47
		10
		10
		Sentences
		15 007
		3 421
		3 303
		Tokens
		387 473
		91 306
		82 253
		Table 2: CRAFT Train data split for development
		cally involved scientiﬁc articles or biomedical do-
		main texts
	</Abstractive Summary>
	<Extractive Summary> =
		The statistics of this split are shown in Table 2
		The original CRAFT corpus syntactic annota-
		tion uses a modiﬁed Penn Treebank (PTB) con-
		stituency formalism (Verspoor et al
	</Extractive Summary>
</Paper ID=ument962>


<Paper ID=ument962> <Table ID =3>
	<Abstractive Summary> =
		22
		Table 3: Development set results with different word vectors
	</Abstractive Summary>
</Paper ID=ument962>


<Paper ID=ument962> <Table ID =4>
	<Abstractive Summary> =
		41
		Table 4: Development set results with extra training data
		Bio, word2vec/CBOW (window 2) and CRAFT
		tokenized, word2vec/CBOW (default parameters)
	</Abstractive Summary>
</Paper ID=ument962>


<Paper ID=ument962> <Table ID =5>
	<Abstractive Summary> =
		536
		Table 5: Final submission results on test data
		Metrics
		Run 1
		Run 2
		Run 3
		Tokens
		99
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Test set results
		The properties of the three runs we submitted to
		the task are summarized in Table 5 together with
		their development and test set LAS scores
	</Extractive Summary>
</Paper ID=ument962>


<Paper ID=ument962> <Table ID =6>
	<Abstractive Summary> =
		544
		Table 6: Final submission test results for all metrics
		limiting the performance of the parser, and that the
		remaining challenges for substantially advancing
		the performance of the system lie speciﬁcally in
		more accurately recovering the dependency struc-
		ture of the sentences
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Analysis of ﬁnal results
		Table 6 provides a detailed look at the perfor-
		mance of our three ﬁnal submissions using all met-
		rics implemented in the CoNLL 2018 shared task
		evaluation script (see Section 4
	</Extractive Summary>
	<Extractive Summary> =
		The
		only metrics showing notable remaining room for
		improvement are dependency-based (last ﬁve rows
		in Table 6)
	</Extractive Summary>
</Paper ID=ument962>


<Paper ID=ument963> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Subset of RDoC constructs used for this task
		and their domain
	</Abstractive Summary>
	<Extractive Summary> =
		Similarly, they per-
		formed well for abstracts with more gold-standard
		sentences (see Table 10)
	</Extractive Summary>
	<Extractive Summary> =
		We would
		like to thank Robell Basset, Lenin Lewis, Ninoo
		Table 10: Variation of Accuracy over the number of
		most relevant (gold-standard) sentences in abstracts
	</Extractive Summary>
</Paper ID=ument963>


<Paper ID=ument963> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Inter-annotator agreement of Task 1 and Task
		2
	</Abstractive Summary>
</Paper ID=ument963>


<Paper ID=ument963> <Table ID =3>
	<Abstractive Summary> =
		Table 3: The number of training examples (positively
		labeled abstracts) provided for Tasks 1 and 2 across
		constructs
	</Abstractive Summary>
	<Extractive Summary> =
		provided in the Table 3 and the distribution of the
		number of most relevant sentences per construct is
		shown in Table 4
	</Extractive Summary>
</Paper ID=ument963>


<Paper ID=ument963> <Table ID =4>
	<Abstractive Summary> =
		edu/rdoc-task/
		221
		Table 4: Distribution of the number of most relevant (gold-standard) sentences in abstracts for each construct in
		the training data
	</Abstractive Summary>
</Paper ID=ument963>


<Paper ID=ument963> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: The number of abstracts in test set for task
		1
	</Abstractive Summary>
	<Extractive Summary> =
		Although included in Table 7, we
		excluded the two constructs, Circadian Rhythms
		and Sleep and Wakefulness, from the ﬁnal anal-
		ysis since these constructs contain one and zero
		negative articles, respectively, leading to perfect
		performance (see Table 5)
	</Extractive Summary>
</Paper ID=ument963>


<Paper ID=ument963> <Table ID =6>
	<Abstractive Summary> =
		We used the def-
		Table 6: The number of abstracts and their percentages
		in test set for task 2
	</Abstractive Summary>
	<Extractive Summary> =
		The distribution of the test set for task 2 across
		constructs is shown in Table 6 and the distribution
		of the number of most relevant sentences per con-
		struct is provided in Table 4
	</Extractive Summary>
</Paper ID=ument963>


<Paper ID=ument963> <Table ID =7>
	<Abstractive Summary> =
		224
		Table 7: Performance of retrieving PubMed Abstracts related to the corresponding RDoC construct (Task 1)
	</Abstractive Summary>
	<Extractive Summary> =
		Although included in Table 7, we
		excluded the two constructs, Circadian Rhythms
		and Sleep and Wakefulness, from the ﬁnal anal-
		ysis since these constructs contain one and zero
		negative articles, respectively, leading to perfect
		performance (see Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		The last column in Table 7 reports the aver-
		age score for the corresponding construct
	</Extractive Summary>
</Paper ID=ument963>


<Paper ID=ument963> <Table ID =8>
	<Abstractive Summary> =
		86
		–
		–
		Table 8: Performance of extracting the most relevant sentence from each abstract related to the corresponding
		RDoC construct (Task 2)
	</Abstractive Summary>
</Paper ID=ument963>


<Paper ID=ument963> <Table ID =9>
	<Abstractive Summary> =
		58
		–
		–
		Table 9: Variation of Accuracy over various size of ab-
		stract
	</Abstractive Summary>
	<Extractive Summary> =
		Typically, participating teams performed rela-
		tively better on shorter abstracts (see Table 9),
		which is intuitive due to that fact the models have
		a higher chance of ﬁnding the most similar sen-
		tences for shorter abstracts
	</Extractive Summary>
</Paper ID=ument963>


<Paper ID=ument963> <Table ID =10>
	<Abstractive Summary> =
		We would
		like to thank Robell Basset, Lenin Lewis, Ninoo
		Table 10: Variation of Accuracy over the number of
		most relevant (gold-standard) sentences in abstracts
	</Abstractive Summary>
	<Extractive Summary> =
		Similarly, they per-
		formed well for abstracts with more gold-standard
		sentences (see Table 10)
	</Extractive Summary>
</Paper ID=ument963>


<Paper ID=ument964> <Table ID =1>
	<Abstractive Summary> =
		Table 1: RDoC construct - This table shows two PubMed abstracts labelled with two different RDoC construct
		and PubMed ID (PMID)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the association be-
		tween PubMed abstracts and RDoC constructs de-
		pending on the semantic knowledge of the high-
		lighted content words
	</Extractive Summary>
</Paper ID=ument964>


<Paper ID=ument964> <Table ID =2>
	<Abstractive Summary> =
		Here, rsup
		siamese refers to a relevance
		233
		L1
		L2
		L3
		L4
		L5
		L6
		L7
		L8
		Total
		All data
		39
		38
		47
		21
		28
		27
		48
		18
		266
		Train set
		31
		30
		37
		16
		22
		21
		38
		14
		209
		Dev set
		8
		8
		10
		5
		6
		6
		10
		4
		57
		Test set (Task1)
		79
		108
		123
		144
		138
		139
		122
		146
		999
		Test set (Task2)
		19
		26
		30
		35
		34
		34
		30
		36
		244
		Table 2: Data statistics - # of PubMed abstracts belong-
		ing to each RDoC construct in different data partitions
	</Abstractive Summary>
	<Extractive Summary> =
		Number of abstracts for each RDoC construct is
		described in Table 2, where ﬁrst row describes
		the statistics for all abstracts and second & third
		row shows the split of those abstracts into training
		and development sets maintaining a 80-20 ratio for
		each RDoC construct
	</Extractive Summary>
	<Extractive Summary> =
		(Refer to Table 2 for header notations) (mAP: “Mean Average Precision”; MAA:
		Macro-Average Accuracy)
		PubMed Abstract
		(PMID: “23386529”; RDoC construct: “Loss”)
		Most Relevant Sentence
		(using reRank(BM25-Extra))
		Sentence
		ID
		Gold
		Label
		Nurses are expected to care for
		grieving women and families
		suffering from perinatal loss
	</Extractive Summary>
</Paper ID=ument964>


<Paper ID=ument964> <Table ID =3>
	<Abstractive Summary> =
		994
		Table 3:
		RDoC Task-1 results (on development
		set): Classiﬁcation accuracy and mean Average Pre-
		cision (mAP) of a-supDocNADE and SVM mod-
		els
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results: RDoC Task-1
		Table 3 shows the performance of supervised Doc-
		ument Ranker models i
	</Extractive Summary>
</Paper ID=ument964>


<Paper ID=ument964> <Table ID =4>
	<Abstractive Summary> =
		41
		PTA
		28316567
		Loss
		Table 4:
		RDoC Task-1 analysis:
		Ranking of
		PubMed abstracts within “Potential Threat Anxiety
		(PTA)” cluster using supervised prediction probabil-
		ities (p(q|v))
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Analysis: RDoC Task-1
		Table 4 shows an impure “Potential Threat Anxi-
		ety” cluster of abstracts containing an intruder ab-
		stract with label (RDoC construct) “Loss”
	</Extractive Summary>
</Paper ID=ument964>


<Paper ID=ument964> <Table ID =5>
	<Abstractive Summary> =
		737
		Table 5: RDoC Task-2 results (on development set):
		Performance of unsupervised and supervised sentence
		rankers (Figure 2) under different conﬁgurations
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results: RDoC Task-2
		Table 5 shows results for Task-2 using three unsu-
		pervised and two supervised sentence ranker mod-
		els
	</Extractive Summary>
	<Extractive Summary> =
		#6
		Relevant
		Table 7: RDoC Task-2 analysis: This table shows
		that the most relevant sentence predicted using
		reRank(BM25-Extra) is actually not a relevant
		sentence, but Ensemble {#1, #2, #4} (Table 5)
		predicts the correct sentence as the most relevant
	</Extractive Summary>
</Paper ID=ument964>


<Paper ID=ument964> <Table ID =6>
	<Abstractive Summary> =
		46
		Table 6: RDoC Tasks ofﬁcial results - performance on test set of different competing systems
	</Abstractive Summary>
	<Extractive Summary> =
		6
		Results: RDoC Task 1 & 2 on Test set
		Table 6 shows the ﬁnal evaluation scores of differ-
		ent competing systems for both the RDoC Task-1
		& Task-2 on ﬁnal test set
	</Extractive Summary>
</Paper ID=ument964>


<Paper ID=ument964> <Table ID =7>
	<Abstractive Summary> =
		#6
		Relevant
		Table 7: RDoC Task-2 analysis: This table shows
		that the most relevant sentence predicted using
		reRank(BM25-Extra) is actually not a relevant
		sentence, but Ensemble {#1, #2, #4} (Table 5)
		predicts the correct sentence as the most relevant
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Analysis: RDoC Task-2
		Table 7 shows that the most relevant sentence
		predicted by reRank(BM25-Extra) is actually a
		non-relevant sentence
	</Extractive Summary>
</Paper ID=ument964>


<Paper ID=ument965> <Table ID =1>
	<Abstractive Summary> =
		2
		Dataset
		Question (Q)
		Context (C)
		|Q|
		|C|
		Q ⊥⊥ C
		Train
		Dev
		Test
		I
		SQuAD
		Crowdsourced
		Wikipedia
		11
		137
		
		86,588
		10,507
		-
		NewsQA
		Crowdsourced
		News articles
		8
		599
		
		74,160
		4,212
		-
		TriviaQA♠
		Trivia
		Web snippets
		16
		784
		
		61,688
		7,785
		-
		SearchQA♠
		Jeopardy
		Web snippets
		17
		749
		
		117,384
		16,980
		-
		HotpotQA
		Crowdsourced
		Wikipedia
		22
		232
		
		72,928
		5,904
		-
		Natural Questions
		Search logs
		Wikipedia
		9
		153
		
		104,071
		12,836
		-
		II
		BioASQ♠
		Domain experts
		Science articles
		11
		248
		
		-
		1,504
		1,518
		DROP♦
		Crowdsourced
		Wikipedia
		11
		243
		
		-
		1,503
		1,501
		DuoRC♦
		Crowdsourced
		Movie plots
		9
		681
		
		-
		1,501
		1,503
		RACE♥
		Domain experts
		Examinations
		12
		349
		
		-
		674
		1,502
		RelationExtraction♠
		Synthetic
		Wikipedia
		9
		30
		
		-
		2,948
		1,500
		TextbookQA♥
		Domain experts
		Textbook
		11
		657
		
		-
		1,503
		1,508
		III
		BioProcess♥
		Domain experts
		Textbook
		9
		94
		
		-
		-
		219
		ComplexWebQ♠
		Crowdsourced
		Web snippets
		14
		583
		
		-
		-
		1,500
		MCTest♥
		Crowdsourced
		Crowdsourced
		9
		244
		
		-
		-
		1,501
		QAMR♦
		Crowdsourced
		Wikipedia
		7
		25
		
		-
		-
		1,524
		QAST
		Domain experts
		Transcriptions
		10
		298
		
		-
		-
		220
		TREC♠
		Crowdsourced
		Wikipedia
		8
		792
		
		-
		-
		1,021
		Table 1: MRQA sub-domain datasets2
		Table 1: Performance of different models on SQuAD
		development set
		collection
		#docs
		#natural
		#generated
		questions
		questions
		V_antiq
		61
		274
		4672
		WP_arch
		96
		302
		36259
		CT_1GM
		16
		241
		7502
		WP_1GM
		123
		319
		50971
		total
		296
		1136
		99404
		Table 1: Description of CALOR-QUEST corpus
		4
		Evaluation
		The main objective of our work is to create in a
		semi-automatic fashion a training corpus for rea-
		ding comprehension model
		Table 1: A sample problem from a multiple-choice QA
		task OpenBookQA (Mihaylov et al
		Q1: Did they release any albums
		A1: Skid Row, released in January 1989
		Q2: How did it do
		A2: instant success
		Q2’: How did Skid Row do
		Q3: Did it go on tour
		A3: ﬁrst supporting Bon Jovi’s outdoor show
		Q3’: Did Skid Row go on tour
		Q4: Did the Tour have a name
		A4: New Jersey tour
		Q4’: Did the outdoor show have a name
		Q5: How long did the tour last
		A5: CANNOTANSWER
		Q5’: How long did the New Jersey tour last
		Table 1: An example of conversational machine comprehension from QuAC dataset (Choi et al6
		Table 1: Answer passage prediction performance, measured
		by Hits@10 on dev bridge questions7
		Table 1: Results on the MRC datasets
		Accuracy
		P@1
		Random Guess
		754
		background vocabulary size
		8616
		2008
		3988
		situation vocabulary size
		6949
		1077
		2736
		question vocabulary size
		1457
		1411
		1885
		Table 1: Key statistics of ROPES67
		Table 1: Question patterns (n = {1, 2, 3}, N = 200)
		Table 1: Example/comparison of our abstractive sum-
		mary on a Debatepedia sample with the output of the
		diversity driven attention model of Nema et al The full-text task has only been addressed
		78
		Who is Emily in love with?
		Who is Emily imprisoned by?
		Who helps Emily escape from the castle?
		Who owns the castle in which Emily is imprisoned?
		Who became Emily’s guardian after her father’s death?
		Table 1: Who questions from NarrativeQA for the book
		The Mysteries of Udolpho, by Ann Radcliffe1
		100
		100
		Table 1: Conversational QA results on CoQA and QuAC, where (N-ctx) refers to using previous N QA pairs (%)15
		Table 1: Baseline results for HotpotReader and BERT
		We show our proposed improvements in Table
		2 and 3php?KNP
		99
		Case
		Question
		Nominative
		［述語］の主語は何か？
		(What is the subject of [predicate]?)
		Accusative
		〇〇を［述語］、の〇〇に入るものは何か？
		(What is the accusative of [predicate]? )
		Dative
		〇〇に［述語］、の〇〇に入るものは何か？
		(What is the dative of [predicate]? )
		Table 1: Question templates of PAS-QA datasets654
		Table 1: Retrieval performance of models on the HOT-
		POTQA benchmark
		Which test would
		see reactions taking
		place slower, test A
		or test B?
		test A
		Table 1: Examples for the datasets we use in our study01M
		Table 1: Dataset Summary
		well-understood, and it tests a model’s tolerance
		for paraphrasing and coreferences between the
		question and context
		SQuAD
		NQ
		Questions
		87,599
		74,097
		Candidate Sentences
		91,707
		239,013
		Candidate Paragraphs
		18,896
		58,699
		Table 1: The number of questions and candidates in the
		constructed datasets ReQA SQuAD and ReQA NQ7
		Table 1: Dataset Statistics
		from summaries [SEP] Where did Super Bowl 50 take place [SEP] [MASK]
		Table 1: BERT-SQG Running Example
		Figure 3: The BERT-SQG architecture
		In C′, we design and insert a new token (i
		Class
		Original
		After Downsampling
		What
		50385
		4000
		Which
		6111
		4000
		Where
		3731
		3731
		When
		5437
		4000
		Who
		9162
		4000
		Why
		1224
		1224
		How
		9408
		4000
		Others
		9408
		4000
		Table 1: SQuAD training set statistics
		Table 1: Example questions (Q) from the NarrativeQA data set, with gold free-text answers (G), the most relevant
		sentence as automatically extracted from the summary (E) and the most relevant sub-sentence level span (boldface)
		∗Authors contributed equally
		Dataset
		Size
		Context
		Question
		SQuAD
		96K
		wikipedia
		crowd
		NewsQA
		78K
		newswire
		crowd
		TriviaQA
		69K
		snippets
		quiz
		SearchQA
		133K
		snippets
		quiz
		HotpotQA
		78K
		wikipedia
		crowd
		NaturalQuestions
		116K
		wikipedia
		crowd
		DROP
		1,503
		wikipedia
		crowd
		RACE
		674
		exam
		handcraft
		BioASQ
		1,504
		biomedical
		handcraft
		TextbookQA
		1,503
		textbook
		handcraft
		RelationExtraction
		2,948
		wikipedia
		KB
		DuoRC
		1,501
		plot
		crowd
		Table 1: Characteristics of released datasets for the
		MRQA shared task49
		Table 1: Development Datasets Results0
		1k lessons and 26k multi-modal questions, from
		middle school science curriculum
		Table 1: Statistics of out-of-domain validation dataset
		204
		Dataset
		Source
		Question
		Multi-
		hop
		In-Domain Datasets
		SQuAD
		Wikipedia
		Crowd
		No
		NewsQA
		News
		Crowd
		No
		TriviaQA
		Snippets
		Trivia
		No
		SearchQA
		Snippets
		Trivia
		No
		HotpotQA
		Wikipedia
		Crowd
		Yes
		NQ
		Wikipedia
		Query
		No
		Out-of-Domain Datasets
		DROP
		Wikipedia
		Crowd
		Yes
		RACE
		Exam
		Expert
		Yes
		DuoRC
		Movie Plot
		Crowd
		No
		BioASQ
		Biomedical
		Crowd
		No
		TQA
		Textbook
		Crowd
		No
		RE
		Wikipedia
		Crowd
		No
		Table 1: Characterization of the training and devel-
		opment datasets
		Matching
		✓
		Table 1: The datasets of MRQA 2019 Shared Task include 6 training sets and 12 testing sets3
		Table 1: Number of examples (question-context pairs),
		segments (question-context chunks), and the percent-
		age of No Answer (NA) segments within each dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Please see Table 1 as well as the
		associated dataset papers for more details on each
		sub-domain’s properties
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, the uniﬁed model out-
		performs previous state-of-the-art models and the
		baseline model
	</Extractive Summary>
	<Extractive Summary> =
		To correctly answer the question in Table 1, for
		example, scientiﬁc facts1 from the provided refer-
		ence corpus — {“a magnet attracts magnetic met-
		als through magnetism” and “iron is always mag-
		netic”}, as well as general world knowledge ex-
		tracted from an external source such as {“a belt
		buckle is often made of iron” and “iron is metal”}
		are required
	</Extractive Summary>
	<Extractive Summary> =
		meaningful by considering the previous questions
		and answers history (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 gives an exam-
		ple of conversational machine comprehension in
		QuAC dataset
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results of the MRC Task
		Table 1 details the performances of models on
		MRC datasets
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the
		key statistics of the dataset
	</Extractive Summary>
	<Extractive Summary> =
		the fact that pairs
		of questions like in Table 1 will have opposite an-
		swers
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 displays some question patterns and their
		frequencies
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 exempliﬁes the diversity and com-
		plexity of Who questions in the data, by listing a
		set of questions from a single book, which require
		increasingly complex types of reasoning
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 1 reports model performance on CoQA and
		QuAC
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results
		In Table 1, the original HotpotReader method does
		not show signiﬁcant performance improvement
		when comparing the Single-Oracle setting with
		the Ordered-Oracle setting
	</Extractive Summary>
	<Extractive Summary> =
		8
		Table 1 reports the accuracy(@k) of retrieving
		7https://lucene
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows
		that, indeed modeling the chain of documents is
		important
	</Extractive Summary>
	<Extractive Summary> =
		A
		unique characteristic of ROPES is that questions
		generally present two possible answer choices, one
		of which is incorrect (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 summarizes key characteristics for the
		datasets
	</Extractive Summary>
	<Extractive Summary> =
		WikiQA consists of 3,047 questions and
		29,258 candidate answers, while ReQA SQuAD
		and ReQA NQ each contain over 20x that num-
		ber of questions and over 3x that number of candi-
		dates (see Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 gives
		summary statistics of these datasets
	</Extractive Summary>
	<Extractive Summary> =
		In
		Table 1, we give an example of the actual running
		of the model
	</Extractive Summary>
	<Extractive Summary> =
		, Table 1 for examples)
	</Extractive Summary>
	<Extractive Summary> =
		The goal of this competition is to
		demonstrate high performances on out-of-domain
		datasets (the bottom part of Table 1 and addition-
		ally unseen test datasets) by the trained model
		which only utilizes in-domain datasets (the top
		part of Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		4
		Results
		The results of our model on all the development
		datasets are summarized in Table 1 and the results
		on all the test datasets are summarized in Table 2
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows the statistics and description
		of these datasets
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 1, the major challenge of the
		shared task is that the train and test datasets differ
		in the following ways:
		• Questions:
		They
		come
		from
		different
		sources, e
	</Extractive Summary>
	<Extractive Summary> =
		Since the testing
		set differs from the training set in terms of docu-
		ment sources (see Table 1), we divide the testing
		set into two subsets: (1) Wiki & Web & News and
		(2) Other
	</Extractive Summary>
	<Extractive Summary> =
		In Table 1 we tabulate the number of “examples”
		(question-context pairs), “segments” (the question
		combined with a portion of the context), and “no-
		answer” (NA) segments (those without a valid an-
		swer span)
	</Extractive Summary>
	<Extractive Summary> =
		(q, c) →
		�
		(q, ci·D:M+i·D), ∀i ∈ [0, k]
		�
		(1)
		The frequencies presented in Table 1 are based
		on our settings of M = 512 and D = 128
	</Extractive Summary>
	<Extractive Summary> =
		Addition-
		ally, its long contexts generate 657K segments,
		double that of the next largest dataset (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		3% of all input segments are NA,
		as shown in Table 1, its unsurprising their inclu-
		sion signiﬁcantly impacted training time and re-
		sults
	</Extractive Summary>
</Paper ID=ument965>


<Paper ID=ument965> <Table ID =2>
	<Abstractive Summary> =
		, 2019)
		Alexandria University
		Table 2: List of participants, ordered by the macro-averaged F1 score on the hidden evaluation set0
		Table 2: Results obtained with BERT-SQUAD on
		CALOR-QUEST with two conditions : V1 correspond
		to SQUAD1 ✓
		Table 2: A sample problem from the ARC-Challenge
		dataset (Clark et al
		Type
		dataPretrain
		QuAC
		train
		val
		test
		train
		val
		test
		questions
		20k
		5k
		3k
		81k
		7k
		7k
		dialogs
		3k
		600
		400
		11k
		1k
		1k
		Table 2: data statistics26
		Table 2: QA performance on HotpotQA7%
		Table 2: Results on Sentence Matching
		implementation (Chollet et al
		Table 2: Types of relations in the background passages21
		Table 2: Qualities (BLEU scores) of generated ques-
		tions (without considering question patterns)
		Table 2:
		Example of our extractive summary on an
		example from the query-based version of CNN/Daily
		Mail (Hermann et al006
		Table 2: Precision scores (P@1, P@5), and Mean Reciprocal Rank (MRR) for frequency-based baselines and our
		system, with and without pretraining8
		Table 2: The ablation study of BERT-FlowDelta (%)26
		Table 2: Results for HotpotReader on 3 oracle settings
		BERT achieved promising results even in the
		Single-Oracle setting which proves its original ca-
		pacity for QA5%)
		－
		Table 2: Classiﬁcation of questions in the RC-QA
		dataset26
		Table 2: Performance on QA task on hidden test set of
		HOTPOTQA after adding the retrieved paragraphs
		containing the answer spans then it might be a lit-
		tle easier for a downstream QA model to identify
		the answer span540
		Table 2: Human Judgments and Metrics: Correlation between metrics and human judgments using Spearman’s
		rho (ρ) and Kendall’s tau (τ) rank correlation coefﬁcients00
		Table 2: Results from all experiments
		We evaluate models on every dataset’s dev set,
		sample 100 question-answer pairs to character-
		ize the linguistic phenomena and inference type
		needed to answer correctly, and then inspect per-
		formance on the sampled pairs9
		Table 2:
		Token-level statistics of the constructed
		datasets
		Dataset
		IC
		MWC
		Imp
		No-Ans
		SEARs
		NewsQA
		0
		0
		501
		347
		16009
		QuoRef
		0
		0
		79
		385
		11759
		DROP
		1377
		457
		113
		284
		16382
		SQuAD
		16
		0
		875
		594
		28188
		ROPES
		637
		119
		0
		201
		2909
		DuoRC
		22
		0
		2706
		-
		45020
		Table 2: Yields of augmented datasets
		No Answer
		substitutes a name in the question for
		a different name from the passage to create with
		high probability a new question with no answer [SEP] Where did Super Bowl 50 take place [SEP] [MASK]
		Table 2: BERT-HLSQG Running Example
		Figure 4: The BERT-HLSQG architecture
		Train
		Test
		Dev
		SQuAD 73K
		73240
		11877
		10570
		SQuAD 81K
		81577
		8964
		8964
		Table 3: Dataset statistics: SQuAD 73K is the setting
		of (Du et al94
		Table 2: Comparison of our model with the baselines if unsuccessful: considering any sentence in
		the summary, return the longest substring
		174
		train
		valid
		test
		# QA-pairs
		32,170
		3,461
		10,557
		# documents
		1,102
		115
		355
		Table 2: Statistics of the NarrativeQA data set (Ko-
		cisky et al
		Test
		SQuAD
		76,079
		10,507
		10,509
		NewsQA
		69,947
		4,212
		4,213
		TriviaQA
		53,902
		7,785
		7,786
		SearchQA
		100,403
		16,980
		16,981
		HotpotQA
		67,010
		5,904
		5,902
		NaturalQuestions
		91,234
		12,836
		12,837
		DROP
		-
		1,503
		-
		RACE
		-
		674
		-
		BioASQ
		-
		1,504
		-
		TextbookQA
		-
		1,503
		-
		RelationExtraction
		-
		2,948
		-
		DuoRC
		-
		1,501
		-
		Table 2: Statistics of datasets for RC tasks71
		Table 2: Test Datasets Results68
		Table 2: Model performance on validation and test set
		GPU
		TPU
		Fine-tuned Layers
		12 - 24 (13)
		1 - 24 (24)
		Floating Point
		16
		32
		Training Batch Size
		4
		48
		Sequence Length
		340
		512
		MLP Layer Size
		512, 384, 1
		1024, 1
		Table 2: Difference of hyper-parameters and the MLP
		structure when ﬁne-tuning XLNet model on GPU and
		TPU8
		Table 2: The conﬁgurations and hyper-parameters of the eleven models used in our experiments16
		Table 2: Model performance including or excluding
		No-Answer (NA) segments in training
	</Abstractive Summary>
	<Extractive Summary> =
		7 In total, we re-
		ceived submissions from 10 different teams for the
		ﬁnal evaluation (Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		For
		example, in the sample problem in Table 2, we ex-
		tract concept mentions such as “Mercury”
	</Extractive Summary>
	<Extractive Summary> =
		For example, the ambiguous concept
		mention “Mercury” in Table 2 should be linked
		to the concept Mercury (planet) rather than
		Mercury (element) in Wikipedia
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 de-
		scribes the data statistics
	</Extractive Summary>
	<Extractive Summary> =
		Question Answering Results
		Table 2 shows the
		ﬁnal multi-hop QA performance
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the breakdown
		of the kinds of relations in the dataset
	</Extractive Summary>
	<Extractive Summary> =
		1
		Quality of the Generated Questions
		The results shown in Table 2 establish our primary
		assumption, which states that a question coherent
		to the current conversational context can be gener-
		ated primarily by knowing the current focus of in-
		terrogation
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the ablation study of BERT-
		FlowDelta, where two proposed modules are
		both important for achieving such results
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 2, the RC-QA
		dataset contains nearly 40% of questions asking
		arguments of nominative, accusative and dative,
		and a few questions asking for omitted arguments,
		which are similar to the PAS-QA dataset
	</Extractive Summary>
	<Extractive Summary> =
		2
		Performance on HOTPOTQA
		Table 2 shows the performance on the QA task
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 lists the average number of tokens in
		question and sentence-level answer text, as well
		as the “query coverage”, which is the percentage
		of tokens in the question that also appear in the
		answer
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the num-
		ber of augmentations generated by each augmenta-
		tion technique-dataset pair
	</Extractive Summary>
	<Extractive Summary> =
		3
		Performance Comparison
		Table 2 shows the performance evaluation results
		of models on out-of-domain datasets
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 lists the detailed conﬁgurations and the
		hyper-parameters of these models
	</Extractive Summary>
	<Extractive Summary> =
		Please refer to Table 2 with corresponding model ID for details about the model conﬁgurations
	</Extractive Summary>
	<Extractive Summary> =
		Please refer to Table 2 with
		corresponding model ID for details about the model conﬁgurations
	</Extractive Summary>
	<Extractive Summary> =
		We ﬁnd that this simple form of Nega-
		tive Sampling yields non-trivial improvements on
		MRQA (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		4
		Experiments and Discussion
		During our experimentation process we used our
		smallest model BERT Base Cased (BBC) for the
		most expensive sampling explorations (Figure 1),
		XLNet Base Cased (XBC) to conﬁrm our ﬁndings
		extended to XLNet (Table 2), and XLNet Large
		Cased (XLC) as the initial basis for our ﬁnal sub-
		mission contenders (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2 we examine
		the impact of including No Answer segments in
		our training set
	</Extractive Summary>
</Paper ID=ument965>


<Paper ID=ument965> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Performance as F1 score on the shared task3
		Table 3: F1 results on questions associated to 10 se-
		mantic frames of CALOR-QUEST with a model trai-
		ned on the whole corpus (all) and one trained on a cor-
		pus where all the generated questions corresponding to
		these 10 frames have been removed (w/o)
		4 When the
		input sequence length exceeds 512, we truncate
		the longest sequence among q, oi, and di (deﬁned
		Dataset
		Train
		Dev
		Test
		Total
		RACE
		87,866
		4,887
		4,934
		97,687
		ARC-Easy
		2,251
		570
		2,376
		5,197
		ARC-Challenge
		1,119
		299
		1,172
		2,590
		OpenBookQA
		4,957
		500
		500
		5,957
		Table 3: The number of questions in RACE and the
		multiple-choice subject-area QA datasets for evalua-
		tion: ARC-Easy, ARC-Challenge, and OpenBookQA75
		Table 3: BLEU-1,2,3,4, EM, ROUGE L and F1 scores on the test dataset in the dataPretrain67
		Table 3: QA performance ablation on the development set
		Table 3: Types of grounding found in ROPES25
		Table 3: Qualities (BLEU scores) of generated ques-
		tions
		Table 3:
		Examples of some of our paraphrased sen-
		tences using an MT system7%
		full evidence found in context
		BM25F
		27%
		partial evidence found in context
		47%
		no evidence found in context
		26%
		Table 3: Percentage of contexts where the correct
		character is mentioned (top)1
		Table 3: Dialogue accuracy for SCONE test (%)77
		Table 3: Results for BERT on 3 oracle settings
		Among both approaches, co-matching shows
		promising performance improvement especially
		for the well pre-trained BERT model
		Training method
		Dataset
		MC-single
		PAS-QA
		Joint
		training
		MC-merged
		PAS-QA + RC-QA
		MC-stepwise
		RC-QA → PAS-QA
		Table 3: Three training methods for PAS analysis41
		Table 3: Zero-shot (zs) IR results on WIKIHOP992
		Table 3: Inter-annotator agreement computed using
		Cohen’s kappa (κ), Pearson correlation (r), and Spear-
		man’s correlation (ρ)
		Table 3: Examples of frequent error types from all 4 datasets
		error type1
		Table 3:
		The distribution of question types in
		ReQA SQuAD and ReQA NQ
		Q: What did histori-
		ans
		compare
		to
		the
		Coptic
		language?
		A: hieroglyphs
		Q:
		What’d
		histori-
		ans
		compare
		to
		the
		Coptic
		language?
		A: hieroglyphs
		Table 3: Examples of generated augmentations with various templates [SEP] Where did Super Bowl 50 take place [SEP] [MASK]
		Table 2: BERT-HLSQG Running Example
		Figure 4: The BERT-HLSQG architecture
		Train
		Test
		Dev
		SQuAD 73K
		73240
		11877
		10570
		SQuAD 81K
		81577
		8964
		8964
		Table 3: Dataset statistics: SQuAD 73K is the setting
		of (Du et al65
		Table 3: Performance of the QG model with respect to the accuracy of the interrogative-word classiﬁer86
		Table 3: Results on summary-level sentence-relevance
		classiﬁcation on the NQA test set of 25K question-
		answer pairs
		SNLI
		550,152
		10,000
		FICTION
		77,348
		2,000
		GOVERNMENT
		77,350
		2,000
		SLATE
		77,306
		2,000
		TELEPHONE
		83,348
		2,000
		TRAVEL
		77,350
		2,000
		Table 3: Statistics of datasets for NLI tasks31
		Table 3: Results for XLNet models that are only ﬁne-tuned on a single training set but tested on all the in-domain
		and out-of-domain development sets42
		-
		Table 3: System performance on the development and test set55
		Table 3: F1 scores for data augmentation using different proportions of query and context paraphrasing and dif-
		ferent sampling distributions on XLNet Large Cased, on individual datasets
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Main Results
		Table 3 lists the macro-averaged F1 scores of all
		the submissions on both the development and test-
		ing portions of the MRQA dataset
	</Extractive Summary>
	<Extractive Summary> =
		As seen in Table 3, many of the submis-
		sions outperform our BERT-Large baseline signif-
		icantly
	</Extractive Summary>
	<Extractive Summary> =
		We evaluate all the submissions on the in-
		domain datasets (Split I) in Table 3 and ﬁnd that
		there is a very strong correlation between in-
		domain and out-of-domain performance
	</Extractive Summary>
	<Extractive Summary> =
		3
		Utilization of External Knowledge from
		In-Domain Data
		Since there are a relatively small number of train-
		ing instances available for a single subject-area
		QA task (see Table 3), instead of ﬁne-tuning a
		pre-ﬁne-tuned model on a single target dataset, we
		also investigate into ﬁne-tuning a pre-ﬁne-tuned
		model on multiple in-domain datasets simulta-
		neously
	</Extractive Summary>
	<Extractive Summary> =
		Our experiments show that this sim-
		ple approach can tremendously increase the an-
		1As shown in Table 3 of Chen et al
	</Extractive Summary>
	<Extractive Summary> =
		Ablation Study
		Table 3 gives ablation results on
		the dev set, where both entity linking and the aux-
		iliary objective slightly improve the performance
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows examples and
		breakdown of these three phenomena
	</Extractive Summary>
	<Extractive Summary> =
		Given these discussions, Table 3 displays the
		qualities of generated questions under several con-
		ditions, and it conﬁrms the above mentioned
		prospect may be probable
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows
		examples of paraphrased sentences using back-
		translation
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows
		our model outperforms both models in zero-shot
		setting
	</Extractive Summary>
	<Extractive Summary> =
		We ﬁnd strong agreement be-
		tween the two annotators across the three datasets
		(see Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		We also introduce
		categories for common errors observed across all
		datasets below; Table 3 shows examples for every
		128
		Error Type
		Question
		Answer
		Prediction
		Random Guess
		How high do plague fevers run?
		38-41C
		near 100%
		Same Entity Type
		What team lost Super Bowl XXXIII?
		Atlanta Falcons
		Denver
		Sentence Selection
		What did Marlee Matlin translate?
		the national anthem
		American Sign Language
		Copying From Question
		What was Apple Talk?
		proprietary suite of
		AppleTalk
		networking protocols
		Factually Correct
		How long are car loans typically?
		60-month
		5 years
		Reasonable Answer
		What did Edison offer Tesla
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 shows the distribution of question
		types for each dataset
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 summarizes statis-
		tics for the compared datasets
	</Extractive Summary>
	<Extractive Summary> =
		Table 3 and
		Figure 3 show a linear relationship between the
		accuracy of the classiﬁer and the IWAQG
	</Extractive Summary>
	<Extractive Summary> =
		Results are shown in Table 3, and show
		that the model detects the most relevant summary
		sentence for a question accurately across a variety
		of metrics
	</Extractive Summary>
	<Extractive Summary> =
		1
		The Main Results and the Effects of
		Pre-trained Models
		Table 3 shows the main results and the results for
		the effects of pre-trained models
	</Extractive Summary>
	<Extractive Summary> =
		From Table 3,
		we have the following observations:
		(1) Our submitted system signiﬁcantly outper-
		forms the ofﬁcial baseline by about 10 F1 score,
		and it is ranked at top 1 of all the participants in
		terms of averaged F1 score 9
	</Extractive Summary>
	<Extractive Summary> =
		4
		Experiments and Discussion
		During our experimentation process we used our
		smallest model BERT Base Cased (BBC) for the
		most expensive sampling explorations (Figure 1),
		XLNet Base Cased (XBC) to conﬁrm our ﬁndings
		extended to XLNet (Table 2), and XLNet Large
		Cased (XLC) as the initial basis for our ﬁnal sub-
		mission contenders (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		We present the best results
		in Table 3 and conclude that these data augmenta-
		tions did not help in-domain or out-domain perfor-
		mance
	</Extractive Summary>
</Paper ID=ument965>


<Paper ID=ument965> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4:
		Macro-averaged F1 scores based on the
		dataset characteristics as deﬁned in Table 1, 2018)
		Table 4: Results obtained on the French SQuAD test
		corpus with a model trained on CALOR-QUEST and
		the original BERT-SQUAD model for English with
		back-translation
		5
		Conclusion
		In this work, we have proposed a semi-
		automatic method to generate question/answer
		pairs from a corpus of documents annotated in
		semantic frames, with the purpose of building a
		large training corpus for machine reading com-
		prehension in French0
		Table 4: Accuracy (%) of the pre-ﬁne-tuned model on
		the RACE dataset, which contains two subsets: RACE-
		M and RACE-H, representing problems collected from
		middle and high school language exams, respectively8
		100
		100
		Table 4: F1, HEQ-Q and HEQ-D scores on the test
		dataset of QuAC dataset
		Which
		experiment
		would be less appro-
		priate for case C, case
		A or case B?
		case A
		Table 4: Example questions and answers from ROPES, showing the relevant parts of the associated passage and
		the reasoning required to answer the question10
		Table 4: Comparison of the qualities (BLEU scores)
		with the baseline systems: NQG (Zhou et al
		14
		10
		Table 4: Statistics of the dataset test samples after pro-
		cessing by the Wang et al The purpose of these joint training meth-
		ods is to verify whether domain knowledge can be
		learned from the RC-QA dataset and whether it is
		100
		　　　
		Train
		Development
		Test
		Nominative
		11,359
		544
		565
		Accusative
		2,756
		199
		196
		Dative
		967
		50
		52
		Table 4: Split of the PAS-QA dataset58
		Table 4: Common Types of Errors on SQuAD (top)
		and Select SQuAD Regression Features and Odds
		Ratios (bottom)
		6145
		Table 4: Mean reciprocal rank (MRR) and recall@K
		performance of neural baselines on ReQA SQuAD and
		ReQA NQ
		Dataset
		IC
		MWC
		Imp
		No-Ans
		SEARs
		NewsQA
		-
		-
		47
		47
		50
		QuoRef
		-
		-
		45
		48
		50
		DROP
		46
		42
		36
		48
		50
		SQuAD
		15/16
		-
		47
		48
		50
		ROPES
		48
		36
		-
		11
		50
		DuoRC
		18/22
		-
		47
		-
		50
		Table 4: Quality of augmented datasets (# of good ques-
		tions out of 50 sampled)
		tion from the original and has enough independent
		value to be incorporated into the evaluation server06
		Table 4: Reading comprehension evaluation results
		Model
		BLEU 1
		BLEU 2
		BLEU 3
		BLEU 4
		METEOR
		ROUGE-L
		NQG-RC
		4372%
		Table 4: Recall of interrogative words of the QG model305
		Table 4: Fraction of correct answers contained in the
		top {1 / 5} answer candidates, and MRR of the cor-
		rect answer in passages retrieved by the BERT-based
		retrieval method (BERT) or an IR method (BM25f)2
		Table 4: Results on the in-domain development set96
		Table 4: Results of models ﬁne-tuned with different data feeding methods on development datasets83
		Table 4: The experimental results on examining the effects of multi-task learning92
		Table 4: Breakdown of hidden development set results
		by dataset using our best XLNet Large model
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		summarizes our observations: (1) the datasets with
		naturally collected questions (either crowdsourced
		or curated by domain experts) all obtain large
		improvements; (2) The datasets collected from
		Wikipedia or education materials (textbooks and
		Science articles) receive bigger gains compared to
		those collected from Web snippets or transcrip-
		tions; and (3) There is a bigger improvement for
		datasets in which questions are posed dependent
		on the passages compared to those with indepen-
		dently collected questions (11
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the scores on the test dataset of
		QuAC dataset compared with some baseline mod-
		els
	</Extractive Summary>
	<Extractive Summary> =
		Question reasoning
		Table 4 shows the break-
		down and examples of the main types of questions
		by the types of reasoning required to answer them
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 presents the comparison with the base-
		line systems
	</Extractive Summary>
	<Extractive Summary> =
		3
		Sentence Compression
		Sentence extraction often produces results that
		are much longer than those in the reference
		summaries—the training data (Table 4) suggests
		that 20 words is a good upper limit for the length
		of the summaries
	</Extractive Summary>
	<Extractive Summary> =
		1
		Datasets
		We processed the CNN/DM2 and Debatepedia3
		datasets using the respective ofﬁcial Python scripts
		to yield the corpora with passages, queries and
		summaries tailored to the queries (Table 4)
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the error distribution for all mod-
		els
	</Extractive Summary>
	<Extractive Summary> =
		993
		-
		Context Len
		-
		-
		-
		-
		Question Type
		-
		-
		-
		-
		Table 5: Common Types of Errors on HotpotQA (top)
		and Select HotpotQA Regression Features and Odds
		Ratios (bottom, - denotes insigniﬁcant results)
		sults in Table 4 are all signiﬁcant)
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 presents the ReQA results for our base-
		line models
	</Extractive Summary>
	<Extractive Summary> =
		To obtain our baseline using traditional IR
		methods, we constructed a paragraph-level re-
		trieval task which allows a direct comparison be-
		tween the neural systems in Table 4 and BM25
	</Extractive Summary>
	<Extractive Summary> =
		Is the new answer the correct answer for the
		new question?
		Table 4 shows the number of high-quality ques-
		tions generated for each dataset
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results and Analysis
		Table 4 shows the human question and generated
		question experiment comparison results
	</Extractive Summary>
	<Extractive Summary> =
		As shown in the Table 4, the total recall of using
		only the QG module is 68
	</Extractive Summary>
	<Extractive Summary> =
		As we can see, the overall
		recall is high, and it is also higher than just using
		the QG module (Table 4), which proves our hy-
		pothesis that modeling the interrogative-word pre-
		diction task as an independent classiﬁcation prob-
		lem yields to a higher recall than generating them
		with the full question
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 summarizes the results on the
		in-domain development set
	</Extractive Summary>
	<Extractive Summary> =
		5
		Results
		Comparison between Data Feeding Methods
		Table 4 shows the performance of the XLNet mod-
		els ﬁne-tuned with the two data feeding methods
		mentioned in §4
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 shows the
		experimental results:
		(1) From the ﬁrst two rows in Table 4, we can
		observe that the auxiliary task of masked language
		model can improve the performance on both in-
		domain and out-of-domain development set, es-
		pecially on the out-of-domain set
	</Extractive Summary>
	<Extractive Summary> =
		(2) From the last two rows in Table 4, we do
		not observe that the auxiliary tasks of natural lan-
		guage inference and paragraph ranking bring fur-
		ther beneﬁts in terms of generalization
	</Extractive Summary>
	<Extractive Summary> =
		The results of the
		leaderboard Out-Domain Development set and ﬁ-
		nal test set results are shown in Table 4 and Ta-
		ble 5 respectively
	</Extractive Summary>
</Paper ID=ument965>


<Paper ID=ument965> <Table ID =5>
	<Abstractive Summary> =
		1
		Table 5: Pretrained language models used in the shared
		task submissions7
		Table 5: Accuracy (%) on the test sets of ARC-Easy, ARC-Challenge, and OpenBookQA datasets9
		Table 5: Model performance on the validation dataset
		of QuAC dataset0
		Table 5: Performance of baselines and human perfor-
		mance on the dev and test set17
		Table 5: Accuracy of question focus estimation80
		Table 5:
		ROUGE (%) performances of our model and competing models on the Debatepedia dataset51
		Table 5: PAS-QA test results of MC models and NN-
		PAS models993
		-
		Context Len
		-
		-
		-
		-
		Question Type
		-
		-
		-
		-
		Table 5: Common Types of Errors on HotpotQA (top)
		and Select HotpotQA Regression Features and Odds
		Ratios (bottom, - denotes insigniﬁcant results)
		sults in Table 4 are all signiﬁcant)6
		Table 5: Time and space tradeoffs of different models53
		Table 5: Performance on baseline BERT model on dif-
		ferent datasets
		tion server68
		Table 5: Comparison between our model and the published methods using sentence level context
		Model
		BLEU 1
		BLEU 2
		BLEU 3
		BLEU 4
		METEOR
		ROUGE-L
		NQG-RC
		42 Lastly, we also prove that
		169
		id
		Only QG*
		IWAQG
		Upper Bound
		Golden
		Answer
		1
		what
		produces
		a list of require-
		ments
		for
		a
		project?
		who produces a
		list
		of
		require-
		ments
		for
		a
		project?
		who produces a
		list
		of
		require-
		ments
		for
		a
		project?
		who produces a
		list
		of
		require-
		ments
		for
		a
		project,
		giving
		an overall view
		of the project’s
		goals?
		The owner
		2
		how many tun-
		nels
		were
		con-
		structed through
		newcastle
		city
		centre?
		what type of tun-
		nels constructed
		through newcas-
		tle city centre?
		what type of tun-
		nels constructed
		through newcas-
		tle city centre ?
		what
		type
		of
		tunnels are con-
		structed through
		newcastle ’s city
		center?
		deep-level
		tunnels
		3
		who received a
		battering
		during
		the siege of new-
		castle?
		what received a
		battering
		during
		the siege of new-
		castle ?
		what received a
		battering
		during
		the siege of new-
		castle ?
		what received a
		battering
		during
		the siege of new-
		castle?
		The church
		tower
		4
		what system is
		newcastle
		inter-
		national
		airport
		connected to?
		what system is
		newcastle
		inter-
		national
		airport
		connected to?
		how is newcastle
		international air-
		port connected to
		?
		how is newport ’s
		airport connected
		to the city?
		via
		the
		Metro Light
		Rail system
		5
		who
		was
		the
		country
		most
		dependent
		on
		arab oil?
		what
		country
		was
		the
		most
		dependent
		on
		arab oil?
		which
		country
		was
		the
		most
		dependent
		on
		arab oil?
		which country is
		the most depen-
		dent on arab oil?
		Japan
		Table 5: Qualitative Analysis50
		Table 5: Summary-level answer extraction results by previous models and our systems trained on out-of-domain
		SQUAD data (BERT SQUAD *), and our heuristic data set (BERT heur)7
		Table 5: Results on the out-of-domain development set42
		Table 5: Results of multi-task models that are ﬁne-tuned with the methods described in §460
		Table 5: The performance of the submitted system on
		two subsets that contain different document sources50
		Table 5: Macro-Average EM and F1 on the held-out
		leaderboard test sets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 summarizes the pre-
		trained models each submission is based on, along
		with its evaluation F1 score
	</Extractive Summary>
	<Extractive Summary> =
		, RC + EC and
		IRC + IEC in Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		5
		Impact of External Knowledge from an
		Open-Domain Resource
		Table 6 shows some examples of errors produced
		by IRC (Table 5) that do not leverage external
		knowledge from open-domain resources
	</Extractive Summary>
	<Extractive Summary> =
		These
		errors can be corrected by enriching the reference
		corpus with external sentences extracted from
		Wikipedia (IRC + IEC in Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		We compare the BERTII baseline in Table 5 that
		only uses the original reference corpus of a given
		end task with our best model
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the base-
		line based on BERT already outperforms previous
		state-of-the-art methods designed for subject-area
		QA tasks (Yadav et al
	</Extractive Summary>
	<Extractive Summary> =
		At the same time, some ablation studies have
		developed on the validation dataset (Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 presents the results for the baselines,
		Development
		Test
		EM
		F1
		EM
		F1
		RoBERTa BASE
		38
	</Extractive Summary>
	<Extractive Summary> =
		2
		Accuracy of Question Focus Estimation
		Table 5 measures the accuracy of query focus esti-
		mation with varying Nc
	</Extractive Summary>
	<Extractive Summary> =
		2
		Results and Discussion
		Table 5 lists evalution results of the NN-PAS mod-
		els and the MC-single/merged/stepwise models
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the most frequent errors in
		HotpotQA involve distractor sentences and mul-
		tihop inference
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 illustrates the tradeoff between model
		accuracy and resource usage
	</Extractive Summary>
	<Extractive Summary> =
		2
		Existing Dataset Performance
		Table 5 shows the result of evaluating on all of
		the development and test sets using our evalua-
		150
		Dataset
		Dev
		Test
		EM
		F1
		EM
		F1
		NewsQA
		29
	</Extractive Summary>
	<Extractive Summary> =
		5
		Quantitative Results
		Table 5 shows the comparison results using
		sentence-level context texts and Table 6 shows the
		results on paragraph-level context
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 5, BERT-HLSQG outperforms the
		existing best performing model by 4-5% on both
		benchmark datasets
	</Extractive Summary>
	<Extractive Summary> =
		As we can see in Table 5, in the ﬁrst three ex-
		amples the interrogative words generated by the
		baseline are wrong, while our model is right
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 summarizes the evalua-
		tion results for out-of-domain
	</Extractive Summary>
	<Extractive Summary> =
		Please refer to Table 5 for the detailed
		partition
	</Extractive Summary>
	<Extractive Summary> =
		However,
		we observe from Table 5 that the system performs
		similarly on two subsets
	</Extractive Summary>
</Paper ID=ument965>


<Paper ID=ument965> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6: Performance on the six datasets of Split II (test portion) ✓
		Table 6: Examples of corrected errors using the reference corpus enriched by the sentences from Wikipedia62
		Table 6: Accuracy of question pattern classiﬁcation06
		Table 6: ROUGE (%) scores of our models and the competing model on the CNN/Daily Mail dataset
		17%
		11%
		11%
		4%
		Yes/No Choice
		8%
		11%
		4%
		0%
		Table 6: Common Types of Errors on MSMARCO
		that long answers are more likely to be faulty or
		badly chosen175
		Table 6: Performance of various models on paragraph-
		level retrieval85
		Template Type
		Answered Incorrectly
		Answered Correctly
		Invert Choice
		Original: Which art gallery was founded
		ﬁrst, Art Euphoric or Trescott Street?
		Generated:
		Which art gallery was
		founded ﬁrst, Trescott Street or Art Eu-
		phoric?
		Original:
		Who scored more ﬁeld
		goals, Nate Kaeding or Dan Carpenter?
		Generated: Who scored more ﬁeld goals,
		Dan Carpenter or Nate Kaeding?
		Implication
		Original:
		When
		did
		the
		Huguenots
		secure
		the
		right
		to
		own
		land
		in
		the
		Baronies?
		Generated:
		Who secured the right
		to own land in baronies in 1697?
		Original:
		When
		did
		Henry
		issue
		the
		Edict
		of
		Nantes?
		Generated:
		What did Henry issue
		in 1598?
		SEARs
		Original:
		What
		was
		the
		theme
		of
		Super
		Bowl
		50?
		Generated: So what was the theme of
		Super Bowl 50?
		Original: Who won Super Bowl 50?
		Generated: So who won Super Bowl
		50?
		Table 6: Quantitative and qualitative analysis of generated augmentations68
		Table 6: Comparison between our model and the published methods using paragraph level context
		mans, and the use of reading comprehension tasks
		also has effective8%
		Table 6: Ablation Study of our interrogative-word clas-
		siﬁer15
		Table 6: Book-level answer extraction results by previous models and our systems trained on out-of-domain
		SQUAD data (BERT SQUAD *), and our heuristic data set (BERT heur)1
		Table 6: Results of submission run76
		Table 6: Results on test datasets53
		Table 6: The performance of the submitted system
		on three subsets that require different language un-
		derstanding ability
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Impact of External Knowledge from an
		Open-Domain Resource
		Table 6 shows some examples of errors produced
		by IRC (Table 5) that do not leverage external
		knowledge from open-domain resources
	</Extractive Summary>
	<Extractive Summary> =
		3
		Accuracy of Question Pattern
		Identiﬁcation
		On the other hand, Table 6 and Table 7 show em-
		barrassingly unsatisfactory results of question pat-
		tern identiﬁcation
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 shows the distribution of common errors
		on MSMARCO
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 6, the
		USE-QA neural baseline outperforms BM25 on
		paragraph retrieval
	</Extractive Summary>
	<Extractive Summary> =
		3
		Synthetic Augmentations
		Table 6 shows the performance of the baseline
		model on various development sets and heuris-
		tically generated questions
	</Extractive Summary>
	<Extractive Summary> =
		5
		Quantitative Results
		Table 5 shows the comparison results using
		sentence-level context texts and Table 6 shows the
		results on paragraph-level context
	</Extractive Summary>
	<Extractive Summary> =
		Second, the results in Table 6 further show that
		BERT-SQG successfully processes the paragraph-
		level contexts and further push the state-of-the-art
		from 16
	</Extractive Summary>
	<Extractive Summary> =
		5
		Ablation Study
		We tried to combine different features shown in
		Table 6 for the interrogative-word classiﬁer
	</Extractive Summary>
	<Extractive Summary> =
		Table 6 summarizes the submission run re-
		sults
	</Extractive Summary>
	<Extractive Summary> =
		Comparison with Baseline
		The results on the test sets shown in Table 6 in-
		dicate that the multi-task XLNet-large model ﬁne-
		tuned with a larger linear layer on the TPU con-
		sistently outperforms the BERT-large baseline by
		a huge margin
	</Extractive Summary>
	<Extractive Summary> =
		Please refer to Table 6 for the detailed par-
		tition
	</Extractive Summary>
	<Extractive Summary> =
		From Table 6, we ob-
		serve that the system performs much worse on the
		the subsets of Reasoning and Arithmetic
	</Extractive Summary>
</Paper ID=ument965>


<Paper ID=ument965> <Table ID =7>
	<Abstractive Summary> =
		7
		Table 7: Results on the six datasets of Split III3
		1,191,347
		Table 7: Percentage (%) of retrieved sentences from
		each source17
		Table 7: Accuracy (BLEU scores) of question pattern
		generation50
		Table 7: Common Types of Errors on SearchQA (top)
		and Select SearchQA Regression Features and Odds
		Ratios (bottom, - denotes insigniﬁcant results)
		are frequently evaluated on a single dataset, and
		even when multiple datasets are used, they tend
		to be similar0%
		Table 7: Recall and precision of interrogative words of
		our interrogative-word classiﬁer
	</Abstractive Summary>
	<Extractive Summary> =
		Appendix
		We present the per-dataset performances in Ta-
		ble 6 and Table 7 for shared task submissions and
		our baselines
	</Extractive Summary>
	<Extractive Summary> =
		We report the statistics of the sentences (without
		redundancy removal) extracted from each source
		in Table 7, used as inputs to our methods IRC +
		IEC and IRC + IEC + MD in Table 5
	</Extractive Summary>
	<Extractive Summary> =
		3
		Accuracy of Question Pattern
		Identiﬁcation
		On the other hand, Table 6 and Table 7 show em-
		barrassingly unsatisfactory results of question pat-
		tern identiﬁcation
	</Extractive Summary>
	<Extractive Summary> =
		From Table 7, we see that the Same Entity Type
		is the major error across all models
	</Extractive Summary>
	<Extractive Summary> =
		Based on the regressions done on model scores
		(Table 7), an interesting common trend is sug-
		gested across all models
	</Extractive Summary>
	<Extractive Summary> =
		In addition, we provide the recall and precision
		per class for our ﬁnal interrogative-word classiﬁer
		(CLS + AT in Table 7)
	</Extractive Summary>
</Paper ID=ument965>


<Paper ID=ument965> <Table ID =8>
	<Abstractive Summary> =
		7
		8
		Table 8: Accuracy (%) on the ARC-Easy test set
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 8,
		we also observe that compared to only ﬁne-tuning
		on ARC-Easy, ﬁne-turning on external in-domain
		data hurts the performance
	</Extractive Summary>
	<Extractive Summary> =
		Table 8 shows an example for each inference type
	</Extractive Summary>
</Paper ID=ument965>


<Paper ID=ument965> <Table ID =9>
	<Abstractive Summary> =
		0
		Table 9: Accuracy (%) by different categories on the
		annotated test sets of ARC-Easy and ARC-Challenge,
		which are released by Sugawara et al
	</Abstractive Summary>
</Paper ID=ument965>


<Paper ID=ument967> <Table ID =1>
	<Abstractive Summary> =
		3
		Dataset
		Question (Q)
		Context (C)
		|Q|
		|C|
		Q ⊥⊥ C
		Train
		Dev
		Test
		I
		SQuAD
		Crowdsourced
		Wikipedia
		11
		137
		
		86,588
		10,507
		-
		NewsQA
		Crowdsourced
		News articles
		8
		599
		
		74,160
		4,212
		-
		TriviaQA♠
		Trivia
		Web snippets
		16
		784
		
		61,688
		7,785
		-
		SearchQA♠
		Jeopardy
		Web snippets
		17
		749
		
		117,384
		16,980
		-
		HotpotQA
		Crowdsourced
		Wikipedia
		22
		232
		
		72,928
		5,904
		-
		Natural Questions
		Search logs
		Wikipedia
		9
		153
		
		104,071
		12,836
		-
		II
		BioASQ♠
		Domain experts
		Science articles
		11
		248
		
		-
		1,504
		1,518
		DROP♦
		Crowdsourced
		Wikipedia
		11
		243
		
		-
		1,503
		1,501
		DuoRC♦
		Crowdsourced
		Movie plots
		9
		681
		
		-
		1,501
		1,503
		RACE♥
		Domain experts
		Examinations
		12
		349
		
		-
		674
		1,502
		RelationExtraction♠
		Synthetic
		Wikipedia
		9
		30
		
		-
		2,948
		1,500
		TextbookQA♥
		Domain experts
		Textbook
		11
		657
		
		-
		1,503
		1,508
		III
		BioProcess♥
		Domain experts
		Textbook
		9
		94
		
		-
		-
		219
		ComplexWebQ♠
		Crowdsourced
		Web snippets
		14
		583
		
		-
		-
		1,500
		MCTest♥
		Crowdsourced
		Crowdsourced
		9
		244
		
		-
		-
		1,501
		QAMR♦
		Crowdsourced
		Wikipedia
		7
		25
		
		-
		-
		1,524
		QAST
		Domain experts
		Transcriptions
		10
		298
		
		-
		-
		220
		TREC♠
		Crowdsourced
		Wikipedia
		8
		792
		
		-
		-
		1,021
		Table 1: MRQA sub-domain datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Please see Table 1 as well as the
		associated dataset papers for more details on each
		sub-domain’s properties
	</Extractive Summary>
</Paper ID=ument967>


<Paper ID=ument967> <Table ID =2>
	<Abstractive Summary> =
		, 2019)
		Alexandria University
		Table 2: List of participants, ordered by the macro-averaged F1 score on the hidden evaluation set
	</Abstractive Summary>
	<Extractive Summary> =
		7 In total, we re-
		ceived submissions from 10 different teams for the
		ﬁnal evaluation (Table 2)
	</Extractive Summary>
</Paper ID=ument967>


<Paper ID=ument967> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Performance as F1 score on the shared task
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Main Results
		Table 3 lists the macro-averaged F1 scores of all
		the submissions on both the development and test-
		ing portions of the MRQA dataset
	</Extractive Summary>
	<Extractive Summary> =
		As seen in Table 3, many of the submis-
		sions outperform our BERT-Large baseline signif-
		icantly
	</Extractive Summary>
	<Extractive Summary> =
		We evaluate all the submissions on the in-
		domain datasets (Split I) in Table 3 and ﬁnd that
		there is a very strong correlation between in-
		domain and out-of-domain performance
	</Extractive Summary>
</Paper ID=ument967>


<Paper ID=ument967> <Table ID =4>
	<Abstractive Summary> =
		9
		Table 4:
		Macro-averaged F1 scores based on the
		dataset characteristics as deﬁned in Table 1
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4
		summarizes our observations: (1) the datasets with
		naturally collected questions (either crowdsourced
		or curated by domain experts) all obtain large
		improvements; (2) The datasets collected from
		Wikipedia or education materials (textbooks and
		Science articles) receive bigger gains compared to
		those collected from Web snippets or transcrip-
		tions; and (3) There is a bigger improvement for
		datasets in which questions are posed dependent
		on the passages compared to those with indepen-
		dently collected questions (11
	</Extractive Summary>
</Paper ID=ument967>


<Paper ID=ument967> <Table ID =5>
	<Abstractive Summary> =
		1
		Table 5: Pretrained language models used in the shared
		task submissions
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 summarizes the pre-
		trained models each submission is based on, along
		with its evaluation F1 score
	</Extractive Summary>
</Paper ID=ument967>


<Paper ID=ument967> <Table ID =6>
	<Abstractive Summary> =
		5
		Table 6: Performance on the six datasets of Split II (test portion)
	</Abstractive Summary>
</Paper ID=ument967>


<Paper ID=ument967> <Table ID =7>
	<Abstractive Summary> =
		7
		Table 7: Results on the six datasets of Split III
	</Abstractive Summary>
	<Extractive Summary> =
		Appendix
		We present the per-dataset performances in Ta-
		ble 6 and Table 7 for shared task submissions and
		our baselines
	</Extractive Summary>
</Paper ID=ument967>


<Paper ID=ument968> <Table ID =1>
	<Abstractive Summary> =
		2
		Table 1: Performance of different models on SQuAD
		development set
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, the uniﬁed model out-
		performs previous state-of-the-art models and the
		baseline model
	</Extractive Summary>
</Paper ID=ument968>


<Paper ID=ument969> <Table ID =1>
	<Abstractive Summary> =
		collection
		#docs
		#natural
		#generated
		questions
		questions
		V_antiq
		61
		274
		4672
		WP_arch
		96
		302
		36259
		CT_1GM
		16
		241
		7502
		WP_1GM
		123
		319
		50971
		total
		296
		1136
		99404
		Table 1: Description of CALOR-QUEST corpus
		4
		Evaluation
		The main objective of our work is to create in a
		semi-automatic fashion a training corpus for rea-
		ding comprehension model
	</Abstractive Summary>
</Paper ID=ument969>


<Paper ID=ument969> <Table ID =2>
	<Abstractive Summary> =
		0
		Table 2: Results obtained with BERT-SQUAD on
		CALOR-QUEST with two conditions : V1 correspond
		to SQUAD1
	</Abstractive Summary>
</Paper ID=ument969>


<Paper ID=ument969> <Table ID =3>
	<Abstractive Summary> =
		3
		Table 3: F1 results on questions associated to 10 se-
		mantic frames of CALOR-QUEST with a model trai-
		ned on the whole corpus (all) and one trained on a cor-
		pus where all the generated questions corresponding to
		these 10 frames have been removed (w/o)
		4
	</Abstractive Summary>
</Paper ID=ument969>


<Paper ID=ument969> <Table ID =4>
	<Abstractive Summary> =
		, 2018)
		Table 4: Results obtained on the French SQuAD test
		corpus with a model trained on CALOR-QUEST and
		the original BERT-SQUAD model for English with
		back-translation
		5
		Conclusion
		In this work, we have proposed a semi-
		automatic method to generate question/answer
		pairs from a corpus of documents annotated in
		semantic frames, with the purpose of building a
		large training corpus for machine reading com-
		prehension in French
	</Abstractive Summary>
</Paper ID=ument969>


<Paper ID=ument97> <Table ID =1>
	<Abstractive Summary> =
		Some works explore enhancing the slot ﬁlling task
		1052
		#
		Utterance
		Slot tag
		Intent
		1
		play Roy Orbison tunes now
		artist
		PlayMusic
		2
		add this Roy Orbison song onto Women of Comedy
		artist
		AddToPlaylist
		3
		book a spot for seven at a bar with chicken french
		served dish
		BookRestaurant
		4
		book french food for me and angeline at a restaurant
		cuisine
		BookRestaurant
		Table 1: Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words
	</Abstractive Summary>
</Paper ID=ument97>


<Paper ID=ument97> <Table ID =2>
	<Abstractive Summary> =
		65
		# Intents
		7
		18
		11
		# Slots
		72
		128
		75
		# Train Set
		13084
		4478
		7995
		# Validation Set
		700
		500
		994
		# Test Set
		700
		893
		1012
		Table 2: Dataset statistics
	</Abstractive Summary>
</Paper ID=ument97>


<Paper ID=ument97> <Table ID =3>
	<Abstractive Summary> =
		10
		Table 3: Results on test sets of the SNIPS and ATIS, where our CM-Net achieves state-of-the-art performances
		in most cases
	</Abstractive Summary>
</Paper ID=ument97>


<Paper ID=ument97> <Table ID =4>
	<Abstractive Summary> =
		57
		Table 4: Ablation experiments on the SNIPS to in-
		vestigate the impacts of various components, where
		“- slot memory” indicates removing the slot memory
		and its interactions with other components correspond-
		ingly
	</Abstractive Summary>
</Paper ID=ument97>


<Paper ID=ument97> <Table ID =5>
	<Abstractive Summary> =
		32
		Table 5: Results on the SNIPS benchmark with the as-
		sistance of pre-trained language model, where we es-
		tablish new state-of-the-art results on the SNIPS
	</Abstractive Summary>
	<Extractive Summary> =
		The results
		emerged in Table 5 show that we establish new
		state-of-the-art results on both tasks of the SNIPS
	</Extractive Summary>
</Paper ID=ument97>


<Paper ID=ument97> <Table ID =6>
	<Abstractive Summary> =
		56
		Table 6: Results on our CAIS dataset, where “†” indi-
		cates our implementation of the S-LSTM
	</Abstractive Summary>
	<Extractive Summary> =
		The
		results listed in Table 6 demonstrate the generaliz-
		ability and effectiveness of our CM-Net when han-
		dling various domains and different languages
	</Extractive Summary>
</Paper ID=ument97>


<Paper ID=ument970> <Table ID =1>
	<Abstractive Summary> =
		Table 1: A sample problem from a multiple-choice QA
		task OpenBookQA (Mihaylov et al
	</Abstractive Summary>
	<Extractive Summary> =
		To correctly answer the question in Table 1, for
		example, scientiﬁc facts1 from the provided refer-
		ence corpus — {“a magnet attracts magnetic met-
		als through magnetism” and “iron is always mag-
		netic”}, as well as general world knowledge ex-
		tracted from an external source such as {“a belt
		buckle is often made of iron” and “iron is metal”}
		are required
	</Extractive Summary>
</Paper ID=ument970>


<Paper ID=ument970> <Table ID =2>
	<Abstractive Summary> =
		✓
		Table 2: A sample problem from the ARC-Challenge
		dataset (Clark et al
	</Abstractive Summary>
	<Extractive Summary> =
		For
		example, in the sample problem in Table 2, we ex-
		tract concept mentions such as “Mercury”
	</Extractive Summary>
	<Extractive Summary> =
		For example, the ambiguous concept
		mention “Mercury” in Table 2 should be linked
		to the concept Mercury (planet) rather than
		Mercury (element) in Wikipedia
	</Extractive Summary>
</Paper ID=ument970>


<Paper ID=ument970> <Table ID =3>
	<Abstractive Summary> =
		When the
		input sequence length exceeds 512, we truncate
		the longest sequence among q, oi, and di (deﬁned
		Dataset
		Train
		Dev
		Test
		Total
		RACE
		87,866
		4,887
		4,934
		97,687
		ARC-Easy
		2,251
		570
		2,376
		5,197
		ARC-Challenge
		1,119
		299
		1,172
		2,590
		OpenBookQA
		4,957
		500
		500
		5,957
		Table 3: The number of questions in RACE and the
		multiple-choice subject-area QA datasets for evalua-
		tion: ARC-Easy, ARC-Challenge, and OpenBookQA
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Utilization of External Knowledge from
		In-Domain Data
		Since there are a relatively small number of train-
		ing instances available for a single subject-area
		QA task (see Table 3), instead of ﬁne-tuning a
		pre-ﬁne-tuned model on a single target dataset, we
		also investigate into ﬁne-tuning a pre-ﬁne-tuned
		model on multiple in-domain datasets simulta-
		neously
	</Extractive Summary>
</Paper ID=ument970>


<Paper ID=ument970> <Table ID =4>
	<Abstractive Summary> =
		0
		Table 4: Accuracy (%) of the pre-ﬁne-tuned model on
		the RACE dataset, which contains two subsets: RACE-
		M and RACE-H, representing problems collected from
		middle and high school language exams, respectively
	</Abstractive Summary>
</Paper ID=ument970>


<Paper ID=ument970> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: Accuracy (%) on the test sets of ARC-Easy, ARC-Challenge, and OpenBookQA datasets
	</Abstractive Summary>
	<Extractive Summary> =
		, RC + EC and
		IRC + IEC in Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		5
		Impact of External Knowledge from an
		Open-Domain Resource
		Table 6 shows some examples of errors produced
		by IRC (Table 5) that do not leverage external
		knowledge from open-domain resources
	</Extractive Summary>
	<Extractive Summary> =
		These
		errors can be corrected by enriching the reference
		corpus with external sentences extracted from
		Wikipedia (IRC + IEC in Table 5)
	</Extractive Summary>
	<Extractive Summary> =
		We compare the BERTII baseline in Table 5 that
		only uses the original reference corpus of a given
		end task with our best model
	</Extractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the base-
		line based on BERT already outperforms previous
		state-of-the-art methods designed for subject-area
		QA tasks (Yadav et al
	</Extractive Summary>
</Paper ID=ument970>


<Paper ID=ument970> <Table ID =6>
	<Abstractive Summary> =
		✓
		Table 6: Examples of corrected errors using the reference corpus enriched by the sentences from Wikipedia
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Impact of External Knowledge from an
		Open-Domain Resource
		Table 6 shows some examples of errors produced
		by IRC (Table 5) that do not leverage external
		knowledge from open-domain resources
	</Extractive Summary>
</Paper ID=ument970>


<Paper ID=ument970> <Table ID =7>
	<Abstractive Summary> =
		3
		1,191,347
		Table 7: Percentage (%) of retrieved sentences from
		each source
	</Abstractive Summary>
	<Extractive Summary> =
		We report the statistics of the sentences (without
		redundancy removal) extracted from each source
		in Table 7, used as inputs to our methods IRC +
		IEC and IRC + IEC + MD in Table 5
	</Extractive Summary>
</Paper ID=ument970>


<Paper ID=ument970> <Table ID =8>
	<Abstractive Summary> =
		7
		8
		Table 8: Accuracy (%) on the ARC-Easy test set
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 8,
		we also observe that compared to only ﬁne-tuning
		on ARC-Easy, ﬁne-turning on external in-domain
		data hurts the performance
	</Extractive Summary>
</Paper ID=ument970>


<Paper ID=ument970> <Table ID =9>
	<Abstractive Summary> =
		0
		Table 9: Accuracy (%) by different categories on the
		annotated test sets of ARC-Easy and ARC-Challenge,
		which are released by Sugawara et al
	</Abstractive Summary>
</Paper ID=ument970>


<Paper ID=ument971> <Table ID =1>
	<Abstractive Summary> =
		Q1: Did they release any albums
		A1: Skid Row, released in January 1989
		Q2: How did it do
		A2: instant success
		Q2’: How did Skid Row do
		Q3: Did it go on tour
		A3: ﬁrst supporting Bon Jovi’s outdoor show
		Q3’: Did Skid Row go on tour
		Q4: Did the Tour have a name
		A4: New Jersey tour
		Q4’: Did the outdoor show have a name
		Q5: How long did the tour last
		A5: CANNOTANSWER
		Q5’: How long did the New Jersey tour last
		Table 1: An example of conversational machine comprehension from QuAC dataset (Choi et al
	</Abstractive Summary>
	<Extractive Summary> =
		meaningful by considering the previous questions
		and answers history (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 gives an exam-
		ple of conversational machine comprehension in
		QuAC dataset
	</Extractive Summary>
</Paper ID=ument971>


<Paper ID=ument971> <Table ID =2>
	<Abstractive Summary> =
		Type
		dataPretrain
		QuAC
		train
		val
		test
		train
		val
		test
		questions
		20k
		5k
		3k
		81k
		7k
		7k
		dialogs
		3k
		600
		400
		11k
		1k
		1k
		Table 2: data statistics
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 de-
		scribes the data statistics
	</Extractive Summary>
</Paper ID=ument971>


<Paper ID=ument971> <Table ID =3>
	<Abstractive Summary> =
		75
		Table 3: BLEU-1,2,3,4, EM, ROUGE L and F1 scores on the test dataset in the dataPretrain
	</Abstractive Summary>
</Paper ID=ument971>


<Paper ID=ument971> <Table ID =4>
	<Abstractive Summary> =
		8
		100
		100
		Table 4: F1, HEQ-Q and HEQ-D scores on the test
		dataset of QuAC dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the scores on the test dataset of
		QuAC dataset compared with some baseline mod-
		els
	</Extractive Summary>
</Paper ID=ument971>


<Paper ID=ument971> <Table ID =5>
	<Abstractive Summary> =
		9
		Table 5: Model performance on the validation dataset
		of QuAC dataset
	</Abstractive Summary>
	<Extractive Summary> =
		At the same time, some ablation studies have
		developed on the validation dataset (Table 5)
	</Extractive Summary>
</Paper ID=ument971>


<Paper ID=ument972> <Table ID =1>
	<Abstractive Summary> =
		6
		Table 1: Answer passage prediction performance, measured
		by Hits@10 on dev bridge questions
	</Abstractive Summary>
</Paper ID=ument972>


<Paper ID=ument972> <Table ID =2>
	<Abstractive Summary> =
		26
		Table 2: QA performance on HotpotQA
	</Abstractive Summary>
	<Extractive Summary> =
		Question Answering Results
		Table 2 shows the
		ﬁnal multi-hop QA performance
	</Extractive Summary>
</Paper ID=ument972>


<Paper ID=ument972> <Table ID =3>
	<Abstractive Summary> =
		67
		Table 3: QA performance ablation on the development set
	</Abstractive Summary>
	<Extractive Summary> =
		Our experiments show that this sim-
		ple approach can tremendously increase the an-
		1As shown in Table 3 of Chen et al
	</Extractive Summary>
	<Extractive Summary> =
		Ablation Study
		Table 3 gives ablation results on
		the dev set, where both entity linking and the aux-
		iliary objective slightly improve the performance
	</Extractive Summary>
</Paper ID=ument972>


<Paper ID=ument973> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Results on the MRC datasets
		Accuracy
		P@1
		Random Guess
		75
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results of the MRC Task
		Table 1 details the performances of models on
		MRC datasets
	</Extractive Summary>
</Paper ID=ument973>


<Paper ID=ument973> <Table ID =2>
	<Abstractive Summary> =
		7%
		Table 2: Results on Sentence Matching
		implementation (Chollet et al
	</Abstractive Summary>
</Paper ID=ument973>


<Paper ID=ument974> <Table ID =1>
	<Abstractive Summary> =
		4
		background vocabulary size
		8616
		2008
		3988
		situation vocabulary size
		6949
		1077
		2736
		question vocabulary size
		1457
		1411
		1885
		Table 1: Key statistics of ROPES
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the
		key statistics of the dataset
	</Extractive Summary>
	<Extractive Summary> =
		the fact that pairs
		of questions like in Table 1 will have opposite an-
		swers
	</Extractive Summary>
</Paper ID=ument974>


<Paper ID=ument974> <Table ID =2>
	<Abstractive Summary> =
		Table 2: Types of relations in the background passages
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the breakdown
		of the kinds of relations in the dataset
	</Extractive Summary>
</Paper ID=ument974>


<Paper ID=ument974> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Types of grounding found in ROPES
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows examples and
		breakdown of these three phenomena
	</Extractive Summary>
</Paper ID=ument974>


<Paper ID=ument974> <Table ID =4>
	<Abstractive Summary> =
		Which
		experiment
		would be less appro-
		priate for case C, case
		A or case B?
		case A
		Table 4: Example questions and answers from ROPES, showing the relevant parts of the associated passage and
		the reasoning required to answer the question
	</Abstractive Summary>
	<Extractive Summary> =
		Question reasoning
		Table 4 shows the break-
		down and examples of the main types of questions
		by the types of reasoning required to answer them
	</Extractive Summary>
</Paper ID=ument974>


<Paper ID=ument974> <Table ID =5>
	<Abstractive Summary> =
		0
		Table 5: Performance of baselines and human perfor-
		mance on the dev and test set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 presents the results for the baselines,
		Development
		Test
		EM
		F1
		EM
		F1
		RoBERTa BASE
		38
	</Extractive Summary>
</Paper ID=ument974>


<Paper ID=ument975> <Table ID =1>
	<Abstractive Summary> =
		67
		Table 1: Question patterns (n = {1, 2, 3}, N = 200)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 displays some question patterns and their
		frequencies
	</Extractive Summary>
</Paper ID=ument975>


<Paper ID=ument975> <Table ID =2>
	<Abstractive Summary> =
		21
		Table 2: Qualities (BLEU scores) of generated ques-
		tions (without considering question patterns)
	</Abstractive Summary>
	<Extractive Summary> =
		1
		Quality of the Generated Questions
		The results shown in Table 2 establish our primary
		assumption, which states that a question coherent
		to the current conversational context can be gener-
		ated primarily by knowing the current focus of in-
		terrogation
	</Extractive Summary>
</Paper ID=ument975>


<Paper ID=ument975> <Table ID =3>
	<Abstractive Summary> =
		25
		Table 3: Qualities (BLEU scores) of generated ques-
		tions
	</Abstractive Summary>
	<Extractive Summary> =
		Given these discussions, Table 3 displays the
		qualities of generated questions under several con-
		ditions, and it conﬁrms the above mentioned
		prospect may be probable
	</Extractive Summary>
</Paper ID=ument975>


<Paper ID=ument975> <Table ID =4>
	<Abstractive Summary> =
		10
		Table 4: Comparison of the qualities (BLEU scores)
		with the baseline systems: NQG (Zhou et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents the comparison with the base-
		line systems
	</Extractive Summary>
</Paper ID=ument975>


<Paper ID=ument975> <Table ID =5>
	<Abstractive Summary> =
		17
		Table 5: Accuracy of question focus estimation
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Accuracy of Question Focus Estimation
		Table 5 measures the accuracy of query focus esti-
		mation with varying Nc
	</Extractive Summary>
</Paper ID=ument975>


<Paper ID=ument975> <Table ID =6>
	<Abstractive Summary> =
		62
		Table 6: Accuracy of question pattern classiﬁcation
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Accuracy of Question Pattern
		Identiﬁcation
		On the other hand, Table 6 and Table 7 show em-
		barrassingly unsatisfactory results of question pat-
		tern identiﬁcation
	</Extractive Summary>
</Paper ID=ument975>


<Paper ID=ument975> <Table ID =7>
	<Abstractive Summary> =
		17
		Table 7: Accuracy (BLEU scores) of question pattern
		generation
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Accuracy of Question Pattern
		Identiﬁcation
		On the other hand, Table 6 and Table 7 show em-
		barrassingly unsatisfactory results of question pat-
		tern identiﬁcation
	</Extractive Summary>
</Paper ID=ument975>


<Paper ID=ument976> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example/comparison of our abstractive sum-
		mary on a Debatepedia sample with the output of the
		diversity driven attention model of Nema et al
	</Abstractive Summary>
</Paper ID=ument976>


<Paper ID=ument976> <Table ID =2>
	<Abstractive Summary> =
		Table 2:
		Example of our extractive summary on an
		example from the query-based version of CNN/Daily
		Mail (Hermann et al
	</Abstractive Summary>
</Paper ID=ument976>


<Paper ID=ument976> <Table ID =3>
	<Abstractive Summary> =
		Table 3:
		Examples of some of our paraphrased sen-
		tences using an MT system
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		examples of paraphrased sentences using back-
		translation
	</Extractive Summary>
</Paper ID=ument976>


<Paper ID=ument976> <Table ID =4>
	<Abstractive Summary> =
		14
		10
		Table 4: Statistics of the dataset test samples after pro-
		cessing by the Wang et al
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Sentence Compression
		Sentence extraction often produces results that
		are much longer than those in the reference
		summaries—the training data (Table 4) suggests
		that 20 words is a good upper limit for the length
		of the summaries
	</Extractive Summary>
	<Extractive Summary> =
		1
		Datasets
		We processed the CNN/DM2 and Debatepedia3
		datasets using the respective ofﬁcial Python scripts
		to yield the corpora with passages, queries and
		summaries tailored to the queries (Table 4)
	</Extractive Summary>
</Paper ID=ument976>


<Paper ID=ument976> <Table ID =5>
	<Abstractive Summary> =
		80
		Table 5:
		ROUGE (%) performances of our model and competing models on the Debatepedia dataset
	</Abstractive Summary>
</Paper ID=ument976>


<Paper ID=ument976> <Table ID =6>
	<Abstractive Summary> =
		06
		Table 6: ROUGE (%) scores of our models and the competing model on the CNN/Daily Mail dataset
	</Abstractive Summary>
</Paper ID=ument976>


<Paper ID=ument977> <Table ID =1>
	<Abstractive Summary> =
		The full-text task has only been addressed
		79
		Who is Emily in love with?
		Who is Emily imprisoned by?
		Who helps Emily escape from the castle?
		Who owns the castle in which Emily is imprisoned?
		Who became Emily’s guardian after her father’s death?
		Table 1: Who questions from NarrativeQA for the book
		The Mysteries of Udolpho, by Ann Radcliffe
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 exempliﬁes the diversity and com-
		plexity of Who questions in the data, by listing a
		set of questions from a single book, which require
		increasingly complex types of reasoning
	</Extractive Summary>
</Paper ID=ument977>


<Paper ID=ument977> <Table ID =2>
	<Abstractive Summary> =
		006
		Table 2: Precision scores (P@1, P@5), and Mean Reciprocal Rank (MRR) for frequency-based baselines and our
		system, with and without pretraining
	</Abstractive Summary>
</Paper ID=ument977>


<Paper ID=ument977> <Table ID =3>
	<Abstractive Summary> =
		7%
		full evidence found in context
		BM25F
		27%
		partial evidence found in context
		47%
		no evidence found in context
		26%
		Table 3: Percentage of contexts where the correct
		character is mentioned (top)
	</Abstractive Summary>
</Paper ID=ument977>


<Paper ID=ument978> <Table ID =1>
	<Abstractive Summary> =
		1
		100
		100
		Table 1: Conversational QA results on CoQA and QuAC, where (N-ctx) refers to using previous N QA pairs (%)
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		Table 1 reports model performance on CoQA and
		QuAC
	</Extractive Summary>
</Paper ID=ument978>


<Paper ID=ument978> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: The ablation study of BERT-FlowDelta (%)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the ablation study of BERT-
		FlowDelta, where two proposed modules are
		both important for achieving such results
	</Extractive Summary>
</Paper ID=ument978>


<Paper ID=ument978> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Dialogue accuracy for SCONE test (%)
	</Abstractive Summary>
</Paper ID=ument978>


<Paper ID=ument979> <Table ID =1>
	<Abstractive Summary> =
		15
		Table 1: Baseline results for HotpotReader and BERT
		We show our proposed improvements in Table
		2 and 3
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results
		In Table 1, the original HotpotReader method does
		not show signiﬁcant performance improvement
		when comparing the Single-Oracle setting with
		the Ordered-Oracle setting
	</Extractive Summary>
</Paper ID=ument979>


<Paper ID=ument979> <Table ID =2>
	<Abstractive Summary> =
		26
		Table 2: Results for HotpotReader on 3 oracle settings
		BERT achieved promising results even in the
		Single-Oracle setting which proves its original ca-
		pacity for QA
	</Abstractive Summary>
</Paper ID=ument979>


<Paper ID=ument979> <Table ID =3>
	<Abstractive Summary> =
		77
		Table 3: Results for BERT on 3 oracle settings
		Among both approaches, co-matching shows
		promising performance improvement especially
		for the well pre-trained BERT model
	</Abstractive Summary>
</Paper ID=ument979>


<Paper ID=ument98> <Table ID =1>
	<Abstractive Summary> =
		8
		Table 1: The F1 scores on WSJ-test
	</Abstractive Summary>
</Paper ID=ument98>


<Paper ID=ument98> <Table ID =2>
	<Abstractive Summary> =
		6
		Table 2: The F1 scores on WSJ-10
	</Abstractive Summary>
</Paper ID=ument98>


<Paper ID=ument98> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3: Recall of constituents by labels
	</Abstractive Summary>
</Paper ID=ument98>


<Paper ID=ument98> <Table ID =4>
	<Abstractive Summary> =
		6
		Table 4: The perplexity of masked words
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 4, the models are trained on WSJ-train
		with BERT masked LM and evaluated on WSJ-
		test
	</Extractive Summary>
</Paper ID=ument98>


<Paper ID=ument980> <Table ID =1>
	<Abstractive Summary> =
		php?KNP
		100
		Case
		Question
		Nominative
		［述語］の主語は何か？
		(What is the subject of [predicate]?)
		Accusative
		〇〇を［述語］、の〇〇に入るものは何か？
		(What is the accusative of [predicate]? )
		Dative
		〇〇に［述語］、の〇〇に入るものは何か？
		(What is the dative of [predicate]? )
		Table 1: Question templates of PAS-QA datasets
	</Abstractive Summary>
</Paper ID=ument980>


<Paper ID=ument980> <Table ID =2>
	<Abstractive Summary> =
		5%)
		－
		Table 2: Classiﬁcation of questions in the RC-QA
		dataset
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 2, the RC-QA
		dataset contains nearly 40% of questions asking
		arguments of nominative, accusative and dative,
		and a few questions asking for omitted arguments,
		which are similar to the PAS-QA dataset
	</Extractive Summary>
</Paper ID=ument980>


<Paper ID=ument980> <Table ID =3>
	<Abstractive Summary> =
		Training method
		Dataset
		MC-single
		PAS-QA
		Joint
		training
		MC-merged
		PAS-QA + RC-QA
		MC-stepwise
		RC-QA → PAS-QA
		Table 3: Three training methods for PAS analysis
	</Abstractive Summary>
</Paper ID=ument980>


<Paper ID=ument980> <Table ID =4>
	<Abstractive Summary> =
		The purpose of these joint training meth-
		ods is to verify whether domain knowledge can be
		learned from the RC-QA dataset and whether it is
		101
		　　　
		Train
		Development
		Test
		Nominative
		11,359
		544
		565
		Accusative
		2,756
		199
		196
		Dative
		967
		50
		52
		Table 4: Split of the PAS-QA dataset
	</Abstractive Summary>
</Paper ID=ument980>


<Paper ID=ument980> <Table ID =5>
	<Abstractive Summary> =
		51
		Table 5: PAS-QA test results of MC models and NN-
		PAS models
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results and Discussion
		Table 5 lists evalution results of the NN-PAS mod-
		els and the MC-single/merged/stepwise models
	</Extractive Summary>
</Paper ID=ument980>


<Paper ID=ument982> <Table ID =1>
	<Abstractive Summary> =
		654
		Table 1: Retrieval performance of models on the HOT-
		POTQA benchmark
	</Abstractive Summary>
	<Extractive Summary> =
		8
		Table 1 reports the accuracy(@k) of retrieving
		7https://lucene
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows
		that, indeed modeling the chain of documents is
		important
	</Extractive Summary>
</Paper ID=ument982>


<Paper ID=ument982> <Table ID =2>
	<Abstractive Summary> =
		26
		Table 2: Performance on QA task on hidden test set of
		HOTPOTQA after adding the retrieved paragraphs
		containing the answer spans then it might be a lit-
		tle easier for a downstream QA model to identify
		the answer span
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Performance on HOTPOTQA
		Table 2 shows the performance on the QA task
	</Extractive Summary>
</Paper ID=ument982>


<Paper ID=ument982> <Table ID =3>
	<Abstractive Summary> =
		41
		Table 3: Zero-shot (zs) IR results on WIKIHOP
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows
		our model outperforms both models in zero-shot
		setting
	</Extractive Summary>
</Paper ID=ument982>


<Paper ID=ument983> <Table ID =1>
	<Abstractive Summary> =
		Which test would
		see reactions taking
		place slower, test A
		or test B?
		test A
		Table 1: Examples for the datasets we use in our study
	</Abstractive Summary>
	<Extractive Summary> =
		A
		unique characteristic of ROPES is that questions
		generally present two possible answer choices, one
		of which is incorrect (Table 1)
	</Extractive Summary>
</Paper ID=ument983>


<Paper ID=ument983> <Table ID =2>
	<Abstractive Summary> =
		540
		Table 2: Human Judgments and Metrics: Correlation between metrics and human judgments using Spearman’s
		rho (ρ) and Kendall’s tau (τ) rank correlation coefﬁcients
	</Abstractive Summary>
</Paper ID=ument983>


<Paper ID=ument983> <Table ID =3>
	<Abstractive Summary> =
		992
		Table 3: Inter-annotator agreement computed using
		Cohen’s kappa (κ), Pearson correlation (r), and Spear-
		man’s correlation (ρ)
	</Abstractive Summary>
	<Extractive Summary> =
		We ﬁnd strong agreement be-
		tween the two annotators across the three datasets
		(see Table 3)
	</Extractive Summary>
</Paper ID=ument983>


<Paper ID=ument984> <Table ID =1>
	<Abstractive Summary> =
		01M
		Table 1: Dataset Summary
		well-understood, and it tests a model’s tolerance
		for paraphrasing and coreferences between the
		question and context
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 summarizes key characteristics for the
		datasets
	</Extractive Summary>
</Paper ID=ument984>


<Paper ID=ument984> <Table ID =2>
	<Abstractive Summary> =
		00
		Table 2: Results from all experiments
		We evaluate models on every dataset’s dev set,
		sample 100 question-answer pairs to character-
		ize the linguistic phenomena and inference type
		needed to answer correctly, and then inspect per-
		formance on the sampled pairs
	</Abstractive Summary>
</Paper ID=ument984>


<Paper ID=ument984> <Table ID =3>
	<Abstractive Summary> =
		Table 3: Examples of frequent error types from all 4 datasets
		error type
	</Abstractive Summary>
	<Extractive Summary> =
		We also introduce
		categories for common errors observed across all
		datasets below; Table 3 shows examples for every
		129
		Error Type
		Question
		Answer
		Prediction
		Random Guess
		How high do plague fevers run?
		38-41C
		near 100%
		Same Entity Type
		What team lost Super Bowl XXXIII?
		Atlanta Falcons
		Denver
		Sentence Selection
		What did Marlee Matlin translate?
		the national anthem
		American Sign Language
		Copying From Question
		What was Apple Talk?
		proprietary suite of
		AppleTalk
		networking protocols
		Factually Correct
		How long are car loans typically?
		60-month
		5 years
		Reasonable Answer
		What did Edison offer Tesla
	</Extractive Summary>
</Paper ID=ument984>


<Paper ID=ument984> <Table ID =4>
	<Abstractive Summary> =
		58
		Table 4: Common Types of Errors on SQuAD (top)
		and Select SQuAD Regression Features and Odds
		Ratios (bottom)
		6
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the error distribution for all mod-
		els
	</Extractive Summary>
	<Extractive Summary> =
		993
		-
		Context Len
		-
		-
		-
		-
		Question Type
		-
		-
		-
		-
		Table 5: Common Types of Errors on HotpotQA (top)
		and Select HotpotQA Regression Features and Odds
		Ratios (bottom, - denotes insigniﬁcant results)
		sults in Table 4 are all signiﬁcant)
	</Extractive Summary>
</Paper ID=ument984>


<Paper ID=ument984> <Table ID =5>
	<Abstractive Summary> =
		993
		-
		Context Len
		-
		-
		-
		-
		Question Type
		-
		-
		-
		-
		Table 5: Common Types of Errors on HotpotQA (top)
		and Select HotpotQA Regression Features and Odds
		Ratios (bottom, - denotes insigniﬁcant results)
		sults in Table 4 are all signiﬁcant)
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 5, the most frequent errors in
		HotpotQA involve distractor sentences and mul-
		tihop inference
	</Extractive Summary>
</Paper ID=ument984>


<Paper ID=ument984> <Table ID =6>
	<Abstractive Summary> =
		17%
		11%
		11%
		4%
		Yes/No Choice
		8%
		11%
		4%
		0%
		Table 6: Common Types of Errors on MSMARCO
		that long answers are more likely to be faulty or
		badly chosen
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the distribution of common errors
		on MSMARCO
	</Extractive Summary>
</Paper ID=ument984>


<Paper ID=ument984> <Table ID =7>
	<Abstractive Summary> =
		50
		Table 7: Common Types of Errors on SearchQA (top)
		and Select SearchQA Regression Features and Odds
		Ratios (bottom, - denotes insigniﬁcant results)
		are frequently evaluated on a single dataset, and
		even when multiple datasets are used, they tend
		to be similar
	</Abstractive Summary>
	<Extractive Summary> =
		From Table 7, we see that the Same Entity Type
		is the major error across all models
	</Extractive Summary>
	<Extractive Summary> =
		Based on the regressions done on model scores
		(Table 7), an interesting common trend is sug-
		gested across all models
	</Extractive Summary>
</Paper ID=ument984>


<Paper ID=ument984> <Table ID =8>
	<Abstractive Summary> =
		
	</Abstractive Summary>
	<Extractive Summary> =
		Table 8 shows an example for each inference type
	</Extractive Summary>
</Paper ID=ument984>


<Paper ID=ument985> <Table ID =1>
	<Abstractive Summary> =
		SQuAD
		NQ
		Questions
		87,599
		74,097
		Candidate Sentences
		91,707
		239,013
		Candidate Paragraphs
		18,896
		58,699
		Table 1: The number of questions and candidates in the
		constructed datasets ReQA SQuAD and ReQA NQ
	</Abstractive Summary>
	<Extractive Summary> =
		WikiQA consists of 3,047 questions and
		29,258 candidate answers, while ReQA SQuAD
		and ReQA NQ each contain over 20x that num-
		ber of questions and over 3x that number of candi-
		dates (see Table 1)
	</Extractive Summary>
</Paper ID=ument985>


<Paper ID=ument985> <Table ID =2>
	<Abstractive Summary> =
		9
		Table 2:
		Token-level statistics of the constructed
		datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists the average number of tokens in
		question and sentence-level answer text, as well
		as the “query coverage”, which is the percentage
		of tokens in the question that also appear in the
		answer
	</Extractive Summary>
</Paper ID=ument985>


<Paper ID=ument985> <Table ID =3>
	<Abstractive Summary> =
		1
		Table 3:
		The distribution of question types in
		ReQA SQuAD and ReQA NQ
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 shows the distribution of question
		types for each dataset
	</Extractive Summary>
</Paper ID=ument985>


<Paper ID=ument985> <Table ID =4>
	<Abstractive Summary> =
		145
		Table 4: Mean reciprocal rank (MRR) and recall@K
		performance of neural baselines on ReQA SQuAD and
		ReQA NQ
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 presents the ReQA results for our base-
		line models
	</Extractive Summary>
	<Extractive Summary> =
		To obtain our baseline using traditional IR
		methods, we constructed a paragraph-level re-
		trieval task which allows a direct comparison be-
		tween the neural systems in Table 4 and BM25
	</Extractive Summary>
</Paper ID=ument985>


<Paper ID=ument985> <Table ID =5>
	<Abstractive Summary> =
		6
		Table 5: Time and space tradeoffs of different models
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 illustrates the tradeoff between model
		accuracy and resource usage
	</Extractive Summary>
</Paper ID=ument985>


<Paper ID=ument985> <Table ID =6>
	<Abstractive Summary> =
		175
		Table 6: Performance of various models on paragraph-
		level retrieval
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 6, the
		USE-QA neural baseline outperforms BM25 on
		paragraph retrieval
	</Extractive Summary>
</Paper ID=ument985>


<Paper ID=ument986> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Dataset Statistics
		from summaries
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 gives
		summary statistics of these datasets
	</Extractive Summary>
</Paper ID=ument986>


<Paper ID=ument986> <Table ID =2>
	<Abstractive Summary> =
		Dataset
		IC
		MWC
		Imp
		No-Ans
		SEARs
		NewsQA
		0
		0
		501
		347
		16009
		QuoRef
		0
		0
		79
		385
		11759
		DROP
		1377
		457
		113
		284
		16382
		SQuAD
		16
		0
		875
		594
		28188
		ROPES
		637
		119
		0
		201
		2909
		DuoRC
		22
		0
		2706
		-
		45020
		Table 2: Yields of augmented datasets
		No Answer
		substitutes a name in the question for
		a different name from the passage to create with
		high probability a new question with no answer
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 shows the num-
		ber of augmentations generated by each augmenta-
		tion technique-dataset pair
	</Extractive Summary>
</Paper ID=ument986>


<Paper ID=ument986> <Table ID =3>
	<Abstractive Summary> =
		Q: What did histori-
		ans
		compare
		to
		the
		Coptic
		language?
		A: hieroglyphs
		Q:
		What’d
		histori-
		ans
		compare
		to
		the
		Coptic
		language?
		A: hieroglyphs
		Table 3: Examples of generated augmentations with various templates
	</Abstractive Summary>
</Paper ID=ument986>


<Paper ID=ument986> <Table ID =4>
	<Abstractive Summary> =
		Dataset
		IC
		MWC
		Imp
		No-Ans
		SEARs
		NewsQA
		-
		-
		47
		47
		50
		QuoRef
		-
		-
		45
		48
		50
		DROP
		46
		42
		36
		48
		50
		SQuAD
		15/16
		-
		47
		48
		50
		ROPES
		48
		36
		-
		11
		50
		DuoRC
		18/22
		-
		47
		-
		50
		Table 4: Quality of augmented datasets (# of good ques-
		tions out of 50 sampled)
		tion from the original and has enough independent
		value to be incorporated into the evaluation server
	</Abstractive Summary>
	<Extractive Summary> =
		Is the new answer the correct answer for the
		new question?
		Table 4 shows the number of high-quality ques-
		tions generated for each dataset
	</Extractive Summary>
</Paper ID=ument986>


<Paper ID=ument986> <Table ID =5>
	<Abstractive Summary> =
		53
		Table 5: Performance on baseline BERT model on dif-
		ferent datasets
		tion server
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Existing Dataset Performance
		Table 5 shows the result of evaluating on all of
		the development and test sets using our evalua-
		151
		Dataset
		Dev
		Test
		EM
		F1
		EM
		F1
		NewsQA
		29
	</Extractive Summary>
</Paper ID=ument986>


<Paper ID=ument986> <Table ID =6>
	<Abstractive Summary> =
		85
		Template Type
		Answered Incorrectly
		Answered Correctly
		Invert Choice
		Original: Which art gallery was founded
		ﬁrst, Art Euphoric or Trescott Street?
		Generated:
		Which art gallery was
		founded ﬁrst, Trescott Street or Art Eu-
		phoric?
		Original:
		Who scored more ﬁeld
		goals, Nate Kaeding or Dan Carpenter?
		Generated: Who scored more ﬁeld goals,
		Dan Carpenter or Nate Kaeding?
		Implication
		Original:
		When
		did
		the
		Huguenots
		secure
		the
		right
		to
		own
		land
		in
		the
		Baronies?
		Generated:
		Who secured the right
		to own land in baronies in 1697?
		Original:
		When
		did
		Henry
		issue
		the
		Edict
		of
		Nantes?
		Generated:
		What did Henry issue
		in 1598?
		SEARs
		Original:
		What
		was
		the
		theme
		of
		Super
		Bowl
		50?
		Generated: So what was the theme of
		Super Bowl 50?
		Original: Who won Super Bowl 50?
		Generated: So who won Super Bowl
		50?
		Table 6: Quantitative and qualitative analysis of generated augmentations
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Synthetic Augmentations
		Table 6 shows the performance of the baseline
		model on various development sets and heuris-
		tically generated questions
	</Extractive Summary>
</Paper ID=ument986>


<Paper ID=ument987> <Table ID =1>
	<Abstractive Summary> =
		[SEP] Where did Super Bowl 50 take place [SEP] [MASK]
		Table 1: BERT-SQG Running Example
		Figure 3: The BERT-SQG architecture
		In C′, we design and insert a new token (i
	</Abstractive Summary>
	<Extractive Summary> =
		In
		Table 1, we give an example of the actual running
		of the model
	</Extractive Summary>
</Paper ID=ument987>


<Paper ID=ument987> <Table ID =2>
	<Abstractive Summary> =
		[SEP] Where did Super Bowl 50 take place [SEP] [MASK]
		Table 2: BERT-HLSQG Running Example
		Figure 4: The BERT-HLSQG architecture
		Train
		Test
		Dev
		SQuAD 73K
		73240
		11877
		10570
		SQuAD 81K
		81577
		8964
		8964
		Table 3: Dataset statistics: SQuAD 73K is the setting
		of (Du et al
	</Abstractive Summary>
</Paper ID=ument987>


<Paper ID=ument987> <Table ID =3>
	<Abstractive Summary> =
		[SEP] Where did Super Bowl 50 take place [SEP] [MASK]
		Table 2: BERT-HLSQG Running Example
		Figure 4: The BERT-HLSQG architecture
		Train
		Test
		Dev
		SQuAD 73K
		73240
		11877
		10570
		SQuAD 81K
		81577
		8964
		8964
		Table 3: Dataset statistics: SQuAD 73K is the setting
		of (Du et al
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 summarizes statis-
		tics for the compared datasets
	</Extractive Summary>
</Paper ID=ument987>


<Paper ID=ument987> <Table ID =4>
	<Abstractive Summary> =
		06
		Table 4: Reading comprehension evaluation results
		Model
		BLEU 1
		BLEU 2
		BLEU 3
		BLEU 4
		METEOR
		ROUGE-L
		NQG-RC
		43
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Results and Analysis
		Table 4 shows the human question and generated
		question experiment comparison results
	</Extractive Summary>
</Paper ID=ument987>


<Paper ID=ument987> <Table ID =5>
	<Abstractive Summary> =
		68
		Table 5: Comparison between our model and the published methods using sentence level context
		Model
		BLEU 1
		BLEU 2
		BLEU 3
		BLEU 4
		METEOR
		ROUGE-L
		NQG-RC
		42
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Quantitative Results
		Table 5 shows the comparison results using
		sentence-level context texts and Table 6 shows the
		results on paragraph-level context
	</Extractive Summary>
	<Extractive Summary> =
		As
		shown in Table 5, BERT-HLSQG outperforms the
		existing best performing model by 4-5% on both
		benchmark datasets
	</Extractive Summary>
</Paper ID=ument987>


<Paper ID=ument987> <Table ID =6>
	<Abstractive Summary> =
		68
		Table 6: Comparison between our model and the published methods using paragraph level context
		mans, and the use of reading comprehension tasks
		also has effective
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Quantitative Results
		Table 5 shows the comparison results using
		sentence-level context texts and Table 6 shows the
		results on paragraph-level context
	</Extractive Summary>
	<Extractive Summary> =
		Second, the results in Table 6 further show that
		BERT-SQG successfully processes the paragraph-
		level contexts and further push the state-of-the-art
		from 16
	</Extractive Summary>
</Paper ID=ument987>


<Paper ID=ument988> <Table ID =1>
	<Abstractive Summary> =
		Class
		Original
		After Downsampling
		What
		50385
		4000
		Which
		6111
		4000
		Where
		3731
		3731
		When
		5437
		4000
		Who
		9162
		4000
		Why
		1224
		1224
		How
		9408
		4000
		Others
		9408
		4000
		Table 1: SQuAD training set statistics
	</Abstractive Summary>
</Paper ID=ument988>


<Paper ID=ument988> <Table ID =2>
	<Abstractive Summary> =
		94
		Table 2: Comparison of our model with the baselines
	</Abstractive Summary>
</Paper ID=ument988>


<Paper ID=ument988> <Table ID =3>
	<Abstractive Summary> =
		65
		Table 3: Performance of the QG model with respect to the accuracy of the interrogative-word classiﬁer
	</Abstractive Summary>
	<Extractive Summary> =
		Table 3 and
		Figure 3 show a linear relationship between the
		accuracy of the classiﬁer and the IWAQG
	</Extractive Summary>
</Paper ID=ument988>


<Paper ID=ument988> <Table ID =4>
	<Abstractive Summary> =
		72%
		Table 4: Recall of interrogative words of the QG model
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in the Table 4, the total recall of using
		only the QG module is 68
	</Extractive Summary>
	<Extractive Summary> =
		As we can see, the overall
		recall is high, and it is also higher than just using
		the QG module (Table 4), which proves our hy-
		pothesis that modeling the interrogative-word pre-
		diction task as an independent classiﬁcation prob-
		lem yields to a higher recall than generating them
		with the full question
	</Extractive Summary>
</Paper ID=ument988>


<Paper ID=ument988> <Table ID =5>
	<Abstractive Summary> =
		Lastly, we also prove that
		170
		id
		Only QG*
		IWAQG
		Upper Bound
		Golden
		Answer
		1
		what
		produces
		a list of require-
		ments
		for
		a
		project?
		who produces a
		list
		of
		require-
		ments
		for
		a
		project?
		who produces a
		list
		of
		require-
		ments
		for
		a
		project?
		who produces a
		list
		of
		require-
		ments
		for
		a
		project,
		giving
		an overall view
		of the project’s
		goals?
		The owner
		2
		how many tun-
		nels
		were
		con-
		structed through
		newcastle
		city
		centre?
		what type of tun-
		nels constructed
		through newcas-
		tle city centre?
		what type of tun-
		nels constructed
		through newcas-
		tle city centre ?
		what
		type
		of
		tunnels are con-
		structed through
		newcastle ’s city
		center?
		deep-level
		tunnels
		3
		who received a
		battering
		during
		the siege of new-
		castle?
		what received a
		battering
		during
		the siege of new-
		castle ?
		what received a
		battering
		during
		the siege of new-
		castle ?
		what received a
		battering
		during
		the siege of new-
		castle?
		The church
		tower
		4
		what system is
		newcastle
		inter-
		national
		airport
		connected to?
		what system is
		newcastle
		inter-
		national
		airport
		connected to?
		how is newcastle
		international air-
		port connected to
		?
		how is newport ’s
		airport connected
		to the city?
		via
		the
		Metro Light
		Rail system
		5
		who
		was
		the
		country
		most
		dependent
		on
		arab oil?
		what
		country
		was
		the
		most
		dependent
		on
		arab oil?
		which
		country
		was
		the
		most
		dependent
		on
		arab oil?
		which country is
		the most depen-
		dent on arab oil?
		Japan
		Table 5: Qualitative Analysis
	</Abstractive Summary>
	<Extractive Summary> =
		As we can see in Table 5, in the ﬁrst three ex-
		amples the interrogative words generated by the
		baseline are wrong, while our model is right
	</Extractive Summary>
</Paper ID=ument988>


<Paper ID=ument988> <Table ID =6>
	<Abstractive Summary> =
		8%
		Table 6: Ablation Study of our interrogative-word clas-
		siﬁer
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Ablation Study
		We tried to combine different features shown in
		Table 6 for the interrogative-word classiﬁer
	</Extractive Summary>
</Paper ID=ument988>


<Paper ID=ument988> <Table ID =7>
	<Abstractive Summary> =
		0%
		Table 7: Recall and precision of interrogative words of
		our interrogative-word classiﬁer
	</Abstractive Summary>
	<Extractive Summary> =
		In addition, we provide the recall and precision
		per class for our ﬁnal interrogative-word classiﬁer
		(CLS + AT in Table 7)
	</Extractive Summary>
</Paper ID=ument988>


<Paper ID=ument989> <Table ID =1>
	<Abstractive Summary> =
		Table 1: Example questions (Q) from the NarrativeQA data set, with gold free-text answers (G), the most relevant
		sentence as automatically extracted from the summary (E) and the most relevant sub-sentence level span (boldface)
	</Abstractive Summary>
	<Extractive Summary> =
		, Table 1 for examples)
	</Extractive Summary>
</Paper ID=ument989>


<Paper ID=ument989> <Table ID =2>
	<Abstractive Summary> =
		if unsuccessful: considering any sentence in
		the summary, return the longest substring
		175
		train
		valid
		test
		# QA-pairs
		32,170
		3,461
		10,557
		# documents
		1,102
		115
		355
		Table 2: Statistics of the NarrativeQA data set (Ko-
		cisky et al
	</Abstractive Summary>
</Paper ID=ument989>


<Paper ID=ument989> <Table ID =3>
	<Abstractive Summary> =
		86
		Table 3: Results on summary-level sentence-relevance
		classiﬁcation on the NQA test set of 25K question-
		answer pairs
	</Abstractive Summary>
	<Extractive Summary> =
		Results are shown in Table 3, and show
		that the model detects the most relevant summary
		sentence for a question accurately across a variety
		of metrics
	</Extractive Summary>
</Paper ID=ument989>


<Paper ID=ument989> <Table ID =4>
	<Abstractive Summary> =
		305
		Table 4: Fraction of correct answers contained in the
		top {1 / 5} answer candidates, and MRR of the cor-
		rect answer in passages retrieved by the BERT-based
		retrieval method (BERT) or an IR method (BM25f)
	</Abstractive Summary>
</Paper ID=ument989>


<Paper ID=ument989> <Table ID =5>
	<Abstractive Summary> =
		50
		Table 5: Summary-level answer extraction results by previous models and our systems trained on out-of-domain
		SQUAD data (BERT SQUAD *), and our heuristic data set (BERT heur)
	</Abstractive Summary>
</Paper ID=ument989>


<Paper ID=ument989> <Table ID =6>
	<Abstractive Summary> =
		15
		Table 6: Book-level answer extraction results by previous models and our systems trained on out-of-domain
		SQUAD data (BERT SQUAD *), and our heuristic data set (BERT heur)
	</Abstractive Summary>
</Paper ID=ument989>


<Paper ID=ument99> <Table ID =1>
	<Abstractive Summary> =
		11
		Table 1: Labeled F1 score (including senses) for all languages on the CoNLL-2009 in-domain test set
	</Abstractive Summary>
	<Extractive Summary> =
		6 We select the best con-
		ﬁguration for each language to report the test set
		performance in Table 1 and Table 4
	</Extractive Summary>
</Paper ID=ument99>


<Paper ID=ument99> <Table ID =2>
	<Abstractive Summary> =
		62
		Table 2: Number of sentences and predicates in train-
		ing set of different languages
	</Abstractive Summary>
	<Extractive Summary> =
		As expected, weight tying is beneﬁcial for
		lower-resource
		languages
		such
		as
		Catalan,
		Japanese and Spanish (see Table 2 for dataset
		characteristics)
	</Extractive Summary>
</Paper ID=ument99>


<Paper ID=ument99> <Table ID =3>
	<Abstractive Summary> =
		54
		Table 3: Labeled F1 score (including senses) for all languages on development set for different conﬁgurations
	</Abstractive Summary>
</Paper ID=ument99>


<Paper ID=ument99> <Table ID =4>
	<Abstractive Summary> =
		04
		Table 4: Labeled F1 scores (including senses) on En-
		glish, German, Czech in-domain and out-of-domain
		test sets; we chose the previous models achieving the
		best scores on the out-of-domain test sets
	</Abstractive Summary>
</Paper ID=ument99>


<Paper ID=ument99> <Table ID =5>
	<Abstractive Summary> =
		06
		Table 5: Labeled F1 score (including senses) for all languages on the CoNLL-2009 in-domain test set
	</Abstractive Summary>
</Paper ID=ument99>


<Paper ID=ument99> <Table ID =6>
	<Abstractive Summary> =
		Model
		U
		C
		R
		Gold
		55
		0
		88
		Baseline
		301
		2
		114
		Structured Reﬁnement
		142
		2
		111
		Table 6: Unique core roles violations (U), continuation
		roles violations (C) and reference roles violations (R)
		on English in-domain test set
	</Abstractive Summary>
</Paper ID=ument99>


<Paper ID=ument99> <Table ID =7>
	<Abstractive Summary> =
		3
		Table 7: Labeled roles precision (RP), recall (RR) and
		sense disambiguation accuracy (Sense) on English in-
		domain test set
	</Abstractive Summary>
</Paper ID=ument99>


<Paper ID=ument990> <Table ID =1>
	<Abstractive Summary> =
		∗Authors contributed equally
		Dataset
		Size
		Context
		Question
		SQuAD
		96K
		wikipedia
		crowd
		NewsQA
		78K
		newswire
		crowd
		TriviaQA
		69K
		snippets
		quiz
		SearchQA
		133K
		snippets
		quiz
		HotpotQA
		78K
		wikipedia
		crowd
		NaturalQuestions
		116K
		wikipedia
		crowd
		DROP
		1,503
		wikipedia
		crowd
		RACE
		674
		exam
		handcraft
		BioASQ
		1,504
		biomedical
		handcraft
		TextbookQA
		1,503
		textbook
		handcraft
		RelationExtraction
		2,948
		wikipedia
		KB
		DuoRC
		1,501
		plot
		crowd
		Table 1: Characteristics of released datasets for the
		MRQA shared task
	</Abstractive Summary>
	<Extractive Summary> =
		The goal of this competition is to
		demonstrate high performances on out-of-domain
		datasets (the bottom part of Table 1 and addition-
		ally unseen test datasets) by the trained model
		which only utilizes in-domain datasets (the top
		part of Table 1)
	</Extractive Summary>
</Paper ID=ument990>


<Paper ID=ument990> <Table ID =2>
	<Abstractive Summary> =
		Test
		SQuAD
		76,079
		10,507
		10,509
		NewsQA
		69,947
		4,212
		4,213
		TriviaQA
		53,902
		7,785
		7,786
		SearchQA
		100,403
		16,980
		16,981
		HotpotQA
		67,010
		5,904
		5,902
		NaturalQuestions
		91,234
		12,836
		12,837
		DROP
		-
		1,503
		-
		RACE
		-
		674
		-
		BioASQ
		-
		1,504
		-
		TextbookQA
		-
		1,503
		-
		RelationExtraction
		-
		2,948
		-
		DuoRC
		-
		1,501
		-
		Table 2: Statistics of datasets for RC tasks
	</Abstractive Summary>
</Paper ID=ument990>


<Paper ID=ument990> <Table ID =3>
	<Abstractive Summary> =
		SNLI
		550,152
		10,000
		FICTION
		77,348
		2,000
		GOVERNMENT
		77,350
		2,000
		SLATE
		77,306
		2,000
		TELEPHONE
		83,348
		2,000
		TRAVEL
		77,350
		2,000
		Table 3: Statistics of datasets for NLI tasks
	</Abstractive Summary>
</Paper ID=ument990>


<Paper ID=ument990> <Table ID =4>
	<Abstractive Summary> =
		2
		Table 4: Results on the in-domain development set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 summarizes the results on the
		in-domain development set
	</Extractive Summary>
</Paper ID=ument990>


<Paper ID=ument990> <Table ID =5>
	<Abstractive Summary> =
		7
		Table 5: Results on the out-of-domain development set
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 summarizes the evalua-
		tion results for out-of-domain
	</Extractive Summary>
</Paper ID=ument990>


<Paper ID=ument990> <Table ID =6>
	<Abstractive Summary> =
		1
		Table 6: Results of submission run
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 summarizes the submission run re-
		sults
	</Extractive Summary>
</Paper ID=ument990>


<Paper ID=ument991> <Table ID =1>
	<Abstractive Summary> =
		49
		Table 1: Development Datasets Results
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Results
		The results of our model on all the development
		datasets are summarized in Table 1 and the results
		on all the test datasets are summarized in Table 2
	</Extractive Summary>
</Paper ID=ument991>


<Paper ID=ument991> <Table ID =2>
	<Abstractive Summary> =
		71
		Table 2: Test Datasets Results
	</Abstractive Summary>
</Paper ID=ument991>


<Paper ID=ument992> <Table ID =1>
	<Abstractive Summary> =
		0
		1k lessons and 26k multi-modal questions, from
		middle school science curriculum
		Table 1: Statistics of out-of-domain validation dataset
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows the statistics and description
		of these datasets
	</Extractive Summary>
</Paper ID=ument992>


<Paper ID=ument992> <Table ID =2>
	<Abstractive Summary> =
		68
		Table 2: Model performance on validation and test set
	</Abstractive Summary>
	<Extractive Summary> =
		3
		Performance Comparison
		Table 2 shows the performance evaluation results
		of models on out-of-domain datasets
	</Extractive Summary>
</Paper ID=ument992>


<Paper ID=ument993> <Table ID =1>
	<Abstractive Summary> =
		205
		Dataset
		Source
		Question
		Multi-
		hop
		In-Domain Datasets
		SQuAD
		Wikipedia
		Crowd
		No
		NewsQA
		News
		Crowd
		No
		TriviaQA
		Snippets
		Trivia
		No
		SearchQA
		Snippets
		Trivia
		No
		HotpotQA
		Wikipedia
		Crowd
		Yes
		NQ
		Wikipedia
		Query
		No
		Out-of-Domain Datasets
		DROP
		Wikipedia
		Crowd
		Yes
		RACE
		Exam
		Expert
		Yes
		DuoRC
		Movie Plot
		Crowd
		No
		BioASQ
		Biomedical
		Crowd
		No
		TQA
		Textbook
		Crowd
		No
		RE
		Wikipedia
		Crowd
		No
		Table 1: Characterization of the training and devel-
		opment datasets
	</Abstractive Summary>
</Paper ID=ument993>


<Paper ID=ument993> <Table ID =2>
	<Abstractive Summary> =
		GPU
		TPU
		Fine-tuned Layers
		12 - 24 (13)
		1 - 24 (24)
		Floating Point
		16
		32
		Training Batch Size
		4
		48
		Sequence Length
		340
		512
		MLP Layer Size
		512, 384, 1
		1024, 1
		Table 2: Difference of hyper-parameters and the MLP
		structure when ﬁne-tuning XLNet model on GPU and
		TPU
	</Abstractive Summary>
</Paper ID=ument993>


<Paper ID=ument993> <Table ID =3>
	<Abstractive Summary> =
		31
		Table 3: Results for XLNet models that are only ﬁne-tuned on a single training set but tested on all the in-domain
		and out-of-domain development sets
	</Abstractive Summary>
</Paper ID=ument993>


<Paper ID=ument993> <Table ID =4>
	<Abstractive Summary> =
		96
		Table 4: Results of models ﬁne-tuned with different data feeding methods on development datasets
	</Abstractive Summary>
	<Extractive Summary> =
		5
		Results
		Comparison between Data Feeding Methods
		Table 4 shows the performance of the XLNet mod-
		els ﬁne-tuned with the two data feeding methods
		mentioned in §4
	</Extractive Summary>
</Paper ID=ument993>


<Paper ID=ument993> <Table ID =5>
	<Abstractive Summary> =
		42
		Table 5: Results of multi-task models that are ﬁne-tuned with the methods described in §4
	</Abstractive Summary>
</Paper ID=ument993>


<Paper ID=ument993> <Table ID =6>
	<Abstractive Summary> =
		76
		Table 6: Results on test datasets
	</Abstractive Summary>
	<Extractive Summary> =
		Comparison with Baseline
		The results on the test sets shown in Table 6 in-
		dicate that the multi-task XLNet-large model ﬁne-
		tuned with a larger linear layer on the TPU con-
		sistently outperforms the BERT-large baseline by
		a huge margin
	</Extractive Summary>
</Paper ID=ument993>


<Paper ID=ument994> <Table ID =1>
	<Abstractive Summary> =
		Matching
		✓
		Table 1: The datasets of MRQA 2019 Shared Task include 6 training sets and 12 testing sets
	</Abstractive Summary>
	<Extractive Summary> =
		As shown in Table 1, the major challenge of the
		shared task is that the train and test datasets differ
		in the following ways:
		• Questions:
		They
		come
		from
		different
		sources, e
	</Extractive Summary>
	<Extractive Summary> =
		Since the testing
		set differs from the training set in terms of docu-
		ment sources (see Table 1), we divide the testing
		set into two subsets: (1) Wiki & Web & News and
		(2) Other
	</Extractive Summary>
</Paper ID=ument994>


<Paper ID=ument994> <Table ID =2>
	<Abstractive Summary> =
		8
		Table 2: The conﬁgurations and hyper-parameters of the eleven models used in our experiments
	</Abstractive Summary>
	<Extractive Summary> =
		Table 2 lists the detailed conﬁgurations and the
		hyper-parameters of these models
	</Extractive Summary>
	<Extractive Summary> =
		Please refer to Table 2 with corresponding model ID for details about the model conﬁgurations
	</Extractive Summary>
	<Extractive Summary> =
		Please refer to Table 2 with
		corresponding model ID for details about the model conﬁgurations
	</Extractive Summary>
</Paper ID=ument994>


<Paper ID=ument994> <Table ID =3>
	<Abstractive Summary> =
		42
		-
		Table 3: System performance on the development and test set
	</Abstractive Summary>
	<Extractive Summary> =
		1
		The Main Results and the Effects of
		Pre-trained Models
		Table 3 shows the main results and the results for
		the effects of pre-trained models
	</Extractive Summary>
	<Extractive Summary> =
		From Table 3,
		we have the following observations:
		(1) Our submitted system signiﬁcantly outper-
		forms the ofﬁcial baseline by about 10 F1 score,
		and it is ranked at top 1 of all the participants in
		terms of averaged F1 score 9
	</Extractive Summary>
</Paper ID=ument994>


<Paper ID=ument994> <Table ID =4>
	<Abstractive Summary> =
		83
		Table 4: The experimental results on examining the effects of multi-task learning
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the
		experimental results:
		(1) From the ﬁrst two rows in Table 4, we can
		observe that the auxiliary task of masked language
		model can improve the performance on both in-
		domain and out-of-domain development set, es-
		pecially on the out-of-domain set
	</Extractive Summary>
	<Extractive Summary> =
		(2) From the last two rows in Table 4, we do
		not observe that the auxiliary tasks of natural lan-
		guage inference and paragraph ranking bring fur-
		ther beneﬁts in terms of generalization
	</Extractive Summary>
</Paper ID=ument994>


<Paper ID=ument994> <Table ID =5>
	<Abstractive Summary> =
		60
		Table 5: The performance of the submitted system on
		two subsets that contain different document sources
	</Abstractive Summary>
	<Extractive Summary> =
		Please refer to Table 5 for the detailed
		partition
	</Extractive Summary>
	<Extractive Summary> =
		However,
		we observe from Table 5 that the system performs
		similarly on two subsets
	</Extractive Summary>
</Paper ID=ument994>


<Paper ID=ument994> <Table ID =6>
	<Abstractive Summary> =
		53
		Table 6: The performance of the submitted system
		on three subsets that require different language un-
		derstanding ability
	</Abstractive Summary>
	<Extractive Summary> =
		Please refer to Table 6 for the detailed par-
		tition
	</Extractive Summary>
	<Extractive Summary> =
		From Table 6, we ob-
		serve that the system performs much worse on the
		the subsets of Reasoning and Arithmetic
	</Extractive Summary>
</Paper ID=ument994>


<Paper ID=ument995> <Table ID =1>
	<Abstractive Summary> =
		3
		Table 1: Number of examples (question-context pairs),
		segments (question-context chunks), and the percent-
		age of No Answer (NA) segments within each dataset
	</Abstractive Summary>
	<Extractive Summary> =
		In Table 1 we tabulate the number of “examples”
		(question-context pairs), “segments” (the question
		combined with a portion of the context), and “no-
		answer” (NA) segments (those without a valid an-
		swer span)
	</Extractive Summary>
	<Extractive Summary> =
		(q, c) →
		�
		(q, ci·D:M+i·D), ∀i ∈ [0, k]
		�
		(1)
		The frequencies presented in Table 1 are based
		on our settings of M = 512 and D = 128
	</Extractive Summary>
	<Extractive Summary> =
		Addition-
		ally, its long contexts generate 657K segments,
		double that of the next largest dataset (Table 1)
	</Extractive Summary>
	<Extractive Summary> =
		3% of all input segments are NA,
		as shown in Table 1, its unsurprising their inclu-
		sion signiﬁcantly impacted training time and re-
		sults
	</Extractive Summary>
</Paper ID=ument995>


<Paper ID=ument995> <Table ID =2>
	<Abstractive Summary> =
		16
		Table 2: Model performance including or excluding
		No-Answer (NA) segments in training
	</Abstractive Summary>
	<Extractive Summary> =
		We ﬁnd that this simple form of Nega-
		tive Sampling yields non-trivial improvements on
		MRQA (see Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		4
		Experiments and Discussion
		During our experimentation process we used our
		smallest model BERT Base Cased (BBC) for the
		most expensive sampling explorations (Figure 1),
		XLNet Base Cased (XBC) to conﬁrm our ﬁndings
		extended to XLNet (Table 2), and XLNet Large
		Cased (XLC) as the initial basis for our ﬁnal sub-
		mission contenders (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		In Table 2 we examine
		the impact of including No Answer segments in
		our training set
	</Extractive Summary>
</Paper ID=ument995>


<Paper ID=ument995> <Table ID =3>
	<Abstractive Summary> =
		55
		Table 3: F1 scores for data augmentation using different proportions of query and context paraphrasing and dif-
		ferent sampling distributions on XLNet Large Cased, on individual datasets
	</Abstractive Summary>
	<Extractive Summary> =
		4
		Experiments and Discussion
		During our experimentation process we used our
		smallest model BERT Base Cased (BBC) for the
		most expensive sampling explorations (Figure 1),
		XLNet Base Cased (XBC) to conﬁrm our ﬁndings
		extended to XLNet (Table 2), and XLNet Large
		Cased (XLC) as the initial basis for our ﬁnal sub-
		mission contenders (Table 3)
	</Extractive Summary>
	<Extractive Summary> =
		We present the best results
		in Table 3 and conclude that these data augmenta-
		tions did not help in-domain or out-domain perfor-
		mance
	</Extractive Summary>
</Paper ID=ument995>


<Paper ID=ument995> <Table ID =4>
	<Abstractive Summary> =
		92
		Table 4: Breakdown of hidden development set results
		by dataset using our best XLNet Large model
	</Abstractive Summary>
	<Extractive Summary> =
		The results of the
		leaderboard Out-Domain Development set and ﬁ-
		nal test set results are shown in Table 4 and Ta-
		ble 5 respectively
	</Extractive Summary>
</Paper ID=ument995>


<Paper ID=ument995> <Table ID =5>
	<Abstractive Summary> =
		50
		Table 5: Macro-Average EM and F1 on the held-out
		leaderboard test sets
	</Abstractive Summary>
</Paper ID=ument995>


<Paper ID=ument996> <Table ID =1>
	<Abstractive Summary> =
		000
		—
		Table 1: Results for the 13 workers (W1–W13) who annotate 50 or more Penn Treebank trees, including the total
		number of trees annotated, unlabeled attachment scores (UAS), full tree match rate (FTM), and median time in
		seconds ( �
		time) between accepting a HIT and submitting results
		6
		Table 1: Example Entry from the ‘Word List by Semantic Principles’
		���� ‘Last Year’: 1
		The American president Donald Trump has ad-
		dressed the General Assembly of the United Na-
		tions for the ﬁrst time
		Table 1: Examples of matching overlapping mentions5791
		Table 1: Number of questions, workers and answers45
		High-agreement pairs
		80%
		67%
		68%
		Low-agreement pairs
		20%
		15%
		15%
		Positive pairs
		20%
		17%
		25%
		Table 1: Speech-level annotation statistics (top) and re-
		sults (bottom), comparing the use of 3 different groups
		of annotators
		41
		type
		#videos
		#video clips
		in-house
		10 (group A)
		1,736
		crowdsourced
		10 (group B)
		1,861
		all
		20
		3,597
		Table 1: Statistics of the data
		label
		(score)
		guidelines
		up
		(+1)
		Watch the video clip and select up if your
		feeling matches one of the pictures below6%
		Table 1: WER and relaxed WER for measuring Quality
		of Transcriptions
		5
	</Abstractive Summary>
	<Extractive Summary> =
		The scores for these 13
		most proliﬁc annotators are presented in Table 1,
		along with the scores for the primary author (who
		is, ideally, representative of an expert annotator)
		included as well for comparison
	</Extractive Summary>
	<Extractive Summary> =
		Table 1 shows an example of
		the word ‘�� (Last Year)’, which is assigned a
		value of 1
	</Extractive Summary>
	<Extractive Summary> =
		As shown in these tables,
		the nominal action and subject categories tend
		to be character-based biased, whereas the voca-
		Table 10: Character-based Biased Categories
		Category
		Ch-Vo
		1
	</Extractive Summary>
	<Extractive Summary> =
		07
		Ch-Vo: WRITE + READ - SPEAK - LISTEN
		Table 11: Voice-based Biased Categories
		Category
		Ch-Vo
		4
	</Extractive Summary>
	<Extractive Summary> =
		Therefore, the
		12
		Table 12: Production Biased Categories
		Category
		P-R
		4
	</Extractive Summary>
	<Extractive Summary> =
		36
		P-R: WRITE + SPEAK - READ - LISTEN
		Table 13: Reception Biased Categories
		Category
		P-R
		1
	</Extractive Summary>
	<Extractive Summary> =
		For instance, in Table 1, examples 1
		and 2 show cases where annotators disagreed on
		including descriptive clauses in the event men-
		tion
	</Extractive Summary>
	<Extractive Summary> =
		First, Table 1 lists the statistics of the three collec-
		tions
	</Extractive Summary>
	<Extractive Summary> =
		Results
		The Experts column of Table 1 sum-
		marizes the annotation statistics and results
	</Extractive Summary>
	<Extractive Summary> =
		Results
		Column Crowd in Table 1 shows the
		agreement and quality measurements of this ex-
		periment
	</Extractive Summary>
	<Extractive Summary> =
		Results
		After several iterations, we assembled
		a group of 28 annotators which achieved simi-
		lar agreement to that of the expert annotators (see
		column Channel in Table 1), working at a much
		higher pace
	</Extractive Summary>
	<Extractive Summary> =
		First, as shown in Table 1,
		10 TED Talks videos were divided into group A
		and group B
	</Extractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =2>
	<Abstractive Summary> =
		Tables 2 and 3 display the top 10 known and un-
		known words for the perspective KNOW, respec-
		Table 2: The Top 10 Known Words (KNOW)
		Words
		KNOW
		��
		all
		2
		18
		Annotator A
		Annotator B
		De Amerikaanse president Donald Trump heeft
		voor het eerst de Algemene Vergadering van de
		Verenigde Naties toegesproken
		de Algemene Vergadering van de Verenigde
		Naties
		The American president Donald Trump ad-
		dressed the General Assembly of the United Na-
		tions for the ﬁrst time
		the General Assembly of the United Nations
		[president, heeft, vergadering, van, verenigde
		naties, toegesproken]
		[vergadering, van, verenigde naties]
		Table 2: Overlapping but non-matching mentions and the sets of their syntactic heads3637
		(b) GLEU
		Table 2: Results of extractive answer aggregation
		Speech
		Explicit
		Implicit
		No mention
		Positive
		150
		137
		102
		Negative
		301
		436
		1,889
		Table 2: A comparison of speech-level labels (Explicit,
		Implicit, No mention) to sentence-level based labels: a
		Positive claim is one which is positive for least one of
		the labeled sentences; a Negative claim is one which
		is negative for all labeled sentences
		Table 2: Annotation guidelines for the change in ten-
		sion
		5
		Annotation
		5
	</Abstractive Summary>
	<Extractive Summary> =
		Such a matching function must mimic hu-
		man judgment in ﬁnding that the span pairs in Ta-
		ble 1 match, but the pair in Table 2 does not
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 shows the syntactic heads extracted in this
		way from two non-matching annotations
	</Extractive Summary>
	<Extractive Summary> =
		Table 2)
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 lists the results
	</Extractive Summary>
	<Extractive Summary> =
		Table 2 compares labels from both setups
	</Extractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =3>
	<Abstractive Summary> =
		36
		Table 3: The Top 10 Unknown Words
		Words
		KNOW
		����
		embeded gutter
		-1 The negative skew
		indicates that most overlapping annotations are not
		Set of pairs
		Count
		Total
		182
		OT
		44
		OP
		61
		OF
		77
		HT
		17
		HF
		44
		Table 3: Annotation pair statistics over the four evalu-
		ation documents
		type
		down
		similar
		up
		sum
		train
		153
		819
		243
		1,215
		test
		69
		342
		110
		521
		all
		222
		1,161
		353
		1,736
		Table 3: Statistics of the data for training the model
		5
	</Abstractive Summary>
	<Extractive Summary> =
		2
		Crowdsourcing Annotation
		Of the data collected via in-house annotations to
		the Group A videos, 70% were used as the training
		set and 30% were used as the test set to train and
		evaluate the model (Table 3)
	</Extractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =4>
	<Abstractive Summary> =
		Voice-based
		Next, we surveyed the difference between the
		character-based (WRITE/READ) and voice-based
		(SPEAK/LISTEN) results by evaluating the values
		10
		Table 4: Character-based Biased Words
		Words
		Ch-Vo
		��
		the abovementioned
		372
		Table 4: Results of the different matching functions
		the same events annotated differently, but simply
		mentions of different events that happen to overlap
		(e63
		Table 4: Comparison of the performance on the test set
		according to the features used
		5
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the positively-valued examples
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 reports on the different matching func-
		tions (Section 4
	</Extractive Summary>
	<Extractive Summary> =
		Table 4 compares the perfor-
		mance according to the features used
	</Extractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =5>
	<Abstractive Summary> =
		00
		Ch-Vo: WRITE + READ - SPEAK - LISTEN
		Table 5: Voice-based Biased Words
		Words
		Ch-Vo
		���
		shopping bag
		-328
		Table 5: Scores of all Dice functions69
		Table 5: Statistics for agreement, time of annotation results
		As the result of the annotation, 11,166 anno-
		tation values were obtained for 10 videos with
		1,861 video clips (group B)
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the
		negatively-valued examples
	</Extractive Summary>
	<Extractive Summary> =
		Table 5 gives the scores for Dice functions on
		each threshold
	</Extractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =6>
	<Abstractive Summary> =
		The difference between production and recep-
		Table 6: Production Biased Words
		Words
		P-R
		��
		capillary tube
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the production biased words, which
		tend to be technical terms
	</Extractive Summary>
	<Extractive Summary> =
		in the medical
		or music ﬁelds) explain certain words in Table 6,
		such as ‘�� (capillary tube)’ and ‘��� (adhe-
		sive tape)’ or traditional music ‘���� (sing a
		song)’
	</Extractive Summary>
	<Extractive Summary> =
		The word
		‘���� (Takamori Saigo)’ also appears as a re-
		ception biased word in Table 6, which is the main
		character in a TV drama
	</Extractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =7>
	<Abstractive Summary> =
		66
		P-R: WRITE + SPEAK - READ - LISTEN
		Table 7: Reception Biased Words
		Word
		P-R
		����
		commit
		someone
		to
		trial
		-2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the reception biased words,
		and the negative words (‘�� (murder)’ and ‘��
		�� (ﬁlling charges)’) are conﬁrmed
	</Extractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =8>
	<Abstractive Summary> =
		11
		Table 8: The Top 10 Known Categories
		Category
		KNOW
		3
	</Abstractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =9>
	<Abstractive Summary> =
		26
		Table 9: The Top 10 Unknown Categories
		Category
		KNOW
		3
	</Abstractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =10>
	<Abstractive Summary> =
		As shown in these tables,
		the nominal action and subject categories tend
		to be character-based biased, whereas the voca-
		Table 10: Character-based Biased Categories
		Category
		Ch-Vo
		1
	</Abstractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =11>
	<Abstractive Summary> =
		07
		Ch-Vo: WRITE + READ - SPEAK - LISTEN
		Table 11: Voice-based Biased Categories
		Category
		Ch-Vo
		4
	</Abstractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =12>
	<Abstractive Summary> =
		Therefore, the
		12
		Table 12: Production Biased Categories
		Category
		P-R
		4
	</Abstractive Summary>
</Paper ID=ument996>


<Paper ID=ument996> <Table ID =13>
	<Abstractive Summary> =
		36
		P-R: WRITE + SPEAK - READ - LISTEN
		Table 13: Reception Biased Categories
		Category
		P-R
		1
	</Abstractive Summary>
</Paper ID=ument996>


<Paper ID=ument998> <Table ID =1>
	<Abstractive Summary> =
		000
		—
		Table 1: Results for the 13 workers (W1–W13) who annotate 50 or more Penn Treebank trees, including the total
		number of trees annotated, unlabeled attachment scores (UAS), full tree match rate (FTM), and median time in
		seconds ( �
		time) between accepting a HIT and submitting results
	</Abstractive Summary>
	<Extractive Summary> =
		The scores for these 13
		most proliﬁc annotators are presented in Table 1,
		along with the scores for the primary author (who
		is, ideally, representative of an expert annotator)
		included as well for comparison
	</Extractive Summary>
</Paper ID=ument998>


<Paper ID=ument999> <Table ID =1>
	<Abstractive Summary> =
		7
		Table 1: Example Entry from the ‘Word List by Semantic Principles’
		���� ‘Last Year’: 1
	</Abstractive Summary>
	<Extractive Summary> =
		Table 1 shows an example of
		the word ‘�� (Last Year)’, which is assigned a
		value of 1
	</Extractive Summary>
	<Extractive Summary> =
		As shown in these tables,
		the nominal action and subject categories tend
		to be character-based biased, whereas the voca-
		Table 10: Character-based Biased Categories
		Category
		Ch-Vo
		1
	</Extractive Summary>
	<Extractive Summary> =
		07
		Ch-Vo: WRITE + READ - SPEAK - LISTEN
		Table 11: Voice-based Biased Categories
		Category
		Ch-Vo
		4
	</Extractive Summary>
	<Extractive Summary> =
		Therefore, the
		13
		Table 12: Production Biased Categories
		Category
		P-R
		4
	</Extractive Summary>
	<Extractive Summary> =
		36
		P-R: WRITE + SPEAK - READ - LISTEN
		Table 13: Reception Biased Categories
		Category
		P-R
		1
	</Extractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =2>
	<Abstractive Summary> =
		Tables 2 and 3 display the top 10 known and un-
		known words for the perspective KNOW, respec-
		Table 2: The Top 10 Known Words (KNOW)
		Words
		KNOW
		��
		all
		2
	</Abstractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =3>
	<Abstractive Summary> =
		36
		Table 3: The Top 10 Unknown Words
		Words
		KNOW
		����
		embeded gutter
		-1
	</Abstractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =4>
	<Abstractive Summary> =
		Voice-based
		Next, we surveyed the difference between the
		character-based (WRITE/READ) and voice-based
		(SPEAK/LISTEN) results by evaluating the values
		11
		Table 4: Character-based Biased Words
		Words
		Ch-Vo
		��
		the abovementioned
		3
	</Abstractive Summary>
	<Extractive Summary> =
		Table 4 shows the positively-valued examples
	</Extractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =5>
	<Abstractive Summary> =
		00
		Ch-Vo: WRITE + READ - SPEAK - LISTEN
		Table 5: Voice-based Biased Words
		Words
		Ch-Vo
		���
		shopping bag
		-3
	</Abstractive Summary>
	<Extractive Summary> =
		Table 5 shows the
		negatively-valued examples
	</Extractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =6>
	<Abstractive Summary> =
		The difference between production and recep-
		Table 6: Production Biased Words
		Words
		P-R
		��
		capillary tube
		0
	</Abstractive Summary>
	<Extractive Summary> =
		Table 6 shows the production biased words, which
		tend to be technical terms
	</Extractive Summary>
	<Extractive Summary> =
		in the medical
		or music ﬁelds) explain certain words in Table 6,
		such as ‘�� (capillary tube)’ and ‘��� (adhe-
		sive tape)’ or traditional music ‘���� (sing a
		song)’
	</Extractive Summary>
	<Extractive Summary> =
		The word
		‘���� (Takamori Saigo)’ also appears as a re-
		ception biased word in Table 6, which is the main
		character in a TV drama
	</Extractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =7>
	<Abstractive Summary> =
		66
		P-R: WRITE + SPEAK - READ - LISTEN
		Table 7: Reception Biased Words
		Word
		P-R
		����
		commit
		someone
		to
		trial
		-2
	</Abstractive Summary>
	<Extractive Summary> =
		Table 7 shows the reception biased words,
		and the negative words (‘�� (murder)’ and ‘��
		�� (ﬁlling charges)’) are conﬁrmed
	</Extractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =8>
	<Abstractive Summary> =
		12
		Table 8: The Top 10 Known Categories
		Category
		KNOW
		3
	</Abstractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =9>
	<Abstractive Summary> =
		26
		Table 9: The Top 10 Unknown Categories
		Category
		KNOW
		3
	</Abstractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =10>
	<Abstractive Summary> =
		As shown in these tables,
		the nominal action and subject categories tend
		to be character-based biased, whereas the voca-
		Table 10: Character-based Biased Categories
		Category
		Ch-Vo
		1
	</Abstractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =11>
	<Abstractive Summary> =
		07
		Ch-Vo: WRITE + READ - SPEAK - LISTEN
		Table 11: Voice-based Biased Categories
		Category
		Ch-Vo
		4
	</Abstractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =12>
	<Abstractive Summary> =
		Therefore, the
		13
		Table 12: Production Biased Categories
		Category
		P-R
		4
	</Abstractive Summary>
</Paper ID=ument999>


<Paper ID=ument999> <Table ID =13>
	<Abstractive Summary> =
		36
		P-R: WRITE + SPEAK - READ - LISTEN
		Table 13: Reception Biased Categories
		Category
		P-R
		1
	</Abstractive Summary>
</Paper ID=ument999>


